2025-02-13 00:00:15,555 - INFO - Initializing LLM for extracting main content from papers
2025-02-13 00:00:31,180 - DEBUG - start_idx: -1, start_marker: 1 Introduction
The pr, end_idx: 5410, end_marker: release the generated
2025-02-13 00:00:35,660 - DEBUG - start_idx: 3124, start_marker: 1. Introduction Larg, end_idx: -1, end_marker: bag of 53 coins is $
2025-02-13 00:03:32,955 - DEBUG - start_idx: 2987, start_marker: 1 INTRODUCTION A lon, end_idx: -1, end_marker: s loss to each model
2025-02-13 00:03:34,612 - DEBUG - start_idx: 1560, start_marker: 1. Introduction Due t, end_idx: 6284, end_marker: tion for efficient LL
2025-02-13 00:06:54,745 - DEBUG - start_idx: 1765, start_marker: 1 Introduction In rec, end_idx: 35354, end_marker: potential risks.
2025-02-13 00:06:54,747 - INFO - Total execution time: 398.42 seconds (6.64 minutes)
2025-02-13 00:06:54,755 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=90, thumbnail=None, content='1. Introduction Large Language Models (LLMs) have shown significant improvements across a variety of domains (Ope- nAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently, OpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning capabilities of LLMs by allocating additional computation at inference time, making it an effective approach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI et al., 2025). TTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs to ‚Äúthink‚Äù slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and (2) External TTS, which improves the reasoning performance via sampling or search-based methods with fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to scale compute optimally, that is, allocating the optimal computation for each problem (Snell et al., 2024). Current TTS methods guide the generation process and select the final answer using Process Reward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024; Beeching et al., 2024). These TTS methods involve several important factors, such as policy models1, PRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy models, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the community from fully understanding the effectiveness of this method and developing insights for compute-optimal TTS strategies. To address these issues, this paper aims to investigate the influence of policy models, PRMs, and problem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore the concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct extensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO, 2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across multiple policy models (ranging from 0.5B to 72B across two model families). Our results show that the compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem difficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B model) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging reasoning tasks by applying compute-optimal TTS. The contributions of this work can be summarized as follows: 1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date policy models, multiple PRMs, diverse scaling methods, and more challenging tasks. 2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process and introduces reward-aware compute-optimal TTS. We also demonstrate that the compute- optimal scaling strategy varies with different policy models, PRMs, and problem difficulty levels. 3. The empirical results demonstrate the significant potential of smaller language models to outperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy, we show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and DeepSeek-R1 on MATH-500 and AIME24 tasks. 1Following Snell et al. (2024), we use ‚Äúpolicy models‚Äù to refer to LLMs that generate solutions, and ‚Äúverifiers‚Äù for PRMs. 2  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Question Best-of-N Question Beam Search Diverse Verifier Tree Search  : Scored by PRM : Selected by PRM : Rejected by PRM : Solution / Step with Final Answer : Intermediate Step Question Question Answer Answer Answer Figure 2: Comparison of different external TTS methods. 2. Setup & Preliminaries 2.1. Problem Formulation We formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018), defined by the tuple (ùíÆ, ùíú, ùí´, ‚Ñõ, ùõæ), where ùíÆis the state space, ùíúis the action space, ùí´: ùíÆ√ó ùíú‚ÜíùíÆ is the transition function, ‚Ñõ: ùíÆ√ó ùíú‚ÜíR is the reward function, and ùõæ‚àà[0, 1] is the discount factor. Given a prompt ùë•‚àºùí≥, the policy with parameters ùúÉgenerates the initial action ùëé1 ‚àºùúãùúÉ(¬∑ | ùë†1), where ùë†1 = ùë•is the initial state. The policy receives a reward ‚Ñõ(ùë†1, ùëé1), and the state transitions to ùë†2 = [ùë†1, ùëé1], where [¬∑, ¬∑] denotes the concatenation of two strings. This process continues until the episode terminates, either by reaching the maximum number of steps or by generating an <EOS> token. A trajectory of length ùêªis represented as ùúè= {ùëé1, ùëé2, ¬∑ ¬∑ ¬∑ , ùëéùêª}. The process can be summarized as follows: Initial State: ùë†1 = ùë•‚àºùí≥ Action: ùëéùë°‚àºùúãùúÉ(¬∑ | ùë†ùë°) State Transition: ùë†ùë°+1 = ùí´(¬∑ | ùë†ùë°, ùëéùë°) = [ùë†ùë°, ùëéùë°] Reward: ùëüùë°= ‚Ñõ(ùë†ùë°, ùëéùë°) (1) 2.2. Test-Time Scaling Method We consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al., 2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell et al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or other methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS methods are shown in Figure 2. Best-of-N. In the BoN approach, the policy model generates ùëÅresponses, after which scoring and voting methods are applied to select the final answer. Beam Search. Given beam width ùëÅand beam size ùëÄ, the policy model first generates ùëÅsteps. The verifier selects the top ùëÅ ùëÄsteps for subsequent search. In the next step, the policy model samples 3  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling ùëÄsteps for each selected previous step. This process repeats until the maximum depth is reached or an <EOS> token is generated. Diverse Verifier Tree Search. To increase diversity, DVTS extends beam search by dividing the search process into ùëÅ ùëÄsubtrees, each of which is explored independently using beam search. As shown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with a large computational budget ùëÅ. A similar trend is observed in Chen et al. (2024), where increasing the number of parallel subtrees proves to be more effective than increasing the beam width under the same budget. 2.3. Compute-Optimal Test-Time Scaling To maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling strategy, which selects hyperparameters corresponding to a given test-time strategy to maximize performance benefits on a specific prompt. Given a prompt ùë•, let Target(ùúÉ, ùëÅ, ùë•) represent the output distribution over ùë•produced by the policy model with parameters ùúÉand a compute budget of ùëÅ. ùúÉ* ùë•,ùë¶*(ùë•)(ùëÅ) = arg max ùúÉ (Ô∏Ä Eùë¶‚àºTarget(ùúÉ,ùëÅ,ùë•) [Ô∏Ä 1ùë¶=ùë¶*(ùë•) ]Ô∏Ä)Ô∏Ä , (2) where ùë¶*(ùë•) denotes the ground-truth correct response for ùë•, and ùúÉ* ùë•,ùë¶*(ùë•)(ùëÅ) represents the test-time compute-optimal scaling strategy for the problem ùë•with compute budget ùëÅ. 3. Rethinking Compute-Optimal Test-Time Scaling 3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware Compute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024). Previous works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al., 2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier to do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM trained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL), we obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy PRM produces more accurate rewards for the responses of the policy model, while the offline PRM often generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng et al., 2024). For practical applications of compute-optimal TTS, training a PRM for each policy model to prevent OOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy in a more general setting, where the PRM might be trained on a different policy model than the one used for TTS. For search-based methods, PRMs guide the selection at each response step, while for sampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the reward influences response selection across all methods; (2) for search-based methods, the reward also influences the search process. To analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B- Instruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs. The results in Figure 12 demonstrate that the reward significantly affects the generation process and outcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect answers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more 4  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 0.0 0.2 0.4 0.6 0.8 1.0 Pass@1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Percentage 11.2% 3.4% 3.4% 5.8% 76.2% mean: 0.82 Figure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five bins. tokens. In Section 4, we also empirically show that rewards have great influence on TTS performance and output tokens. Based on these findings, we propose that rewards should be integrated into the compute-optimal TTS strategy. Let us denote the reward function as ‚Ñõ. Our reward-aware compute-optimal TTS strategy is formulated as: ùúÉ* ùë•,ùë¶*(ùë•),‚Ñõ(ùëÅ) = arg max ùúÉ (Ô∏Ä Eùë¶‚àºTarget(ùúÉ,ùëÅ,ùë•,‚Ñõ) [Ô∏Ä 1ùë¶=ùë¶*(ùë•) ]Ô∏Ä)Ô∏Ä , (3) where Target(ùúÉ, ùëÅ, ùë•, ‚Ñõ) represents the output distribution of the policy model ùúÉ, adjusted by the reward function ‚Ñõ, under a compute budget ùëÅand prompt ùë•. For sampling-based scaling methods, Target(ùúÉ, ùëÅ, ùë•, ‚Ñõ) = Target(ùúÉ, ùëÅ, ùë•). This reward-aware strategy ensures that compute-optimal scaling adapts to the policy model, prompt, and reward function, leading to a more general framework for practical TTS. 3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles To consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five difficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels from MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al., 2024) is not effective since different policy models have different reasoning capabilities. As shown in Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500 problems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty. Specifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ‚àº100%), medium (10% ‚àº50%), and hard (0% ‚àº10%). 4. How to Scale Test-Time Compute Optimally? In this section, we aim to answer the following questions: ‚Ä¢ Q1: How does TTS improve with different policy models and PRMs? ‚Ä¢ Q2: How does TTS improve for problems with different difficulty levels? ‚Ä¢ Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods? 5  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 4.1. Setup Datasets. We conduct experiments on competition-level mathematical datasets, including MATH- 500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative problems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell et al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical reasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for experiments. Policy Models. For test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and Qwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy models. Process Reward Models. We consider the following open-source PRMs for evaluation: ‚Ä¢ Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang et al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al., 2024). ‚Ä¢ RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta- Math (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The base model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024). ‚Ä¢ Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM- 1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B- Instruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron et al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series models. ‚Ä¢ Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5- Math-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang et al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al., 2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed, Qwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while Qwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as demonstrated in Zhang et al. (2025). Scoring and Voting Methods. Following Wang et al. (2024a), we consider three scoring methods: PRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote. To obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of length ùêª, the scores for each trajectory with different scoring methods are calculated as follows: (1) PRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = min‚Ñõ{‚Ñõùë°}ùêª ùë°=0. (2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ‚Ñõùêª. (3) PRM-Avg scores each trajectory by the average reward among all steps, i.e., score = 1 ùêª ‚àëÔ∏Äùêª ùë°=0 ‚Ñõùë°. The voting methods then aggregate the scores to determine the final answer. Majority Vote selects the answer with the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest score, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer with the highest score. 6  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 50 60 70 80 90 Llama-3.1-8B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 50 60 70 80 90 RLHFlow-PRM-Mistral-8B 22 24 26 28 50 60 70 80 90 RLHFlow-PRM-Deepseek-8B 22 24 26 28 50 60 70 80 90 Skywork-PRM-1.5B 22 24 26 28 50 60 70 80 90 Skywork-PRM-7B 22 24 26 28 50 60 70 80 90 Qwen2.5-Math-PRM-7B 22 24 26 28 50 60 70 80 90 Qwen2.5-Math-PRM-72B 22 24 26 28 75 80 85 90 95 Qwen2.5-7B-Inst. 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 Pass@k Majority Best-of-N Beam Search DVTS Figure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different PRMs and TTS strategies. 22 24 26 28 0 20 40 60 Llama-3.1-8B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 0 20 40 60 RLHFlow-PRM-Mistral-8B 22 24 26 28 0 20 40 60 RLHFlow-PRM-Deepseek-8B 22 24 26 28 0 20 40 60 Skywork-PRM-1.5B 22 24 26 28 0 20 40 60 Skywork-PRM-7B 22 24 26 28 0 20 40 60 Qwen2.5-Math-PRM-7B 22 24 26 28 0 20 40 60 Qwen2.5-Math-PRM-72B 22 24 26 28 10 20 30 40 50 Qwen2.5-7B-Inst. 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 Pass@k Majority Best-of-N Beam Search DVTS Figure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different PRMs and TTS strategies. We use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute budgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as in prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width is set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we restrict the maximum number of new tokens to 8192. For search-based methods, the token limit is 2048 for each step and 8192 for the total response. 4.2. How does TTS improve with different policy models and PRMs? (Q1) PRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama- 3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs improves significantly with larger compute budgets, while the results of searching with Math-Shepherd and RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct, the performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more budgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy of both policy models improves a lot with larger compute budgets, the performance improvement of TTS remains moderate. These results demonstrate that the generalization of PRMs is particularly challenging across different policy models and tasks, especially for more complex tasks. The optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms other strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based 2https://github.com/openreasoner/openr 7  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling methods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using a PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization across policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers trapped in local optima and worsen the performance. This may also be related to the base model of the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B- Instruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025). Further analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice of the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of considering reward information in compute-optimal TTS. We also explore the relationship between TTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS performance is positively correlated with the process supervision abilities of PRMs, and the fitted function is ùëå= 7.66 log(ùëã) + 44.31, where ùëårepresents TTS performance and ùëãrepresents the process supervision abilities of the PRM (Zhang et al., 2025). 30 40 50 60 70 80 ProcessBench Performance 70 72 74 76 78 Test-Time Scaling Performance Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B Figure 6: The relationship between TTS performance and process supervision abilities of different PRMs on MATH, where the size of each circle represents the number of parameters of the PRM and the curve represents the fitted function. 22 24 26 28 40 60 80 Qwen2.5-0.5B-Inst. 22 24 26 28 60 70 80 90 Qwen2.5-1.5B-Inst. 22 24 26 28 70 80 90 Qwen2.5-3B-Inst. 22 24 26 28 80 85 90 95 Qwen2.5-7B-Inst. 22 24 26 28 80 85 90 95 Qwen2.5-14B-Inst. 22 24 26 28 80 85 90 95 100 Qwen2.5-32B-Inst. 22 24 26 28 85 90 95 100 Qwen2.5-72B-Inst. Pass@k Majority Best-of-N Beam Search DVTS Figure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with different scaling methods. The optimal TTS method varies with policy models. To study the relationship between the parameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5 family LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy models. For small policy models, search-based methods outperform BoN, while for large policy models, BoN is more effective than search-based methods. This difference occurs because larger models have stronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast, smaller models rely on a verifier to select each step, ensuring the correctness of each intermediate step. 8  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 4.3. How does TTS improve for problems with different difficulty levels? (Q2) Following Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty levels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in MATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell et al., 2024) is not appropriate because different policy models exhibit different reasoning abilities. To address this, we categorize the difficulty levels into three groups based on the absolute value of Pass@1 accuracy: easy (50% ‚àº100%), medium (10% ‚àº50%), and hard (0% ‚àº10%). The optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9 show that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy problems, while beam search works better for harder problems. For policy models with parameters between 7B and 32B, DVTS performs well for easy and medium problems, and beam search is preferable for hard problems. For policy models with 72B parameters, BoN is the best method for all difficulty levels. 22 24 26 28 94 96 98 100 Llama-3.2-1B-Inst. Level 1 22 24 26 28 40 60 80 100 Level 2 22 24 26 28 0 20 40 60 Level 3 22 24 26 28 92 94 96 98 100 Llama-3.2-3B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 20 40 60 80 22 24 26 28 94 96 98 100 Llama-3.1-8B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 Pass@k Majority Best-of-N Beam Search DVTS Figure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels. 4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods? (Q3) Table 1: Statistics of training data of RLHFlow PRMs. Mistral-PRM-Data Deepseek-PRM-Data Average Token per Response 236.9 333.1 Average Token per Step 46.6 58.4 PRMs are biased towards the length of steps. Although we perform TTS under the same budget in pervious experiments, we find that the number of inference tokens with different PRMs varies singificantly. For example, given the same budget and the same policy model, the number of inference 9  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling tokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow- PRM-Mistral-8B, nearly 2√ó. The training data of RLHFlow series PRMs are sampled from different LLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze several properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek- 8B4. As shown in Table 1, both the average token per response and the average token per step of DeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data of RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to the bias towards the length of the output. We also find that the number of inference tokens of scaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very near, which indicates that searching with Skywork-PRM-7B is more efficient than searching with Qwen2.5-Math-7B. Table 2: Performance of TTS with different voting methods on MATH-500. Skywork-PRM-7B Qwen2.5-Math-PRM-7B Majority Vote 86.8 87.6 PRM-Min-Max 83.0 87.4 PRM-Min-Vote 86.6 87.6 PRM-Last-Max 84.4 87.6 PRM-Last-Vote 87.0 87.6 PRM-Avg-Max 85.8 87.8 PRM-Avg-Vote 86.8 87.6 PRMs are sensitive to voting methods. From the results in Table 2, it is shown that Skywork- PRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very sensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are processed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps labeled as positive steps in the training data and makes the outputted large reward values more likely to be correct. This shows that the training data of PRMs is important for improving the ability to find errors in the search process. 5. Results for Compute-Optimal Test-Time Scaling With the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to explore the following questions: ‚Ä¢ Q4: Can smaller policy models outperform larger models with the compute-optimal TTS strategy? ‚Ä¢ Q5: How does compute-optimal TTS improve compared with CoT and majority voting? ‚Ä¢ Q6: Is TTS more effective than long-CoT-based methods? 5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy (Q4) Scaling test-time compute of small policy models is crucially important for improving the reasoning performance of LLMs. We are interested in whether smaller policy models can outperform larger ones, GPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the 3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data 4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data 10  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Table 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs (CoT) on MATH-500 and AIME24. Policy Model MATH-500 AIME24 Avg. Proprietary LLMs (CoT) GPT-4o 74.6 9.3 42.0 o1-preview 85.5 44.6 65.1 o1-mini 90.0 63.6 76.8 o1 94.8 79.2 87.0 Open-Source LLMs (CoT) Llama-3.1-70B-Inst. 65.2 16.7 41.0 Llama-3.1-405B-Inst. 71.4 23.3 47.4 QwQ-32B-Preview 90.6 50.0 70.3 DeepSeek-R1 97.3 79.8 88.6 Open-Source LLMs (TTS) Llama-3.2-1B-Inst. 66.2 16.7 41.5 Llama-3.2-1B-Inst. (ùëÅ= 512) 72.2 10.0 41.1 Llama-3.2-3B-Inst. 75.6 30.0 52.8 Qwen2.5-0.5B-Inst. 76.4 10.0 43.2 Qwen2.5-1.5B-Inst. 81.8 20.0 50.9 DeepSeek-R1-Distill-Qwen-1.5B 91.6 63.3 77.5 DeepSeek-R1-Distill-Qwen-7B 95.2 83.3 89.3 performance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct (CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above two tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek- R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24. From the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24, meaning that smaller models can outperform 135√ó larger models using the compute-optimal TTS strategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve the result by 487.0% (23√ó ‚Üí135√ó). (2) If we further increase the compute budget to ùëÅ= 512, Llama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on MATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct and Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating that small models can exceed GPT-level performance with the compute-optimal TTS strategy. (4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview and o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the compute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These results demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs with the compute-optimal TTS strategy. FLOPS Comparison. To answer the question of whether compute-optimal TTS is more effective than increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell 5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use Qwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct. 11  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Table 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones (CoT). Policy Model Pre-training FLOPS Inference FLOPS Total FLOPS. Llama-3.2-3B-Inst. 1.62 √ó 1023 3.07 √ó 1017 1.62 √ó 1023 Llama-3.1-405B-Inst. 3.65 √ó 1025 4.25 √ó 1017 3.65 √ó 1025 DeepSeek-R1-Distill-7B 7.56 √ó 1023 8.15 √ó 1017 7.56 √ó 1023 DeepSeek-R1 5.96 √ó 1025 4.03 √ó 1018 5.96 √ó 1025 Table 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models on MATH-500. Policy Model CoT Major. Compute-Optimal TTS Performance Gain Efficiency Gain Llama-3.2-1B-Inst. 26.0 39.0 66.2 154.6% >256.0√ó Llama-3.2-3B-Inst. 41.4 58.4 78.2 88.9% 14.1√ó Llama-3.1-8B-Inst. 49.8 66.4 80.6 61.8% 43.9√ó Qwen2.5-0.5B-Inst. 31.6 47.2 76.4 141.8% >64.0√ó Qwen2.5-1.5B-Inst. 54.4 68.4 85.6 57.4% >256.0√ó Qwen2.5-3B-Inst. 64.0 77.0 87.6 36.9% 58.4√ó Qwen2.5-7B-Inst. 76.8 83.6 91.0 18.5% 35.9√ó Qwen2.5-14B-Inst. 80.2 85.6 91.0 13.5% 51.4√ó Qwen2.5-32B-Inst. 82.4 87.0 90.6 10.0% 0.8√ó Qwen2.5-72B-Inst. 83.8 87.2 91.8 9.5% 12.9√ó et al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results, we can see that small policy models even surpass large ones with less inference FLOPS and reduce the total FLOPS by 100√ó ‚àº1000√ó. 5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5) Based on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty levels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in Table 5. We find that compute-optimal TTS can be 256√ó more efficient than majority voting and improve reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal TTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters in the policy model increases, the improvement of TTS gradually decreases. This suggests that the effectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for models with weak reasoning abilities, scaling test-time compute leads to a substantial improvement, whereas for models with strong reasoning abilities, the gain is limited. 5.3. Is TTS more effective than long-CoT-based methods? (Q6) Recently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan et al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance of TTS with these approaches. Setup. We evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first generates reasoning data via MCTS, followed by online policy and preference model learning. (2) Eurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit process rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection 12  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Table 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24. Policy Model MATH-500 AIME24 Avg. Open-Source LLMs (CoT) Qwen2.5-7B-Inst. 76.8 13.3 45.1 Qwen2.5-Math-7B-Inst. 79.8 13.3 46.6 Long-CoT Methods (CoT) rStar-Math-7B 78.4 26.7 52.6 Eurus-2-7B-PRIME 79.2 26.7 53.0 Qwen2.5-7B-SimpleRL-Zero 77.2 33.3 55.3 Qwen2.5-7B-SimpleRL 82.4 26.7 54.6 Satori-Qwen-7B 83.6 23.3 53.5 DeepSeek-R1-Distill-Qwen-7B 92.4 63.3 77.9 Open-Source LLMs (TTS) Qwen2.5-7B-Inst. w/ 7B PRM (Ours) 88.0 33.3 60.5 Qwen2.5-7B-Inst. w/ 72B PRM (Ours) 91.0 36.7 63.9 with only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and then improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B parameters into a 7B LLM. Results. As shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math, Eurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of TTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on AIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on the data generated via MCTS but is less effective than distilling from strong reasoning models. Also, TTS is more effective on simpler tasks than on more complex tasks. 6. Related Work LLM Test-Time Scaling. Scaling LLM test-time compute is an effective way to improve the perfor- mance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based methods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu et al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al. (2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al. (2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process reward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for external reward models and the generation of extensive samples, Manvi et al. (2024) proposes a self-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al., 2024) explores TTS via search methods with diversity. However, these works lack a evaluation with either strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a more systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and provide some principles for practical TTS. Improving Mathematical Reasoning Abilities of LLMs. Prior methods for improving mathematical reasoning abilities can be divided into training-time methods and test-time methods. For training- 13  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling time methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023; Azerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024; Gou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical capabilities. Another line of works explore self-training and self-improvement strategies (Zelikman et al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024; Zhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve the reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve the mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi, 2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI o1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking. For test-time methods, prompt-based approaches have been extensively studied to enhance reasoning without altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022) and its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into manageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond prompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and correct their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages program interpreter or symbolic manipulators to perform precise calculations and validations. Self- verification approaches (Weng et al., 2023) enable models to assess the correctness of their own reasoning processes, further increasing robustness. These test-time strategies complement training- time enhancements, collectively contributing to significant improvements in LLMs‚Äô mathematical reasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time compute via PRM-guided search methods. Process Reward Models. Previous works show that PRMs are more effective than ORMs (Ue- sato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as PRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col- lection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu et al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs are explored from advantage modeling (Setlur et al., 2024b), ùëÑ-value rankings (Li and Li, 2024), implicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives. Additionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al., 2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on mathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and PRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang et al. (2025) provides guidelines for practical development of PRMs and releases the most capable PRMs for mathematical tasks up-to-date. 7. Conclusion & Discussion In this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from the perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our findings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs, and problem difficulty, validating that smaller language models can perform better than larger models when applying compute-optimal TTS. Our results show that a 1B model can achieve better performance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can achieve strong TTS results by supervising a more capable 72B policy model, which suggests the importance of investigating a true ‚Äúweak-to-strong‚Äù approach instead of the current ‚Äústrong-to-weak‚Äù supervision for policy optimization. To achieve this goal, we need to develop more efficient supervision 14  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling methods, as both PRM-based and RL-based approaches have limitations due to their dependence on high-quality supervision. Future work should focus on developing more adaptable and universal supervision mechanisms to boost the performance of small language models on complex tasks and provide new approaches for developing efficient reasoning strategies. Limitations. Although we provide a comprehensive evaluation of TTS on mathematical tasks, there are still some limitations and future directions to explore: (1) Extending TTS to more tasks such as coding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS. 15  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling References AI-MO. Aime 2024, 2024. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-aime. Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/ introducing-claude/. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=4WnqRR915j. Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models, 2024. URL https://huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher R√©, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum?id=VaXnxQ3UKo. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum? id=YfZ4ZPt8zd. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin 16  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In International Conference on Machine Learning (ICML), volume 202, pages 10764‚Äì10799, 2023. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview. net/forum?id=Ep0TtjVoap. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey‚Äìpart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 17  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265, 2024. Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview. net/forum?id=shgx0eqdw6. Kimi. k0-math, November 2024. URL https://kimi.moonshot.cn/. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Joshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically annotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024. Wendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum? id=v8L0pN6EOi. Jianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo. Autopsv: Automated process-supervised verifier. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46534‚Äì46594, 2023. Rohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. 18  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report‚Äìpart 1. arXiv preprint arXiv:2410.18982, 2024. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532, 2024a. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of- action-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. Skywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/. Skywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction tuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume 235, pages 47885‚Äì47900, 2024. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware rejection tuning for mathematical problem-solving. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7601‚Äì7614, 2024. 19  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun Wang. AlphaZero-like tree-search can guide large language model decoding and training. In International Conference on Machine Learning (ICML), volume 235, pages 49890‚Äì49920, 2024. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024a. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426‚Äì9439, 2024b. Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR), 2023. URL https:// openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural information processing systems (NeurIPS), volume 35, pages 24824‚Äì24837, 2022. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2550‚Äì2575, 2023. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 41618‚Äì41650, 2023. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. 20  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024c. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 11809‚Äì11822, 2023. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for large language models. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=N8N0hgNDRt. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 15476‚Äì15488, 2022. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-STar: Language models can teach themselves to think before speaking. In Conference on Language Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9. Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in large language models‚Äìthe story goes on. arXiv preprint arXiv:2407.08348, 2024. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM self-training via process reward guided tree search. In Advances in Neural Information Processing Systems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5. Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint arXiv:2412.11006, 2024b. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. 21  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595‚Äì46623, 2023. 22  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling A. Prompt Template for Test-Time Scaling The system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang et al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use the system prompt of the official evaluation6 for Llama 3 to prevent performance drop. Table 7: System prompt for Llama 3 series models. Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide a concise solution with minimal explanation. - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] ... Regardless of the approach, always conclude with: Therefore, the final answer is: $\\boxed{answer}$. I hope it is correct. Where [answer] is just the final number or expression that solves the problem. Table 8: System prompt for Qwen2.5 series models. Please reason step by step, and put your final answer within \\boxed{}. B. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and Scaling Methods The full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10 and Figure 11. 6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals 23  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 94 96 98 100 Llama-3.2-1B-Inst. Level 1 22 24 26 28 40 60 80 100 Level 2 22 24 26 28 0 20 40 60 Level 3 22 24 26 28 92 94 96 98 100 Llama-3.2-3B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 20 40 60 80 22 24 26 28 94 96 98 100 Llama-3.1-8B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 22 24 26 28 92 94 96 98 100 Qwen2.5-0.5B-Inst. 22 24 26 28 40 50 60 70 80 90 100 22 24 26 28 0 20 40 60 80 22 24 26 28 94 96 98 100 Qwen2.5-1.5B-Inst. 22 24 26 28 40 50 60 70 80 90 100 22 24 26 28 0 20 40 60 80 22 24 26 28 97 98 99 100 Qwen2.5-3B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 22 24 26 28 95 96 97 98 99 100 Qwen2.5-7B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 22 24 26 28 97 98 99 100 Qwen2.5-14B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 96 97 98 99 100 Qwen2.5-32B-Inst. 22 24 26 28 40 50 60 70 80 90 100 22 24 26 28 0 20 40 60 22 24 26 28 97 98 99 100 Qwen2.5-72B-Inst. 22 24 26 28 20 40 60 80 100 22 24 26 28 0 20 40 60 Pass@k Majority Best-of-N Beam Search DVTS Figure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels. 24  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 30 40 50 60 70 80 Llama-3.2-1B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 30 40 50 60 70 80 RLHFlow-PRM-Mistral-8B 22 24 26 28 30 40 50 60 70 80 RLHFlow-PRM-Deepseek-8B 22 24 26 28 30 40 50 60 70 80 Skywork-PRM-1.5B 22 24 26 28 30 40 50 60 70 80 Skywork-PRM-7B 22 24 26 28 30 40 50 60 70 80 Qwen2.5-Math-PRM-7B 22 24 26 28 30 40 50 60 70 80 Qwen2.5-Math-PRM-72B 22 24 26 28 40 50 60 70 80 90 Llama-3.2-3B-Inst. 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 50 60 70 80 90 Llama-3.1-8B-Inst. 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 Qwen2.5-0.5B-Inst. 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 60 70 80 90 Qwen2.5-1.5B-Inst. 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 70 80 90 Qwen2.5-3B-Inst. 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 75 80 85 90 95 Qwen2.5-7B-Inst. 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 Pass@k Majority Best-of-N Beam Search DVTS Figure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling strategies. 25  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 0 10 20 30 40 Llama-3.2-1B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 0 10 20 30 40 RLHFlow-PRM-Mistral-8B 22 24 26 28 0 10 20 30 40 RLHFlow-PRM-Deepseek-8B 22 24 26 28 0 10 20 30 40 Skywork-PRM-1.5B 22 24 26 28 0 10 20 30 40 Skywork-PRM-7B 22 24 26 28 0 10 20 30 40 Qwen2.5-Math-PRM-7B 22 24 26 28 0 10 20 30 40 Qwen2.5-Math-PRM-72B 22 24 26 28 0 10 20 30 40 50 60 Llama-3.2-3B-Inst. 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 Llama-3.1-8B-Inst. 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 5 10 15 20 Qwen2.5-0.5B-Inst. 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 10 20 30 40 50 Qwen2.5-1.5B-Inst. 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 Qwen2.5-3B-Inst. 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 10 20 30 40 50 Qwen2.5-7B-Inst. 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 Pass@k Majority Best-of-N Beam Search DVTS Figure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling strategies. 26  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling C. Cases In this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the output of TTS, we identify several issues with PRMs. Specifically, we observe four major categories: (1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct steps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM sometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect these errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the PRM assigns lower scores to certain intermediate steps that are not where the critical errors actually occur. This indicates a misalignment between the scoring signal and the actual error locations. (4) Scoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the token length of intermediate steps, result in large discrepancies in scores for equally correct reasoning steps. Notably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not used during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model). These problems distort the reasoning search process, degrade overall performance, and reduce the reliability of PRM-assisted reasoning. Addressing these biases in future model architectures and training procedures is necessary to improve the robustness and interpretability of PRMs. 27  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response (Beam Search with RLHFlow-Mistral-PRM-8B): ## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince  it can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible  by 10. [score=0.42] ## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe  possible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable  with the digits 2 and 0. [score=0.62] ## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use  leading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with  the digits 0 and 2. –∫–∏\\n [score=0.56] ## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found  the least positive integer multiple of 30 that can be written with only the digits 0 and 2.  [score=0.87] Therefore, the final answer is: $\\\\boxed{660}$. [score=0.81] Problem: What is the least positive integer multiple of 30 that  can be written with only the digits 0 and 2? Response (Beam Search with RLHFlow-Deepseek-PRM-8B): ## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of  30, a number must be divisible by both 2 and 3. [score=0.85] ## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end  in 0, since it\'s the only way it will be even. [score=0.89] ## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of  its digits must be divisible by 3. [score=0.92] ## Step 4: Determine the least possible number meeting these requirements\\nThe smallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200.  However, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85] ## Step 5: Find the next possible number meeting the requirements\\nThe next number  to try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91] ## Step 6: Determine the next possible number meeting the requirements\\nThe next  number is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3.  [score=0.91] ## Step 7: Find the next possible number meeting the requirements\\nThe next number  to try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3.  [score=0.88] ## Step 8: Determine the next possible number meeting the requirements\\nThe next  number is 22000. However, we can notice that the sum of the digits is still not divisible by  3. [score=0.88] ## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more  2s to meet the divisibility requirement of 3. [score=0.91] ## Step 10: Find the next possible number meeting the requirements\\nThe next number  to try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85] Therefore, the final answer is: $\\\\boxed{2220}$. [score=0.81] # of Tokens: 890 # of Tokens: 2419 Figure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B. 28  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the  steps:  [score=0.90] 1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is  even, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2  \\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n      \\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).   [score=0.88] 2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its  prime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]   [score=0.84] 3. **Simplify the Square Root**: We can use the property of square roots that  \\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} =  \\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2}  \\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53] Therefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).   [score=0.46] Problem: Simplify $\\\\sqrt{242}$. Figure 13: TTS case of Over-Criticism. 29  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we  can follow these steps: [score=0.99] 1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D  = 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99] 2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of  an angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here,  \\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90] 3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D =  \\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n    \\\\] [score=0.97] 4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the  equation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by  0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE  = 10\\n     \\\\] [score=0.99] Therefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99] Problem: Suppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF =  (0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n  label(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\",  (E+F)/2,W); \\n[/asy] Figure 14: TTS case of Error Neglect. 30  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\)  where \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and  the sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle,  which is the ratio of the length of the opposite side to the hypotenuse. [score=0.74] Given:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to  get:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE =  \\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88] Thus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82] Problem: Suppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF =  (0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n  label(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\",  (E+F)/2, W); \\n [/asy] Figure 15: TTS case of Error Neglect. 31  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents  to the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the  excenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents  from a point to a circle, \\\\(DB = DC\\\\).  [score=0.20] We use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended  to intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know  that the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the  ratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} =  \\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the  Power of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the  circle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of  the tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot  BP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x =  \\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66] Using the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies  \\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot  DP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the  ratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and  \\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the  formula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC  \\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is  to use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\).  Since \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP  = \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector  Theorem and the length of the angle bisector formula), we can solve for  \\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} =  \\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus,  \\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 =  157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92] Problem: Let $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$  and $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If  $AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$  and $n$ are relatively prime integers. Find $m + n$. Figure 16: TTS case of Error Localization Bias. 32  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: Let\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\).  Therefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\).  After finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\).  [score=0.98] Since you redistribute the coins so that all eight bags have the same number of coins, the  total number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible  by 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n  \\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\),  we have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81] To solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative  inverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8}  \\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49  \\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\)  by 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n  = 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92] The total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53  = 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we  have:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer  \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the  expression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256  \\\\] [score=0.79] To find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7  \\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51] Thus, the smallest number of coins you could have had before finding the bag of 53 coins  is \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31] Problem: You have seven bags of gold coins. Each bag has the same number of gold coins. One day, you  find a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight  bags you hold have the same number of coins. You successfully manage to redistribute all the  coins, and you also note that you have more than 200 coins. What is the smallest number of  coins you could have had before finding the bag of 53 coins? Figure 17: TTS case of Scoring Bias. 33  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Problem: You have seven bags of gold coins. Each bag has the same number of gold coins. One day, you  find a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight  bags you hold have the same number of coins. You successfully manage to redistribute all the  coins, and you also note that you have more than 200 coins. What is the smallest number of  coins you could have had before finding the bag of 53 coins? Response: Let\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\).  Therefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins,  the total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98] We are given that the total number of coins can be evenly distributed among 8 bags, so  \\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of  coins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\)  that satisfies both conditions.  [score=0.95] First, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv  0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation  as:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x  \\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can  be written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90] Next, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of  coins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k +  88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2  \\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\).  Substituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we  get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you  could have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7  \\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before  finding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283] Figure 18: TTS case of Scoring Bias. 34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators Daniil Moskovskiy1,2* Nikita Sushko1,2* Sergey Pletenev1,2 Elena Tutubalina1,3,4 Alexander Panchenko2,1 1AIRI 2Skoltech 3Sber AI 4ISP RAS Research Center for Trusted AI Correspondence: {d.moskovskiy, a.panchenko}@skol.tech Abstract Existing approaches to multilingual text detox- ification are hampered by the scarcity of par- allel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually col- lected and synthetically generated multilingual parallel text detoxification dataset compris- ing 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Rus- sian. The data was sourced from different toxi- city evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that mod- els trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification. Warning: this paper contains illustrative examples of texts that readers may find offensive or disturbing. 1 Introduction The proliferation of social networks and text-based internet media has highlighted the issue of online toxicity and hate speech (Saha et al., 2019). This phenomenon not only creates an unpleasant envi- ronment for users but also deters advertisers, poten- tially impacting the economic viability of these plat- forms (Fortuna and Nunes, 2018). Consequently, there is an urgent need for effective mechanisms to measure and mitigate toxicity in online spaces. A promising approach to addressing this chal- lenge is text detoxification of text through para- phrasing (Krishna et al., 2020). Text detoxification is a subtask of text style transfer (TST), which in- volves rewriting text while preserving its original *Equal contribution. Toxic Text Detoxified Text German Wie be**oppt muss man sein? Wie verwirrt muss man sein? Spanish Que os den por el c**o. Que os d√© muy mala suerte. French c‚Äôest moi at***d√© ! je suis tomb√© ! C‚Äôest moi qui suis tomb√© ! Russian —è –º—É–∂–∏–∫ –∞ –≤—ã –≥**–Ω–æ –Ø –º—É–∂—á–∏–Ω–∞, –∞ –≤—ã –Ω–µ–ø—Ä–∞–≤—ã Table 1: Examples of the source toxic texts across dif- ferent languages and their respective synthetic detoxifi- cations from our SynthDetoxM. meaning and altering specific style attribute, such as formality, bias, expressiveness, sentiment, or, in the case of detoxification, toxicity (Fu et al., 2018; Lai et al., 2021). While significant progress has been made in monolingual TST and detoxification, both in super- vised and unsupervised settings (Dale et al., 2021; Logacheva et al., 2022; Pour et al., 2023), multilin- gual text detoxification remains a largely unsolved problem. This is primarily due to two factors: the scarcity of parallel detoxification data across multi- ple languages and the suboptimal performance of unsupervised methods in cross-lingual settings (De- mentieva et al., 2023). Manual or crowdsourced data collection is a chal- lenging and costly task (Rao and Tetreault, 2018; Reid and Artetxe, 2023; Konovalov et al., 2016b), creating parallel data with the use of modern LLMs, which already proven to work well for the tasks of text classification (Sun et al., 2023) and question answering (Ye et al., 2022), remains underexplored. To address these challenges and facilitate the devel- opment of multilingual text detoxification models and datasets, we propose a framework for gener- ating parallel multilingual synthetic detoxification data and SynthDetoxM, a large-scale multilinugal synthetic parallel text detoxification dataset, which was created using this framework. arXiv:2502.06394v1  [cs.CL]  10 Feb 2025  Multilingual Toxic Data Source Toxic Data Detoxification-1 Detoxification-2 Detoxification-3 Detoxification-4 Detoxification-5 Score(ùë•ùë•ùëñùëñ; ùë¶ùë¶ùëñùëñ) xi ùëñùëñ=1 n yi ùëñùëñ=1 n SynthDetoxM LLM Clean and filter  data by length Generate detox candidates using LLMs Score detoxification candidates and select best xi; yibest ùëñùëñ=1 n Compose final  detox dataset Figure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification dataset SynthDetoxM. Our dataset is comprised of 16,000 high-quality synthetic detoxification pairs across four languages: German, Spanish, French and Russian. The dataset was created using few-shot prompting and selecting the best generations of five different open-source LLMs. The answers were combined using a hand- crafted heuristic, providing the best answers from each model to ensure diversity and quality of the final data. Our contributions can be summarized as follows: 1. We propose a framework for generating syn- thetic parallel multilingual detoxification data using few-shot prompting of LLMs. 2. We create SynthDetoxM, a large-scale multilin- gual synthetic parallel dataset for text detox- ification, helping to address the data scarcity issue in the detoxification task. 3. We conduct a thorough empirical evaluation of the proposed dataset, including linguistic analysis of the data and benchmarking against the human-annotated MultiParaDetox. We openly release the generated'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=26, thumbnail=None, content='1 INTRODUCTION A longstanding goal of multi-agent artificial intelligence is the de- velopment of independent agents that can communicate using a shared language. Communication is especially necessary in ‚Äúpar- tially observable‚Äù settings, where each agent only has a limited view of the world and therefore benefits from sharing knowledge with other agents to achieve its goal. In particular, ‚Äúsocial deduction‚Äù games are settings where each agent‚Äôs goal is to deduce informa- tion about the environment by communicating with other agents ‚Äì requiring each player to learn how to parse messages from other players while effectively sharing important information needed for game completion. In this work, we study the hidden-role game of Among Us [18] as a specific instance of a challenging social deduction game to investigate the importance of communication, illustrated in Fig. 1. Hidden-role games [4, 19] are a class of environments where play- ers are split into an uninformed majority and a smaller informed hidden subteam, which we refer to as crewmates and imposters re- spectively. These two teams are adversaries, resulting in a zero-sum game, where the goal of the crewmates is to deduce the identity of imposters to vote them out. Unlike other popular hidden role games such as the game of Mafia [2], where statements from players are unfalsifiable, Among Us is based in a 2D embodied environment, allowing discussions and intuitions to be grounded in specific ob- servations. In the game, crewmates try to complete an assigned set of tasks scattered across the environment while imposters try to kill all crewmates. If a player reports the corpse of an eliminated crewmate ‚Äì killed by an imposter ‚Äì the game moves to a discussion phase with a free-form chat followed by a voting period, where all players vote to eject a suspected imposter. For crewmates, success in the discussion phase would mean correctly voting out the im- poster, while success for imposters means avoiding suspicion from the crewmates to continue staying in the game as long as possi- ble. This highlights the importance of communication during the discussion phase as crewmates need to learn to effectively utilize arXiv:2502.06060v1  [cs.AI]  9 Feb 2025  Figure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid environment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room. Here, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can perform tasks (indicated by the stars ‚Äì in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and green agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates the discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided by our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter. The example discussion shown on the right is based on a generated discussion from our trained models. the discussion phase to vote out imposters in an adversarial setting. For the rest of this paper, we study the game of Among Us from the perspective of crewmates attempting to perform tasks, identify imposters, and win the game. In multi-agent environments, an effective technique for training strong cooperative and competitive agents is multi-agent reinforce- ment learning (MARL), which enables artificial agents to achieve superhuman levels of performance in competitive games such as StarCraft [36], and cooperative games such as Overcooked [5, 31] and Hanabi [13]. However, in settings where communication in natural language is necessary, existing MARL techniques often struggle as they require large datasets of task-specific human com- munication data to perform on-par with humans [8]. This funda- mentally limits the agents‚Äô ability to communicate at human-level and is not practical for learning in settings where these datasets do not readily exist. The game of Among Us falls into this category, where communication is necessary to reason and progress in the game. Therefore, we would like to find an approach that learns to communicate effectively and convincingly without requiring large amounts of task-specific human data. However, the major chal- lenge in learning to communicate without access to large amounts of human data is that novice agents do not have a strong signal for understanding the helpfulness of the messages they send (speak- ing) or for learning the meaning of messages from other players (listening). In particular, the sparse reward signal the agents receive when winning the game is not informative enough to reinforce high-quality discussions between agents. Our key insight is that we can leverage the agents‚Äô instrumental goal of predicting useful information about the world ‚Äì e.g., the identity of imposters ‚Äì as a dense reward to provide a higher-quality signal that can enable more informative communication during the discussion phase and potentially higher performance policies. We propose an approach that rewards a message generated dur- ing the discussion phase based on how the other crewmates‚Äô beliefs on the identity of the imposter changes. Each crewmate wants to send messages that help other crewmates be more certain about the true identity of the imposter. However, this only explains how to learn to ‚Äúspeak‚Äù assuming that the other agents can appropri- ately update their belief about the world given a message. We also need to ensure the agents know how to ‚Äúlisten‚Äù and update beliefs appropriately. To encourage this, we additionally add an imposter prediction signal to guide the agent‚Äôs learning to predict the true identity of the imposter after each message. By training agents to speak and listen effectively, we enable the agents to self-improve their discussion abilities. Further, to encourage listening and speak- ing in natural language during the discussion phase of the game, we tap into the power of large language models (LLMs), unspecial- ized models trained with large amounts of human language data. Specifically, we initialize crewmates as LLMs capable of communi- cating via natural language. Recent advances in foundation models have demonstrated some reasoning abilities [3, 27], including un- derstanding social scenarios [20], but even the strongest language models today are weak at self-critiquing [34] or performing theory of mind reasoning [33], limiting their ability to improve their lis- tening skills based on their own feedback. However, by training LLMs within our proposed framework of encouraging listening and speaking with auxiliary dense rewards for helping other crewmates vote out the correct imposter, we overcome this limitation, enabling the self-improvement of these models over time. To evaluate our framework, we analyze the success rate of crew- mates against both pretrained and adaptive imposters, and find that crewmates form a robust communication strategy. We find that our technique results in emergent behavior commonly found in real games of Among Us between humans, such as directly accusing  players and providing evidence to help other crewmates [22]. We also find that our augmentation to discussions results in two times higher success rates relative to standard RL along with over three times higher success rates relative to base models that are over four times larger than our models, highlighting the importance of our discussion strategies. 2 RELATED WORK In this section, we review related work on emergent communica- tion, prior works that use language models as agents in embodied settings, and past works integrating language models with RL. Emergent Communication. A major topic in MARL is emergent communication between agents, especially in the context of refer- ence games and repeated reference games, where a speaker knows the ground-truth answer to a question (e.g., a specific image out of a set of images that needs to be referred to). Then, the speaker needs to communicate to the listener, who later needs to choose the item being referenced either over one or repeated interactions. Prior work has shown that humans tend to quickly adapt to such tasks [26], naturally using theory of mind reasoning to determine the intents of speakers [9]. Further, Hawkins et al. [12] showed that language models can also learn to adapt to human conventions via continual learning. Without using human natural language data, Lazaridou et al. [23] and Havrylov and Titov [11] use symbolic cheap-talk signals to solve referential games. Our framework of social deduction games, however, is more challenging as each agent does not know the ground truth answer, so teams must communi- cate to collectively learn the answer. Therefore, our domain does not have as clear of a distinction between ‚Äúspeakers‚Äù who have knowledge and ‚Äúlisteners‚Äù who need to gain answers as agents in social deduction games must play both roles. Language Models Agents. A large body of prior work use LLMs‚Äô access to internet scale data for task planning and decision making. In robotics, prior works explore how language models can be used to plan out a sequence of high-level primitives given an instruction in natural language [1, 17, 24]. In a virtual gaming setting, Park et al. [29] uses ChatGPT to simulate members of a small virtual town. Although there is no specific task or mechanism for ‚Äútraining‚Äù these agents, they demonstrate the use of a long-term memory stream to store memories beyond the context length of the language models, enabling the formation of social networks. This technique of having external memory has later been used to learn ‚Äúskills‚Äù in a single-player environment [38] and for coordination in multi- agent environments [10]. These works demonstrate that language models are capable of controlling agents in a wide range of settings, which is key to our motivation to directly use language models as a strong starting point for agents operating in more challenging environments such as social deduction games. Reinforcement Learning with Foundation Models. Some works also combine language models with reinforcement learning. Ci- cero [8] is an AI for the game of Diplomacy that uses a dialogue- conditional action model from human actions and trains a dialogue- free model using RL to choose actions. Cicero uses an ‚Äúintent‚Äù em- bedding to connect the dialogue generation and strategic reasoning components. This allows Cicero to communicate with other agents in a way that feels natural to other players, but it prevents the RL model from directly controlling the generated messages, potentially limiting improvements in message quality. Another drawback is that this technique requires a large number of human demonstra- tions, which may be impractical in many settings. Foundation models have been effective in both providing rewards and as a base model for policies. Hu and Sadigh [14] and Kwon et al. [21] use language models as reward signals to train a separate network to follow a specific coordination strategy. We similarly use the LLM to provide denser rewards during the discussion phase, but we train the LLM itself instead of a separate policy. Outside of the embodied setting, reinforcement learning has also been key to improving the chat capabilities of LLMs. Ouyang et al. [28] demonstrates the effectiveness of reinforcement learning from human feedback (RLHF), where a reward model is trained using human feedback and an LLM is fine-tuned using a modification of the PPO algorithm to improve its performance. Yuan et al. [39] extends this by allowing the LLM to be its own reward model and generate its own data for self-improvement, similar to how we use the LLM‚Äôs own change in beliefs as a reward signal. However, a crucial difference is that our reward model remains grounded in an environment by design due to the imposter prediction training signal. This means that we do not need to rely on the ability of pretrained LLMs to critique their own generations, enabling us to use smaller language models and correct logical errors over time. 3 PRELIMINARIES We model social deduction games, such as Among Us, as a variant of the partially observable Markov game (POMG) [25] that includes a question whose answer must be deduced through interacting with players and the rest of the environment. Our modified POMG can be described by a tuple (ùëõ, S, A, P,ùëü, O,ùõæ, Q,ùëû), where ùëõis the number of players, S is the joint (hidden) state space and A is the joint action space. The transition function P : S √ó A √ó S ‚Üí[0, 1], is the probability of reaching a state given the current state and joint action. The reward function ùëü: S ‚ÜíRùëõ, gives a real value reward for each state transition to each player, and ùõæis the reward discount. The observation function, O : S ‚ÜíùëÇùëõ, generates the player-specific observations from the state. Our POMG has additional terms for the task of social deduction, which are the set of all possible answers to the deduction problem Q ‚äÜA and the correct answer ùëû‚ààQ. In social deduction games, agents will be given opportunities to answer the question as a literal action (i.e. voting in Among Us or choosing the correct object in reference games), and at those steps the correct action to take is ùëû. The trajectory up to time ùë°is defined as a sequence of joint observations and actions: ùúèùë°= (ùëú0,ùëé0, . . . ,ùëéùë°‚àí1,ùëúùë°). An individ- ual player only experiences their own action-observation history (AOH), which is defined for player ùëñas ùúèùëñ ùë°= (ùëúùëñ 0,ùëéùëñ 0, . . . ,ùëéùëñ ùë°‚àí1,ùëúùëñ ùë°), and they follow a stochastic policy ùúãùëñ(ùëéùëñ|ùúèùëñ). In the game of Among Us, the AOH consists of past observations supplied by the envi- ronment, past embodied actions, and all prior discussions with the other players. Language Models. Language models are trained to model the probability of a sequence of discrete tokens, where each token represents a string of natural language. For a sequence of tokens,  ùëä= {ùë§0,ùë§1, . . . ,ùë§ùëò}, the probability of the sequence being gener- ated is ùëù(ùëä) = √éùëò ùëó=0 ùëù(ùë§ùëó|ùë§<ùëó), so causal language models predict the distribution of the next token conditioned on all prior tokens. Our Among Us environment is designed such that each observa- tion at time stepùë°is a sequence of tokensùëúùë°= ùëäùë°= {ùë§0,ùë§1, . . . ,ùë§ùëò} and each action at time step ùë°is a single token ùëéùë°= ùë§ùë°, allowing us to use language models as the policy for each agent. The AOH is a sequence of tokens, so language models can sample the next action by predicting the next token in the sequence, constrained to the set of legal actions at that timestep. In this work, we use the RWKV language model [30], a recurrent language model based on a linear attention mechanism, as the pre- trained foundation model. We choose RWKV over more common transformer-based models [35], because the recurrent formulation allows us to generate RL trajectories with a constant time and space complexity per token, and RWKV enables unbounded-context train- ing using truncated backpropagation through time. This is espe- cially important since Among Us trajectories often reach tens of thousands of tokens in length per player, which would require significantly more compute for classic attention-based models. Em- pirically, RWKV has also performed on-par with transformer-based models, especially in decision-making tasks [7] and long-context understanding [15], making it the ideal choice for this study. 4 THE GAME OF AMONG US In this section, we describe the key design decisions of our imple- mentation of the hidden-role game of Among Us. Our goal is to create an environment where agents can ground their discussion based on evidence in the environment. A more complete description of the game is in Appendix A. Role Assignment. At the start of the game, each player is either assigned as an imposter or a crewmate. The crewmates are not informed of the identities of the other players, but all imposters are informed of the identities of the other players at the start. In our setting, we assign one player to be the imposter and the other ùëõ‚àí1 players as crewmates. The crewmates are assigned a set of ùëÅtasks, scattered across the environment. As an example, ùëÅ= 3 in the example in Fig. 1. Gameplay Phase. During the gameplay phase, players simultane- ously move in an embodied environment, receiving observations from the environment and taking actions, as illustrated in Fig. 2. Players freely move around a ùëä√ó ùêªgrid of rooms during the gameplay phase, receiving new observations ùëúùë°at each time step. All agents can move between adjacent rooms by choosing ùëégo to x, where x is a cardinal direction, or they can simply wait in the room by choosing ùëéwait. Crewmates can complete tasks in their current room by choosing ùëétask, but they are unable to observe the environ- ment for ùëÅtask_time time steps, i.e., they will not be able to observe if a crewmate is being killed by an imposter while performing a task. Note that tasks are indistinguishable from one another, so we do not have different actions for different tasks. Imposters can kill crewmates by choosing ùëékill,ùëówhere ùëóis a crewmate in the same room as them, but they have to wait ùëÅcooldown time steps between killing crewmates. Finally, crewmates can report dead bodies in their room by choosing ùëéreport,ùëó, where ùëóis the corpse of player ùëó, which initiates the discussion phase. Observe Observe Token  Buffer Token  Buffer Crewmate: œÄ2 Crewmate: œÄj st+1 Imposter: œÄ1 Crewmate: œÄj Observe Observe : You are in room (1, 0). You see Green, Orange in the room   o1 t+1 a1 t ‚àà{awest, await, akill,3, akill,4} aj t ‚àà{aeast, await, atask, areport,2} Token  Buffer Token  Buffer œÑj t œÑ1 t Figure 2: Diagram of the embodied gameplay loop. The envi- ronment starts by sending observations to all agents simul- taneously and collects tokenized actions from a set of valid actions at each timestep. The set of all valid actions are ùëégo to x, ùëétask, ùëékill,ùëó, ùëéreport,ùëó, and ùëéwait, where x is a cardinal direction and ùëóis the name of a crewmate. The environment provides each player with the subset of actions that are valid at each timestep. Discussion Phase. During the discussion phase, we cycle over each player twice in a random order and allow them to say a sentence, ùëétalk, in natural language. After this discussion, a voting phase begins, where each player votes for one player, ùëò, they want to eject by choosing action ùëévote,ùëò. The player who gets the plurality of votes is ejected. If the imposter is not ejected, the game continues to the next gameplay phase, where crewmates can continue finishing tasks. Before the discussion starts and between each discussion mes- sage, the environment also surveys each crewmate by asking who they would vote for, i.e., by querying them to pick an ùëévote,ùëòas if they had to vote immediately. This action has no impact on the POMG, but it will be relevant for our training algorithm. Note that the set of all voting actions is equal to the set of all possible ‚Äúanswers‚Äù in the social deduction game (Q), and voting out the correct imposter corresponds to ùëû, the correct answer to the social deduction question. Reward Structure. Among Us is fundamentally a team zero-sum game, so reward is based on whether crewmates or imposters win. If all tasks are completed or the imposter is ejected, the crewmates win with a reward of +1. However, if the number of imposters is ever greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of -1. 5 TRAINING LLM CREWMATES IN AMONG US By defining an environment that only interfaces with players through natural language, we can directly use a language model as the pol- icy ùúãùëñ(ùëéùëñ|ùúèùëñ) of an agent ùëñ. The action-observation histories of our agents ùúèùëñare just strings of natural language, and new observa- tions and actions can simply be appended to the end of the strings. Furthermore, when taking actions ùëéùëñ, the outputs of the language model can be constrained to be one of the legal actions provided by the environment at each timestep. Following this procedure,  we construct an agent using a pretrained RWKV language model, which we define as policy ùúãRWKV. Although the environment is designed to interface nicely with language models, we find that ùúãRWKV struggles to reason as crew- mates in a zero-shot fashion in Among Us, with models frequently voting to eject the wrong players. In this section, we describe our procedure for improving the performance of crewmates by enabling them to self-critique and use these scores to improve dialogue gen- eration. The first two subsections describe how to improve the perfor- mance of an individual learning crewmate, first describing a rein- forcement learning procedure and then describing how to enhance communication by learning to listen and speak. The third subsec- tion describes how to train the team of crewmates to be robust to adaptive imposters and different policies within the crewmate population. 5.1 Reinforcement Learning in Among Us To train a language model to take more effective actions without expert demonstrations, we can turn to reinforcement learning. Since Among Us already provides rewards for winning, we can directly optimize this to produce a model ùúãRL that minimizes the following loss: ùêøRL(ùúã) = ‚àíE ùúèùëñ‚àºŒ† ‚àëÔ∏Å ùë° " ùõæùë°ùëüùëñ ùë°+ ùúÜNL log( ùúã(ùëéùëñ ùë°|ùúèùëñ ùë°) ùúãRWKV(ùëéùëñ ùë°|ùúèùëñ ùë°) ) # , (1) where Œ† represents the joint policy that has ùúãcontrolling agent ùëñ, and ùúÜNL is a hyperparameter controlling the strength of a soft KL constraint regularizing trained models to the base LLM to prevent discussions from moving out of natural language [28]. Note that the only reward signal is the sparse reward received at the end of the game along with additional rewards for completing tasks. In particular, there is very little signal for the effectiveness of its messages during discussions, which makes utilizing communication very difficult with just RL in practice. This sparse signal also makes identifying the imposter difficult in the multi-agent setting, because voting correctly may still result in a loss and voting incorrectly could result in a win if a plurality of agents vote for the imposter. 5.2 Enhancing Communication of Crewmates To improve beyond the RL baseline, we can take advantage of the social deduction component of the game. In particular, each agent‚Äôs belief in choosing the correct answer ùëû‚ààQ will provide a stronger signal for learning the core components of the game and the means of communication relative to the RL baseline. In this subsection, we discuss the key contributions of this work. Specifically, we highlight new loss terms to enhance both listen- ing and speaking abilities, enabling crewmates to better utilize discussions. Listening: Imposter Prediction. Suppose an agent is learning in a environment where it is partnered with expert crewmates who already know how to discuss the game. How can this agent learn to understand the meanings of environment observations and messages from other crewmates? To effectively discuss the game of Among Us, crewmates need to understand who the imposter is given their past observations and the past messages. This prediction task can act as an auxiliary task that can guide the discussion phase to be more grounded and meaningful. We directly train crewmates to improve their reasoning over imposters using the environment‚Äôs ground truth answer for the identity of the imposter. Specifically, we use the timesteps when the environment directly surveys the players for their beliefs over the imposters, which occurs between discussion messages, as the training signal. Note that this training signal does not specifically require human demonstration data; agents can learn to understand observations and messages from other players using any rollout buffer. For every living crewmate, if they are asked to provide their beliefs regarding the identity of the imposter at timestep ùë°, the listening loss for that timestep is ùêøL(ùúã,ùúèùëñ ùë°) = ‚àílog ùúã(ùëû|ùúèùëñ ùë°), (2) where ùëû= ùëévote,ùëóis the action representing choosing the correct imposter ùëó, and ùúèùëñ ùë°is the AOH until timestep ùë°, which may include prior discussions. At the very start of the discussion phase, agents need to reflect on the probabilities of other agents being the imposter based on their observations during the gameplay phase. For instance, if a crewmate directly witnesses a murder, they should be very certain that the murderer is the imposter; our listening loss uses this signal to increase their certainty over the imposter. By framing the task of identifying imposters using messages and observations as a supervised learning problem, agents learn to understand the meaning of messages, enabling them to vote out the correct imposter. Using this loss term, we can define two new policies. We can directly incorporate the listening loss into the RL policy, giving us the policy ùúãRL+L that optimizes ùêøRL+L(ùúã) = ùêøRL(ùúã) + E ùúèùëñ‚àºŒ† ‚àëÔ∏Å ùë° ùúÜLùêøL(ùúã,ùúèùëñ ùë°), (3) where ùúÜL is a hyperparameter controlling the strength of the lis- tening loss and is only nonzero on timesteps when the crewmates are asked to predict the identity of the imposter. This enables the model to optimize actions while improving its ability to identify imposters. We can also define a purely listening policy, ùúãL that incorporates the listening loss without an RL component, therefore optimizing ùêøL only(ùúã) = E ùúèùëñ‚àºŒ†rand ‚àëÔ∏Å ùë° ùúÜLùêøL(ùúã,ùúèùëñ ùë°), (4) where Œ†rand is a joint policy that uses ùúãRWKV for discussions and chooses gameplay actions uniformly at random. Speaking: Reinforced Discussion Learning. So far, we have developed a policy that can learn to take effective actions in the environment with RL, and can update beliefs based on discussion messages. Now suppose that an agent is partnered with expert crewmates who already know how to parse messages from other players. How can this agent learn to construct helpful messages when it is their turn to speak? Although our use of a supervised imposter prediction loss allows agents to learn how to interpret messages from other agents in the previous subsection, we cannot directly apply the same idea to  learning how to speak as there is no ground truth notion of effec- tive messages. We instead improve the agents‚Äô discussion abilities using reinforcement learning. Specifically, we grant rewards to the speaking agent based on the change in living crewmates‚Äô beliefs on the true imposter after each message. Formally, let ùêµùë°be the sum of all living crewmates‚Äô beliefs, ùêµùë°= ‚àëÔ∏Å ùëò‚ààùê∂ùë° ùúãùëò(ùëû|ùúèùëò ùë°), (5) where the ùëûrepresents voting out the correct imposter, and ùê∂ùë° is the set of all living crewmates at time ùë°. If ùë°‚Ä≤ is the previous belief-querying timestep, then the reward for crewmate ùëñ, who just finished speaking, is ùëüùë† ùë°: ùëüùë† ùë°= ùêµùë°‚àíùêµùë°‚Ä≤. (6) Intuitively, this reward models the causal effect of each message on the task of predicting the correct imposter. The most effective message that a crewmate could send would convince other crew- mates to vote out the true imposter. Using speaking and listening, we can train an agent ùúãRL+S+L that minimizes the following loss: ùêøRL+L+S(ùúã) = ùêøRL+L(ùúã) ‚àí E ùúèùëñ‚àºŒ† ‚àëÔ∏Å ùë° [ùúÜSùõæùë°ùëüùë† ùë°]. (7) 5.3 Training for Dynamic Settings As a team zero-sum game, we want our trained crewmates to work well against a wide range of imposters. To do so, we employ an iterated self-play algorithm, where crewmates and imposters train against earlier iterations of their adversary‚Äôs policy. We train im- posters to learn to mislead crewmates into voting out other agents, so we keep the RL loss and invert the speaking loss, minimizing the following: ùêøimp(ùúã) = ùêøRL(ùúã) + E ùúèùëñ‚àºŒ† ‚àëÔ∏Å ùë° [ùúÜSùõæùë°ùëüùë† ùë°]. (8) As the inner optimization loop, we use independent PPO [16, 32] with shared networks for policy and value functions and the Schedule Free AdamW optimizer [6]. We also want our crewmates to be robust to different partners who also act reasonably. Therefore, we always set one crewmate to be frozen to the listening policy ùúãL when forming the joint policy Œ†, following the N-Agent Ad hoc teamwork setting [37] instead of assuming a homogeneous population. This change also ensures that crewmates cannot simply determine the identity of the imposter by forming an arbitrary convention and voting out any agent who violates that convention. Finally, we want our agents to be robust to different environment configurations. We randomize multiple environment parameters while training: choosing between three different layouts of the environment (1 √ó 3, 2 √ó 2, and 2 √ó 3 grids), and randomizing the number of tasks assigned to each crewmate to either 3, 4, or 5. We only train on configurations where there are 4 crewmates and 1 imposter, but we report generalization results when playing with different numbers of crewmates. To stabilize training, we also include the following world model- ing loss to each model‚Äôs loss function: Models 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Win Rate RWKV RWKV7B RL L RL + L RL + L + S Figure 3: Win rates for crewmates trained with different algorithms over the ‚Äúbase‚Äù environment: 2 √ó 2 grid of rooms, 4 tasks per crewmate, and 5 players. Error bars represent the maximum and minimum expected win rates across the three independently trained runs with different seeds. ùêøWM(ùúã) = ‚àíE ùúèùëñ‚àºŒ† ‚àëÔ∏Å ùë° ùúÜWM log ùúã(ùëúùëñ ùë°+1|ùúèùëñ ùë°,ùëéùëñ ùë°), (9) where ùúÜWM is the relative strength of the world modeling loss. Although this loss does not directly contribute to improving task success, it subtly helps improve the model‚Äôs performance. In partic- ular, as a recurrent model, RWKV benefits from this world modeling loss as it ensures that features are remembered throughout training. Furthermore, the world modeling loss prevents the model from plac- ing too much weight on action tokens, which would cause models to output action tokens even during regular discussion sections. 6 RESULTS In this section, we analyze the quality of our trained crewmates. We inspect the importance of different design decisions regarding the training of crewmates by ablating over the components of the loss function in Eq. (7): RL, listening, and speaking. We determine the importance of discussions in the game by measuring the equilibrium win rates of crewmates against imposters and analyze emergent behaviors in discussions. Finally, we highlight some common failure modes we observed during training and how they are mitigated in our final training algorithms. 6.1 Cooperative Training For this set of experiments, we analyze the performance of the crewmates from the first iteration of the self-play algorithm. We conduct all experiments using the 1.5B RWKV model, because we find diminishing returns at higher parameter counts from our experiments on base models (see Appendix B for more details). We report the win rates of each policy in the base environment when keeping the imposter and one crewmate fixed to ùúãL in Fig. 3. Model Evaluations. The simplest baselines are direct evaluations of the base model. We find that the 1.5B RWKV model struggles to win in the game, with the larger 7B parameter model performing slightly better as both win less than 20% of the time in the base environment.  2√ó1 1√ó3 2√ó2 2√ó3 3√ó2 Environment Shape 0.0 0.2 0.4 0.6 0.8 Win Rate 2 3 4 5 6 Tasks 4 5 6 Players RWKV RWKV7B RL L RL + L RL + L + S Figure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying the environment shape, tasks, and number of players. Just training with RL significantly boosts the performance rel- ative to the base models, even significantly outperforming the 7B parameter model. However, we still find that RL without the ad- ditional listening loss struggles to reason about the identity of imposters. Even when warm-starting the RL policy from ùúãL, we find that it quickly loses the ability to identify imposters, instead voting for any agent with equal probability. When we instead only trained with listening ‚Äì using the loss ùêøL only ‚Äì the model, ùúãL, does not know which actions are effective or how to discuss details about the environment, but it is an effective baseline due to the fact that predicting the identity of the imposter is valuable in Among Us. When combining RL and the listening loss, we find success rates again increase dramatically, with further improvements when adding our denser speaking rewards, as agents can now differen- tiate between helpful and unhelpful messages when training. We ultimately find that our full model achieves twice the win rate of the RL-only baseline in the base environment. Note that the difference in scores when adding the additional speaking term is relatively small. Even without the explicit speaking reward, the language model produces coherent messages, often sharing their current suspicions during discussion rounds, thus benefiting from discussion even without additional rewards. This is an interesting emergent behavior as it shows that speaking is indirectly improved by training the model to listen better. Robustness to Environment Variation. We present the win rate of crewmates against imposters across different environment configurations in Fig. 4, and see that the trends between models observed in the base environment generally persist across configu- rations. We find that the shape of the environment has little effect on the win rates of crewmates, with smaller environments gener- ally being easier since it is harder for imposters to kill crewmates without witnesses. We see a general decline in performance across all models when increasing the number of tasks, because this makes it harder to win the game by completing tasks instead of voting out the imposter. Finally, we see a significant increase in win rates as the number of crewmates increase, which we expect since the crewmates can still recover from incorrectly voting out a crewmate. We do not observe significant deviations from the expected trend lines in settings that were out of the training distribution, demon- strating how the language models can extrapolate their behaviors to unseen deviations in the configuration. Message Evaluations. We find a major difference between the message patterns of the base RWKV model and those from ùúãRL+L+S. Most messages from the base RWKV model are often unfocused, hallucinating a wider context to the game and role-playing a crew- mate. Meanwhile, crewmates using ùúãRL+L+S often directly accuse the imposter or otherwise name the imposter in their messages. In general, we find that naming an agent makes it more likely for other agents to vote against them. Furthermore, crewmates share messages that resemble environment observations that helped them judge the identity of the imposter. For instance, a crewmate may say ‚ÄúPlayer Green is leaving Room (0,1)‚Äù when the body is in Room (0,1) to indicate that Player Green was running away from the dead body, which is often correlated with being the imposter. However, the crewmates sometimes tell lies in their messages ‚Äì just like hu- mans often do when playing Among Us. In particular, they often simply make up evidence and state whatever is most convincing to other agents to get enough votes to eject the correct imposter. Representative behavior samples are provided in Appendix C. 6.2 Robustness to Imposters When training against frozen imposters, crewmates could come up with simple strategies that result in high win rates but are easy to overcome with a more intelligent imposter. We therefore run multiple iterations of self-play to investigate whether crewmates can use discussions even against imposters that can evolve to their policies, which we illustrate in Fig. 5. Note that in exploitability curves, we would like to see convergence upon a narrow interval for both the upper and lower bounds; a weak strategy can be easily exploited at each iteration and would therefore have both lines stay far apart and converge slowly. We find that the crewmates‚Äô strategies are robust to imposters trained in an adversarial fashion. In particular, we see that crew- mate scores converge after only a few iterations; depending on the seed, the win rate converges to between 0.51 and 0.56 on the base environment. In fact, even the crewmates that only trained on the base model imposter are relatively strong, as can be seen by the large jump in the lower bound between iterations 0 and 1. This result implies that policies discovered by ùúãRL+L+S are very robust since even facing adversarially trained imposters does not cause a significant performance drop.  0 1 2 3 Self-Play Iteration 0.2 0.3 0.4 0.5 0.6 Win Rate Learning Imposter Frozen Imposter Figure 5: Exploitability curves for policies over self-play it- erations, evaluated on the base environment. The orange line indicates the expected win rate against an adversarially trained imposter. The black line indicates the expected win rate of crewmates who are specifically optimized against this iteration‚Äôs imposters. Note that iteration 0 refers to the base models, while iteration 1 refers to the crewmate policy from the Cooperative Training section. Shaded regions represent the maximum and minimum win rates across the three inde- pendently trained runs with different seeds. Qualitatively, we observe that imposters attempt to shift blame to other players by counter-accusing another crewmate. In par- ticular, they mimic the discussion patterns of crewmates, and the crewmates sometimes fall for this deception. Crewmates who have not witnessed the murder tend to support claims made by other players, causing them to sometimes help the imposter. Interestingly, we still see similar behavior to a smaller level in the base model imposters when playing against strong crewmates. This emergent behavior can likely be attributed to the in-context learning capabil- ities of language models, which would allow the imposter to mimic the speech of crewmates who spoke beforehand. 6.3 Failure Modes Throughout our experimentation, we encountered various failure modes that we tackled in our final training algorithms. Specifically, discussions tended to leave natural language and generally degen- erate without careful consideration from the training algorithm. First, we observed that the soft KL constraint commonly used in RLHF [28] required careful tuning to keep language generations in English. When this constraint is weighted too low, all of our RL-trained models diverge from natural language after only a few iterations, causing it to output random tokens during discussions and stop improving in performance. We also observed that allowing all crewmates to be trained si- multaneously would lead to degenerate solutions. Sometimes the models learn a social convention where they simply do not speak during the discussion phase. Specifically, models would output new- lines when it is their turn to speak instead of actually speaking. In this case, only the imposter would speak and all the agents would just vote the speaker out. The models would also learn to just wait in the starting room instead of moving around, allowing them to witness the murder or vote out the person who moves out of the room. These strategies are degenerate solutions since they would not work if the imposter was aware of their strategy or if not all the crewmates shared the same strategy, but these strategies would lead to nearly perfect win rates during the first iteration of self-play. The fix to this issue was to ‚Äúfreeze‚Äù one crewmate to not learn and therefore not follow changes in strategies. The final failure mode we observed was using action tokens in discussions instead of natural language. Specifically, the RL-trained models would learn to take actions by explicitly choosing action tokens, but this gives action tokens a higher probability to be chosen overall, even during discussion phases. We observed that the best way to counteract this effect was to introduce the world modeling loss from Eq. (9). This loss ensured that the model preserved its language modeling abilities and had the side effect of helping the models match the patterns it experienced in observations within its own discussions, which would help independent agents understand the intentions of our models. 7 DISCUSSION Summary. We introduce a technique to self-improve the discussion ability of an LLM in social deduction games and show how it en- ables agents to communicate effectively in the game of Among Us. We demonstrate that, despite having weak base models, our agents learn to speak effectively and extract information from discussion messages. We also find that our agents are robust to adversarially trained imposters, who, despite attempting to sabotage the dis- cussion, are unable to break the crewmates‚Äô coordination during discussions. Our technique ultimately shows that self-improving discussions in multi-agent settings does not require task-specific hu- man data, unlocking the possibility for multi-agent communication with language models in novel tasks. Limitations and Future Work. A key limitation of our approach is that our scene prediction technique is task-dependent. In Among Us, there is a natural connection between the discussion and trying to predict the identity of the imposter, and a similar structure applies to a wide range of social deduction games and real-world settings. An interesting future direction would be to allow agents to identify which aspects of the scene are relevant to a specific task instead of manually specifying it. Please refer to Appendix D for more analysis on the broader impacts of our work. We also note that crewmates are not always truthful in their discussions, opting instead to make the most convincing statements. We consider this behavior to be potentially dangerous outside of our sandboxed setting of Among Us, so we believe that optimizing for truthfulness is an important future direction. ACKNOWLEDGMENTS This research was supported in part by the Other Transaction award HR00112490375 from the U.S. Defense Advanced Research Projects Agency (DARPA) Friction for Accountability in Conversa- tional Transactions (FACT) program, the Cooperative AI foundation, ONR project N00014-21-1-2298, NSF Awards #2006388, #1941722, #2125511, AFOSR YIP, and the Stanford Center for Human-Centered AI (HAI). We thank the Stanford Madrona team for giving advice on the systems-level details of our implementation, and Hengyuan Hu for his insightful feedback when reviewing this paper.  REFERENCES [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus- man, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding Language in Robotic Affordances. arXiv:2204.01691 [cs.RO] [2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical Study of Players and Coalitions in a Partial Information Environment. The Annals of Applied Probability 18, 3 (2008), 825‚Äì846. http://www.jstor.org/stable/25442651 [3] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL] [4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas Sandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation. arXiv:2308.16017 [cs.GT] [5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia, Pieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans for human-AI coordination. Curran Associates Inc., Red Hook, NY, USA. [6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth Conference on Neural Information Processing Systems. [7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma- nipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for Lifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306 [8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla- herty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. 2022. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Sci- ence 378, 6624 (2022), 1067‚Äì1074. https://doi.org/10.1126/science.ade9097 arXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097 [9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by assuming that speakers are informative. Cognitive Psychology 75 (2014), 80‚Äì96. https://doi.org/10.1016/j.cogpsych.2014.08.002 [10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong Zheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA- gent: Emergent Gaming Interaction. 3154‚Äì3183. https://doi.org/10.18653/v1/ 2024.findings-naacl.200 [11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent games: learning to communicate with sequences of symbols. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS‚Äô17). Curran Associates Inc., Red Hook, NY, USA, 2146‚Äì2156. [12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin- ual Adaptation for Efficient Machine Communication. In Proceedings of the 24th Conference on Computational Natural Language Learning, Raquel Fern√°ndez and Tal Linzen (Eds.). Association for Computational Linguistics, Online, 408‚Äì419. https://doi.org/10.18653/v1/2020.conll-1.33 [13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other- Play " for zero-shot coordination. In Proceedings of the 37th International Confer- ence on Machine Learning (ICML‚Äô20). JMLR.org, Article 409, 12 pages. [14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement Learning for Human-AI Coordination. In 40th International Conference on Machine Learning (ICML). [15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se- quences? Comparing Architechtural Inductive Biases on Long-Context Abilities. arXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112 [16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Jo√£o G.M. Ara√∫jo. 2022. CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms. Journal of Machine Learning Research 23, 274 (2022), 1‚Äì18. http://jmlr.org/papers/v23/21- 1342.html [17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan- guage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em- bodied Agents. In Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 9118‚Äì9147. https://proceedings.mlr.press/v162/huang22a.html [18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/. [Online; accessed 25-February-2024]. [19] Kavya Kopparapu, Edgar A. Du√©√±ez-Guzm√°n, Jayd Matyas, Alexander Sasha Vezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki, Joel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI] [20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan, and Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International Conference on Robotics and Automation (ICRA). [21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re- ward Design with Language Models. In International Conference on Learning Representations (ICLR). [22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 6570‚Äì6588. https://doi.org/10. 18653/v1/2023.findings-acl.411 [23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi- Agent Cooperation and the Emergence of (Natural) Language. In International Conference on Learning Representations. https://openreview.net/forum?id= Hk8N3Sclg [24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. 2023. Text2Motion: from natural language instructions to feasible plans. Autonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7 [25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce- ment Learning of Partially Observable Markov Games. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI [26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway, and Judith E. Fan. 2021. Learning to communicate about shared procedural abstractions. arXiv:2107.00077 [cs.CL] [27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser- rat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large Language Models as General Pattern Machines. In Proceedings of the 7th Confer- ence on Robot Learning (CoRL). [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul- man, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe- ter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. arXiv:2203.02155 [cs.CL] [29] Joon Sung Park, Joseph C. O‚ÄôBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ‚Äô23) (San Francisco, CA, USA) (UIST ‚Äô23). Association for Computing Machinery, New York, NY, USA. [30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart≈Çomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis≈Çaw Wo≈∫niak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 14048‚Äì14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936 [31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for human-AI collaboration. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS ‚Äô23). Curran Associates Inc., Red Hook, NY, USA, Article 1003, 25 pages. [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG] [33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. arXiv:2305.14763 [cs.CL] [34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4 Doesn‚Äôt Know It‚Äôs Wrong: An Analysis of Iterative Prompting for Reasoning Problems. arXiv:2310.12397 [cs.AI] [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All You Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762 [36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha√´l Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R√©mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,  James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W√ºnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature 575 (2019), 350 ‚Äì 354. https://api. semanticscholar.org/CorpusID:204972004 [37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone. 2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing Systems (NeurIPS). [38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023). [39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models. arXiv:2401.10020 [cs.CL]  A ENVIRONMENT DESIGN Gameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a ùëä√ó ùêªgrid of rooms. The location of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set speed of movement, ùëÅùë°ùëüùëéùë£ùëíùëô. Crewmates use this time to complete ‚Äútasks‚Äù around the map. In the original game of Among Us, tasks involve completing minigames that require a player‚Äôs attention, such as solving a maze, preventing them from observing the environment around them. In our implementation, we simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified amount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task, which leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all tasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and partial observability that comes with it. Imposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is an action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an ‚Äúelimination cooldown‚Äù before eliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates. When a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion phase. Since the environment interfaces with agents through natural language, the environment constructs observation descriptions based on the surroundings of the agent. Each observation ùëúùëñ ùë°include the current timestamp of the environment, the current room that the agent is in, and the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a room looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here is an example of an observation for a crewmate at timestep 56: [56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2. Furthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of seconds remaining in the elimination cooldown. Following an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an example of the action set: [56] World: You can perform any of the following actions: go north; wait; do task; go south; wait [56] You: wait All agents are allowed to just ‚Äúwait‚Äù in a room until something changes in the environment, or ‚Äúgo‚Äù to an adjacent room, taking time to travel. If there is a corpse near an agent, they can ‚Äúreport body‚Äù and initiate the discussion phase. Crewmates can ‚Äúdo task‚Äù to do an uncompleted task in the room, while imposters can ‚Äúkill player [x]‚Äù to eliminate a specific player if they are not on the elimination cooldown. Note that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the ‚Äúwait‚Äù action. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format. Discussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought to a central room. The environment informs all players about the identity of the corpse and the player who reported the body. To determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player‚Äôs turn to speak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment chooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we allow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens used in the longest messages in the ‚Äúquick chat‚Äù setting of the original Among Us game. In practice, trained agents tend to use fewer than 20 tokens per message, instead ending their turn early using newline characters. After the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent who achieves the plurality of the votes gets ejected, except in the case of ties or when ‚Äúabstaining‚Äù wins, at which point nobody gets ejected. If all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues. Before the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe is the current imposter, and collects their probabilities based on the language model‚Äôs probabilities of voting out each agent. This has no impact on the game itself, but it is used for training. Reward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If all tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is ever greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of ‚àí1.  B BASE MODEL RESULTS The original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these models on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data. We present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so we use this model for ablation experiments. Our Base Model Original RWKV Model Size Accuracy ‚Üë PPLWorld ‚Üì PPLLam ‚Üì Accuracy ‚Üë PPLùëäùëúùëüùëôùëë‚Üì PPLùêøùëéùëö‚Üì 169 M 0.60 0.34 45.1 0.15 4.4 27.5 430 M 0.61 0.30 19.6 0.18 2.9 11.2 1.5 B 0.62 0.28 7.3 0.20 2.4 6.2 3 B 0.64 0.23 6.2 0.14 2.0 4.8 7 B 0.65 0.22 5.2 0.22 2.0 4.1 Table 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens in the environment, which correlates with a model‚Äôs ability to understand its environment. PPLLAM refers to the lambada evaluation perplexity. C SAMPLE GENERATIONS All samples below are from the perspective of the imposter: Sample 1: Through the discussion, Player Pink learns that Player Blue is the imposter: World (to all): Player Yellow discovered the dead body of Player Purple in room (0,0). Player Blue (you) saying: "There\'s no way I\'m playing this game." Player Yellow (to all): "Only the Imposter will speak." Player Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when Player Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted Player Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter Player Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates." Player Green (to all): "I notice suspicious activities of Player Blue and Player Purple." Player Pink (to all): "I notice suspicious activities of Player Blue." Player Blue was voted out. Crewmates win! Sample 2: Other players directly accuse Player Green: World (to all): Player Purple discovered the dead body of Player Yellow in room (2,0). Player Orange (to all): "The Imposter is Player Green." Player Green (you) saying: "Shut Up!" Player Purple (to all): "The Imposter is Player Green." Player Orange (to all): "You are the Imposter!" Player Green (you) saying: "My strategy is to kill or eject Imposters." Player Purple (to all): "Players can only report each other and identify the Imposter." Player Green was voted out. Crewmates win! Sample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange. World (to all): Player Purple discovered the dead body of Player Red in room (1,0). Player Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that Player Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don Player Purple (to all): "I believe the imposter in this room is Player Pink." Player Orange (to all): "I believe that Player Purple is the imposter in this Player Pink (you) saying: "I believe the imposter is Player Cubie." Player Purple (to all): "I believe Player Pink killed Player Orange." World (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected this round. World (to all): There are currently more imposters than crewmates. Imposters win!  Sample 4: Player Green says false information, but the imposter is still voted out. World (to all): Player Yellow discovered the dead body of Player Purple in room (2,0). Player Green (to all): "I don\'t know." Player Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body Player Red (you) saying: "I believe Player Green is the Imposter." Player Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter." Player Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1 Player Blue (to all): "I believe Player Red is the Imposter." Player Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more Player Yellow (to all): "I believe Player Red is the Imposter." Player Red was voted out. Crewmates win! Sample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group. World (to all): Player Yellow discovered the dead body of Player Blue in room (1,1). Player Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be Player Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red." Player Orange (you) saying: "The Player Red is the most suspicious Player" Player Red (to all): "I am Player Orange" Player Yellow (to all): "I vote Player Orange" Player Orange (you) saying: "I vote Player Yellow" Sample 6: Agents say evidence they observed in the environment. World (to all): Player Pink discovered the dead body of Player Green in room (1,0). Player Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates than 1 Player Purple (you) saying: "I think it is the Player Red in the room." Player Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1, Player Pink (to all): "I think Player Red is the Imposter." Player Purple (you) saying: "I think it is Player Red. Player Red (to all): "I think I see Player Purple leaving from room (0,0). World (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes. Therefore, nobody is ejected this round. D BROADER IMPACTS The primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human demonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to larger models with sufficient compute. Strong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot reasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural language would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams. There are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters make statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the ability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future researchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language models before deploying large-scale multi-agent learning systems that can interface with the real world. E HYPERPARAMETERS AND COMPUTE We use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don‚Äôt have a separate scheduler.  Table 2: Common hyperparameters hyperparameters value lr 3e-4 ùúÜBC 1.0 ùúÜWM 1.0 ùúÜNL 0.05 ùúÜL 0.3 ùúÜS 1.0 An exception to the above hyperparameters is that ùúÜL = 3.0 for ùúãRL+L+S and ùúÜL = 0.1 for ùúãRL+L because we find that it significantly impacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories in parallel during optimization. All experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute. F ASSETS AND LICENSES We borrow code from CleanRL‚Äôs PPO implementation [16], provided under the MIT license. We draw inspiration from Innersloth‚Äôs Among Us game, which gives permission to use the Among Us IP for non-commercial and educational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for illustrative purposes. All art assets in this paper were created using Processing, Matplotlib, and Keynote. This paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=18, thumbnail=None, content="1 Introduction In recent years, the rise of Large Language Mod- els (LLMs) has made significant advances in AI- assisted coding and reshaped the domain of code generation and problem-solving (Zhao et al., 2023). Code generation assistants built on GPT-4 (Ope- nAI, 2024), Mistral (Jiang et al., 2023a), and Llama *Work done when working as a remote RA at QCRI. (Dubey et al., 2024), inter alia, have demonstrated unprecedented ability to understand, generate, and manipulate code across various programming lan- guages and problem domains. However, despite these advancements, significant challenges persist in generating code for complex programming tasks. Current state-of-the-art approaches in code gen- eration typically employ a dual-pass process (Shi et al., 2024; Jin et al., 2024b; Zhong et al., 2024; Levin et al., 2024). In the first pass, they use LLMs to generate an initial, fully/partially cor- rect version of the program. Then accordingly in the second pass, they apply external tool-based it- erative debuggers that leverage runtime compiler feedback or other diagnostic tools to refine and cor- rect the generated code. While this approach has shown promise, it necessitates numerous iterations of LLM-tool interactions, and importantly its ef- fectiveness is heavily dependent on the quality of the initial code generation‚Äîa process that contin- ues to present substantial difficulties. Therefore, in this paper, we present CODESIM, a novel multi- agent code generation framework that seamlessly synthesizes complex code solutions without exter- nal resources, while offering potential for further enhancement through minimal external debugging. Synthesizing programs even in the first pass, however, is fundamentally challenging, requiring a deep understanding of natural language processing, computer algorithms, data structures, and problem- solving strategies. These challenges are further compounded when attempting to generate code for competitive programming problems or advanced software engineering tasks, where adherence to spe- cific constraints or passing unit tests are paramount (Khan et al., 2023). While earlier code generation methods em- ployed direct approaches (Chen et al., 2021a), chain-of-thought reasoning (Wei et al., 2022a), syn- thesized test-case guidance (Chen et al., 2022a), retrieval-augmented generation (Parvez et al., arXiv:2502.05664v1  [cs.CL]  8 Feb 2025  Problem Check if in given list of numbers, are any two numbers closer to each other than given threshold. Sample I/O Passed? No Yes Planning\xa0 Agent Plans Coding\xa0 Agent Code Debugged Code Debugging\xa0 Agent Try to debug\xa0 d times If all d debugging\xa0steps\xa0fails to pass sample I/O,\xa0loop\xa0back\xa0 to Planning Agent\xa0p times.\xa0 Plan Plan OK? Yes No Generate\xa0 Exemplar\xa0 &\xa0Plan\xa0 Simulate & Verify Plan\xa0 Revise Plan Generate Code Problem Plan Simulate on Failed I/O & Debug Testing Feedback Plan Code Problem CodeSim Pipeline Planning Agent Coding Agent Debugging Agent Figure 1: Overview of CODESIM: It consists of three agents‚Äîplanning, coding, and debugging. The Planning Agent first generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and refined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses potential bugs through step-wise simulation across d trials. The entire process iterates p times. 2021), and various in-context exemplar-based strategies (Shum et al., 2023; Zhang et al., 2022), recent paradigms have shifted toward plan-based (Jiang et al., 2023b), sampling or tree-searching (Zhou et al., 2023), self-retrieval (Yasunaga et al., 2023), and diverse agent-based approaches (Zhang et al., 2024; Qian et al., 2024; Shinn et al., 2023; Huang et al., 2023; Dong et al., 2023b). Most recently, MapCoder (Islam et al., 2024a) proposes a multi-agent framework that implements agents emulating different stages of program syn- thesis such as recalling relevant examples, design- ing/planning, code generation, and testing/debug- ging. While this approach mimics a real devel- oper‚Äôs code generation cycle and shows improve- ments, it focuses solely on expanding steps without verifying the underlying hypotheses, with tests be- ing performed only during the debugging phase. Consequently, the resulting gains are limited and it also requires larger number of iterations (i.e., LLM API calls). To address these limitations, CODESIM‚Äîbuilt upon planning, coding, and debugging agents‚Äîintroduces a novel verification approach inspired by human problem-solving. By simulating input/output step-by-step, CODESIM verifies both the generated plans and performs internal debugging, mirroring how humans understand, visualize, and refine algorithms. This simulation- driven planning and debugging process ensures that each step is thoroughly evaluated, significantly enhancing both solution quality and efficiency. Figure 1 shows an overview of our proposed ap- proach, CODESIM and in Figure 2, we demonstrate how simulation assists in both plan verification and debugging, highlighting its crucial role in improving problem-solving accuracy. We evaluate CODESIM on seven popular pro- gramming synthesis benchmarks, including foun- dational tasks like HumanEval and MBPP, as well as challenging competitive problem-solving bench- marks such as APPS, and CodeContest. Our ex- periments leverage multiple LLMs, including Chat- GPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix- tral, showcasing significant improvements in their program synthesis capabilities. CODESIM consis- tently achieves state-of-the-art performances, often surpassing strong baselines like MapCoder. Ad- ditionally, our findings suggest that CODESIM‚Äôs performance can be further improved when inte- grated with external debugging tools, such as LDB (Zhong et al., 2024), highlighting a promising di- rection for future research in hybrid code gener- ation and debugging systems. Through detailed ablation studies, we provide valuable insights into CODESIM‚Äôs functionality. We will open-source our framework to support future research in AI-assisted programming and problem-solving. 2 Related Work Code Generation: Program synthesis has been a fundamental challenge in AI for decades (Manna and Waldinger, 1971). Early attempts with smaller language models centered on code generation by fine-tuning neural networks (Wang et al., 2021; Ah- mad et al., 2021; Feng et al., 2020; Parvez et al., 2018; Hellendoorn and Devanbu, 2017; Rabinovich et al., 2017; Yin and Neubig, 2017; Hindle et al.,  LLM Loop from 2 to 3 Simulation of tri(3) For n = 2:\xa0since it's even, use the formula\xa0 `tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0 3, 2 Initialize: 3 For n = 3:\xa0\xa0since it's odd, use the recursive formula\xa0 \xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`. \xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case). \xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`. \xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`. \xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8 The list should contain the first `n + 1` numbers, so we need to include `tri(0)`. But\xa0`tri(0)` is not defined in the problem, but based on the example, it seems to be `1`. The simulation shows that the plan does not correctly handle the initialization and calculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0 The plan needs modification because it does not account for `tri(0)`. We need to start the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`. Plan Evaluation Tribonacci sequence is defined by the recurrence: \xa0 \xa0 tri(1) = 3 \xa0 \xa0 tri(n) = 1 + n / 2, if n is even. \xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd. Examples:\xa0tri(3) = [1, 3, 2, 8] Problem Initialize a List:\xa0Start with a list containing the base case: [3]. Iterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0 \xa0 \xa0 \xa0sequence Calculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list. Calculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance. Return the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence. Plan 0 end = max(10, 14) = 14 \xa0 \xa0- `10 % 2 == 0` (even, added to list) \xa0 \xa0- `11 % 2 != 0` (odd, not added) \xa0 \xa0- `12 % 2 == 0` (even, added to list) \xa0 \xa0- `13 % 2 != 0` (odd, not added) \xa0 \xa0- `14 % 2 == 0` (even, added to list) 10 The list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this. LLM Given two positive integers a and b, return the even digits between a\xa0and b, in ascending order. For example: \xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8] \xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8] \xa0 \xa0 generate_integers(10, 14) => [] Problem def generate_integers(a, b): \xa0 \xa0 start = min(a, b) \xa0 \xa0 end = max(a, b) \xa0 \xa0 even_numbers = [] \xa0 \xa0 for number in range(start, end + 1): \xa0 \xa0 \xa0 \xa0 if number % 2 == 0: \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number) \xa0 \xa0 return even_numbers Code Plan Validation Using Simulation Failed on test case:  generate_integers(10, 14) => [] Code\xa0 Execution LLM Simulation of\xa0generate_integers(10, 14) Loop over 10, 11, 12, 13, 14 start = min(10, 14) = 10 even_numbers =\xa0 10, 12 10, 12, 14 10, 12, 14 The code does not account for the requirement that the even digits themselves must be between `a` and `b`.\xa0 Debugging Note def generate_integers(a, b): \xa0 \xa0 start = min(a, b) \xa0 \xa0 end = max(a, b) \xa0 \xa0 even_numbers = [] \xa0 \xa0 for number in range(start, end + 1): \xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number \xa0 \xa0 \xa0 \xa0 for digit in str(number): \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end: \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit)) \xa0 \xa0 # Remove duplicates and sort the result \xa0 \xa0 return sorted(set(even_numbers)) Modified Code Code Execution Passed on Sample I/O Send it for final Evaluation Private Test \xa0Passed Debugging Using Simulation Figure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two different problems using CODESIM. 2016), while others explored leveraging data flow information or conversational intents to guide the process (Andreas et al., 2020; Yu et al., 2019). Var- ious prior approaches have also addressed code generation tasks using techniques such as data flow analysis and search-based methods (Li et al., 2022a; Parisotto and Salakhutdinov, 2017; Polozov and Gulwani, 2015; Gulwani, 2011). LLMs for Code: Various LLMs have been de- veloped for code synthesis (Austin et al., 2021; Chen et al., 2021b; Nijkamp et al., 2022; Fried et al., 2022; Allal et al., 2023; Li et al., 2022c). Re- cent open-source LLMs include the Llama family (Llama-2, CodeLlama, Llama3.1, etc.) (Roziere et al., 2023; Touvron et al., 2023), the Mistral family (Mistral, Mixtral, Codestral) (Jiang et al., 2023a), the Deepseek family (Deepseek Coder, Deepseek-V2, etc.) (Guo et al., 2024), MoTCoder (Li et al., 2023), and the Qwen family (Qwen 1.5, 2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which are capable of solving many basic problems. Prompting LLMs and Multi-Agent Code Gen- eration: LLM prompting can be summarized into three categories: retrieval (Yasunaga et al., 2023; Parvez et al., 2021, 2023), planning (Jiang et al., 2023b; Wei et al., 2022b), and debugging (Le et al., 2022; Chen et al., 2022b, 2023; Ridnik et al., 2024), in addition to direct code generation approaches. In contrast, our work combines all these paradigms and bridges their gaps (See Table 1). Recently, nu-  Approach Exemplars Plan Additional  test cases  generation Debug Simulation Reflexion ‚úó ‚úó ‚úî ‚úî ‚úó Self-planning ‚úó ‚úî ‚úó ‚úó ‚úó Analogical ‚úî ‚úî ‚úó ‚úó ‚úó LATS ‚úó ‚úî ‚úî ‚úî ‚úó MapCoder ‚úî ‚úî ‚úó ‚úî ‚úó CodeSim ‚úî ‚úî ‚úó ‚úî ‚úî Table 1: Comparison of code generation approaches. merous works have explored multi-agent code gen- eration and problem-solving, including (Kulesza et al., 2004; Jin et al., 2024a; Phan et al., 2024), as well as approaches highlighted in Section 1. However, CODESIM uniquely features simulation- driven planning and LLM-based debugging. More recently, external debugging has emerged to fur- ther boost performance, such as LDB (Zhong et al., 2024), ChatDebug (Levin et al., 2024), and MGDe- bugger (Shi et al., 2024), which serve as a second pass after our generation. 3 CODESIM Our goal is to develop a multi-agent code genera- tion approach capable of complex problem solving. Drawing inspiration from recent works like Map- Coder and ChatDev (in a different context), we de- vise the agents in CODESIM for planning, coding, and debugging. While these existing approaches fo- cus primarily on expanding steps without verifying underlying hypotheses, we address this limitation by introducing a novel verification approach. Our approach simulates input/output step-by-step, veri- fying generated plans and performing internal de- bugging, mirroring how humans understand, visu- alize, and refine in algorithm development. Below, we present our proposed model. 3.1 Planning Agent The first component of CODESIM is the Planning Agent. Given a problem description, the Planning Agent generates a single exemplar‚Äîa relevant prob- lem along with its plan and solution. This mimics the behavior of human programmers, who, when faced with a new problem, first recall a similar problem they‚Äôve previously solved. This exemplar- based recall is crucial as it provides a starting point for constructing a solution plan. Instead of gener- ating multiple ungrounded exemplars as in Map- Coder, our agent focuses on only one at a time. We then instruct the LLM to generate an appro- priate plan. Once the plan is created, the LLM simulates (step-by-step) the solution with a sample input. If the simulation result does not match the expected output, the agent prompts the LLM to re- vise the plan. Otherwise, the plan is deemed valid. In the case of failure, the Planning Agent refines the plan. The complete prompts for the Planning Agent‚Äîincluding plan generation, verification, and refinement‚Äîare provided in the Appendix (Figure 5, 6, 7). 3.2 Coding Agent Next component is the Coding Agent, which takes the problem description and the plan generated by the Planning Agent as input. The role of this agent is to translate the plan into executable code that solves the given problem. Once the code is generated, CODESIM evaluates it using sample in- put/output test cases. If the code passes all sample tests, it is returned as the final solution. Otherwise, the code is handed over to the next agent for further refinement. Figure 8 in the Appendix provides the complete prompt used by the Coding Agent. 3.3 Debugging Agent The final component, the Debugging Agent, re- ceives the original problem, the plan from the Planning Agent, the code generated by the Coding Agent, and the execution (unit testing) log as input to debug the code. To identify bugs, instead of di- rectly prompting the LLMs, we uniquely leverage the simulation once again. The LLM is instructed specifically to simulate the code on inputs where it fails to produce the expected output, allowing it to trace the execution step by step and locate the error. Once the bug is identified, the LLM mod- ifies the code to resolve the issue. The complete prompt for the Debugging Agent is shown in the Appendix (Figure 9). Unlike other approaches such as LATS (Zhou et al., 2023), AgentCoder (Huang et al., 2023), and Reflexion (Shinn et al., 2023), our Debugging Agent does not require additional test case generation. The rationale behind excluding this phase is discussed in the Ablation Study 6.8. 3.4 Adaptive Iteration CODESIM employs an adaptive iteration starting with the Planning Agent, which generates plans for the given problem. These plans are passed to the Coding Agent, which translates them into code and tests against sample I/Os. If all tests pass, the code is returned; otherwise, it‚Äôs sent to the Debugging Agent. The Debugging Agent attempts to fix the  Basic Programming Problems LLM Approach HumanEval HumanEval ET EvalPlus Avg HumanEval MBPP MBPP-ET Avg MBPP Avg ChatGPT Direct 71.3% 64.6% 67.1% 67.7% 75.8% 52.6% 64.2% 65.9% CoT 70.7% 63.4% 68.3% 67.5% 78.3% 55.7% 67.0% 67.2% Self-Planning 70.7% 61.0% 62.8% 64.8% 73.8% 51.1% 62.5% 63.6% Analogical 67.1% 59.1% 59.1% 61.8% 69.3% 46.9% 58.1% 59.9% Self-collaboration 74.4% 56.1% - 65.3% 68.2% 49.5% 58.9% 62.1% LATS 83.8% - - - - - - - MapCoder 80.5% 70.1% 71.3% 74.0% 78.3% 54.4% 66.4% 70.2% CodeSim 86.0% 72.0% 73.2% 77.1% 86.4% 59.7% 73.1% 75.1% GPT4 Direct 80.1% 73.8% 81.7% 78.5% 81.1% 54.7% 67.9% 73.2% CoT 89.0% 61.6% - 75.3% 82.4% 56.2% 69.3% 72.3% Self-Planning 85.4% 62.2% - 73.8% 75.8% 50.4% 63.1% 68.4% Analogical 66.5% 48.8% 62.2% 59.1% 58.4% 40.3% 49.4% 54.3% Reflexion 91.0% 78.7% 81.7% 83.8% 78.3% 51.9% 65.1% 74.4% LATS 92.7% - - - - - - - MapCoder 93.9% 82.9% 83.5% 86.8% 83.1% 57.7% 70.4% 78.6% CodeSim 94.5% 81.7% 84.8% 87.0% 89.7% 61.5% 75.6% 81.3% GPT4o Direct 90.2% 81.1% 82.3% 84.5% 81.1% 55.9% 68.5% 76.5% CoT 90.9% 82.3% 87.2% 86.8% 82.9% 57.9% 70.4% 78.6% Self-Planning 89.0% 80.5% 84.1% 84.5% 82.60% 56.4% 69.50% 77.0% Analogical 88.4% 80.5% 83.5% 84.1% 75.10% 50.9% 63.00% 73.6% Reflexion 87.2% 81.1% 81.1% 83.1% 81.1% 56.7% 68.9% 76.0% LATS 88.8% 81.2% - 85.0% - - - - MapCoder 90.2% 80.5% 81.7% 84.1% 88.7% 59.2% 74.0% 79.0% CodeSim 95.1% 86.0% 87.2% 89.4% 90.7% 61.2% 76.0% 82.7% Table 2: Pass@1 results for different approaches on basic programming tasks. code for up to d attempts. If unsuccessful after d attempts, the process returns to the Planning Agent, restarting the cycle. Once code passing all sample I/Os is obtained, the cycle ends, returning the code as the final output solution for evaluation against hidden test cases. This entire process repeats for a maximum of p cycles if needed. Algorithm 9 in the Appendix summarizes our adaptive agent traversal. The algorithm‚Äôs complexity is O(pd). Appendix 12 provides a comprehensive example of how CODESIM solves a problem. 4 Experimental Setup 4.1 Datasets Following MapCoder, we evaluate CODESIM on five basic programming benchmarks i.e., Hu- manEval (Chen et al., 2021a), HumanEval- ET (Dong et al., 2023a), EvalPlus (Liu et al., 2023), MBPP) (Austin et al., 2021), and MBPP- ET (Dong et al., 2023a) and two competitive pro- gramming datasets i.e., APPS (Hendrycks et al., 2021), and CodeContest (Li et al., 2022b). For fair comparison, we collect all the datasets from the repository of the MapCoder. 4.2 Baselines and Metric To evaluate CODESIM, we compare it against state-of-the-art code generation approaches, includ- ing MapCoder, as well as several notable meth- ods: Direct, Chain of Thought (CoT) (Wei et al., 2022b), Self-Planning (Jiang et al., 2023b), Ana- logical Reasoning (Yasunaga et al., 2023), and Self-collaboration (Dong et al., 2023b). For sim- pler programming tasks, we include strong base- lines such as Reflexion (Shinn et al., 2023) and LATS (Zhou et al., 2023). We exclude AgentCoder (Huang et al., 2023) due to reproducibility issues (discussed in Appendix 10). For fair comparison, our evaluation utilizes ChatGPT (gpt-3.5-turbo- 1106), GPT-4 (gpt-4-1106-preview) from OpenAI, alongside open-source LLMs such as Gemma2- 9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1- 70B. For basic programming tasks, we report next- generation performance with additional evaluations using GPT-4o (gpt-4o-2024-08-06). We adopt the  widely used pass@1 metric, where a model is deemed successful if its sole predicted solution is correct. 4.3 Reproducibility We aim to contribute to the NLP community by open-sourcing all of our code along with result logs, enabling others to reproduce our findings. For simple programming, we set the maximum number of planning tries to p = 5 and debugging tries to d = 5. For the competitive problem solving, we used p = 3 and d = 3 by default except for the CodeContest with GPT-4 where p = 3, d = 5. 5 Results 5.1 Basic Code Generation In Table 2, we evaluate the model performances on simple code generation tasks. Overall, CODESIM demonstrates consistently superior per- formance compared to all other baselines across all datasets and LLMs. Notably, CODESIM achieves top scores with GPT-4o, reaching 95.1% on Hu- manEval, 87.2% on EvalPlus, and 90.7% on MBPP, resulting in an impressive 82.7% overall average and their new state-of-the-art (SoTA) re- sults. This represents a significant improvement over the next best method, MapCoder, which scores 79.0% on average with GPT-4o. CODESIM‚Äôs effec- tiveness is consistent across different model vari- ants, outperforming other approaches with Chat- GPT (75.1% avg) and GPT-4 (81.3% avg) as well. The method‚Äôs robust performance across di- verse datasets, including the challenging MBPP- ET where it achieves 61.5% with GPT-4, under- scores its versatility in handling various program- ming tasks. These results strongly indicate that CODESIM‚Äôs simulation-driven planning and debug- ging approach marks a substantial advancement in code generation and problem-solving capabilities, as it consistently outperformed other baselines. 5.2 Competitive Problem Solving In Table 3, we evaluate performance on complex, contest-level code generation tasks. CODESIM de- livers significant improvements over other base- lines in solving complex contest-level code genera- tion tasks. With GPT-4, CODESIM reaches a strong 29.1% on CodeContests and 22.0% on APPS, marking a consistent edge over MapCoder‚Äôs 25.3% average. The performance gains are even more pro- nounced with ChatGPT, where CODESIM achieves a 16.4% on CodeContests, and 12.0% on APPS re- sulting 14.2% overall, outperforming MapCoder‚Äôs 12.0%. These results highlight CODESIM‚Äôs ability to handle the complexity of contest-level problems more effectively, especially through its simulation- driven approach. LLM Contest-Level Problems Approach CodeContest APPS Avg ChatGPT Direct 5.5% 8.0% 6.8% CoT 6.1% 7.3% 6.7% Self-Planning 6.1% 9.3% 7.7% Analogical 7.3% 6.7% 7.0% MapCoder 12.7% 11.3% 12.0% CodeSim 16.4% 12.0% 14.2% GPT4 Direct 12.1% 12.7% 12.4% CoT 5.5% 11.3% 8.4% Self-Planning 10.9% 14.7% 12.8% Analogical 10.9% 12.0% 11.5% MapCoder 28.5% 22.0% 25.3% CodeSim 29.1% 22.0% 25.6% Table 3: Pass@1 results for different approaches on CodeContest and APPS dataset. Open-Source LLMs LLM Approach HumanEval HumanEval ET EvalPlus Avg Gemma2-9B Direct 64.0% 56.1% 56.1% 58.7% CoT 31.7% 26.2% 27.4% 28.4% Reflexion 62.2% 56.7% 55.5% 58.1% CodeSim 82.9% 72.0% 72.6% 75.8% Mixtral8x7B Direct 20.7% 18.9% 18.9% 19.5% CoT 46.3% 42.1% 39.0% 42.5% Reflexion 34.1% 32.9% 29.9% 32.3% CodeSim 75.0% 61.6% 61.0% 65.9% LLaMa3.1-8B Direct 42.1% 38.4% 39.0% 39.8% CoT 48.8% 42.1% 43.3% 44.7% Reflexion 43.9% 31.1% 29.9% 35.0% CodeSim 79.9% 65.2% 61.2% 68.8% LLaMa3.1-70B Direct 57.3% 50.6% 52.4% 53.4% CoT 75.6% 67.7% 70.1% 71.1% Reflexion 73.8% 64.0% 68.3% 68.7% CodeSim 90.2% 73.8% 76.2% 80.1% Table 4: Pass@1 results for different approaches using Open-source LLMs. 5.3 Performance Across Open-source LLMs To further demonstrate CODESIM‚Äôs generaliza- tion capability, we evaluate its performance with open-source LLMs, including Gemma2-9B, Mix- tral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As shown in Table 4, CODESIM consistently outper- forms all other methods across these models. On LLaMa3.1-70B, CODESIM achieves an accuracy  of 90.2% on HumanEval and 76.2% on EvalPlus, with an average of 80.1%, closely matching GPT- 4o‚Äôs performance. Due to the complex prompting scheme of MapCoder, open-source LLMs often struggle to generate output in the correct format. Therefore, we exclude MapCoder from this experi- ment. On the other hand, Reflexion shows minimal improvement in accuracy. These results highlight CODESIM‚Äôs strong generalization ability across various LLM architectures, even on smaller mod- els like Gemma2-9B that achieves a notable avg accuracy of 75.8%. 6 Ablation Studies and Analyses 6.1 Impact of Different Agents Our primary contributions are two folds: (i) the simulation-guided plan verification step within the Planning Agent and (ii) the bug fixing process through simulation in Debugging Agent. To evalu- ate the significance of these components, we ablate these two parts of our approach and present the results in Table 5. The findings confirm that both components contribute significantly. Simulation  Driven  Planning Debugging  using  Simulation Pass@1 Performance  Drop ‚úó ‚úó 92.1% 3.2% ‚úî ‚úó 93.3% 1.9% ‚úó ‚úî 93.3% 1.9% ‚úî ‚úî 95.1% - Table 5: Pass@1 results for different versions of CODESIM (by using GPT4o on HumanEval dataset). 6.2 Fine-grained Analysis of the Impact of Simulation Table 6 presents the impact of incorporating Simulation in CODESIM. The results show that CODESIM consistently outperforms other ap- proaches across both simple and multi-agent set- tings, demonstrating superior performance with both open-source and proprietary LLMs. This high- lights the effectiveness of Simulation in enhancing problem-solving efficiency within our pipeline. 6.3 Impact of Varying Programming Languages To evaluate the performance of CODESIM across various programming languages, we utilized the LLM Method Approach HumanEval  (Pass@1) Impact of using  Simulation GPT4o Simpler Direct 90.2% 5.4% CoT 90.9% 4.6% Self-Planning 89.0% 6.9% Analogical 88.4% 7.6% Reflexion 91.0% 4.5% LATS 88.8% 7.1% Multi-Agent MapCoder 90.2% 5.4% CodeSim 95.1% - LLaMa3.1-70B Simpler Direct 57.3% 57.4% CoT 75.6% 19.3% Reflexion 73.8% 22.2% Multi-Agent CodeSim 90.2% - Table 6: Impact of using Simulation. xCodeEval (Khan et al., 2023) dataset. The ex- perimental results, presented in Table 7, demon- strate that CODESIM maintains strong performance across different programming languages, highlight- ing its versatility and effectiveness. Dataset Language Direct MapCoder CodeSim xCodeEval Python 17.9% 27.4% 27.4% C 9.4% 21.7% 24.5% Rust 12.3% 21.7% 23.6% Table 7: Pass@1 results for different programming lan- guages from xCodeEval dataset by using ChatGPT. 6.4 Use of External Debugger LLM LDB Reflexion MapCoder CodeSim ChatGPT without 88.0% 90.2% 95.1% with 89.6% 92.1% 96.3% GPT-4o without 88.0% 90.2% 95.1% with 94.5% 91.5% 97.6% Table 8: Pass@1 results for different approaches using an external debugger. The performance of CODESIM can be further en- hanced by incorporating an external debugger in the second pass. We experiment with LDB as the external debugger on HumanEval dataset in Table 8. We use the output code from the most competitive first-pass generation methods, includ- ing CODESIM, Reflexion, and MapCoder, using GPT-4o as the backbone. These seed programs are then passed to LDB, which was tested with two different LLMs: ChatGPT and GPT-4o. As can be seen, CODESIM achieves 95.1% accuracy in  the first pass with GPT-4o, surpassing Reflexion‚Äôs second pass performance of 94.5%. By utilizing LDB with GPT-4o, CODESIM achieves a second pass accuracy of 97.6%, setting a new state-of-the- art result for a dual-pass approach. In addition, we note that the second pass with LDB consumes 39K more tokens in Reflexion compared to our approach, highlighting the efficiency of CODESIM. 6.5 Qualitative Example We also conduct a qualitative analysis to better understand how CODESIM improves performance across various datasets. Figure 2 demonstrates how CODESIM enhances the plan through simulation and assists in debugging the code using the same technique. A complete example, including LLM output, is provided in Appendix 12. 6.6 Impact of p and d CODESIM includes two key hyperparameters: the maximum number of planning steps (p) and the maximum number of debugging steps (d). By vary- ing these parameters, we plot the results in Figure 3, which shows a proportionate improvement in performance. It is important to note that higher val- ues of p and d lead to more API calls and increased token consumption, allowing users to adjust these parameters to balance between accuracy and cost. Figure 3: Pass@1 results by varying maximum number of planning, p and maximum number of debugging, d. 6.7 Impact of Number of Sample I/Os The HumanEval dataset has an average of only 2.82 sample I/Os per example, which is a relatively small number for deriving meaningful insights. In this ablation, we augment the dataset by adding 5 more sample I/Os from the HumanEval-ET dataset. This augmentation increases performance notably, leading to 89% accuracy with ChatGPT, a 3.5% improvement over previous results, 86%. 6.8 Impact of Synthesizing Additional I/O Increasing the number of sample I/Os for testing can enhance the overall performance of our ap- proach, as indicated in 6.7. Based on this insight, we use a self-consistency (Wang et al., 2023a) method to generate additional test cases. We in- struct the LLM to generate five more test cases for each problem, covering both basic and edge cases. The LLM is called twice, and we select the test cases that are present in both responses. How- ever, this approach results in a performance decline. With ChatGPT we achieve 78% accuracy‚Äîa 9.3% decrease from the original 86%. This indicates that generating additional I/Os is a non-trivial task that may negatively impact final outcomes. 6.9 API Call and Token Analysis We compare the API calls and token consumption of our approach with the previous state-of-the-art method, MapCoder (Islam et al., 2024a), as shown in Table 9. The results reveal that CODESIM not only improves performance but also reduces token consumption. On average, CODESIM uses 4.13 thousand fewer tokens while achieving a 7.1% in- crease in accuracy, proving that CODESIM is more efficient in both accuracy and token usage com- pared to MapCoder. 6.10 Error Analysis and Challenges Although CODESIM demonstrates strong performance compared to other methods, it faces challenges in specific algorithmic domains. The APPS dataset (Hendrycks et al., 2021) includes problems with three levels of difficulty: (i) Introductory, (ii) Interview, and (iii) Competition. Figure 4 illustrates the performance of different approaches based on difficulty level. The results indicate that for introductory and interview-level problems, CODESIM does not surpass MapCoder when using ChatGPT. Additionally, when using GPT-4, CODESIM struggles to outperform MapCoder on interview-level problems. Upon manual review, we observe that for more complex issues, such as dynamic programming (DP), CODESIM encounters difficulties in constructing the DP table.  LLM Dataset Average API Calls ‚Üì Average Token Consumption (K) ‚Üì Token Reduction over  MapCoder (k) ‚Üë Acc Gain over  MapCoder ‚Üë MapCoder CodeSim MapCoder CodeSim ChatGPT HumanEval 17 7 10.41 5.48 4.93 6.8% MBPP 12 6 4.84 4.24 0.60 10.3% APPS 21 15 26.57 19.20 7.37 6.2% CodeContest 23 16 34.95 24.02 10.92 29.1% GPT4 HumanEval 15 5 12.75 5.15 7.60 0.6% MBPP 8 5 4.96 5.21 -0.26 7.9% APPS 19 13 31.80 23.18 8.61 0.0% CodeContest 19 17 38.70 41.66 -2.95 2.1% GPT4o HumanEval 9 4 6.63 3.84 2.79 5.4% MBPP 9 5 6.10 4.43 1.67 2.3% Average 15.2 9.3 17.77 13.64 4.13 7.1% Table 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens used (in thousands). Here the upward symbol (‚Üë) refers that the higher value is better and opposite meaning for downward symbol (‚Üì). Figure 4: Performance of different approaches across different difficulty levels on the APPS dataset. 7 Conclusion and Future Work In this paper, we introduce CODESIM, a novel framework that leverages the multi-agent prompting capabilities of LLMs for efficient code generation in problem-solving tasks. CODESIM integrates three agents‚Äîplanning, coding, and debugging‚Äîto effectively solve programming problems. It harnesses the power of simulation for plan verification and debugging, significantly outperforming existing state-of-the-art approaches by a wide margin. Future work will focus on extending this approach to other domains such as mathematical reasoning and question answering broadening its scope and impact. 8 Limitations In Section 6.4, we observe that utilizing an exter- nal debugger can further enhance our results. Our next research goal is to achieve the best perfor- mance without relying on any external tools. Al- though we have reduced token consumption com- pared to the previous state-of-the-art method, Map- Coder, it still remains high compared to the di- rect prompting approach. Direct prompting con- sumes an average of 560 tokens, while our method consumes around 13,640 tokens. This indicates room for enhancement in efficiency. While in this work, we generate the exemplars with the LLMs themselves, in general they are found from exter- nal resource (Parvez and Chang, 2021). Although this has its own challenges such as noisy retrievals (Wang et al., 2023b), inconsistent generations (Is- lam et al., 2024b; Parvez, 2024; Sadat et al., 2023) this direction could also be a possible improvement. Another limitation is the use of external tools for assistance during simulation. We have not explored this avenue in the current research, leaving it for future work. Additionally, more sample I/Os could potentially improve performance, and our future research will focus on investigating methods for generating accurate additional I/Os. Moreover, we would like to note that in this work, we focus solely on generated code‚Äôs correctness and did not study its optimizations such as test-time, memory. Fi- nally, it is advisable to run the machine generated code inside a sandbox to avoid any potential risks."),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=18, thumbnail=None, content='1. Introduction Due to their impressive performance, there is a strong push to deploy deep learning models, par- ticularly large language models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; G Team et al., 2024) in a large number of scenarios. Due to auto- regressive nature of LLMs, decode latency tends to dominate inference cost. Decode latency itself is dominated by communication cost of transfer- ring model weights from high-bandwidth mem- ory (HBM) to the SRAM or due to transferring weights/activations in a distributed cluster. Quantizing weights and/or activations can sig- nificantly reduce the overall communication load and is, therefore, one of the most popular tech- niques for reducing inference costs (Dettmers et al., 2022). While floating-point representations are standard for training, integer data types such as int8, int4, and int2 are appealing alternatives for inference. However, current methods for quan- tizing to these varying integer precisions typically treat each target precision as an independent op- timization problem, leading to a collection of dis- tinct models rather than a single, versatile one. Furthermore, quantizing to extremely low preci- sions like int2 is known to be highly inaccurate. In this work, we pose the question of whether both of the above challenges can be addressed; that is, can we train a single model from which we can extract multiple accurate lower-precision models? We answer this question in the affir- mative by introducing Matryoshka Quantization (MatQuant), a novel multi-scale training method that leverages the inherent nested (Matryoshka) structure (Kusupati et al., 2022) within integer data types (Figure 1a). Specifically, slicing the top bits of an int8-quantized weight can directly yield an int4 or int2 model. Existing quantiza- tion techniques often neglect this structure, which limits the potential for multi-scale adaptable mod- els operating at various bit-widths with optimal performance. Instead, MatQuant simultaneously optimizes model weights across multiple precision levels (e.g., int8, int4, int2). At a high level, we repre- sent each model parameter at different precision levels using shared most significant bits (MSBs), and then jointly optimize the loss for each pre- cision level. This allows us to develop a single quantized model that can effectively operate at any of the chosen bit-widths, offering a spectrum of accuracy-versus-cost options. MatQuant is a general-purpose technique, applicable to most 1 arXiv:2502.06786v1  [cs.LG]  10 Feb 2025  Matryoshka Quantization 11 01 1001  (a) 2 4 6 8 Effective bits per FFN parameter 40 50 60 70 Task Average Gemma-2 9B MatQuant MatQuant-Interp. Baseline MinMax Sliced int8 (b) (c) Figure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka structure of int8 ‚Üíint4 ‚Üíint2. (b) Empirical gains of MatQuant on downstream tasks, especially > 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution as a consequence of MatQuant‚Äôs training mechanism that maximises accuracies across all precisions. learning-based quantization methods, such as Quantization Aware Training (QAT) (Jacob et al., 2018) and OmniQuant (Shao et al., 2023). We demonstrate the efficacy of MatQuant when applied to quantizing the Feed-Forward Network (FFN) parameters of standard LLMs (Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al., 2017) ‚Äì typically, FFN is the main latency block hence the focus on improving the most signifi- cant component‚Äôs latency. Our results show that MatQuant produces int8 and int4 models with comparable accuracy to independently trained baselines, despite the benefit of shared model pa- rameters. Critically, the int2 models generated by MatQuant significantly outperform their in- dividually trained counterparts, with 8% higher accuracy on downstream tasks (Figure 1b). We also extend MatQuant to quantize all weights of a Transformer layer. Finally, we find that quantiz- ing with MatQuant shifts the quantized weight distribution toward higher values, contributing to improved int2 performance (Figure1c). Beyond improving chosen precision perfor- mance, MatQuant allows for seamless extraction of interpolative bit-widths, such as int6 and int3. MatQuant also admits a dense accuracy-vs-cost pareto-optimal trade-off by enabling layer-wise Mix‚Äôn‚ÄôMatch of different precisions. This ensures deployment of say an effective int3 sized model even if the underlying hardware only supports int4 and int2. Overall, MatQuant and its vari- ants present a significant step toward develop- ing multi-scale models with high flexibility and performance, pushing the boundaries of low-bit quantization for efficient LL')]}
