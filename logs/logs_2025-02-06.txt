2025-02-06 00:13:04,078 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:14:53,377 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:17:59,856 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:18:32,771 - INFO - Papers: {'2025-02-03': [Paper(arxiv_id='2501.14677', authors=['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], published_at=datetime.datetime(2025, 2, 3, 13, 15, 59, 743000, tzinfo=datetime.timezone.utc), title='MatAnyone: Stable Video Matting with Consistent Memory Propagation', summary='Auxiliary-free human video matting methods, which rely solely on input\nframes, often struggle with complex or ambiguous backgrounds. To address this,\nwe propose MatAnyone, a robust framework tailored for target-assigned video\nmatting. Specifically, building on a memory-based paradigm, we introduce a\nconsistent memory propagation module via region-adaptive memory fusion, which\nadaptively integrates memory from the previous frame. This ensures semantic\nstability in core regions while preserving fine-grained details along object\nboundaries. For robust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a novel training\nstrategy that efficiently leverages large-scale segmentation data, boosting\nmatting stability. With this new network design, dataset, and training\nstrategy, MatAnyone delivers robust and accurate video matting results in\ndiverse real-world scenarios, outperforming existing methods.', upvotes=25, thumbnail=None, content='Auxiliary-free human video matting methods, which rely\nsolely on input frames, often struggle with complex or am-\nbiguous backgrounds. To address this, we propose MatAny-\none,\na robust framework tailored for target-assigned\nvideo matting. Specifically, building on a memory-based\nparadigm, we introduce a consistent memory propagation\nmodule via region-adaptive memory fusion, which adap-\ntively integrates memory from the previous frame.\nThis\nensures semantic stability in core regions while preserv-\ning fine-grained details along object boundaries. For ro-\nbust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a\nnovel training strategy that efficiently leverages large-scale\nsegmentation data, boosting matting stability. With this new\n1\narXiv:2501.14677v1  [cs.CV]  24 Jan 2025\n\nnetwork design, dataset, and training strategy, MatAnyone\ndelivers robust and accurate video matting results in diverse\nreal-world scenarios, outperforming existing methods.\n1. Introduction\nAuxiliary-free human video matting (VM) is widely recog-\nnized for its convenience [24, 27, 33], as it only requires\ninput frames without additional annotations. However, its\nperformance often deteriorates in complex or ambiguous\nbackgrounds, especially when similar objects, i.e., other hu-\nmans, appear in the background (Fig. 2(b)). We consider\nauxiliary-free video matting to be under-defined, as their\nresults can be uncertain without a clear target object.\nIn this work, we focus on a problem that is more appli-\ncable to real-world video applications: video matting fo-\ncused on pre-assigned target object(s), with the target seg-\nmentation mask provided in the first frame. This enables\nthe model to perform stable matting via consistent object\ntracking throughout the entire video, while offering bet-\nter interactivity. The setting is well-studied in Video Ob-\nject Segmentation (VOS), where it is referred to as “semi-\nsupervised” [10, 19, 38].\nA common strategy is to use\na memory-based paradigm [8, 12, 38, 51], encoding past\nframes and corresponding segmentation results into mem-\nory, from which a new frame retrieves relevant information\nfor its mask prediction. This allows a lightweight network\nto achieve consistent and accurate tracking of the target ob-\nject. Inspired by this, we adapt the memory-based paradigm\nfor video matting, leveraging its stability across frames.\nVideo matting poses additional challenges compared to\nVOS, as it requires not only accurate semantic detection in\ncore regions but also high-quality detail extraction along the\nboundary (e.g., hair), as defined in Fig. 2(a). A straightfor-\nward approach is to fine-tune matting details using matting\ndata, based on segmentation priors from VOS. Recent ap-\nproaches attempt to achieve both goals, either in a coupled\nor decoupled manner. For instance, AdaM [31] and FTP-\nVM [21] refine the memory-based segmentation mask for\neach frame via a decoder to produce alpha mattes, while\nMaGGIe [22] devises a separate refiner network to process\nsegmentation masks across all frames from an off-the-shelf\nVOS model. However, these methods often lead to subop-\ntimal results due to limitations in the available video mat-\nting data: (i) the quality of VideoMatte240K [32], the most\nwidely used human video matting dataset, is suboptimal.\nIts ground-truth alpha mattes exhibit problematic semantic\naccuracy in core areas (e.g., interior holes) and lack fine de-\ntails along the boundaries (e.g., blurry hair); (ii) video mat-\nting datasets are much smaller in scale compared to VOS\ndatasets; and (iii) video matting data are synthetic due to the\nextreme difficulty of human annotations, limiting their gen-\neralizability to real-world cases [33]. Consequently, fine-\ntuning a strong VOS prior for video matting with existing\nvideo matting data usually disrupts this prior. While bound-\nary details may show improvement compared to segmenta-\ntion results, the matting quality in terms of semantic stabil-\nity in core areas and details in boundary areas remain unsat-\nisfactory, as shown by the results of MaGGIe in Fig. 2(b).\nProducing matting-level details while maintaining se-\nmantic stability of a memory-based approach is challeng-\ning, especially training with suboptimal video matting data.\nTo tackle this, we focus on several key aspects:\nNetwork - we introduce a consistent memory propagation\nmechanism in the memory space. For each current frame,\nthe alpha value change relative to the previous frame is esti-\nmated for every token. This estimation guides the adaptive\nintegration of information from the previous frame. The\n“large-change” regions rely more on the current frame’s in-\nformation queried from the memory bank, while “small-\nchange” regions tend to retain the memory from the previ-\nous frame. This region-adaptive memory fusion inherently\nstabilizes memory propagation throughout the video, im-\nproving matting quality with fine details and temporal con-\nsistency. Specifically, it encourages the network to focus\non boundary regions during training to capture fine details,\nwhile “small-change” tokens in the core regions preserve\ninternally complete foreground and clean background (see\nour results in Fig. 2(b)).\nData - we collect a new training dataset, named VM800,\nwhich is twice as large, more diverse, and of higher quality\nin both core and boundary regions compared to the Video-\nMatte240K dataset [32], greatly enhancing robust train-\ning for video matting. In addition, we introduce a more\nchallenging test dataset, named YoutubeMatte, featuring\nmore diverse foreground videos and improved detail qual-\nity. These new datasets offer a solid foundation for robust\ntraining and reliable evaluation in video matting.\nTraining Strategy - the lack of real video matting data re-\nmains a significant limitation, affecting both stability and\ngeneralizability.\nWe address this problem by leveraging\nlarge-scale real segmentation data via a novel training strat-\negy. Unlike common practices [21, 22, 33] that train with\nsegmentation data on a separate prediction head parallel\nto the matting head, we propose using segmentation data\nwithin the same head as matting for more effective supervi-\nsion. This is achieved by applying region-specific losses –\nfor core regions, we apply a pixel-wise loss to ensure stabil-\nity and generalization in semantics; for boundary regions,\nwhere segmentation data lacks alpha labels, we employ an\nimproved DDC loss [35], scaled to make edges resemble\nmatting rather than segmentation.\nIn summary, our main contributions are as follows:\n• We propose MatAnyone, a practical human video mat-\nting framework supporting target assignment, with sta-\nble performance in both semantics of core regions and\nfine-grained boundary details.\nTarget object(s) can be\neasily assigned using off-the-shelf segmentation methods,\nand reliable tracking is achieved even in long videos with\n2\n\nOurs\nRVM\nMaGGIe\n(a) Definitions for Matting\nCore Areas\nInput\nBoundary Area\n(b) Issues:               Segmentation prior broken                 Confused by ambiguous background\nMaGGIe\nRVM \nFigure 2. Definitions and motivations for MatAnyone. (a) In a matting frame, the image can be broadly divided into two areas based on\nthe alpha value: the core (semantic) and the boundary (fine-details). The core includes the background (alpha values of 0) and the solid\nforeground (alpha values of 1), while the boundary (highlighted in pink) encompasses areas with alpha values between 0 and 1. (b) Due to\nthe under-defined setting, auxiliary-free methods like RVM [33] are easily confused by ambiguous background. Meanwhile, mask-guided\nmethods like MaGGIe [22] tend to break the segmentation prior they aim to leverage, due to the deficiency in video matting data.\ncomplex and ambiguous backgrounds.\n• We introduce a consistent memory propagation mecha-\nnism via region-adaptive memory fusion, improving sta-\nbility in core regions and quality in boundary details.\n• We contribute larger and higher-quality datasets for train-\ning and testing, offering a solid foundation for robust\ntraining and reliable evaluation in video matting.\n• To overcome the scarcity of real video matting data, we\nleverage real segmentation data for core-area supervision,\nlargely improving semantic stability over prior methods.\n2. Related Work\nVideo Matting.\nDue to the intrinsic ambiguity in the\nauxiliary-free setting [24, 27, 33, 39, 57, 61], such tasks\ngenerally are object-specific. Among them, human video\nmatting [24, 27, 43, 61] without auxiliary inputs is popular\ndue to its wide applications. Challenging as the auxiliary-\nfree setting, being in the video domain brings in additional\ndifficulties in temporal coherency. MODNet [24] extends\nits portrait matting setting to video domain with a flicker-\ning reduction trick (non-learning) within a local sequence.\nRVM [33] steps further to design for videos specifically\nwith ConvGRU [1] as its recurrent architecture.\nRobust\nas RVM, it is still easy to be confused by humans in the\nbackground.\nWith the success of promptable segmenta-\ntion [25, 40, 58, 62], obtaining segmentation mask for a\ntarget human object only requires minimal human efforts.\nRecent mask-guided image [3, 29, 55, 56] and video mat-\nting [21, 22, 28, 31] thus leverages this convenience for\na more robust performance.\nAdam [31] propagates the\nfirst-frame segmentation mask across all frames while FTP-\nVM [21] propagates the first-frame trimap. Taking the prop-\nagated mask as a rough result, their decoder serves for mat-\nting details refinement.\nMaGGIe [22] enjoys a stronger\nprior by taking the segmentation mask across all frames in-\nstead of the first one. Taking all the segmentation masks at a\ntime, the network is able to perform bidirectional temporal\nfusion for coherency. To mitigate the poor generalizabil-\nity of synthetic video matting data, a common practice is to\nsimultaneously train with real segmentation data for seman-\ntics supervision [21, 31, 33].\nMemory-based VOS. Semi-supervised VOS segments the\ntarget object with a first-frame annotation across frames [8–\n12, 18, 30, 37, 42].\nThe memory matching paradigm\nby Space-Time Correspondence Network (STCN) [10] is\nwidely followed by current VOS methods [8, 12, 46, 51],\nand achieves good performance. We thus take the memory-\nbased paradigm as our framework since it is similar to our\nsetting except that our outputs are alpha mattes.\nVideo Consistency in Low-level Vision.\nTo enhance\ntemporal consistency across adjacent frames, the recur-\nrent frame fusion [47, 59] and optical flow-guided prop-\nagation [4–6] are commonly utilized in the video restora-\ntion networks. Recent methods also employ temporal lay-\ners such as 3D convolution [2, 48] and temporal atten-\ntion [2, 7, 49, 60] during training, while other training-free\nmethods resorts to cross-frame attention [50, 53] and flow-\nguided attention [13, 15] in the pretrained models. In this\nwork, we find that the memory-based paradigm is effective\nenough to maintain video consistency for video matting.\n3. Methodology\nOverview. Achieving matting-level details while preserv-\ning the semantic stability of a memory-based approach\nposes challenges, especially when training with suboptimal\nvideo matting data. To tackle this, we propose our MatAny-\none, as illustrated in Fig. 3.\nSimilar to semi-supervised\nVOS, MatAnyone only requires the segmentation mask for\nthe first frame as a target assignment (e.g., the yellow mask\nin Fig. 3(a)). The alpha matte for the assigned object is then\ngenerated frame by frame in a sequential manner. Specif-\nically, for an incoming frame t, it is first encoded into F t\nas ×16 downsampled feature representation, which is then\ntransformed into key and query for consistent memory prop-\nagation (Sec. 3.1), and output the pixel memory readout P t.\nWe employ the object transformer proposed by Cutie [12]\nto group the pixel memory by object-level semantics for ro-\nbustness against noise brought by low-level pixel matching.\n3\n\n(a) Overall Framework\n(c) Training Strategy\n(b) Consistent Memory Propagation\nEncoder\nObject \nTransformer\nConsistent\nMemory Propagation\n!!\n"!\n#!\n#0\n#t\n#N\n…\n…\n$!\nDecoder\nValue\nEncoder\nUpdate Alpha Memory\n%!\nMatting\nData\nsynthetic\nsmall scale\nw/ matting details\nSegment.\nData\nreal\nlarge scale\nw/o matting details\nSegmentation Data (w/o GT alpha matte)\nMatting Data (w/ GT alpha matte)\nMatting Loss\nOutput\nGT\nGT\nOutput\nUncertain Loss\nMatAnyone\nCertain Loss\n&"\n\'#, )#\n*"\nAttention\n+"\nkey\nvalue\nkey\nquery\nCurrent Frame\nLast Frame Memory\n\'"\n\'"$%\n)"$%\nUpdate Alpha Memory !! (every frame)\nUpdate Alpha Memory !! (every r-th frame)\n)"#\n,"\nkey\nvalue\nAlpha Memory Bank\nUncertainty \nPrediction\nMatAnyone\nFigure 3. An overview of MatAnyone. MatAnyone is a memory-based framework for video matting. Given a target segmentation map\nin the first frame, our model achieves stable and high-quality matting through consistent memory propagation, with a region-adaptive\nmemory fusion module to combine information from the previous and current frame. To overcome the scarcity of real video matting data,\nwe incorporate a new training strategy that effectively leverages matting data for fine-grained matting details and segmentation data for\nsemantic stability, with designed losses separately.\nThe refined memory readout Ot acts as the final feature to\nbe sent into the decoder for alpha matte prediction. The pre-\ndicted alpha matte M t is then encoded to memory value V t,\nwhich is used to update the alpha memory bank.\nDue to limitations in the quality and quantity of video\nmatting data, training with such data makes it difficult to\nachieve satisfactory stability in core regions. To mitigate\nthis, RVM [33] proposes a parallel head for real segmenta-\ntion data alongside the matting head, guiding the network\nto be robust in real-world cases. However, this is not suffi-\ncient, as the matting head itself cannot receive supervision\nfrom real data. Inspired by the DDC loss [35] designed for\nalpha-free image matting, we devise a training strategy for\ncore regions, which provides direct supervision to the mat-\nting head with segmentation data (Sec. 3.2), leading to sub-\nstantial improvements in semantic stability.\nWe also propose two practical inference strategies that\nallow for flexible application, 1) a recurrent refinement ap-\nproach based on the memory-driven paradigm, and 2) an\nauxiliary-free variant that eliminates the need for a target\nsegmentation mask in the first frame (Sec. 3.3).\n3.1. Consistent Memory Propagation\nAlpha Memory Bank. In this study, we introduce a con-\nsistent memory propagation (CMP) module specifically de-\nsigned for video matting, as illustrated in Fig.3(b). Exist-\ning memory-based VM methods store either segmentation\nmasks [31] or trimaps [21] in memory and use a decoder\nto refine the matting details. Such approaches do not fully\nleverage the stability provided by the memory paradigm\nin boundary regions, leading to instability such as flicker-\ning. To address this, building on the memory-based frame-\nwork [10], our MatAnyone stores the alpha matte in an al-\npha memory bank to enhance stability in boundary regions.\nRegion-Adaptive Memory Fusion. Given the inherent dif-\nference between the segmentation map (values of 0 or 1)\nand the matting map (values between 0 and 1), the memory-\nmatching approach needs to be adjusted. Specifically, in\nSTCN [10], memory values for the query frame are based\non the similarity between query and memory key, assum-\ning equal importance for all query tokens. However, this\nassumption does not hold for video matting. As shown in\nFig. 2(a), a query frame can be divided into core and bound-\nary regions. When compared with frame t −1, only a small\nfraction of tokens in frame t change significantly in alpha\nvalues, with these “large-change” tokens mainly located in\nobject boundaries, while the “small-change” tokens reside\nin the core regions. This highlights the need to treat core\nand boundary regions separately to enforce stability.\nSpecifically, we introduce a boundary-area prediction\nmodule to estimate the change probability Ut of each query\ntoken for adaptive memory fusion, where higher Ut indi-\ncates “large-change” regions and lower Ut indicates “small-\nchange” regions.\nThe prediction module is lightweight,\n4\n\nconsisting of three convolution layers. We formulate the\nprediction as a binary segmentation problem with loss\nLbin seg and use the actual alpha change between frame\nt −1 and t as supervision. Specifically, we define U GT\nt\n:\n|M GT\nt−1 −M GT\nt\n| >= δ, where δ is a threshold. Using the\noutput of the module ˆUt, we compute the binary cross en-\ntropy loss against U GT\nt\n. During the region-adaptive mem-\nory fusion process, we apply the sigmoid function on ˆUt to\ntransform it as a probability. The final pixel memory read-\nout is a soft merge:\nPt = V m\nt\n∗Ut + Vt−1 ∗(1 −Ut),\n(1)\nwhere Ut ∈[0, 1], V m\nt\nare current values queried from\nmemory bank, and Vt−1 are values propagated from the\nlast frame. This approach significantly improves stability\nin core regions by maintaining internal completeness and a\nclean background (Fig. 2(b) and Fig. 4). It also enhances\nstability in boundary regions, as it directs the network to fo-\ncus on object boundaries with soft alpha values, while the\nmemory-based paradigm inherently stabilizes the matched\nvalues (see Table 3(c)). A detailed analysis is provided in\nthe ablation study of Sec. 5.2 and Sec. J.2.\n3.2. Core-area Supervision via Segmentation\nNew Training Scheme. Most recent video matting meth-\nods follow RVM’s approach of using real segmentation data\nto address the limitations of video matting data. In these\nmethods, segmentation and matting data are fed to the main\nshared network, but are directed to produce outputs at sep-\narate heads. Although segmentation data do supervise the\nmain network to empower generalizability and robustness\nto the matting model, the stability they provide falls short\nof what a VOS model could achieve. As shown in Fig. 2,\nboth RVM and MaGGIe perform significantly worse than\nthe VOS outputs (white masks on inputs) by XMem [8] in\ncore areas, where semantic information is key. We believe\nthe parallel head training scheme may not fully exploit the\nrich segmentation prior in the data. To address this, we pro-\npose to supervise the matting head directly with segmenta-\ntion data. Specifically, we predict the alpha matte for seg-\nmentation inputs and optimize the matting outputs accord-\ningly, as illustrated in Fig. 3(c).\nScaled DDC Loss.\nA natural challenge arises with the\naforementioned approach: how can we compute the loss\non matting outputs for segmentation data when there is no\nground truth (GT) alpha matte? For core areas, the GT la-\nbels are readily available in the segmentation data, where\nan l1 loss suffices, and we denote it as Lcore. The real dif-\nficulty lies in the boundary region. A recent paper proposes\nDDC loss [35], which supervises boundary areas using the\ninput image without requiring a GT alpha matte.\nLDDC = 1\nN\nN\nP\ni\nP\nj\n|αi −αj −∥Ii −Ij∥2|,\nj ∈argtopk{−∥Ii −Ij∥2}.\n(2)\nHowever, we find that the underlying assumption of this de-\nsign, that ∥αi −αj∥2 = ∥Ii −Ij∥2 for αi > αj, does\nnot always hold true. For two image pixels Ii and Ij, their\ndifference is given by:\nIi −Ij = [αiFi +(1−αi)Bi]−[αjFj +(1−αj)Bj], (3)\nwhere Fi, Bi represent the foreground and background val-\nues at pixel i, and similarly for Fj and Bj at pixel j. Since\nwe impose the constraint j ∈argtopk{−∥Ii −Ij∥2}, we\ncan assume Fi = Fj = F, Bi = Bj = B within a small\nwindow. This simplifies Eq. (3) to:\nIi −Ij = (αi −αj)(F −B).\n(4)\nThis shows that the assumptions for DDC loss hold only\nwhen |F −B| = 1. To account for this, we devise a scaled\nversion as our boundary loss Lboundary:\nLboundary = 1\nN\nN\nP\ni\nP\nj\n|(αi −αj)(F −B) −∥Ii −Ij∥2|,\nj ∈argtopk{−∥Ii −Ij∥2},\n(5)\nwhere F is approximated by the average of the top k largest\npixel values in the small window, and B by the average\nof the top k smallest pixel values. In the ablation study\n(Sec. 5.2), we show that training with our scaled DDC loss\n(Eq. (5)) yields more natural matting results than training\nwith the original version (Eq. (2)), which tends to produce\nsegmentation-like jagged and stair-stepped edges.\n3.3. Practical Inference Strategies\nRecurrent Refinement.\nThe first-frame matte is pre-\ndicted from the given first-frame segmentation mask, and\nits quality will affect the matte prediction for the subse-\nquent frames. The sequential prediction in the memory-\nbased paradigm enables recurrent refinement during infer-\nence. Leveraging this mechanism, we introduce an optional\nfirst-frame warm-up module for inference. Specifically, we\nrepeat the first frame n times, treating each repetition as\nthe initial frame, and use only the nth alpha output as the\nfirst frame to initialize the alpha memory bank. This (1)\nenhances robustness against the given segmentation mask\nand (2) refines matting details in the first frame to achieve\nimage-matting quality (see Fig. 6 and Fig. 13).\nAuxiliary-free Variant. To enable comparison with an ar-\nbitrary auxiliary-free video matting approach, we design an\nauxiliary-free version by removing the segmentation prior\nfrom the initial frame. Instead, we use the first-frame al-\npha matte generated by an auxiliary-free method of interest,\nsuch as RVM, and binarize it as the given mask for our set-\nting. Table 1 presents a comparison between RVM and our\nauxiliary-free variant (Ours-AF) on synthetic benchmarks.\n4. Data\nWe briefly introduce our new training datasets and bench-\nmarks for evaluation, including both synthetic and real-\n5\n\nTable 1. Quantitative comparisons on different video matting benchmarks from diverse sources. The best and second-best performances\nare marked in red and orange , respectively. † indicates that MaGGIe [22] requires the instance mask as guidance for each frame, while\nour method only requires it in the first frame.\nMetrics\nAuxiliary-free (AF) Methods\nMask-guided Methods\nMODNet [24]\nRVM [33]\nRVM-Large [33]\nOurs-AF\nAdaM [31]\nFTP-VM [20]\nMaGGIe† [22]\nOurs\nVideoMatte (512 × 288)\nMAD↓\n9.41\n6.08\n5.32\n5.99\n5.30\n6.13\n5.49\n5.07\nMSE↓\n4.30\n1.47\n0.62\n1.72\n0.78\n1.31\n0.60\n0.87\nGrad↓\n1.89\n0.88\n0.59\n0.88\n0.72\n1.14\n0.57\n0.62\ndtSSD↓\n2.23\n1.36\n1.24\n1.10\n1.33\n1.60\n1.39\n1.16\nConn↓\n0.81\n0.41\n0.30\n0.38\n0.30\n0.41\n0.31\n0.25\nVideoMatte (1920 × 1080)\nMAD↓\n11.13\n6.57\n5.81\n5.66\n4.42\n8.00\n4.42\n4.27\nMSE↓\n5.54\n1.93\n0.97\n1.68\n0.39\n3.24\n0.40\n0.36\nGrad↓\n15.30\n10.55\n9.65\n5.75\n5.12\n23.75\n4.03\n4.04\ndtSSD↓\n3.08\n1.90\n1.78\n1.27\n1.39\n2.37\n1.31\n1.24\nYoutubeMatte (512 × 288)\nMAD↓\n19.37\n4.08\n3.36\n3.95\n-\n3.08\n3.54\n2.57\nMSE↓\n16.21\n1.97\n1.04\n2.25\n-\n1.29\n1.23\n0.94\nGrad↓\n2.05\n1.34\n1.03\n1.26\n-\n1.16\n1.10\n0.91\ndtSSD↓\n2.79\n1.81\n1.62\n1.52\n-\n1.83\n1.88\n1.53\nConn↓\n2.68\n0.60\n0.50\n0.57\n-\n0.41\n0.49\n0.36\nYoutubeMatte (1920 × 1080)\nMAD↓\n15.29\n4.37\n3.50\n3.70\n-\n6.49\n2.37\n2.05\nMSE↓\n12.68\n2.25\n1.19\n2.35\n-\n4.58\n0.98\n0.76\nGrad↓\n8.42\n15.1\n12.64\n11.45\n-\n29.78\n7.69\n9.67\ndtSSD↓\n2.74\n2.28\n2.08\n1.81\n-\n2.41\n1.77\n1.75\nworld. More details are provided in the appendix (Sec. I).\n4.1. Training Datasets\nTo address limitations in video matting datasets in both\nquality and quantity, we collect abundant green screen\nvideos, process them with Adobe After Effects, and conduct\nmanual selection to remove common artifacts also found in\nVideoMatte240K [32] (see Fig. 8). Compared to Video-\nMatte240K, our dataset, VM800, is (1) twice as large, (2)\nmore diverse in terms of hairstyles, outfits, and motion,\nand (3) higher in quality. Ablation studies (Table 3(b) and\nSec. J.1) further demonstrate the advantages of our dataset.\n4.2. Synthetic Benchmark\nThe standard benchmark, VideoMatte [32], derived from\nVideoMatte240K, includes only 5 unique foreground\nvideos, which is under representative. Additionally, their\nforegrounds lack sufficient boundary details, limiting their\nability to discern matting precision in boundary regions. To\ncreate a more comprehensive benchmark, we compile 32\ndistinct 1920 × 1080 green-screen foreground videos from\nYouTube, and process them similarly to our training dataset.\nOur benchmark, YouTubeMatte, provides enhanced detail\nrepresentation, as reflected by higher Grad [41] values.\n4.3. Real-world Benchmark and Metric\nReal-world benchmarks are essential to facilitate the prac-\ntical use of video matting models.\nAlthough real-world\nvideos lack ground truth (GT) alpha mattes, we can generate\nframe-wise segmentation masks as GT for core areas ben-\nefiting from the high capability of existing VOS methods.\nSpecifically, we select a subset of 25 real-world videos [33]\n(100 frames each) with high-quality core GT masks verified\nmanually. MAD, MSE, and dtSSD [14] are then calculated\nat the core region as core region metrics, representing se-\nmantic stability that is critical for visual perception.\n5. Experiments\nTraining Schedule.\nStage 1.\nFollowing the practice of\nRVM [33], we start by training the entire model on our\nVM800 for 80k iterations. The sequence length is initially\nset to 3 and extended to 8 with increasing sampling intervals\nfor more complex scenarios. Stage 2. As the key stage, we\napply the core supervision training strategy introduced in\nSection 3.2. Real segmentation data COCO [34], SPD [45]\nand YouTubeVIS [52] are added for supervising the matting\nhead. The loss function applied are specified in Section 3.2.\nStage 3. Finally, we fine-tune the model with image matting\ndata D646 [39] and AIM [26] for finer matting details.\n6\n\nVideo Frame\nRVM\nFTP-VM\nMaGGIe\nOurs\nFrame t\nFrame t+5\nFigure 4. Qualitative comparisons on real-world videos. Our MatAnyone significantly outperforms existing auxiliary-free (RVM [33]) and\nmask-guided (FTP-VM [21] and MaGGIe [22]) approaches in both detail extraction and semantic accuracy. For the lowest row, while other\nmethods all miss out on important body parts (i.e., head) and mistakenly take background pixels as foreground (due to similar colors), thus\ngenerating messy outputs, our method presents an accurate and visually clean output by even identifying the shadow near the boundary.\n5.1. Comparisons\nWe compare MatAnyone with several state-of-the-art meth-\nods, including auxiliary-free (AF) methods: MODNet [24],\nRVM [33], and RVM-Large [33], and mask-guided meth-\nods: AdaM [31], FTP-VM [21], and MaGGIe [22].\nTable 2. Quantitative comparisons on real-world benchmark [33].\nThe best and second performances are marked in\nred\nand\norange , respectively.\nMethods\nMAD↓\nMSE↓\ndtSSD↓\nAuxiliary-free\nMODNet [24]\n11.67\n10.12\n3.37\nRVM [33]\n1.21\n0.77\n1.43\nRVM-Large [33]\n0.95\n0.50\n1.30\nMask-guided\nFTP-VM [21]\n4.77\n4.11\n1.68\nMaGGIe [22]\n1.94\n1.53\n1.63\nMatAnyone (Ours)\n0.18\n0.11\n0.95\n5.1.1\nQuantitative Evaluations\nSynthetic Benchmarks. For a comprehensive evaluation\non synthetic benchmarks, we employ MAD (mean abso-\nlute difference) and MSE (mean squared error) for seman-\ntic accuracy, Grad (spatial gradient) [41] for detail extrac-\ntion, Conn (connectivity) [41] for perceptual quality, and\ndtSSD [14] for temporal coherence. In Table 1, our method\nachieves the best MAD and dtSSD across all datasets at\nboth high and low resolutions, demonstrating exceptional\nspatial accuracy for alpha mattes and remarkable temporal\nstability. Our auxiliary version, which shares the same first-\nframe prediction as RVM, outperforms RVM in both dtSSD\nand Conn metrics across all datasets, highlighting the ad-\nvantages of our design in stability and visual quality.\nReal Benchmark. For evaluation on real benchmarks, we\nTable 3. Ablation study of the new training dataset (New Data),\nconsistent memory propagation module (CMP), and new training\nscheme (New Training) on real benchmark (about 1080p).\nExp.\nNew Data\nCMP\nNew Training\nMAD↓\nMSE↓\ndtSSD↓\n(a)\n3.16\n2.65\n1.37\n(b)\n✓\n2.55\n2.25\n1.36\n(c)\n✓\n✓\n1.85\n1.67\n1.25\n(d)\n✓\n✓\n✓\n0.42\n0.34\n0.94\nuse the core region metrics in Section 4.3.\nIn Table 2,\nour method demonstrates superior generalizability on real\ncases, achieving the best metric values with a substantial\nmargin over both auxiliary-free and mask-guided methods.\n5.1.2\nQualitative Evaluations\nVisual results on real-world videos are in Fig. 4 and Fig. 5.\nGeneral Video Matting. MatAnyone outperforms existing\nauxiliary-free and mask-guided approaches in both detail\nextraction (boundary) and semantic accuracy (core). Fig. 4\nshows that MatAnyone excels at fine-grained details (e.g.,\nhair in the middle row) and differentiates full human body\nagainst complicated or ambiguous backgrounds when fore-\nground and background colors are similar (e.g., last row).\nInstance Video Matting. The assignment of target object\nat the first frame gives us flexibility for instance video mat-\nting. In Fig. 5, although MaGGIe [22] benefits from us-\ning instance masks as guidance for each frame, our method\ndemonstrates superior performance in instance video mat-\nting, particularly in maintaining object tracking stability and\npreserving fine-grained details of alpha mattes.\n5.2. Ablation Study\nEnhancement from New Training Data. In Table 3, by\ncomparing (a) and (b), it is observed that training with\nnew data noticeably improves the semantic performance\n7\n\nMaGGIe\nOurs\nVideo Frame\nInstance #1\n#1\n#3\n#2\nInstance #2\nInstance #3\n#1\n#3\n#2\nMaGGIe\nOurs\nVideo Frame\nInstance #1\nInstance #2\nInstance #3\nFigure 5. Quantitative comparisons with MaGGIe [22] on instance video matting. Despite MaGGIe using instance mask as guidance for\neach frame, our method shows better performance, achieving better stability in object tracking and finer alpha matte details.\nVideo Frame\nSegmentation Mask\n!! = 1\n!! = 2\n!! = 5\n!! = 10\nFigure 6. Improvement with Recurrent refinement. (Zoom-in for best view)\nVideo Frames\nDDC Loss\nScaled DDC loss\nFigure 7. Comparison of matting results training with original\nDDC loss [35] and with scaled DDC loss, where the latter gives\nmore stable and natural matting results.\nwith decreased MAD and MSE, showing that our newly-\ncollected VM800 indeed contributes to robust training with\nits upgraded quantity, quality, and diversity.\nEffectiveness of Consistent Memory Propagation. We\nfurther investigate the effectiveness of the consistent mem-\nory propagation (CMP) module. From Table 3 (b) to (c), im-\nprovement can be seen across all metrics with CMP added,\nindicating its effectiveness in improving semantic stability\nand temporal coherency. In particular, dtSSD in (c) is al-\nready lower than all the other methods in Table 2, showing\nthe superiority of CMP in terms of temporal consistency.\nEffectiveness of New Training Scheme. Our new training\nscheme brings our model to the next level with a noticeable\nimprovement in all metrics. It already outperforms all the\nother methods in Table 2 without further fine-tuning.\nScaled DDC Loss. We examine the merit of the scaled\nversion of DDC loss by training with Lcore and Lboundary\nonly to maximize its effect. In Fig. 7, training with vanilla\nDDC loss produces segmentation-like jaggedness, espe-\ncially among the boundary region. Our scaled DDC loss\nyields more stable and natural matting results.\nEffectiveness of Recurrent Refinement. Fig. 6 shows the\neffectiveness of recurrent refinement in a progressive man-\nner. Given a rough segmentation mask, our method can pro-\nduce alpha matte with descent details within 10 iterations.\n6. Conclusion\nWe introduce MatAnyone, a practical framework for target-\nassigned human video matting that ensures stable and ac-\ncurate results across diverse real-world scenarios.\nOur\nmethod leverages a region-adaptive memory fusion ap-\nproach, which combines memory from previous frames to\nmaintain semantic consistency in core areas while preserv-\ning fine details along object boundaries. With a new train-\ning dataset that is larger, high-quality, and diverse and a\nnovel training strategy that effectively leverages segmenta-\ntion data, MatAnyone achieves robust and stable matting\nperformance, even with complex backgrounds. These ad-\nvancements position MatAnyone a practical solution for\nreal-world video matting, also setting a solid foundation for\nfuture research in memory-based video processing.\n8\n\nReferences\n[1] Nicolas Ballas, Li Yao, Christopher J Pal, and Aaron\nCourville. Delving deeper into convolutional networks for\nlearning video representations. In ICLR, 2016. 3\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your Latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 3\n[3] Huanqia Cai, Fanglei Xue, Lele Xu, and Lili Guo. Trans-\nMatting: Enhancing transparent objects matting with trans-\nformers. In ECCV, 2022. 3\n[4] Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, and\nChen Change Loy. BasicVSR: The search for essential com-\nponents in video super-resolution and beyond.\nIn CVPR,\n2021. 3\n[5] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Improving video super-resolution with\nenhanced propagation and alignment. In CVPR, 2022.\n[6] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy.\nInvestigating tradeoffs in real-world\nvideo super-resolution. In CVPR, 2022. 3\n[7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideoCrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 3\n[8] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In ECCV, 2022. 2, 3, 5, 12, 13, 14, 16\n[9] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular\ninteractive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR, 2021.\n[10] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethink-\ning space-time networks with improved memory coverage\nfor efficient video object segmentation. In NeurIPs, 2021. 2,\n3, 4, 12\n[11] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander\nSchwing, and Joon-Young Lee. Tracking anything with de-\ncoupled video segmentation. In ICCV, 2023.\n[12] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young\nLee, and Alexander Schwing. Putting the object back into\nvideo object segmentation. In CVPR, 2024. 2, 3, 12, 13, 14,\n16\n[13] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen,\nJiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo\nRosenhahn, Tao Xiang, and Sen He.\nFLATTEN: optical\nflow-guided attention for consistent text-to-video editing. In\nICLR, 2024. 3\n[14] Mikhail Erofeev, Yury Gitman, Dmitriy S Vatolin, Alexey\nFedorov, and Jue Wang. Perceptually motivated benchmark\nfor video matting. In BMVC, 2015. 6, 7, 16\n[15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenFlow:\nConsistent diffusion features for consistent\nvideo editing. In ICLR, 2024. 3\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 12\n[17] Qiqi Hou and Feng Liu. Context-aware image matting for si-\nmultaneous foreground and alpha estimation. In ICCV, 2019.\n13\n[18] Li Hu, Peng Zhang, Bang Zhang, Pan Pan, Yinghui Xu,\nand Rong Jin. Learning position and target consistency for\nmemory-based video object segmentation. In CVPR, 2021.\n3\n[19] Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing.\nMaskRNN: Instance level video object segmentation.\nIn\nNeurIPS, 2017. 2\n[20] Wei-Lun Huang and Ming-Sui Lee. End-to-end video mat-\nting with trimap propagation. In CVPR, 2023. 6\n[21] Wei-Lun Huang and Ming-Sui Lee. End-to-end video mat-\nting with trimap propagation. In CVPR, 2023. 2, 3, 4, 7, 13,\n18, 20, 21, 22\n[22] Chuong Huynh, Seoung Wug Oh, , Abhinav Shrivastava, and\nJoon-Young Lee. MaGGIe: Masked guided gradual human\ninstance matting. In CVPR, 2024. 2, 3, 6, 7, 8, 13, 18, 20,\n21, 22\n[23] Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson WH\nLau. Harmonizer: Learning to perform white-box image and\nvideo harmonization. In ECCV, 2022. 16\n[24] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Ryn-\nson W.H. Lau. MODNet: Real-time trimap-free portrait mat-\nting via objective decomposition. In AAAI, 2022. 2, 3, 6, 7\n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 3, 19\n[26] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic\nnatural image matting. In IJCAI, 2021. 6\n[27] Jiachen\nLi,\nVidit\nGoel,\nMarianna\nOhanyan,\nShant\nNavasardyan, Yunchao Wei, and Humphrey Shi. VMFormer:\nEnd-to-end video matting with transformer. In WACV, 2024.\n2, 3\n[28] Jiachen Li,\nRoberto Henschel,\nVidit Goel,\nMarianna\nOhanyan, Shant Navasardyan, and Humphrey Shi.\nVideo\ninstance matting. In WACV, 2024. 3\n[29] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting Any-\nthing. In CVPR, 2024. 3\n[30] Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang\nCheng, Jiangmiao Pang, and Chen Change Loy. Tube-link:\nA flexible cross tube framework for universal video segmen-\ntation. In ICCV, 2023. 3\n[31] Chung-Ching Lin, Jiang Wang, Kun Luo, Kevin Lin, Linjie\nLi, Lijuan Wang, and Zicheng Liu. Adaptive human matting\nfor dynamic videos. In CVPR, 2023. 2, 3, 4, 6, 7, 13\n[32] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta,\nBrian L Curless, Steven M Seitz, and Ira Kemelmacher-\nShlizerman. Real-time high-resolution background matting.\nIn CVPR, 2021. 2, 6, 12, 14, 15, 16, 17\n[33] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip\nSengupta. Robust high-resolution video matting with tempo-\nral guidance. In WACV, 2022. 1, 2, 3, 4, 6, 7, 13, 16, 18, 20,\n21, 22\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 6, 13\n9\n\n[35] Wenze Liu, Zixuan Ye, Hao Lu, Zhiguo Cao, and Xiangyu\nYue. Training matting models without alpha labels. arXiv\npreprint arXiv:2408.10539, 2024. 2, 4, 5, 8\n[36] I Loshchilov. Decoupled weight decay regularization. In\nICLR, 2019. 12\n[37] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV, 2019. 3\n[38] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV, 2019. 2\n[39] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang\nXu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hier-\narchical structure aggregation for image matting. In CVPR,\n2020. 3, 6\n[40] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll´ar, and Christoph Feicht-\nenhofer. SAM 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714, 2024. 3\n[41] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit\nGelautz, Pushmeet Kohli, and Pamela Rott. A perceptually\nmotivated online benchmark for image matting. In CVPR,\n2009. 6, 7\n[42] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized\nmemory network for video object segmentation. In ECCV,\n2020. 3\n[43] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and\nJiaya Jia. Deep automatic portrait matting. In ECCV, 2016.\n3\n[44] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, and\nYu-Wing Tai. Deep video matting via spatio-temporal align-\nment and aggregation. In CVPR, 2021. 13\n[45] Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid.\nLearning video object segmentation with visual memory. In\nICCV, 2017. 6, 13\n[46] Haochen Wang, Xiaolong Jiang, Haibing Ren, Yao Hu, and\nSong Bai. Swiftnet: Real-time video object segmentation. In\nCVPR, 2021. 3\n[47] Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, and\nChen Change Loy. EDVR: Video restoration with enhanced\ndeformable convolutional networks. In CVPRW, 2019. 3\n[48] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. VideoComposer: Compositional video synthesis\nwith motion controllability. In NeurIPS, 2024. 3\n[49] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yum-\ning Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua\nLin, Yu Qiao, and Ziwei Liu. LaVie: High-quality video\ngeneration with cascaded latent diffusion models. In IJCV,\n2024. 3\n[50] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-A-Video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, 2023. 3\n[51] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping\nZhang, and Wenxiu Sun. Efficient regional memory network\nfor video object segmentation. In CVPR, 2021. 2, 3\n[52] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-\nmentation. In ICCV, 2019. 6, 13\n[53] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change\nLoy.\nRerender A Video: Zero-shot text-guided video-to-\nvideo translation. In SIGGRAPH Asia, 2023. 3\n[54] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating ob-\njects with transformers for video object segmentation.\nIn\nNeurIPS, 2021. 14\n[55] Jingfeng Yao,\nXinggang Wang,\nShusheng Yang,\nand\nBaoyuan Wang.\nViTMatte: Boosting image matting with\npre-trained plain vision transformers. Information Fusion,\n2024. 3\n[56] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu.\nMatte Anything: Interactive natural image matting with seg-\nment anything model. Image and Vision Computing, page\n105067, 2024. 3, 17, 19\n[57] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing\nHuang, Hujun Bao, and Weiwei Xu. A late fusion cnn for\ndigital matting. In CVPR, 2019. 3\n[58] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai.\nEdgeSAM: Prompt-in-the-loop distillation for on-device de-\nployment of sam. arXiv preprint, 2023. 3\n[59] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie,\nWangmeng Zuo, and Jimmy Ren.\nSpatio-temporal filter\nadaptive network for video deblurring. In ICCV, 2019. 3\n[60] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo,\nand Chen Change Loy.\nUpscale-A-Video:\nTemporal-\nconsistent diffusion model for real-world video super-\nresolution. In CVPR, 2024. 3\n[61] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo\nZhang, and Ming Tang. Fast deep matting for portrait ani-\nmation on mobile phone. In ACMMM, 2017. 3\n[62] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae\nLee. Segment everything everywhere all at once. In NeurIPS,\n2024. 3\n10\n\nAppendix\nIn this supplementary material, we provide additional discussions and results to supplement the main paper. In Sec-\ntion G, we present the network details of our MatAnyone. In Section H, we discuss more training details, including training\nschedules, training augmentations, and loss functions. In Section I, we provide more details on our new training and testing\ndatasets, including the generation pipeline and some examples for demonstration. We present comprehensive results in Sec-\ntion J to further show our performance, including those for ablation studies and qualitative comparisons. It is noteworthy that\nwe also include a demo video (Section J.6) to showcase a Hugging Face demo and additional results on real-world cases in\nvideo format.\nContents\n1. Introduction\n2\n2. Related Work\n3\n3. Methodology\n3\n3.1. Consistent Memory Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.2. Core-area Supervision via Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.3. Practical Inference Strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4. Data\n5\n4.1. Training Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2. Synthetic Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.3. Real-world Benchmark and Metric\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5. Experiments\n6\n5.1. Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.1.1\nQuantitative Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.1.2\nQualitative Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.2. Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n6. Conclusion\n8\nG. Architecture\n12\nG.1. Network Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nH. Training\n12\nH.1. Training Schedules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nH.2. Training Augmentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nH.3. Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nI. Dataset\n14\nI.1 . New Training Dataset - VM800 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nI.2 . New Test Dataset - YouTubeMatte . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nI.3 . Real Benchmark and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nJ. More Results\n17\nJ.1 . Enhancement from New Training Data\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.2 . Effectiveness of Consistent Memory Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.3 . Effectiveness of New Training Scheme\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.4 . Effectiveness of Recurrent Refinement\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.5 . More Qualitative Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nJ.6 . Demo Video\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n11\n\nG. Architecture\nG.1. Network Designs\nAs illustrated in Fig. 3 in the main paper, our MatAnyone mainly has five important components: (1) an encoder for key and\nquery transformation, (2) a consistent memory propagation module for pixel memory readout, (3) an object transformer [12]\nfor memory grouping by object-level semantics, (4) a decoder for alpha matte decoding, (5) a value encoder for alpha matte\nencoding, which is used to update the alpha memory bank.\nEncoder. We adopt ResNet-50 [16] for encoder following common practices in memory-based VOS [8, 10, 12]. Discarding\nthe last convolution stage, we take ×16 downsampled feature as F t for key and query transformation, while features at scales\n×8, ×4, ×2, and ×1 are used as skip connections for the decoder.\nConsistent Memory Propagation. The process of consistent memory propagation is detailed in Fig. 3(b) in the main paper.\nAlpha memory bank serves as the main working memory for past information query as in [8, 12], which is updated every rth\nframe across the whole time span. The query of the current frame to the alpha memory bank is implemented in an attention\nmanner following [8, 12]. For the query QHW ×C 1 and alpha memory bank KT HW ×C, V T HW ×Cv 2, the affinity matrix\nA ∈[0, 1]HW ×T HW of the query to alpha memory is computed as:\nAij =\nexp(d(Qi, Kj))\nP\nz exp(d(Qi, Kz)),\n(6)\nwhere d(·, ·) is the anisotropic L2 function, H and W are the height and width at ×16 downsampled input scale, and T is\nthe number of memory frames stored in alpha memory bank. The queried values V m\nt\nin Fig. 3(b) in the main manuscript is\nobtained as:\nV m\nt\n= AVm.\n(7)\nIn addition to that, we also maintain last frame memory solely for the uncertainty prediction module we propose, and it is\nupdated every frame. The boundary-area prediction module is lightweight with one 1 × 1 convolution and two 3 × 3\nconvolutions. By taking the input of a concatenation of current frame feature Kt, last frame feature Kt−1, and last alpha\nmatte prediction Mt−1, it outputs a one-channel change probability mask Ut of each query token, where higher Ut indicates\nsuch token is likely to change more in the alpha value compared with Mt−1. As mentioned in Sec. 3.1 in the manuscript, the\nground truth Ut label is obtained by: U GT\nt\n: |M GT\nt−1 −M GT\nt\n| >= δ, where δ is set at 0 for segmentation data, and 0.001 for\nmatting data as noise tolerance. Since Ut is predicted at a ×16 downsampled scale in the memory space, the ground truth\nmask U GT\nt\nis also downsampled in the mode of area.\nObject Transformer. Our object transformer is derived from Cutie [12] with three consecutive object transformer blocks.\nPixel memory readout P t obtained from the consistent memory propagation module is then grouped through several attention\nlayers and feed-forward networks. In this way, the noise brought by low-level pixel matching could be effectively reduced\nfor a more robust matching against distractors. We do not claim contributions for this module.\nDecoder. Our decoder is inspired by common practices in VOS [8, 12] with modified designs specifically for the matting\ntasks. The mask decoder is VOS generally consists of two interactive upsampling from ×16 to ×4, and then a bilinear\ninterpolation is applied to the input scale. However, since the boundary region for an alpha matte requires much more\nprecision than a segmentation mask, we enrich the decoder with two more upsampling layers until ×1, where skip connections\nfrom the encoder are applied at each scale to enhance the boundary precision.\nValue Encoder. Similar to the encoder, we adopt ResNet-18 [16] for value encoder following common practices in memory-\nbased VOS [8, 10, 12]. Different from the encoder for key and query, the value encoder takes the predicted alpha matte M t\nas well as the image features as input, the encoded values are then used to update the alpha memory bank and last frame\nmemory according to their updating rules.\nH. Training\nH.1. Training Schedules\nStage 1. To initialize our model on memory propagation learning, we train with our new video matting data VM800, which\nis of larger scale, higher quality, and better diversity than VideoMatte240K [32]. We use the AdamW [36] optimizer with a\nlearning rate of 1 × 10−4 with a weight decay 0.001. The batch size is set to 16. We train with a short sequence length of\n3 for 80K first, and then we train with a longer sequence length of 8 for another 5K for more complex scenarios. Video and\n1We ignore the subscript t in Qt for simplicity\n2We ignore the subscript m in Km and Vm for simplicity\n12\n\nTable 4. Training settings and losses used in different training stages. † indicates that segmentation loss is computed as an auxiliary loss\non a segmentation head, which will be abandoned during inference. Other than that, matting loss and core supervision loss are computed\non the matting head for semantic stability in core regions and matting details in the boundary region.\nTraining Stage\n#Iterations\nMatting Data\nSegmentation Data\nSequence Length\nMatting Loss\nSegmentation Loss†\nCore Supervision Loss\nStage 1\n85K\nvideo\nimage & video\n3 (80K) →8 (5K)\n✓\n✓\nStage 2\n40K\nvideo\nimage & video\n8\n✓\n✓\n✓\nStage 3\n5K\nimage\nimage & video\n8\n✓\n✓\n✓\nimage segmentation data COCO [34], SPD [45] and YouTubeVIS [52] are used to train the segmentation head parallel to the\nmatting head at the same time, as previous practices [21, 31, 33].\nStage 2. We apply our key training strategy - core-area supervision in this stage. On the basis of the previous stage, we add\nadditional supervision on the matting head with segmentation data to enhance the semantics robustness and generalizability\ntowards real cases. In this stage, the learning rate is set to be 1 × 10−5, and we train with a sequence length of 8 for 40K for\nboth matting and segmentation data.\nStage 3. Due to the inferior quality of video matting data compared with image matting data annotated by humans, we\nfinetune our model with image matting data instead for 5K with a 1 × 10−6 learning rate. Noticeable improvements in\nmatting details, especially among boundary regions, could be seen after this stage.\nH.2. Training Augmentations\nAugmentations for Training Data. As discussed in the manuscript, video matting data are deficient in quantity and diversity.\nIn order to enhance training data variety during the composition process, we follow RVM [33] to apply motion (e.g., affine\ntranslation, scale, rotation, etc.) and temporal (e.g., clip reversal, speed changes, etc.) augmentations to both foreground\nand background videos. Motion augmentations applied to image data also serve to synthesize video sequences from images,\nmaking it possible to fine-tune with higher-quality image data for details.\nAugmentations for Given Mask. Since our setting is to receive the segmentation mask for the first frame and make alpha\nmatte prediction for all the frames including the first one, it is important to have our model robust to the given mask. To\ngenerate the given mask in the training pipeline, we first obtain the original given mask. For segmentation data, it is just the\nground truth (GT) for the first frame, while for matting data, it is the binarization result on the first-frame GT alpha matte,\nwith a threshold of 50. Erosion or dilation is then applied with a probability of 40% each, with kernel sizes ranging from 1\nto 5. In this way, we force the model to learn alpha predictions based on an inaccurate segmentation mask, also enhancing\nthe model robustness towards memory readout if it is not so accurate during the predictions in following frames.\nAugmentations for Assigned Object(s). The assignment of target object(s) as a segmentation mask for the first frame gives\nus flexibility for instance video matting. Given the strong prior, the model is still easy to be confused by other salient humans\nnot assigned as target. To solve this, we find that a small modification in the video segmentation data pipeline has an obvious\neffect. In YouTubeVIS [52], for each video with human existence, suppose the number of human instances is H. Instead of\ncombining all of them as one object (practice in previous auxiliary-free methods [33]), we randomly take h ≤H instance as\nforeground, while unchosen instances are marked as background. In this way, we force the model to distinguish the target\nhuman object(s) even when other salient human object(s) exist, enhancing the robustness in object tracking for instance video\nmatting even without instance mask for each frame as MaGGIe [22] has.\nH.3. Loss Functions\nGiven that we take the first-frame segmentation mask alongside with input frames as input, our model needs to predict alpha\nmatte starting from the first frame, which is different from VOS methods [8, 12]. In addition, since we also apply mask\naugmentation on the given segmentation mask, the prediction from the segmentation head should also start from the first\nframe. As a result, we need to apply losses on all t ∈[0, N] frames for both matting and segmentation heads.\nThere are mainly three kinds of losses involved in our training: (1) matting loss Lmat; (2) segmentation loss Lseg; (3)\ncore supervision (CS) loss Lcs, and their usages in different training stages are summarized in Table 4.\nMatting Loss. For frame t, suppose we have the predicted alpha matte Mt w.r.t. its ground-truth (GT) M GT\nt\n. We follow\nRVM [33] to employ L1 loss for semantics Ll1, pyramid Laplacian loss [17] for matting details Llap, and temporal coherence\nloss [44] Ltc for flickering reduction:\nLl1 = ∥Mt −M GT\nt\n∥1,\n(8)\nLlap =\n5\nX\ns=1\n2s−1\n5\n∥Ls\npyr(Mt) −Ls\npyr(M GT\nt\n)∥1,\n(9)\n13\n\nLtc = ∥dMt\ndt\n−dM GT\nt\ndt\n∥2,\n(10)\nThe overall matting loss is summarized as:\nLmat = Ll1 + 5Llap + Ltc.\n(11)\nSegmentation Loss. For frame t, suppose we have the predicted segmentation mask St w.r.t. its ground-truth (GT) SGT\nt\nfrom the segmentation head. We employ common losses used in VOS [8, 12, 54], Lce and Ldice.\nLce = SGT\nt\n(−log(St)) + (1 −SGT\nt\n)(−log(1 −St)),\n(12)\nLdice = 1 −2StSGT\nt\n+ 1\nSt + SGT\nt\n+ 1.\n(13)\nThe overall segmentation loss is summarized as:\nLseg = Lce + Ldice.\n(14)\nCore Supervision Loss. For core-area supervision, we combine the region-specific losses: Lcore for core region and\nLboundary for boundary region as defined in Sec. 3.2 in the manuscript, and the overall core supervision loss is summa-\nrized as:\nLcs = Lcore + 1.5Lboundary.\n(15)\nI. Dataset\nTable 5. Comparison on Datasets. We compare our new training data and testing data with the old ones, in terms of the number of distinct\nforegrounds, sources, and whether harmonization is applied.\nDatesets\nVideoMatte240K (old train) [32]\nVM800 (new train)\nVideoMatte (old test) [32]\nYouTubeMatte (new test)\n#Foregrounds\n475\n826\n5\n32\nSources\n-\nStoryblocks, Envato Elements, Motion Array\n-\nYouTube\nHarmonized\n-\n-\nx\n✓\nI.1. New Training Dataset - VM800\nOverview. As summarized in Table 5, our new training dataset VM800 has almost twice the number of foreground videos\nthan VideoMatte240K [32] in quantity. To enhance diversity and data distribution, our foreground green screen videos are\ndownloaded from a total of three video footage websites: Storyblocks, Envato Elements, and Motion Array, and thus enjoy\na diversity in hairstyles, outfits, and motion. In addition, we ensure the high quality of our VM800 dataset in fine detail and\nthrough careful manual selection.\nGeneration Pipeline. We employ Adobe After Effects in our data generation pipeline to extract alpha channels from green\nscreen footage videos. Since the amount of green screen footage to be processed is huge, we would like to obtain the\npreliminary results with an automatic pipeline. We first use Keylight and set Screen Color to be the pixel value taken\nfrom the upper left corner for each frame. To obtain a clean alpha matte, we clip the values smaller than 20 to be 0 and those\nlarger than 80 to be 255. To further enhance the alpha matte quality, we post-process with another two keying effects Key\nCleaner and Advanced Spill Supressor, which are generally used together following Keylight. Since we are\nprocessing a video, we also turn on reduce chatter in Key Cleaner to reduce flickering in the boundary region. For\nbatch processing, we compile the above process into a Javascript and XML file for After Effects to run with, and obtain a\nlarge batch of preliminary results for manual selection.\n14\n\nKeylight\n- Screen Color: pixel value of upper left corner\n- Screen Matte:\n- Clip Black: 20\n- Clip White: 80\nKey Cleaner\n- radius: 1\n- reduce chatter: check\nAdvanced Spill Supressor\n(a) Errors in reflective regions (e.g., glasses)\n(b) Inhomogeneous in core regions (e.g., shadow) \nFigure 8. Issues with VideoMatte240K [32]. (a) Errors in alpha values exist in reflective regions (e.g., “a hole” on glasses). (b) Inhomo-\ngeneous alpha values exist in core regions (e.g., caused by shadow), where the alpha value should be exactly 0 or 1.\nFigure 9. Gallery for our new training dataset VM800. High-quality details in the boundary regions and diversity in terms of gender,\nhairstyles, and aspect ratios could be clearly observed.\nQuality - Fine Details. The green screen foreground videos we downloaded are almost in a 4K quality, and we also place\na higher priority on those videos with more details (e.g., hair) in our download choice. Fig. 9 shows the fine details in our\nVM800 dataset.\nQuality - Careful Manual Selection. We notice that alpha mattes extracted with After Effects from green screen videos\noften encounter inhomogeneities in core regions. For example, reflective regions in the foreground will result in a near-zero\nvalue (i.e., a hole) in the alpha matte, as shown in Fig. 8(a). In addition, noise also exists in the green screen background,\nresulting in the fact the alpha values may not homogeneously equal 0, which should not be the case in the core region.\n15\n\nSimilarly, for foregrounds, colors that are similar to the background green, or shadow in the foreground, may also result in\nthe alpha values not homogeneously equal to 1 in the core foreground region, making the alpha matte look noisy, as shown\nin Fig. 8(b). Since VideoMatte240K [32] is also obtained with After Effects, we observe that alpha mattes with the above\nproblems still exist, and thus taking such wrong ground truth for training will inevitably lead to problematic inference results\n(Fig. 11(a)). As a result, we conduct careful manual selection to examine all our processed alpha mattes, and leave out those\nwith the above problems. As shown in Fig. 11(a), training with our VM800 will not lead to such problematic results.\nI.2. New Test Dataset - YouTubeMatte\nOverview. As summarized in Table 5, our new synthetic benchmark YouTubeMatte has over six times larger than the number\nof distinct foreground videos in VideoMatte [32], making it a much more representative benchmark for evaluation with better\ndiversity. In addition, the green screen videos for foregrounds are downloaded from YouTube at a scale of 1920 × 1080\nwith rich boundary details, thus enhancing its ability to discern matting precision in boundary regions. While the generation\npipeline for YouTubeMatte is almost the same as that for VM800, harmonization [23], however, is applied when compositing\nthe foreground on a background. Such an operation effectively makes YouTubeMatte a more challenging benchmark that is\ncloser to the real distribution. As shown in Fig. 10, while RVM [33] is confused by the harmonized frame, our method still\nyields robust performance.\nBefore\nAfter\nVideo Frame\nRVM\nOurs\nHarmonization\nFigure 10. Harmonization on synthetic benchmarks and its effect on model performance. Harmonization [23] is an operation that\nmakes the composited frame more natural and realistic, which also effectively makes our YouTubeMatte a more challenging benchmark\nthat is closer to the real distribution. It is observed that while RVM [33] is confused by the harmonized frame, our method still yields robust\nperformance.\nI.3. Real Benchmark and Evaluation\nOverview. As a technique towards real-world applications (e.g., virtual background in the online meeting), the synthetic\nbenchmark is not enough to test the generalizability of video matting models. Although there are countless of real human\nvideos for testing in the wild, the lack of GT alpha mattes makes them hard to serve as a real benchmark. Here, we select\na subset of 25 real-world videos from [33], where a consecutive of 100 frames for each video are selected with no scene\ntransition, to form our real benchmark. According to our definitions in Fig. 2(a) in the manuscript, we could also divide the\nevaluation metrics for core regions and for boundary separately, making evaluation for real benchmarks feasible.\nEvaluation on Core Regions. Thanks to the recent success of VOS methods [8, 12], frame-wise segmentation masks could\nbe generated with high precision. Here, we employ Cutie [12] for video segmentation results. We first obtain the trimap\nfor each segmentation mask by applying dilation and erosion (with kernel size 21), and then compute the core mask where\ntrimap values equal 0 or 1. In this way, the values of a segmentation mask within its core region could be considered as the\nGT alpha values for the core region, where common metrics including MAD and MSE for semantic accuracy, and dtSSD [14]\nfor temporal coherency could be applied for evaluation.\n16\n\nJ. More Results\nJ.1. Enhancement from New Training Data\nAs discussed in Sec. 4.1 in the manuscript and Section I.1 in the supplementary, our new training data VM800 is upgraded\nin quantity, quality, and diversity. In addition to the quantitative evaluation in Tab. 3 in the manuscript, we further show the\nenhancement from new training data by providing more results when comparing the model trained with VideoMatte240K [32]\nand the model trained with our VM800 in Fig. 11(a).\nErrors in \nreflective objects\nInhomogeneous \ncore regions\nVideo Frame\nOld Training Data \nNew Training Data \nVideo Frame\nw/o Core Supervision w/ Core Supervision \n(a) Enhancement from New Training Data\n(b) Effectiveness of New Training Scheme\nFigure 11. (a) Comparison on results trained with old training data (VideoMatte240K [32]) and new training data (our VM800). It\ncould be observed that training with old data will lead to errors in reflective objects (e.g., holes on the sunglasses) and inhomogeneous alpha\nvalues in the core regions. However, both issues are fixed when training with our new data, indicating a higher quality. (b) Comparison\non results trained without and with core-area supervision. It could be observed that training without it will lead to semantics error due\nto the weak supervision from real segmentation data, while training with core supervision largely improves semantics accuracy thanks to\nthe stronger supervision enabled.\nJ.2. Effectiveness of Consistent Memory Propagation\nAs one of our key designs, the consistent memory propagation (CMP) module improves both stability in core regions and\nquality in boundary details. In addition to the quantitative evaluation in Tab. 3 in the manuscript, we give more qualitative\nresults and analysis in Fig. 12.\nJ.3. Effectiveness of New Training Scheme\nOur new training scheme introduces core-area supervision, which largely enhances the semantic accuracy and stability, as\nshown in Tab. 3 in the manuscript. More qualitative results are shown in Fig. 11(b) for better visualization of its effects.\nJ.4. Effectiveness of Recurrent Refinement\nAs discussed in Sec. 3.3 in the manuscript, the sequential prediction in the memory-based paradigm enables recurrent re-\nfinement without the need for retraining during inference. By repeating the first frame n times and iteratively updating the\nfirst frame prediction based on the last-time prediction, the quality of the first frame alpha matte could be recurrently refined.\nWe show in Fig. 13 that such recurrent refinement can not only (1) enhance the robustness to the given segmentation mask\neven when it is of low quality, but also (2) achieve matting details at an image-matting level when compared with an image\nmatting method (i.e., Matte Anything [56] in the last column).\n17\n\nt\nt+40\nt+80\nt+160\nt+120\nVideo Frames\nw/o CMP\nw/ CMP\nChange Prob.\nFigure 12. Comparison on results with and without Consistent Memory Propagation. It could be observed that when CMP is not\napplied, semantic errors constantly exist across a wide span of video frames. However, when training with CMP, we observe from the\n“Change Probability” mask that usually our model only takes pixels near the boundary as “changed”, and most of the inner regions (i.e.,\nearring) will mainly take the memory values from the last frame. As we can see on the figure, while predictions are both correct at time\nt, the model with CMP successfully keeps the correctness and gives stable results, while the model without CMP quickly breaks the\ncorrectness and never recovers.\nJ.5. More Qualitative Comparisons\nIn this subsection, we provide additional visual comparisons of our method with the state-of-the-art methods, including\nauxiliary-free (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. Fig. 14 presents the\ngeneral video matting results on real videos. To further demonstrate the superiority of our model, Fig. 15 and Fig. 16 both\nshowcase a challenging case respectively, where other methods mostly fail. In addition, Fig. 17 demonstrates the instance\nmatting results compared with MaGGIe [22], a method with instance mask for each frame is given as guidance, while our\nmodel only has the segmentation mask for the first frame as guidance.\nJ.6. Demo Video\nWe also offer a demo video. This video showcases more video matting results and a hugging face demo for applicability,\nboth on real-world videos.\n18\n\nVideo Frame\nSegmentation Mask\n!! = 1\n!! = 5\n!! = 10\nImage Matting \n(Matte Anything)\nFigure 13. Comparison on results with iterative refinement. A noticeable enhancement on details can be observed even with one\niteration of refinement compared with the given segmentation mask. Within 10 iterations, our model is able to achieve matting details at an\nimage-matting level, even better than Matte Anything [56], which is an image matting model also based on the results from SAM [25].\n19\n\nVideo Frame\nRVM\nFTP-VM\nMaGGIe\nOurs\nFigure 14. More qualitative comparisons on general video matting with SOTA methods. We compare our MatAnyone with both\nauxiliary-free (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. It could be observed that our method\nsignificantly outperforms others in both detail extraction and semantic accuracy, across diverse and complex real scenarios. It is noteworthy\nthat although sometimes MaGGIe [22] seems to give acceptable results when compositing with a green screen, its alpha matte turns out\nto be noisy (i.e., inhomogeneous in the core foreground region and blurry in the boundary region), while our alpha matte is clean with\nfine-grained details in the boundary region. As a result, we also include alpha mattes for a more comprehensive comparison. (Zoom in for\nbest view)\n20\n\nVideo Frames\nRVM\nFTP-VM\nMaGGIe\nOurs\nFigure 15. A challenging example of general video matting across a long time span. We compare our MatAnyone with both auxiliary-\nfree (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. It could be observed that our model is able to\ntrack the target object stably even when the object is moving fast in a highly complex scene, where all the other methods present noticeable\nfailures. (Zoom in for best view)\n21\n\nVideo Frames\nRVM\nFTP-VM\nMaGGIe\nOurs\nFigure 16. Another challenging example of general video matting across a long time span. We compare our MatAnyone with both\nauxiliary-free (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. This example showcases that our\nmodel is able to track the target objects even in a highly ambiguous background, where the colors for foreground and background are\nsimilar, and also multiple humans in the background. In addition, it also demonstrates when there is more than one target object, our model\nis still able to handle this challenging case well. (Zoom in for best view)\n#1\n#2\n#1\n#2\n#1\n#2\n#3\nInstance #1 Instance #2 Instance #3\nVideo Frame\nMaGGIe (#1)\nMaGGIe (#2)\nVideo Frame\nOurs (#1)\nOurs (#2)\nMaGGIe\nOurs\nFigure 17. More qualitative comparisons on instance matting. We compare our MatAnyone with MaGGIe [22], a mask-guided method\nthat requires the instance mask for each frame, while our method only requires the mask for the first frame. It could be observed that even\nwith such strong given prior, MaGGIe still performs below our method in terms of semantic accuracy in the core regions. Moreover, in\nterms of the boundary regions, by examining the details there, we could clearly observe that the details generated by MaGGIe are blurry\nand far from fine-grained compared with our results. (Zoom in for best view)\n22')],
 '2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=124, thumbnail=None, content='Since the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15')]}
2025-02-06 00:22:29,089 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:22:53,460 - INFO - Total execution time: 23.65 seconds (0.39 minutes)
2025-02-06 00:22:53,469 - INFO - Papers: {'2025-02-03': [Paper(arxiv_id='2501.14677', authors=['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], published_at=datetime.datetime(2025, 2, 3, 13, 15, 59, 743000, tzinfo=datetime.timezone.utc), title='MatAnyone: Stable Video Matting with Consistent Memory Propagation', summary='Auxiliary-free human video matting methods, which rely solely on input\nframes, often struggle with complex or ambiguous backgrounds. To address this,\nwe propose MatAnyone, a robust framework tailored for target-assigned video\nmatting. Specifically, building on a memory-based paradigm, we introduce a\nconsistent memory propagation module via region-adaptive memory fusion, which\nadaptively integrates memory from the previous frame. This ensures semantic\nstability in core regions while preserving fine-grained details along object\nboundaries. For robust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a novel training\nstrategy that efficiently leverages large-scale segmentation data, boosting\nmatting stability. With this new network design, dataset, and training\nstrategy, MatAnyone delivers robust and accurate video matting results in\ndiverse real-world scenarios, outperforming existing methods.', upvotes=25, thumbnail=None, content='Auxiliary-free human video matting methods, which rely\nsolely on input frames, often struggle with complex or am-\nbiguous backgrounds. To address this, we propose MatAny-\none,\na robust framework tailored for target-assigned\nvideo matting. Specifically, building on a memory-based\nparadigm, we introduce a consistent memory propagation\nmodule via region-adaptive memory fusion, which adap-\ntively integrates memory from the previous frame.\nThis\nensures semantic stability in core regions while preserv-\ning fine-grained details along object boundaries. For ro-\nbust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a\nnovel training strategy that efficiently leverages large-scale\nsegmentation data, boosting matting stability. With this new\n1\narXiv:2501.14677v1  [cs.CV]  24 Jan 2025\n\nnetwork design, dataset, and training strategy, MatAnyone\ndelivers robust and accurate video matting results in diverse\nreal-world scenarios, outperforming existing methods.\n1. Introduction\nAuxiliary-free human video matting (VM) is widely recog-\nnized for its convenience [24, 27, 33], as it only requires\ninput frames without additional annotations. However, its\nperformance often deteriorates in complex or ambiguous\nbackgrounds, especially when similar objects, i.e., other hu-\nmans, appear in the background (Fig. 2(b)). We consider\nauxiliary-free video matting to be under-defined, as their\nresults can be uncertain without a clear target object.\nIn this work, we focus on a problem that is more appli-\ncable to real-world video applications: video matting fo-\ncused on pre-assigned target object(s), with the target seg-\nmentation mask provided in the first frame. This enables\nthe model to perform stable matting via consistent object\ntracking throughout the entire video, while offering bet-\nter interactivity. The setting is well-studied in Video Ob-\nject Segmentation (VOS), where it is referred to as “semi-\nsupervised” [10, 19, 38].\nA common strategy is to use\na memory-based paradigm [8, 12, 38, 51], encoding past\nframes and corresponding segmentation results into mem-\nory, from which a new frame retrieves relevant information\nfor its mask prediction. This allows a lightweight network\nto achieve consistent and accurate tracking of the target ob-\nject. Inspired by this, we adapt the memory-based paradigm\nfor video matting, leveraging its stability across frames.\nVideo matting poses additional challenges compared to\nVOS, as it requires not only accurate semantic detection in\ncore regions but also high-quality detail extraction along the\nboundary (e.g., hair), as defined in Fig. 2(a). A straightfor-\nward approach is to fine-tune matting details using matting\ndata, based on segmentation priors from VOS. Recent ap-\nproaches attempt to achieve both goals, either in a coupled\nor decoupled manner. For instance, AdaM [31] and FTP-\nVM [21] refine the memory-based segmentation mask for\neach frame via a decoder to produce alpha mattes, while\nMaGGIe [22] devises a separate refiner network to process\nsegmentation masks across all frames from an off-the-shelf\nVOS model. However, these methods often lead to subop-\ntimal results due to limitations in the available video mat-\nting data: (i) the quality of VideoMatte240K [32], the most\nwidely used human video matting dataset, is suboptimal.\nIts ground-truth alpha mattes exhibit problematic semantic\naccuracy in core areas (e.g., interior holes) and lack fine de-\ntails along the boundaries (e.g., blurry hair); (ii) video mat-\nting datasets are much smaller in scale compared to VOS\ndatasets; and (iii) video matting data are synthetic due to the\nextreme difficulty of human annotations, limiting their gen-\neralizability to real-world cases [33]. Consequently, fine-\ntuning a strong VOS prior for video matting with existing\nvideo matting data usually disrupts this prior. While bound-\nary details may show improvement compared to segmenta-\ntion results, the matting quality in terms of semantic stabil-\nity in core areas and details in boundary areas remain unsat-\nisfactory, as shown by the results of MaGGIe in Fig. 2(b).\nProducing matting-level details while maintaining se-\nmantic stability of a memory-based approach is challeng-\ning, especially training with suboptimal video matting data.\nTo tackle this, we focus on several key aspects:\nNetwork - we introduce a consistent memory propagation\nmechanism in the memory space. For each current frame,\nthe alpha value change relative to the previous frame is esti-\nmated for every token. This estimation guides the adaptive\nintegration of information from the previous frame. The\n“large-change” regions rely more on the current frame’s in-\nformation queried from the memory bank, while “small-\nchange” regions tend to retain the memory from the previ-\nous frame. This region-adaptive memory fusion inherently\nstabilizes memory propagation throughout the video, im-\nproving matting quality with fine details and temporal con-\nsistency. Specifically, it encourages the network to focus\non boundary regions during training to capture fine details,\nwhile “small-change” tokens in the core regions preserve\ninternally complete foreground and clean background (see\nour results in Fig. 2(b)).\nData - we collect a new training dataset, named VM800,\nwhich is twice as large, more diverse, and of higher quality\nin both core and boundary regions compared to the Video-\nMatte240K dataset [32], greatly enhancing robust train-\ning for video matting. In addition, we introduce a more\nchallenging test dataset, named YoutubeMatte, featuring\nmore diverse foreground videos and improved detail qual-\nity. These new datasets offer a solid foundation for robust\ntraining and reliable evaluation in video matting.\nTraining Strategy - the lack of real video matting data re-\nmains a significant limitation, affecting both stability and\ngeneralizability.\nWe address this problem by leveraging\nlarge-scale real segmentation data via a novel training strat-\negy. Unlike common practices [21, 22, 33] that train with\nsegmentation data on a separate prediction head parallel\nto the matting head, we propose using segmentation data\nwithin the same head as matting for more effective supervi-\nsion. This is achieved by applying region-specific losses –\nfor core regions, we apply a pixel-wise loss to ensure stabil-\nity and generalization in semantics; for boundary regions,\nwhere segmentation data lacks alpha labels, we employ an\nimproved DDC loss [35], scaled to make edges resemble\nmatting rather than segmentation.\nIn summary, our main contributions are as follows:\n• We propose MatAnyone, a practical human video mat-\nting framework supporting target assignment, with sta-\nble performance in both semantics of core regions and\nfine-grained boundary details.\nTarget object(s) can be\neasily assigned using off-the-shelf segmentation methods,\nand reliable tracking is achieved even in long videos with\n2\n\nOurs\nRVM\nMaGGIe\n(a) Definitions for Matting\nCore Areas\nInput\nBoundary Area\n(b) Issues:               Segmentation prior broken                 Confused by ambiguous background\nMaGGIe\nRVM \nFigure 2. Definitions and motivations for MatAnyone. (a) In a matting frame, the image can be broadly divided into two areas based on\nthe alpha value: the core (semantic) and the boundary (fine-details). The core includes the background (alpha values of 0) and the solid\nforeground (alpha values of 1), while the boundary (highlighted in pink) encompasses areas with alpha values between 0 and 1. (b) Due to\nthe under-defined setting, auxiliary-free methods like RVM [33] are easily confused by ambiguous background. Meanwhile, mask-guided\nmethods like MaGGIe [22] tend to break the segmentation prior they aim to leverage, due to the deficiency in video matting data.\ncomplex and ambiguous backgrounds.\n• We introduce a consistent memory propagation mecha-\nnism via region-adaptive memory fusion, improving sta-\nbility in core regions and quality in boundary details.\n• We contribute larger and higher-quality datasets for train-\ning and testing, offering a solid foundation for robust\ntraining and reliable evaluation in video matting.\n• To overcome the scarcity of real video matting data, we\nleverage real segmentation data for core-area supervision,\nlargely improving semantic stability over prior methods.\n2. Related Work\nVideo Matting.\nDue to the intrinsic ambiguity in the\nauxiliary-free setting [24, 27, 33, 39, 57, 61], such tasks\ngenerally are object-specific. Among them, human video\nmatting [24, 27, 43, 61] without auxiliary inputs is popular\ndue to its wide applications. Challenging as the auxiliary-\nfree setting, being in the video domain brings in additional\ndifficulties in temporal coherency. MODNet [24] extends\nits portrait matting setting to video domain with a flicker-\ning reduction trick (non-learning) within a local sequence.\nRVM [33] steps further to design for videos specifically\nwith ConvGRU [1] as its recurrent architecture.\nRobust\nas RVM, it is still easy to be confused by humans in the\nbackground.\nWith the success of promptable segmenta-\ntion [25, 40, 58, 62], obtaining segmentation mask for a\ntarget human object only requires minimal human efforts.\nRecent mask-guided image [3, 29, 55, 56] and video mat-\nting [21, 22, 28, 31] thus leverages this convenience for\na more robust performance.\nAdam [31] propagates the\nfirst-frame segmentation mask across all frames while FTP-\nVM [21] propagates the first-frame trimap. Taking the prop-\nagated mask as a rough result, their decoder serves for mat-\nting details refinement.\nMaGGIe [22] enjoys a stronger\nprior by taking the segmentation mask across all frames in-\nstead of the first one. Taking all the segmentation masks at a\ntime, the network is able to perform bidirectional temporal\nfusion for coherency. To mitigate the poor generalizabil-\nity of synthetic video matting data, a common practice is to\nsimultaneously train with real segmentation data for seman-\ntics supervision [21, 31, 33].\nMemory-based VOS. Semi-supervised VOS segments the\ntarget object with a first-frame annotation across frames [8–\n12, 18, 30, 37, 42].\nThe memory matching paradigm\nby Space-Time Correspondence Network (STCN) [10] is\nwidely followed by current VOS methods [8, 12, 46, 51],\nand achieves good performance. We thus take the memory-\nbased paradigm as our framework since it is similar to our\nsetting except that our outputs are alpha mattes.\nVideo Consistency in Low-level Vision.\nTo enhance\ntemporal consistency across adjacent frames, the recur-\nrent frame fusion [47, 59] and optical flow-guided prop-\nagation [4–6] are commonly utilized in the video restora-\ntion networks. Recent methods also employ temporal lay-\ners such as 3D convolution [2, 48] and temporal atten-\ntion [2, 7, 49, 60] during training, while other training-free\nmethods resorts to cross-frame attention [50, 53] and flow-\nguided attention [13, 15] in the pretrained models. In this\nwork, we find that the memory-based paradigm is effective\nenough to maintain video consistency for video matting.\n3. Methodology\nOverview. Achieving matting-level details while preserv-\ning the semantic stability of a memory-based approach\nposes challenges, especially when training with suboptimal\nvideo matting data. To tackle this, we propose our MatAny-\none, as illustrated in Fig. 3.\nSimilar to semi-supervised\nVOS, MatAnyone only requires the segmentation mask for\nthe first frame as a target assignment (e.g., the yellow mask\nin Fig. 3(a)). The alpha matte for the assigned object is then\ngenerated frame by frame in a sequential manner. Specif-\nically, for an incoming frame t, it is first encoded into F t\nas ×16 downsampled feature representation, which is then\ntransformed into key and query for consistent memory prop-\nagation (Sec. 3.1), and output the pixel memory readout P t.\nWe employ the object transformer proposed by Cutie [12]\nto group the pixel memory by object-level semantics for ro-\nbustness against noise brought by low-level pixel matching.\n3\n\n(a) Overall Framework\n(c) Training Strategy\n(b) Consistent Memory Propagation\nEncoder\nObject \nTransformer\nConsistent\nMemory Propagation\n!!\n"!\n#!\n#0\n#t\n#N\n…\n…\n$!\nDecoder\nValue\nEncoder\nUpdate Alpha Memory\n%!\nMatting\nData\nsynthetic\nsmall scale\nw/ matting details\nSegment.\nData\nreal\nlarge scale\nw/o matting details\nSegmentation Data (w/o GT alpha matte)\nMatting Data (w/ GT alpha matte)\nMatting Loss\nOutput\nGT\nGT\nOutput\nUncertain Loss\nMatAnyone\nCertain Loss\n&"\n\'#, )#\n*"\nAttention\n+"\nkey\nvalue\nkey\nquery\nCurrent Frame\nLast Frame Memory\n\'"\n\'"$%\n)"$%\nUpdate Alpha Memory !! (every frame)\nUpdate Alpha Memory !! (every r-th frame)\n)"#\n,"\nkey\nvalue\nAlpha Memory Bank\nUncertainty \nPrediction\nMatAnyone\nFigure 3. An overview of MatAnyone. MatAnyone is a memory-based framework for video matting. Given a target segmentation map\nin the first frame, our model achieves stable and high-quality matting through consistent memory propagation, with a region-adaptive\nmemory fusion module to combine information from the previous and current frame. To overcome the scarcity of real video matting data,\nwe incorporate a new training strategy that effectively leverages matting data for fine-grained matting details and segmentation data for\nsemantic stability, with designed losses separately.\nThe refined memory readout Ot acts as the final feature to\nbe sent into the decoder for alpha matte prediction. The pre-\ndicted alpha matte M t is then encoded to memory value V t,\nwhich is used to update the alpha memory bank.\nDue to limitations in the quality and quantity of video\nmatting data, training with such data makes it difficult to\nachieve satisfactory stability in core regions. To mitigate\nthis, RVM [33] proposes a parallel head for real segmenta-\ntion data alongside the matting head, guiding the network\nto be robust in real-world cases. However, this is not suffi-\ncient, as the matting head itself cannot receive supervision\nfrom real data. Inspired by the DDC loss [35] designed for\nalpha-free image matting, we devise a training strategy for\ncore regions, which provides direct supervision to the mat-\nting head with segmentation data (Sec. 3.2), leading to sub-\nstantial improvements in semantic stability.\nWe also propose two practical inference strategies that\nallow for flexible application, 1) a recurrent refinement ap-\nproach based on the memory-driven paradigm, and 2) an\nauxiliary-free variant that eliminates the need for a target\nsegmentation mask in the first frame (Sec. 3.3).\n3.1. Consistent Memory Propagation\nAlpha Memory Bank. In this study, we introduce a con-\nsistent memory propagation (CMP) module specifically de-\nsigned for video matting, as illustrated in Fig.3(b). Exist-\ning memory-based VM methods store either segmentation\nmasks [31] or trimaps [21] in memory and use a decoder\nto refine the matting details. Such approaches do not fully\nleverage the stability provided by the memory paradigm\nin boundary regions, leading to instability such as flicker-\ning. To address this, building on the memory-based frame-\nwork [10], our MatAnyone stores the alpha matte in an al-\npha memory bank to enhance stability in boundary regions.\nRegion-Adaptive Memory Fusion. Given the inherent dif-\nference between the segmentation map (values of 0 or 1)\nand the matting map (values between 0 and 1), the memory-\nmatching approach needs to be adjusted. Specifically, in\nSTCN [10], memory values for the query frame are based\non the similarity between query and memory key, assum-\ning equal importance for all query tokens. However, this\nassumption does not hold for video matting. As shown in\nFig. 2(a), a query frame can be divided into core and bound-\nary regions. When compared with frame t −1, only a small\nfraction of tokens in frame t change significantly in alpha\nvalues, with these “large-change” tokens mainly located in\nobject boundaries, while the “small-change” tokens reside\nin the core regions. This highlights the need to treat core\nand boundary regions separately to enforce stability.\nSpecifically, we introduce a boundary-area prediction\nmodule to estimate the change probability Ut of each query\ntoken for adaptive memory fusion, where higher Ut indi-\ncates “large-change” regions and lower Ut indicates “small-\nchange” regions.\nThe prediction module is lightweight,\n4\n\nconsisting of three convolution layers. We formulate the\nprediction as a binary segmentation problem with loss\nLbin seg and use the actual alpha change between frame\nt −1 and t as supervision. Specifically, we define U GT\nt\n:\n|M GT\nt−1 −M GT\nt\n| >= δ, where δ is a threshold. Using the\noutput of the module ˆUt, we compute the binary cross en-\ntropy loss against U GT\nt\n. During the region-adaptive mem-\nory fusion process, we apply the sigmoid function on ˆUt to\ntransform it as a probability. The final pixel memory read-\nout is a soft merge:\nPt = V m\nt\n∗Ut + Vt−1 ∗(1 −Ut),\n(1)\nwhere Ut ∈[0, 1], V m\nt\nare current values queried from\nmemory bank, and Vt−1 are values propagated from the\nlast frame. This approach significantly improves stability\nin core regions by maintaining internal completeness and a\nclean background (Fig. 2(b) and Fig. 4). It also enhances\nstability in boundary regions, as it directs the network to fo-\ncus on object boundaries with soft alpha values, while the\nmemory-based paradigm inherently stabilizes the matched\nvalues (see Table 3(c)). A detailed analysis is provided in\nthe ablation study of Sec. 5.2 and Sec. J.2.\n3.2. Core-area Supervision via Segmentation\nNew Training Scheme. Most recent video matting meth-\nods follow RVM’s approach of using real segmentation data\nto address the limitations of video matting data. In these\nmethods, segmentation and matting data are fed to the main\nshared network, but are directed to produce outputs at sep-\narate heads. Although segmentation data do supervise the\nmain network to empower generalizability and robustness\nto the matting model, the stability they provide falls short\nof what a VOS model could achieve. As shown in Fig. 2,\nboth RVM and MaGGIe perform significantly worse than\nthe VOS outputs (white masks on inputs) by XMem [8] in\ncore areas, where semantic information is key. We believe\nthe parallel head training scheme may not fully exploit the\nrich segmentation prior in the data. To address this, we pro-\npose to supervise the matting head directly with segmenta-\ntion data. Specifically, we predict the alpha matte for seg-\nmentation inputs and optimize the matting outputs accord-\ningly, as illustrated in Fig. 3(c).\nScaled DDC Loss.\nA natural challenge arises with the\naforementioned approach: how can we compute the loss\non matting outputs for segmentation data when there is no\nground truth (GT) alpha matte? For core areas, the GT la-\nbels are readily available in the segmentation data, where\nan l1 loss suffices, and we denote it as Lcore. The real dif-\nficulty lies in the boundary region. A recent paper proposes\nDDC loss [35], which supervises boundary areas using the\ninput image without requiring a GT alpha matte.\nLDDC = 1\nN\nN\nP\ni\nP\nj\n|αi −αj −∥Ii −Ij∥2|,\nj ∈argtopk{−∥Ii −Ij∥2}.\n(2)\nHowever, we find that the underlying assumption of this de-\nsign, that ∥αi −αj∥2 = ∥Ii −Ij∥2 for αi > αj, does\nnot always hold true. For two image pixels Ii and Ij, their\ndifference is given by:\nIi −Ij = [αiFi +(1−αi)Bi]−[αjFj +(1−αj)Bj], (3)\nwhere Fi, Bi represent the foreground and background val-\nues at pixel i, and similarly for Fj and Bj at pixel j. Since\nwe impose the constraint j ∈argtopk{−∥Ii −Ij∥2}, we\ncan assume Fi = Fj = F, Bi = Bj = B within a small\nwindow. This simplifies Eq. (3) to:\nIi −Ij = (αi −αj)(F −B).\n(4)\nThis shows that the assumptions for DDC loss hold only\nwhen |F −B| = 1. To account for this, we devise a scaled\nversion as our boundary loss Lboundary:\nLboundary = 1\nN\nN\nP\ni\nP\nj\n|(αi −αj)(F −B) −∥Ii −Ij∥2|,\nj ∈argtopk{−∥Ii −Ij∥2},\n(5)\nwhere F is approximated by the average of the top k largest\npixel values in the small window, and B by the average\nof the top k smallest pixel values. In the ablation study\n(Sec. 5.2), we show that training with our scaled DDC loss\n(Eq. (5)) yields more natural matting results than training\nwith the original version (Eq. (2)), which tends to produce\nsegmentation-like jagged and stair-stepped edges.\n3.3. Practical Inference Strategies\nRecurrent Refinement.\nThe first-frame matte is pre-\ndicted from the given first-frame segmentation mask, and\nits quality will affect the matte prediction for the subse-\nquent frames. The sequential prediction in the memory-\nbased paradigm enables recurrent refinement during infer-\nence. Leveraging this mechanism, we introduce an optional\nfirst-frame warm-up module for inference. Specifically, we\nrepeat the first frame n times, treating each repetition as\nthe initial frame, and use only the nth alpha output as the\nfirst frame to initialize the alpha memory bank. This (1)\nenhances robustness against the given segmentation mask\nand (2) refines matting details in the first frame to achieve\nimage-matting quality (see Fig. 6 and Fig. 13).\nAuxiliary-free Variant. To enable comparison with an ar-\nbitrary auxiliary-free video matting approach, we design an\nauxiliary-free version by removing the segmentation prior\nfrom the initial frame. Instead, we use the first-frame al-\npha matte generated by an auxiliary-free method of interest,\nsuch as RVM, and binarize it as the given mask for our set-\nting. Table 1 presents a comparison between RVM and our\nauxiliary-free variant (Ours-AF) on synthetic benchmarks.\n4. Data\nWe briefly introduce our new training datasets and bench-\nmarks for evaluation, including both synthetic and real-\n5\n\nTable 1. Quantitative comparisons on different video matting benchmarks from diverse sources. The best and second-best performances\nare marked in red and orange , respectively. † indicates that MaGGIe [22] requires the instance mask as guidance for each frame, while\nour method only requires it in the first frame.\nMetrics\nAuxiliary-free (AF) Methods\nMask-guided Methods\nMODNet [24]\nRVM [33]\nRVM-Large [33]\nOurs-AF\nAdaM [31]\nFTP-VM [20]\nMaGGIe† [22]\nOurs\nVideoMatte (512 × 288)\nMAD↓\n9.41\n6.08\n5.32\n5.99\n5.30\n6.13\n5.49\n5.07\nMSE↓\n4.30\n1.47\n0.62\n1.72\n0.78\n1.31\n0.60\n0.87\nGrad↓\n1.89\n0.88\n0.59\n0.88\n0.72\n1.14\n0.57\n0.62\ndtSSD↓\n2.23\n1.36\n1.24\n1.10\n1.33\n1.60\n1.39\n1.16\nConn↓\n0.81\n0.41\n0.30\n0.38\n0.30\n0.41\n0.31\n0.25\nVideoMatte (1920 × 1080)\nMAD↓\n11.13\n6.57\n5.81\n5.66\n4.42\n8.00\n4.42\n4.27\nMSE↓\n5.54\n1.93\n0.97\n1.68\n0.39\n3.24\n0.40\n0.36\nGrad↓\n15.30\n10.55\n9.65\n5.75\n5.12\n23.75\n4.03\n4.04\ndtSSD↓\n3.08\n1.90\n1.78\n1.27\n1.39\n2.37\n1.31\n1.24\nYoutubeMatte (512 × 288)\nMAD↓\n19.37\n4.08\n3.36\n3.95\n-\n3.08\n3.54\n2.57\nMSE↓\n16.21\n1.97\n1.04\n2.25\n-\n1.29\n1.23\n0.94\nGrad↓\n2.05\n1.34\n1.03\n1.26\n-\n1.16\n1.10\n0.91\ndtSSD↓\n2.79\n1.81\n1.62\n1.52\n-\n1.83\n1.88\n1.53\nConn↓\n2.68\n0.60\n0.50\n0.57\n-\n0.41\n0.49\n0.36\nYoutubeMatte (1920 × 1080)\nMAD↓\n15.29\n4.37\n3.50\n3.70\n-\n6.49\n2.37\n2.05\nMSE↓\n12.68\n2.25\n1.19\n2.35\n-\n4.58\n0.98\n0.76\nGrad↓\n8.42\n15.1\n12.64\n11.45\n-\n29.78\n7.69\n9.67\ndtSSD↓\n2.74\n2.28\n2.08\n1.81\n-\n2.41\n1.77\n1.75\nworld. More details are provided in the appendix (Sec. I).\n4.1. Training Datasets\nTo address limitations in video matting datasets in both\nquality and quantity, we collect abundant green screen\nvideos, process them with Adobe After Effects, and conduct\nmanual selection to remove common artifacts also found in\nVideoMatte240K [32] (see Fig. 8). Compared to Video-\nMatte240K, our dataset, VM800, is (1) twice as large, (2)\nmore diverse in terms of hairstyles, outfits, and motion,\nand (3) higher in quality. Ablation studies (Table 3(b) and\nSec. J.1) further demonstrate the advantages of our dataset.\n4.2. Synthetic Benchmark\nThe standard benchmark, VideoMatte [32], derived from\nVideoMatte240K, includes only 5 unique foreground\nvideos, which is under representative. Additionally, their\nforegrounds lack sufficient boundary details, limiting their\nability to discern matting precision in boundary regions. To\ncreate a more comprehensive benchmark, we compile 32\ndistinct 1920 × 1080 green-screen foreground videos from\nYouTube, and process them similarly to our training dataset.\nOur benchmark, YouTubeMatte, provides enhanced detail\nrepresentation, as reflected by higher Grad [41] values.\n4.3. Real-world Benchmark and Metric\nReal-world benchmarks are essential to facilitate the prac-\ntical use of video matting models.\nAlthough real-world\nvideos lack ground truth (GT) alpha mattes, we can generate\nframe-wise segmentation masks as GT for core areas ben-\nefiting from the high capability of existing VOS methods.\nSpecifically, we select a subset of 25 real-world videos [33]\n(100 frames each) with high-quality core GT masks verified\nmanually. MAD, MSE, and dtSSD [14] are then calculated\nat the core region as core region metrics, representing se-\nmantic stability that is critical for visual perception.\n5. Experiments\nTraining Schedule.\nStage 1.\nFollowing the practice of\nRVM [33], we start by training the entire model on our\nVM800 for 80k iterations. The sequence length is initially\nset to 3 and extended to 8 with increasing sampling intervals\nfor more complex scenarios. Stage 2. As the key stage, we\napply the core supervision training strategy introduced in\nSection 3.2. Real segmentation data COCO [34], SPD [45]\nand YouTubeVIS [52] are added for supervising the matting\nhead. The loss function applied are specified in Section 3.2.\nStage 3. Finally, we fine-tune the model with image matting\ndata D646 [39] and AIM [26] for finer matting details.\n6\n\nVideo Frame\nRVM\nFTP-VM\nMaGGIe\nOurs\nFrame t\nFrame t+5\nFigure 4. Qualitative comparisons on real-world videos. Our MatAnyone significantly outperforms existing auxiliary-free (RVM [33]) and\nmask-guided (FTP-VM [21] and MaGGIe [22]) approaches in both detail extraction and semantic accuracy. For the lowest row, while other\nmethods all miss out on important body parts (i.e., head) and mistakenly take background pixels as foreground (due to similar colors), thus\ngenerating messy outputs, our method presents an accurate and visually clean output by even identifying the shadow near the boundary.\n5.1. Comparisons\nWe compare MatAnyone with several state-of-the-art meth-\nods, including auxiliary-free (AF) methods: MODNet [24],\nRVM [33], and RVM-Large [33], and mask-guided meth-\nods: AdaM [31], FTP-VM [21], and MaGGIe [22].\nTable 2. Quantitative comparisons on real-world benchmark [33].\nThe best and second performances are marked in\nred\nand\norange , respectively.\nMethods\nMAD↓\nMSE↓\ndtSSD↓\nAuxiliary-free\nMODNet [24]\n11.67\n10.12\n3.37\nRVM [33]\n1.21\n0.77\n1.43\nRVM-Large [33]\n0.95\n0.50\n1.30\nMask-guided\nFTP-VM [21]\n4.77\n4.11\n1.68\nMaGGIe [22]\n1.94\n1.53\n1.63\nMatAnyone (Ours)\n0.18\n0.11\n0.95\n5.1.1\nQuantitative Evaluations\nSynthetic Benchmarks. For a comprehensive evaluation\non synthetic benchmarks, we employ MAD (mean abso-\nlute difference) and MSE (mean squared error) for seman-\ntic accuracy, Grad (spatial gradient) [41] for detail extrac-\ntion, Conn (connectivity) [41] for perceptual quality, and\ndtSSD [14] for temporal coherence. In Table 1, our method\nachieves the best MAD and dtSSD across all datasets at\nboth high and low resolutions, demonstrating exceptional\nspatial accuracy for alpha mattes and remarkable temporal\nstability. Our auxiliary version, which shares the same first-\nframe prediction as RVM, outperforms RVM in both dtSSD\nand Conn metrics across all datasets, highlighting the ad-\nvantages of our design in stability and visual quality.\nReal Benchmark. For evaluation on real benchmarks, we\nTable 3. Ablation study of the new training dataset (New Data),\nconsistent memory propagation module (CMP), and new training\nscheme (New Training) on real benchmark (about 1080p).\nExp.\nNew Data\nCMP\nNew Training\nMAD↓\nMSE↓\ndtSSD↓\n(a)\n3.16\n2.65\n1.37\n(b)\n✓\n2.55\n2.25\n1.36\n(c)\n✓\n✓\n1.85\n1.67\n1.25\n(d)\n✓\n✓\n✓\n0.42\n0.34\n0.94\nuse the core region metrics in Section 4.3.\nIn Table 2,\nour method demonstrates superior generalizability on real\ncases, achieving the best metric values with a substantial\nmargin over both auxiliary-free and mask-guided methods.\n5.1.2\nQualitative Evaluations\nVisual results on real-world videos are in Fig. 4 and Fig. 5.\nGeneral Video Matting. MatAnyone outperforms existing\nauxiliary-free and mask-guided approaches in both detail\nextraction (boundary) and semantic accuracy (core). Fig. 4\nshows that MatAnyone excels at fine-grained details (e.g.,\nhair in the middle row) and differentiates full human body\nagainst complicated or ambiguous backgrounds when fore-\nground and background colors are similar (e.g., last row).\nInstance Video Matting. The assignment of target object\nat the first frame gives us flexibility for instance video mat-\nting. In Fig. 5, although MaGGIe [22] benefits from us-\ning instance masks as guidance for each frame, our method\ndemonstrates superior performance in instance video mat-\nting, particularly in maintaining object tracking stability and\npreserving fine-grained details of alpha mattes.\n5.2. Ablation Study\nEnhancement from New Training Data. In Table 3, by\ncomparing (a) and (b), it is observed that training with\nnew data noticeably improves the semantic performance\n7\n\nMaGGIe\nOurs\nVideo Frame\nInstance #1\n#1\n#3\n#2\nInstance #2\nInstance #3\n#1\n#3\n#2\nMaGGIe\nOurs\nVideo Frame\nInstance #1\nInstance #2\nInstance #3\nFigure 5. Quantitative comparisons with MaGGIe [22] on instance video matting. Despite MaGGIe using instance mask as guidance for\neach frame, our method shows better performance, achieving better stability in object tracking and finer alpha matte details.\nVideo Frame\nSegmentation Mask\n!! = 1\n!! = 2\n!! = 5\n!! = 10\nFigure 6. Improvement with Recurrent refinement. (Zoom-in for best view)\nVideo Frames\nDDC Loss\nScaled DDC loss\nFigure 7. Comparison of matting results training with original\nDDC loss [35] and with scaled DDC loss, where the latter gives\nmore stable and natural matting results.\nwith decreased MAD and MSE, showing that our newly-\ncollected VM800 indeed contributes to robust training with\nits upgraded quantity, quality, and diversity.\nEffectiveness of Consistent Memory Propagation. We\nfurther investigate the effectiveness of the consistent mem-\nory propagation (CMP) module. From Table 3 (b) to (c), im-\nprovement can be seen across all metrics with CMP added,\nindicating its effectiveness in improving semantic stability\nand temporal coherency. In particular, dtSSD in (c) is al-\nready lower than all the other methods in Table 2, showing\nthe superiority of CMP in terms of temporal consistency.\nEffectiveness of New Training Scheme. Our new training\nscheme brings our model to the next level with a noticeable\nimprovement in all metrics. It already outperforms all the\nother methods in Table 2 without further fine-tuning.\nScaled DDC Loss. We examine the merit of the scaled\nversion of DDC loss by training with Lcore and Lboundary\nonly to maximize its effect. In Fig. 7, training with vanilla\nDDC loss produces segmentation-like jaggedness, espe-\ncially among the boundary region. Our scaled DDC loss\nyields more stable and natural matting results.\nEffectiveness of Recurrent Refinement. Fig. 6 shows the\neffectiveness of recurrent refinement in a progressive man-\nner. Given a rough segmentation mask, our method can pro-\nduce alpha matte with descent details within 10 iterations.\n6. Conclusion\nWe introduce MatAnyone, a practical framework for target-\nassigned human video matting that ensures stable and ac-\ncurate results across diverse real-world scenarios.\nOur\nmethod leverages a region-adaptive memory fusion ap-\nproach, which combines memory from previous frames to\nmaintain semantic consistency in core areas while preserv-\ning fine details along object boundaries. With a new train-\ning dataset that is larger, high-quality, and diverse and a\nnovel training strategy that effectively leverages segmenta-\ntion data, MatAnyone achieves robust and stable matting\nperformance, even with complex backgrounds. These ad-\nvancements position MatAnyone a practical solution for\nreal-world video matting, also setting a solid foundation for\nfuture research in memory-based video processing.\n8\n\nReferences\n[1] Nicolas Ballas, Li Yao, Christopher J Pal, and Aaron\nCourville. Delving deeper into convolutional networks for\nlearning video representations. In ICLR, 2016. 3\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your Latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 3\n[3] Huanqia Cai, Fanglei Xue, Lele Xu, and Lili Guo. Trans-\nMatting: Enhancing transparent objects matting with trans-\nformers. In ECCV, 2022. 3\n[4] Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, and\nChen Change Loy. BasicVSR: The search for essential com-\nponents in video super-resolution and beyond.\nIn CVPR,\n2021. 3\n[5] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Improving video super-resolution with\nenhanced propagation and alignment. In CVPR, 2022.\n[6] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy.\nInvestigating tradeoffs in real-world\nvideo super-resolution. In CVPR, 2022. 3\n[7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideoCrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 3\n[8] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In ECCV, 2022. 2, 3, 5, 12, 13, 14, 16\n[9] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular\ninteractive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR, 2021.\n[10] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethink-\ning space-time networks with improved memory coverage\nfor efficient video object segmentation. In NeurIPs, 2021. 2,\n3, 4, 12\n[11] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander\nSchwing, and Joon-Young Lee. Tracking anything with de-\ncoupled video segmentation. In ICCV, 2023.\n[12] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young\nLee, and Alexander Schwing. Putting the object back into\nvideo object segmentation. In CVPR, 2024. 2, 3, 12, 13, 14,\n16\n[13] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen,\nJiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo\nRosenhahn, Tao Xiang, and Sen He.\nFLATTEN: optical\nflow-guided attention for consistent text-to-video editing. In\nICLR, 2024. 3\n[14] Mikhail Erofeev, Yury Gitman, Dmitriy S Vatolin, Alexey\nFedorov, and Jue Wang. Perceptually motivated benchmark\nfor video matting. In BMVC, 2015. 6, 7, 16\n[15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenFlow:\nConsistent diffusion features for consistent\nvideo editing. In ICLR, 2024. 3\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 12\n[17] Qiqi Hou and Feng Liu. Context-aware image matting for si-\nmultaneous foreground and alpha estimation. In ICCV, 2019.\n13\n[18] Li Hu, Peng Zhang, Bang Zhang, Pan Pan, Yinghui Xu,\nand Rong Jin. Learning position and target consistency for\nmemory-based video object segmentation. In CVPR, 2021.\n3\n[19] Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing.\nMaskRNN: Instance level video object segmentation.\nIn\nNeurIPS, 2017. 2\n[20] Wei-Lun Huang and Ming-Sui Lee. End-to-end video mat-\nting with trimap propagation. In CVPR, 2023. 6\n[21] Wei-Lun Huang and Ming-Sui Lee. End-to-end video mat-\nting with trimap propagation. In CVPR, 2023. 2, 3, 4, 7, 13,\n18, 20, 21, 22\n[22] Chuong Huynh, Seoung Wug Oh, , Abhinav Shrivastava, and\nJoon-Young Lee. MaGGIe: Masked guided gradual human\ninstance matting. In CVPR, 2024. 2, 3, 6, 7, 8, 13, 18, 20,\n21, 22\n[23] Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson WH\nLau. Harmonizer: Learning to perform white-box image and\nvideo harmonization. In ECCV, 2022. 16\n[24] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Ryn-\nson W.H. Lau. MODNet: Real-time trimap-free portrait mat-\nting via objective decomposition. In AAAI, 2022. 2, 3, 6, 7\n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 3, 19\n[26] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic\nnatural image matting. In IJCAI, 2021. 6\n[27] Jiachen\nLi,\nVidit\nGoel,\nMarianna\nOhanyan,\nShant\nNavasardyan, Yunchao Wei, and Humphrey Shi. VMFormer:\nEnd-to-end video matting with transformer. In WACV, 2024.\n2, 3\n[28] Jiachen Li,\nRoberto Henschel,\nVidit Goel,\nMarianna\nOhanyan, Shant Navasardyan, and Humphrey Shi.\nVideo\ninstance matting. In WACV, 2024. 3\n[29] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting Any-\nthing. In CVPR, 2024. 3\n[30] Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang\nCheng, Jiangmiao Pang, and Chen Change Loy. Tube-link:\nA flexible cross tube framework for universal video segmen-\ntation. In ICCV, 2023. 3\n[31] Chung-Ching Lin, Jiang Wang, Kun Luo, Kevin Lin, Linjie\nLi, Lijuan Wang, and Zicheng Liu. Adaptive human matting\nfor dynamic videos. In CVPR, 2023. 2, 3, 4, 6, 7, 13\n[32] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta,\nBrian L Curless, Steven M Seitz, and Ira Kemelmacher-\nShlizerman. Real-time high-resolution background matting.\nIn CVPR, 2021. 2, 6, 12, 14, 15, 16, 17\n[33] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip\nSengupta. Robust high-resolution video matting with tempo-\nral guidance. In WACV, 2022. 1, 2, 3, 4, 6, 7, 13, 16, 18, 20,\n21, 22\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 6, 13\n9\n\n[35] Wenze Liu, Zixuan Ye, Hao Lu, Zhiguo Cao, and Xiangyu\nYue. Training matting models without alpha labels. arXiv\npreprint arXiv:2408.10539, 2024. 2, 4, 5, 8\n[36] I Loshchilov. Decoupled weight decay regularization. In\nICLR, 2019. 12\n[37] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV, 2019. 3\n[38] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV, 2019. 2\n[39] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang\nXu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hier-\narchical structure aggregation for image matting. In CVPR,\n2020. 3, 6\n[40] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll´ar, and Christoph Feicht-\nenhofer. SAM 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714, 2024. 3\n[41] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit\nGelautz, Pushmeet Kohli, and Pamela Rott. A perceptually\nmotivated online benchmark for image matting. In CVPR,\n2009. 6, 7\n[42] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized\nmemory network for video object segmentation. In ECCV,\n2020. 3\n[43] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and\nJiaya Jia. Deep automatic portrait matting. In ECCV, 2016.\n3\n[44] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, and\nYu-Wing Tai. Deep video matting via spatio-temporal align-\nment and aggregation. In CVPR, 2021. 13\n[45] Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid.\nLearning video object segmentation with visual memory. In\nICCV, 2017. 6, 13\n[46] Haochen Wang, Xiaolong Jiang, Haibing Ren, Yao Hu, and\nSong Bai. Swiftnet: Real-time video object segmentation. In\nCVPR, 2021. 3\n[47] Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, and\nChen Change Loy. EDVR: Video restoration with enhanced\ndeformable convolutional networks. In CVPRW, 2019. 3\n[48] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. VideoComposer: Compositional video synthesis\nwith motion controllability. In NeurIPS, 2024. 3\n[49] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yum-\ning Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua\nLin, Yu Qiao, and Ziwei Liu. LaVie: High-quality video\ngeneration with cascaded latent diffusion models. In IJCV,\n2024. 3\n[50] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-A-Video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, 2023. 3\n[51] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping\nZhang, and Wenxiu Sun. Efficient regional memory network\nfor video object segmentation. In CVPR, 2021. 2, 3\n[52] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-\nmentation. In ICCV, 2019. 6, 13\n[53] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change\nLoy.\nRerender A Video: Zero-shot text-guided video-to-\nvideo translation. In SIGGRAPH Asia, 2023. 3\n[54] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating ob-\njects with transformers for video object segmentation.\nIn\nNeurIPS, 2021. 14\n[55] Jingfeng Yao,\nXinggang Wang,\nShusheng Yang,\nand\nBaoyuan Wang.\nViTMatte: Boosting image matting with\npre-trained plain vision transformers. Information Fusion,\n2024. 3\n[56] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu.\nMatte Anything: Interactive natural image matting with seg-\nment anything model. Image and Vision Computing, page\n105067, 2024. 3, 17, 19\n[57] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing\nHuang, Hujun Bao, and Weiwei Xu. A late fusion cnn for\ndigital matting. In CVPR, 2019. 3\n[58] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai.\nEdgeSAM: Prompt-in-the-loop distillation for on-device de-\nployment of sam. arXiv preprint, 2023. 3\n[59] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie,\nWangmeng Zuo, and Jimmy Ren.\nSpatio-temporal filter\nadaptive network for video deblurring. In ICCV, 2019. 3\n[60] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo,\nand Chen Change Loy.\nUpscale-A-Video:\nTemporal-\nconsistent diffusion model for real-world video super-\nresolution. In CVPR, 2024. 3\n[61] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo\nZhang, and Ming Tang. Fast deep matting for portrait ani-\nmation on mobile phone. In ACMMM, 2017. 3\n[62] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae\nLee. Segment everything everywhere all at once. In NeurIPS,\n2024. 3\n10\n\nAppendix\nIn this supplementary material, we provide additional discussions and results to supplement the main paper. In Sec-\ntion G, we present the network details of our MatAnyone. In Section H, we discuss more training details, including training\nschedules, training augmentations, and loss functions. In Section I, we provide more details on our new training and testing\ndatasets, including the generation pipeline and some examples for demonstration. We present comprehensive results in Sec-\ntion J to further show our performance, including those for ablation studies and qualitative comparisons. It is noteworthy that\nwe also include a demo video (Section J.6) to showcase a Hugging Face demo and additional results on real-world cases in\nvideo format.\nContents\n1. Introduction\n2\n2. Related Work\n3\n3. Methodology\n3\n3.1. Consistent Memory Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.2. Core-area Supervision via Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.3. Practical Inference Strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4. Data\n5\n4.1. Training Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2. Synthetic Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.3. Real-world Benchmark and Metric\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5. Experiments\n6\n5.1. Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.1.1\nQuantitative Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.1.2\nQualitative Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.2. Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n6. Conclusion\n8\nG. Architecture\n12\nG.1. Network Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nH. Training\n12\nH.1. Training Schedules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nH.2. Training Augmentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nH.3. Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nI. Dataset\n14\nI.1 . New Training Dataset - VM800 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nI.2 . New Test Dataset - YouTubeMatte . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nI.3 . Real Benchmark and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nJ. More Results\n17\nJ.1 . Enhancement from New Training Data\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.2 . Effectiveness of Consistent Memory Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.3 . Effectiveness of New Training Scheme\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.4 . Effectiveness of Recurrent Refinement\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nJ.5 . More Qualitative Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nJ.6 . Demo Video\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n11\n\nG. Architecture\nG.1. Network Designs\nAs illustrated in Fig. 3 in the main paper, our MatAnyone mainly has five important components: (1) an encoder for key and\nquery transformation, (2) a consistent memory propagation module for pixel memory readout, (3) an object transformer [12]\nfor memory grouping by object-level semantics, (4) a decoder for alpha matte decoding, (5) a value encoder for alpha matte\nencoding, which is used to update the alpha memory bank.\nEncoder. We adopt ResNet-50 [16] for encoder following common practices in memory-based VOS [8, 10, 12]. Discarding\nthe last convolution stage, we take ×16 downsampled feature as F t for key and query transformation, while features at scales\n×8, ×4, ×2, and ×1 are used as skip connections for the decoder.\nConsistent Memory Propagation. The process of consistent memory propagation is detailed in Fig. 3(b) in the main paper.\nAlpha memory bank serves as the main working memory for past information query as in [8, 12], which is updated every rth\nframe across the whole time span. The query of the current frame to the alpha memory bank is implemented in an attention\nmanner following [8, 12]. For the query QHW ×C 1 and alpha memory bank KT HW ×C, V T HW ×Cv 2, the affinity matrix\nA ∈[0, 1]HW ×T HW of the query to alpha memory is computed as:\nAij =\nexp(d(Qi, Kj))\nP\nz exp(d(Qi, Kz)),\n(6)\nwhere d(·, ·) is the anisotropic L2 function, H and W are the height and width at ×16 downsampled input scale, and T is\nthe number of memory frames stored in alpha memory bank. The queried values V m\nt\nin Fig. 3(b) in the main manuscript is\nobtained as:\nV m\nt\n= AVm.\n(7)\nIn addition to that, we also maintain last frame memory solely for the uncertainty prediction module we propose, and it is\nupdated every frame. The boundary-area prediction module is lightweight with one 1 × 1 convolution and two 3 × 3\nconvolutions. By taking the input of a concatenation of current frame feature Kt, last frame feature Kt−1, and last alpha\nmatte prediction Mt−1, it outputs a one-channel change probability mask Ut of each query token, where higher Ut indicates\nsuch token is likely to change more in the alpha value compared with Mt−1. As mentioned in Sec. 3.1 in the manuscript, the\nground truth Ut label is obtained by: U GT\nt\n: |M GT\nt−1 −M GT\nt\n| >= δ, where δ is set at 0 for segmentation data, and 0.001 for\nmatting data as noise tolerance. Since Ut is predicted at a ×16 downsampled scale in the memory space, the ground truth\nmask U GT\nt\nis also downsampled in the mode of area.\nObject Transformer. Our object transformer is derived from Cutie [12] with three consecutive object transformer blocks.\nPixel memory readout P t obtained from the consistent memory propagation module is then grouped through several attention\nlayers and feed-forward networks. In this way, the noise brought by low-level pixel matching could be effectively reduced\nfor a more robust matching against distractors. We do not claim contributions for this module.\nDecoder. Our decoder is inspired by common practices in VOS [8, 12] with modified designs specifically for the matting\ntasks. The mask decoder is VOS generally consists of two interactive upsampling from ×16 to ×4, and then a bilinear\ninterpolation is applied to the input scale. However, since the boundary region for an alpha matte requires much more\nprecision than a segmentation mask, we enrich the decoder with two more upsampling layers until ×1, where skip connections\nfrom the encoder are applied at each scale to enhance the boundary precision.\nValue Encoder. Similar to the encoder, we adopt ResNet-18 [16] for value encoder following common practices in memory-\nbased VOS [8, 10, 12]. Different from the encoder for key and query, the value encoder takes the predicted alpha matte M t\nas well as the image features as input, the encoded values are then used to update the alpha memory bank and last frame\nmemory according to their updating rules.\nH. Training\nH.1. Training Schedules\nStage 1. To initialize our model on memory propagation learning, we train with our new video matting data VM800, which\nis of larger scale, higher quality, and better diversity than VideoMatte240K [32]. We use the AdamW [36] optimizer with a\nlearning rate of 1 × 10−4 with a weight decay 0.001. The batch size is set to 16. We train with a short sequence length of\n3 for 80K first, and then we train with a longer sequence length of 8 for another 5K for more complex scenarios. Video and\n1We ignore the subscript t in Qt for simplicity\n2We ignore the subscript m in Km and Vm for simplicity\n12\n\nTable 4. Training settings and losses used in different training stages. † indicates that segmentation loss is computed as an auxiliary loss\non a segmentation head, which will be abandoned during inference. Other than that, matting loss and core supervision loss are computed\non the matting head for semantic stability in core regions and matting details in the boundary region.\nTraining Stage\n#Iterations\nMatting Data\nSegmentation Data\nSequence Length\nMatting Loss\nSegmentation Loss†\nCore Supervision Loss\nStage 1\n85K\nvideo\nimage & video\n3 (80K) →8 (5K)\n✓\n✓\nStage 2\n40K\nvideo\nimage & video\n8\n✓\n✓\n✓\nStage 3\n5K\nimage\nimage & video\n8\n✓\n✓\n✓\nimage segmentation data COCO [34], SPD [45] and YouTubeVIS [52] are used to train the segmentation head parallel to the\nmatting head at the same time, as previous practices [21, 31, 33].\nStage 2. We apply our key training strategy - core-area supervision in this stage. On the basis of the previous stage, we add\nadditional supervision on the matting head with segmentation data to enhance the semantics robustness and generalizability\ntowards real cases. In this stage, the learning rate is set to be 1 × 10−5, and we train with a sequence length of 8 for 40K for\nboth matting and segmentation data.\nStage 3. Due to the inferior quality of video matting data compared with image matting data annotated by humans, we\nfinetune our model with image matting data instead for 5K with a 1 × 10−6 learning rate. Noticeable improvements in\nmatting details, especially among boundary regions, could be seen after this stage.\nH.2. Training Augmentations\nAugmentations for Training Data. As discussed in the manuscript, video matting data are deficient in quantity and diversity.\nIn order to enhance training data variety during the composition process, we follow RVM [33] to apply motion (e.g., affine\ntranslation, scale, rotation, etc.) and temporal (e.g., clip reversal, speed changes, etc.) augmentations to both foreground\nand background videos. Motion augmentations applied to image data also serve to synthesize video sequences from images,\nmaking it possible to fine-tune with higher-quality image data for details.\nAugmentations for Given Mask. Since our setting is to receive the segmentation mask for the first frame and make alpha\nmatte prediction for all the frames including the first one, it is important to have our model robust to the given mask. To\ngenerate the given mask in the training pipeline, we first obtain the original given mask. For segmentation data, it is just the\nground truth (GT) for the first frame, while for matting data, it is the binarization result on the first-frame GT alpha matte,\nwith a threshold of 50. Erosion or dilation is then applied with a probability of 40% each, with kernel sizes ranging from 1\nto 5. In this way, we force the model to learn alpha predictions based on an inaccurate segmentation mask, also enhancing\nthe model robustness towards memory readout if it is not so accurate during the predictions in following frames.\nAugmentations for Assigned Object(s). The assignment of target object(s) as a segmentation mask for the first frame gives\nus flexibility for instance video matting. Given the strong prior, the model is still easy to be confused by other salient humans\nnot assigned as target. To solve this, we find that a small modification in the video segmentation data pipeline has an obvious\neffect. In YouTubeVIS [52], for each video with human existence, suppose the number of human instances is H. Instead of\ncombining all of them as one object (practice in previous auxiliary-free methods [33]), we randomly take h ≤H instance as\nforeground, while unchosen instances are marked as background. In this way, we force the model to distinguish the target\nhuman object(s) even when other salient human object(s) exist, enhancing the robustness in object tracking for instance video\nmatting even without instance mask for each frame as MaGGIe [22] has.\nH.3. Loss Functions\nGiven that we take the first-frame segmentation mask alongside with input frames as input, our model needs to predict alpha\nmatte starting from the first frame, which is different from VOS methods [8, 12]. In addition, since we also apply mask\naugmentation on the given segmentation mask, the prediction from the segmentation head should also start from the first\nframe. As a result, we need to apply losses on all t ∈[0, N] frames for both matting and segmentation heads.\nThere are mainly three kinds of losses involved in our training: (1) matting loss Lmat; (2) segmentation loss Lseg; (3)\ncore supervision (CS) loss Lcs, and their usages in different training stages are summarized in Table 4.\nMatting Loss. For frame t, suppose we have the predicted alpha matte Mt w.r.t. its ground-truth (GT) M GT\nt\n. We follow\nRVM [33] to employ L1 loss for semantics Ll1, pyramid Laplacian loss [17] for matting details Llap, and temporal coherence\nloss [44] Ltc for flickering reduction:\nLl1 = ∥Mt −M GT\nt\n∥1,\n(8)\nLlap =\n5\nX\ns=1\n2s−1\n5\n∥Ls\npyr(Mt) −Ls\npyr(M GT\nt\n)∥1,\n(9)\n13\n\nLtc = ∥dMt\ndt\n−dM GT\nt\ndt\n∥2,\n(10)\nThe overall matting loss is summarized as:\nLmat = Ll1 + 5Llap + Ltc.\n(11)\nSegmentation Loss. For frame t, suppose we have the predicted segmentation mask St w.r.t. its ground-truth (GT) SGT\nt\nfrom the segmentation head. We employ common losses used in VOS [8, 12, 54], Lce and Ldice.\nLce = SGT\nt\n(−log(St)) + (1 −SGT\nt\n)(−log(1 −St)),\n(12)\nLdice = 1 −2StSGT\nt\n+ 1\nSt + SGT\nt\n+ 1.\n(13)\nThe overall segmentation loss is summarized as:\nLseg = Lce + Ldice.\n(14)\nCore Supervision Loss. For core-area supervision, we combine the region-specific losses: Lcore for core region and\nLboundary for boundary region as defined in Sec. 3.2 in the manuscript, and the overall core supervision loss is summa-\nrized as:\nLcs = Lcore + 1.5Lboundary.\n(15)\nI. Dataset\nTable 5. Comparison on Datasets. We compare our new training data and testing data with the old ones, in terms of the number of distinct\nforegrounds, sources, and whether harmonization is applied.\nDatesets\nVideoMatte240K (old train) [32]\nVM800 (new train)\nVideoMatte (old test) [32]\nYouTubeMatte (new test)\n#Foregrounds\n475\n826\n5\n32\nSources\n-\nStoryblocks, Envato Elements, Motion Array\n-\nYouTube\nHarmonized\n-\n-\nx\n✓\nI.1. New Training Dataset - VM800\nOverview. As summarized in Table 5, our new training dataset VM800 has almost twice the number of foreground videos\nthan VideoMatte240K [32] in quantity. To enhance diversity and data distribution, our foreground green screen videos are\ndownloaded from a total of three video footage websites: Storyblocks, Envato Elements, and Motion Array, and thus enjoy\na diversity in hairstyles, outfits, and motion. In addition, we ensure the high quality of our VM800 dataset in fine detail and\nthrough careful manual selection.\nGeneration Pipeline. We employ Adobe After Effects in our data generation pipeline to extract alpha channels from green\nscreen footage videos. Since the amount of green screen footage to be processed is huge, we would like to obtain the\npreliminary results with an automatic pipeline. We first use Keylight and set Screen Color to be the pixel value taken\nfrom the upper left corner for each frame. To obtain a clean alpha matte, we clip the values smaller than 20 to be 0 and those\nlarger than 80 to be 255. To further enhance the alpha matte quality, we post-process with another two keying effects Key\nCleaner and Advanced Spill Supressor, which are generally used together following Keylight. Since we are\nprocessing a video, we also turn on reduce chatter in Key Cleaner to reduce flickering in the boundary region. For\nbatch processing, we compile the above process into a Javascript and XML file for After Effects to run with, and obtain a\nlarge batch of preliminary results for manual selection.\n14\n\nKeylight\n- Screen Color: pixel value of upper left corner\n- Screen Matte:\n- Clip Black: 20\n- Clip White: 80\nKey Cleaner\n- radius: 1\n- reduce chatter: check\nAdvanced Spill Supressor\n(a) Errors in reflective regions (e.g., glasses)\n(b) Inhomogeneous in core regions (e.g., shadow) \nFigure 8. Issues with VideoMatte240K [32]. (a) Errors in alpha values exist in reflective regions (e.g., “a hole” on glasses). (b) Inhomo-\ngeneous alpha values exist in core regions (e.g., caused by shadow), where the alpha value should be exactly 0 or 1.\nFigure 9. Gallery for our new training dataset VM800. High-quality details in the boundary regions and diversity in terms of gender,\nhairstyles, and aspect ratios could be clearly observed.\nQuality - Fine Details. The green screen foreground videos we downloaded are almost in a 4K quality, and we also place\na higher priority on those videos with more details (e.g., hair) in our download choice. Fig. 9 shows the fine details in our\nVM800 dataset.\nQuality - Careful Manual Selection. We notice that alpha mattes extracted with After Effects from green screen videos\noften encounter inhomogeneities in core regions. For example, reflective regions in the foreground will result in a near-zero\nvalue (i.e., a hole) in the alpha matte, as shown in Fig. 8(a). In addition, noise also exists in the green screen background,\nresulting in the fact the alpha values may not homogeneously equal 0, which should not be the case in the core region.\n15\n\nSimilarly, for foregrounds, colors that are similar to the background green, or shadow in the foreground, may also result in\nthe alpha values not homogeneously equal to 1 in the core foreground region, making the alpha matte look noisy, as shown\nin Fig. 8(b). Since VideoMatte240K [32] is also obtained with After Effects, we observe that alpha mattes with the above\nproblems still exist, and thus taking such wrong ground truth for training will inevitably lead to problematic inference results\n(Fig. 11(a)). As a result, we conduct careful manual selection to examine all our processed alpha mattes, and leave out those\nwith the above problems. As shown in Fig. 11(a), training with our VM800 will not lead to such problematic results.\nI.2. New Test Dataset - YouTubeMatte\nOverview. As summarized in Table 5, our new synthetic benchmark YouTubeMatte has over six times larger than the number\nof distinct foreground videos in VideoMatte [32], making it a much more representative benchmark for evaluation with better\ndiversity. In addition, the green screen videos for foregrounds are downloaded from YouTube at a scale of 1920 × 1080\nwith rich boundary details, thus enhancing its ability to discern matting precision in boundary regions. While the generation\npipeline for YouTubeMatte is almost the same as that for VM800, harmonization [23], however, is applied when compositing\nthe foreground on a background. Such an operation effectively makes YouTubeMatte a more challenging benchmark that is\ncloser to the real distribution. As shown in Fig. 10, while RVM [33] is confused by the harmonized frame, our method still\nyields robust performance.\nBefore\nAfter\nVideo Frame\nRVM\nOurs\nHarmonization\nFigure 10. Harmonization on synthetic benchmarks and its effect on model performance. Harmonization [23] is an operation that\nmakes the composited frame more natural and realistic, which also effectively makes our YouTubeMatte a more challenging benchmark\nthat is closer to the real distribution. It is observed that while RVM [33] is confused by the harmonized frame, our method still yields robust\nperformance.\nI.3. Real Benchmark and Evaluation\nOverview. As a technique towards real-world applications (e.g., virtual background in the online meeting), the synthetic\nbenchmark is not enough to test the generalizability of video matting models. Although there are countless of real human\nvideos for testing in the wild, the lack of GT alpha mattes makes them hard to serve as a real benchmark. Here, we select\na subset of 25 real-world videos from [33], where a consecutive of 100 frames for each video are selected with no scene\ntransition, to form our real benchmark. According to our definitions in Fig. 2(a) in the manuscript, we could also divide the\nevaluation metrics for core regions and for boundary separately, making evaluation for real benchmarks feasible.\nEvaluation on Core Regions. Thanks to the recent success of VOS methods [8, 12], frame-wise segmentation masks could\nbe generated with high precision. Here, we employ Cutie [12] for video segmentation results. We first obtain the trimap\nfor each segmentation mask by applying dilation and erosion (with kernel size 21), and then compute the core mask where\ntrimap values equal 0 or 1. In this way, the values of a segmentation mask within its core region could be considered as the\nGT alpha values for the core region, where common metrics including MAD and MSE for semantic accuracy, and dtSSD [14]\nfor temporal coherency could be applied for evaluation.\n16\n\nJ. More Results\nJ.1. Enhancement from New Training Data\nAs discussed in Sec. 4.1 in the manuscript and Section I.1 in the supplementary, our new training data VM800 is upgraded\nin quantity, quality, and diversity. In addition to the quantitative evaluation in Tab. 3 in the manuscript, we further show the\nenhancement from new training data by providing more results when comparing the model trained with VideoMatte240K [32]\nand the model trained with our VM800 in Fig. 11(a).\nErrors in \nreflective objects\nInhomogeneous \ncore regions\nVideo Frame\nOld Training Data \nNew Training Data \nVideo Frame\nw/o Core Supervision w/ Core Supervision \n(a) Enhancement from New Training Data\n(b) Effectiveness of New Training Scheme\nFigure 11. (a) Comparison on results trained with old training data (VideoMatte240K [32]) and new training data (our VM800). It\ncould be observed that training with old data will lead to errors in reflective objects (e.g., holes on the sunglasses) and inhomogeneous alpha\nvalues in the core regions. However, both issues are fixed when training with our new data, indicating a higher quality. (b) Comparison\non results trained without and with core-area supervision. It could be observed that training without it will lead to semantics error due\nto the weak supervision from real segmentation data, while training with core supervision largely improves semantics accuracy thanks to\nthe stronger supervision enabled.\nJ.2. Effectiveness of Consistent Memory Propagation\nAs one of our key designs, the consistent memory propagation (CMP) module improves both stability in core regions and\nquality in boundary details. In addition to the quantitative evaluation in Tab. 3 in the manuscript, we give more qualitative\nresults and analysis in Fig. 12.\nJ.3. Effectiveness of New Training Scheme\nOur new training scheme introduces core-area supervision, which largely enhances the semantic accuracy and stability, as\nshown in Tab. 3 in the manuscript. More qualitative results are shown in Fig. 11(b) for better visualization of its effects.\nJ.4. Effectiveness of Recurrent Refinement\nAs discussed in Sec. 3.3 in the manuscript, the sequential prediction in the memory-based paradigm enables recurrent re-\nfinement without the need for retraining during inference. By repeating the first frame n times and iteratively updating the\nfirst frame prediction based on the last-time prediction, the quality of the first frame alpha matte could be recurrently refined.\nWe show in Fig. 13 that such recurrent refinement can not only (1) enhance the robustness to the given segmentation mask\neven when it is of low quality, but also (2) achieve matting details at an image-matting level when compared with an image\nmatting method (i.e., Matte Anything [56] in the last column).\n17\n\nt\nt+40\nt+80\nt+160\nt+120\nVideo Frames\nw/o CMP\nw/ CMP\nChange Prob.\nFigure 12. Comparison on results with and without Consistent Memory Propagation. It could be observed that when CMP is not\napplied, semantic errors constantly exist across a wide span of video frames. However, when training with CMP, we observe from the\n“Change Probability” mask that usually our model only takes pixels near the boundary as “changed”, and most of the inner regions (i.e.,\nearring) will mainly take the memory values from the last frame. As we can see on the figure, while predictions are both correct at time\nt, the model with CMP successfully keeps the correctness and gives stable results, while the model without CMP quickly breaks the\ncorrectness and never recovers.\nJ.5. More Qualitative Comparisons\nIn this subsection, we provide additional visual comparisons of our method with the state-of-the-art methods, including\nauxiliary-free (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. Fig. 14 presents the\ngeneral video matting results on real videos. To further demonstrate the superiority of our model, Fig. 15 and Fig. 16 both\nshowcase a challenging case respectively, where other methods mostly fail. In addition, Fig. 17 demonstrates the instance\nmatting results compared with MaGGIe [22], a method with instance mask for each frame is given as guidance, while our\nmodel only has the segmentation mask for the first frame as guidance.\nJ.6. Demo Video\nWe also offer a demo video. This video showcases more video matting results and a hugging face demo for applicability,\nboth on real-world videos.\n18\n\nVideo Frame\nSegmentation Mask\n!! = 1\n!! = 5\n!! = 10\nImage Matting \n(Matte Anything)\nFigure 13. Comparison on results with iterative refinement. A noticeable enhancement on details can be observed even with one\niteration of refinement compared with the given segmentation mask. Within 10 iterations, our model is able to achieve matting details at an\nimage-matting level, even better than Matte Anything [56], which is an image matting model also based on the results from SAM [25].\n19\n\nVideo Frame\nRVM\nFTP-VM\nMaGGIe\nOurs\nFigure 14. More qualitative comparisons on general video matting with SOTA methods. We compare our MatAnyone with both\nauxiliary-free (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. It could be observed that our method\nsignificantly outperforms others in both detail extraction and semantic accuracy, across diverse and complex real scenarios. It is noteworthy\nthat although sometimes MaGGIe [22] seems to give acceptable results when compositing with a green screen, its alpha matte turns out\nto be noisy (i.e., inhomogeneous in the core foreground region and blurry in the boundary region), while our alpha matte is clean with\nfine-grained details in the boundary region. As a result, we also include alpha mattes for a more comprehensive comparison. (Zoom in for\nbest view)\n20\n\nVideo Frames\nRVM\nFTP-VM\nMaGGIe\nOurs\nFigure 15. A challenging example of general video matting across a long time span. We compare our MatAnyone with both auxiliary-\nfree (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. It could be observed that our model is able to\ntrack the target object stably even when the object is moving fast in a highly complex scene, where all the other methods present noticeable\nfailures. (Zoom in for best view)\n21\n\nVideo Frames\nRVM\nFTP-VM\nMaGGIe\nOurs\nFigure 16. Another challenging example of general video matting across a long time span. We compare our MatAnyone with both\nauxiliary-free (AF) method: RVM [33] and mask-guided methods: FTP-VM [21], and MaGGIe [22]. This example showcases that our\nmodel is able to track the target objects even in a highly ambiguous background, where the colors for foreground and background are\nsimilar, and also multiple humans in the background. In addition, it also demonstrates when there is more than one target object, our model\nis still able to handle this challenging case well. (Zoom in for best view)\n#1\n#2\n#1\n#2\n#1\n#2\n#3\nInstance #1 Instance #2 Instance #3\nVideo Frame\nMaGGIe (#1)\nMaGGIe (#2)\nVideo Frame\nOurs (#1)\nOurs (#2)\nMaGGIe\nOurs\nFigure 17. More qualitative comparisons on instance matting. We compare our MatAnyone with MaGGIe [22], a mask-guided method\nthat requires the instance mask for each frame, while our method only requires the mask for the first frame. It could be observed that even\nwith such strong given prior, MaGGIe still performs below our method in terms of semantic accuracy in the core regions. Moreover, in\nterms of the boundary regions, by examining the details there, we could clearly observe that the details generated by MaGGIe are blurry\nand far from fine-grained compared with our results. (Zoom in for best view)\n22'),
                Paper(arxiv_id='2501.18119', authors=['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], published_at=datetime.datetime(2025, 2, 3, 6, 6, 33, 957000, tzinfo=datetime.timezone.utc), title='Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models', summary='Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.', upvotes=19, thumbnail=None, content='Large Language Models (LLMs) has emerged\nas a significant question. To this end, we pro-\npose a two-stage framework to learn and ap-\nply quantized codes for each entity, aiming for\nthe seamless integration of KGs with LLMs.\nFirstly, a self-supervised quantized representa-\ntion (SSQR) method is proposed to compress\nboth KG structural and semantic knowledge\ninto discrete codes (i.e., tokens) that align the\nformat of language sentences. We further de-\nsign KG instruction-following data by view-\ning these learned codes as features to directly\ninput to LLMs, thereby achieving seamless\nintegration.\nThe experiment results demon-\nstrate that SSQR outperforms existing unsu-\npervised quantized methods, producing more\ndistinguishable codes. Further, the fine-tuned\nLLaMA2 and LLaMA3.1 also have superior\nperformance on KG link prediction and triple\nclassification tasks, utilizing only 16 tokens\nper entity instead of thousands in conventional\nprompting methods.\n1\nIntroduction\nLarge\nLanguage\nModels\n(LLMs),\nsuch\nas\nLLaMA (Touvron et al., 2023a,b) and GPT-4 (Ope-\nnAI, 2023), are initiating considerable transforma-\ntions within the fields of artificial intelligence (AI)\nand natural language processing (NLP). They have\nachieved substantial success (Peng et al., 2023;\nWang et al., 2024; Xu et al., 2024b), and thus,\nhave been regarded as potential pathways towards\nachieving the ultimate goal of artificial general in-\ntelligence (Yang et al., 2024a). However, the spe-\ncific training strategies employed by LLMs render\nthem black-box models and struggle to retrieve the\nrelevant facts necessary for the correct answer (Pan\net al., 2024), resulting in low performance in com-\nplex reasoning scenarios (Xu et al., 2025, 2024a).\nFigure 1: Illustration of different strategies to integrate\nKGs with LLMs. (a) The direct method utilizes (sam-\npled) graph structures and semantic text as inputs. (b)\nOur method for seamlessly integrating KGs with LLMs\nusing learned quantized and discrete codes.\nFurthermore, knowledge hallucination becomes a\nserious issue, which may generate wrong state-\nments that conflict with reality (Bang et al., 2023;\nJi et al., 2023). It presents considerable risks, par-\nticularly in specialized fields like law (Cui et al.,\n2023) and healthcare (Lin et al., 2025; He et al.,\n2025).\nKnowledge Graphs (KGs), also known as knowl-\nedge bases, organizes massive amounts of factual\nknowledge in a structured and interpretable man-\nner by the triple form of (subject, relation, object).\nThey can serve as a vital supplement to LLMs (Pan\net al., 2024), providing an alternative way to ad-\ndress hallucinations and generate more precise an-\nswers using continual fine-tuning (Zhang et al.,\n2024b; Hron et al., 2024) or retrieve-based rea-\nsoning (Sun et al., 2024; Tan et al., 2024; Zhang\net al., 2024a). However, the KGs’ structure is in a\ngraph form, which markedly differs from the dis-\ncrete token format of the natural language in LLMs.\nThus, due to the presence of this natural represen-\ntation gap, the effective integration of comprehen-\nsive structural information of KGs with LLMs has\nemerged as a significant question.\nAs shown in Figure 1 (a), one straightforward\nmethod involves converting relevant triples into\ntextual prompts and then feeding them into LLMs,\ncombined with semantic text. This simple strategy\nwould necessitate a substantial number of tokens,\narXiv:2501.18119v1  [cs.CL]  30 Jan 2025\n\nFigure 2: The statistics of 2-hop sampled neighbors and\nneeded tokens (by LLaMA2) for entities in FB15k-237.\ncausing an enormous resource burden. Supposing\nthe average degree of an entity is d, the number of\nits neighbors grows exponentially and reaches dh in\nthe h-hop. While certain sampling strategies such\nas random walk (Ko et al., 2024) and path prun-\ning (Tan et al., 2024) have been introduced, a con-\nsiderable computational load also exists. As shown\nin Figure 2, when only sample 20% 2-hop neigh-\nbors in FB15k-237 (Toutanova and Chen, 2015)\ndataset, the median and mean number of neighbors\nfor entities are about 10 and 107, which requires\nmedian and mean tokens of about 300 and 3K, re-\nspectively. When with 30% sampling, even the\nmedian needed tokens reach about 2.5K per entity.\nConsidering that KG tasks may involve multiple en-\ntities, even the most advanced long-context LLMs\nmay face challenges in handling them. Meanwhile,\nemploying KGs’ substructures through sampling\ncould disrupt the holistic modeling of the entire\ngraph, potentially resulting in information loss and\nsub-optimal performance for downstream tasks.\nAnother alternate strategy involves integrating\ncontinuous KG embedding with LLMs by a learn-\nable adapter (Zhang et al., 2024b), introducing new\nnetworks in the framework. It requires additional\nprecise alignment between the different latent rep-\nresentation spaces of KG embeddings and LLMs.\nConsidering the above context, we aim to explore\nthe potential to bridge the natural gap between KG\nstructure and natural language and then integrate\nKGs with LLMs. Inspired by the early fusion strat-\negy in multimodal LLMs (Team, 2024), the general\nidea of this study is to learn compressed and dis-\ncrete entity codes (i.e., tokens), rather than continu-\nous embeddings, by quantized techniques to repre-\nsent holistic structural and semantic information of\nentities in KGs. They have the same discrete form\nof natural language, e.g., the quantized codes in\nFigure 1 (b) align the format of language sentences.\nThus, seamlessly integrating KGs with LLMs can\nbe realized by directly inputting the learned codes\ninto LLMs, merely requiring an expansion of the\nLLMs’ tokenizer vocabulary and eliminating the\nneed for any other framework modifications.\nAlthough several studies have conducted quan-\ntized representations on KGs (Galkin et al., 2022;\nChen et al., 2023; Li et al., 2023), they universally\nemploy an unsupervised approach to select anchors\nto represent entities, failing to the holistic struc-\ntural and semantic modeling. In this study, we first\nintroduce a self-supervised quantized representa-\ntion for KGs, aiming to learn discrete codes for\neach entity that can reconstruct KG structures and\nalign with semantic texts. A graph convolutional\nnetwork (GCN) is used as an encoder to model\nneighbor structures of KGs, and vector quantiza-\ntion (Van Den Oord et al., 2017) is implemented for\nthe KG quantized representation learning. Further,\nbased on learned entity codes, we construct specific\ninstructions for KG tasks, which can be seamlessly\nintegrated with LLMs, presenting a new paradigm\nto employ LLMs in KG applications. In summary,\nour contributions lie in the following three folds:\n• We propose a self-supervised quantized repre-\nsentation (SSQR) method that is capable of acquir-\ning both KG structural and semantic knowledge.\nTo our knowledge, this is the first study for KG\nquantization learning in a self-supervised manner.\n• We propose the first study that utilizes the\nderived codes to seamlessly integrate KGs with\nLLMs, which is achieved by viewing codes as input\nfeatures and designing KG instruction-following\ndata. It has extensive potential applications, e.g.,\nKG link prediction and triple classification.\n• From the experiment view, SSQR exhibits su-\nperior performance compared to current unsuper-\nvised quantized methods and the learned codes\nare more distinguishable. Besides, using only 16\ncodes for each entity, the fine-tuned LLaMA2 and\nLLaMA3.1 have superior performance on KG link\nprediction and triple classification tasks.\n2\nQuantized Representation for KGs\nFormally, a KG can be represented as G\n=\n{E, R, F}, which is the combination of entities\nE, relations R, and triples F ⊆E × R × E. Each\ntriple is in the form of (h, r, t). For each entity\ne, it has the structural and semantic information,\nwhere we utilize the entity neighbors N(e) and its\ntextual description Te to describe, respectively. Al-\nthough here we only use one-order neighbors N(e)\n\nFigure 3: The overall architecture of our study. (a) is for SSQR learning. (b) is for instruction tuning for KG tasks,\nwhere the learned quantized representations serve as features. Icons\nand\nrepresent the status of the module\nduring training, indicating if it is frozen or being updated, respectively.\nfor demonstration, our model is capable of captur-\ning high-order structure information by multi-layer\nGCNs. In the following contents, we will detailedly\ndescribe the structural and semantic modeling, as\nwell as the quantized representation for KGs.\nStructural Modeling. Here, we utilize simple but\neffective GCNs (Lin et al., 2022) to embed the\nstructural information of KGs, which follows the\niterative message-passing strategy to update the\nentity embeddings from l-th layer to (l+1)-th:\nel+1\nj\n= Wl\n1ej +\nX\n(ei,r)∈N(ej)\nWl\n2ml\nei,r,ej,\n(1)\nwhere e is the embedding of the entity and the\nW denotes the transformation matrix. m is the\nmessage information of the specific edge. Here, we\nfollow the composition operation (Vashishth et al.,\n2020) for calculation:\nml\nei,r,ej = el\ni ∗vl\nr,\n(2)\nwhere v is the relation embedding and ∗is element-\nwise multiplication for two vectors. Between dif-\nferent layers, relation representation is updated by\nlinear transformation: vl+1 = Wl\nrelvl. After L\nGCN layers, the entity representation eL and rela-\ntion representation vL are all obtained.\nQuantized Representation. Here, we introduce\nthe quantized representation for discrete KG rep-\nresentation. For its implementation, inspired by\nVQ-VAE (Van Den Oord et al., 2017) and VQ-\nGAN (Esser et al., 2021), we first maintain a dis-\ncrete cookbook X = [x1, x2, · · · , xM], where\neach xm ∈Rd is a learnable vector to represent\ncode m. Using this, a d-dimensional vector e can\nbe quantized by matching the nearest code:\nQ(e) = xi, where i = arg min\nm\n∥e −xm∥2\n2, (3)\nwhere Q is quantized function. In this way, each\nvector can be assigned to only one code, which\nmay lack representation capacity and distinguisha-\nbility for KG embedding. So we first transform the\nlearned entity embedding eL to multiple times of\ndimension d, i.e., FFN(eL) ∈RN×d. In this way,\neach entity can be assigned to a code sequence with\nthe length of N. Thus, each entity can be repre-\nsented to [q1, q2, · · · , qN] by Eq. (3), where qn is\nthe code index in the codebook. The quantized\nrepresentation vector can be:\nqe = WqQ(eL), Q(eL) = [xq1, xq2 · · · xqN ]. (4)\nBased on this, the whole model can be optimized\nin an end-to-end manner by the straight-through\ngradient estimator (Van Den Oord et al., 2017):\nLq =\n\r\rsg[eL] −qe\n\r\r2\n2 + β\n\r\reL −sg[qe]\n\r\r2\n2,\n(5)\nwhere sg stands for stop gradient, which is char-\nacterized by its identity function during forward\ncomputation and has zero partial derivatives for\nbackpropagation. ∥sg[eL] −qe\n\r\r2\n2 is codebook loss\nassuring the codes are close to encoder’s outputs\nand ∥eL −sg[qe]\n\r\r2\n2 is commit loss encouraging the\nencoder generating outputs close to codes. β is a\nhyper-parameter to trade off the two terms.\nStructure Reconstruction. To inject the holistic\nstructure information into the quantized represen-\ntations, we hope the learned entity codes can re-\nconstruct KG structures. But directly predicting\nadjacency matrix as is done in research for homo-\ngeneous graphs (Yang et al., 2024b) is inappropri-\nate, because KGs’ structures are more heteroge-\nneous and sparse. Thus, based on quantized em-\nbeddings, we verify the validity of each triplet (h,\n\nr, t) and implicitly reflect the holistic KG, where\nConvE (Dettmers et al., 2018) is implemented:\ns(h, r, t) =\n\x02\nFlat(Conv(¯qh∥¯vL\nr ))\n\x03⊤Wcqt.\n(6)\nFlat and Conv are the flatten and 2D convolution\noperations, respectively. ¯q and ¯v are transforma-\ntion matrices for embeddings q and v. The final\nscore of triple (h, r, t) can be regularized by the\nsigmoid function σ: ˜y = σ(s(h, r, t)). Finally,\ncompared with actual label y, the structure model-\ning can be learned by binary cross-entropy loss:\nLst =−1\n|F|\nX\ni\n[yi log ˜yi+(1−yi) log(1−˜yi)]. (7)\nSemantic Distilling. For semantic modeling, our\ngoal is to ensure that the learned codes for each\nentity can imply the information of its correspond-\ning text descriptions. Considering the substantial\nsuccess of LLMs, we introduce a simple yet effec-\ntive distilling strategy to learn from them. Specif-\nically, we first obtain text embeddings of KG en-\ntities by LLMs: te = LLM(Te). Here, we utilize\nthe text-embedding-3-large as the LLM by Ope-\nnAI API 1 considering its strong ability for text\nembeddings. It embeds each text sequence into a\n3072-dimension vector. Based on this, we make the\nmodel have the ability to align its semantic embed-\nding through the learned quantized output, where\nthe loss of mean square error is utilized:\nLse = −1\n|E|\nX\ni\n\r\rWsqei −tei\n\r\r2\n2.\n(8)\nIn this way, we distil the semantic knowledge from\nthe LLMs to our discrete codes of GCN outputs.\nOn the whole, the entire quantized representa-\ntion model can be updated by the combination of\nquantized, structural, and semantic loss:\nL = Lq + Lst + Lse.\n(9)\n3\nTuning LLMs with SSQR\nEmploying the quantized representation, each en-\ntity in KGs can be illustrated by codes of length N.\nThis can be perceived as the same form of natural\nlanguage, thereby facilitating its seamless integra-\ntion with LLMs. Every learned code can serve as a\nnew token, necessitating only an expansion of the\ntoken vocabulary within the LLM’s tokenizer.\n1https://platform.openai.com/docs/guides/embeddings\nInstruction: This is a knowledge graph completion\ntask, which needs to predict the tail entity for an\nincomplete query triplet.\nInput: The query triplet is (h, r, ?).\nThe quantized representation of entity h is: [Code(h)]\nThe answer candidates and corresponding quantized\nrepresentations are as follows:\nentity 1, [Code(entity 1)]\n· · ·\nentity 20, [Code(entity 20)]\nPlease generate quantized representations of the top-\n3 potential answers, ranked from highest to lowest:\nOutput: 1. [Code(candidate 1)]\n2. [Code(candidate 2)]\n3. [Code(candidate 3)]\nTable 1: Instruction format for link prediction, where\nlearned codes serve as entity features to help ranking.\nThis paradigm can be applied to various KG\ntasks, by constructing corresponding instruction\ndata, where the learned entity codes could act as\nfeatures. For example, the KG link prediction task\ncan be done using the instruction form as shown\nin Table 1. Specifically, the code sequence of en-\ntity e can be Code(e): “[CODEq1] [CODEq2] · · ·\n[CODEqN]”. For each query (h, r, ?), we pro-\nvide the codes Code(h) for query head h. Besides,\nwe give several answer candidates along with their\nassociated codes for LLM ranking. Candidates\ncan be obtained by conventional KG embedding\nmodels, e.g., TransE (Bordes et al., 2013) and\nCompGCN (Vashishth et al., 2020). The goal is\nto predict the actual ranking list of candidates us-\ning their discrete codes. The detailed instructions\nfor triple classification are described in Section C\nof the Appendix. For the LLM fine-tuning, the\nnext token prediction is carried out based on the\ninstruction I and previously generated text tokens:\nLllm = −\nN\nX\nn=1\nlog\n\x00xn|x<n, I\n\x01\n.\n(10)\n4\nExperiments and Analysis\nTo verify the effectiveness of the proposed SSQR\nand its ability to integrate with LLMs, We carry out\nexperiments on the KG link prediction and triple\nclassification tasks, where the popular datasets\nWN18RR (Dettmers et al., 2018) and FB15k-\n237 (Toutanova and Chen, 2015) as well as FB15k-\n237N (Lv et al., 2022) are utilized. For SSQR, a\n2-layer GCN is utilized as the encoder. β is set to\n0.25 in the experiment. The embedding dimension\n\n1.00\n0.28\n1.00\n0.24\n0.17\n1.00\n0.21\n0.14\n0.17\n1.00\n0.24\n0.13\n0.23\n0.24\n1.00\n0.20\n0.20\n0.17\n0.18\n0.23\n1.00\n0.17\n0.18\n0.17\n0.13\n0.19\n0.08\n1.00\n0.12\n0.03\n0.07\n0.15\n0.09\n0.08\n0.16\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(a) Original text embedding.\n1.00\n0.27\n1.00\n0.37\n0.26\n1.00\n0.20\n-0.34 -0.22\n1.00\n0.27\n-0.00\n0.34\n0.14\n1.00\n0.27\n0.12\n0.12\n0.04\n0.01\n1.00\n0.01\n0.14\n0.06\n-0.14 -0.05\n0.08\n1.00\n-0.03 -0.14 -0.36\n0.06\n-0.08\n0.23\n0.22\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(b) SSQR.\n1.00\n0.98\n1.00\n0.98\n0.98\n1.00\n0.99\n0.99\n0.99\n1.00\n1.00\n0.98\n0.98\n0.99\n1.00\n0.99\n0.96\n0.98\n0.97\n0.98\n1.00\n0.98\n0.98\n1.00\n0.99\n0.98\n0.98\n1.00\n0.98\n1.00\n0.99\n0.99\n0.98\n0.97\n0.98\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(c) SSQR w/o GCN.\n1.00\n0.03\n1.00\n0.20\n-0.16\n1.00\n-0.08\n0.05\n0.07\n1.00\n-0.06\n0.32\n0.13\n-0.00\n1.00\n-0.05\n0.07\n0.12\n0.12\n0.18\n1.00\n0.06\n0.40\n0.04\n0.02\n0.30\n0.24\n1.00\n0.14\n0.14\n-0.15\n0.22\n0.08\n0.33\n0.38\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(d) SSQR w/o semantics.\nFigure 4: The cosine similarity of quantized representations on the WN18RR dataset (sampled 8 entities).\nTable 2: The results of baselines are from Li et al. (2023).\n† means the improvement of SSQR compared to the best\nperformance in each metric. ‡ means the ablation results\ncompared to the results of SSQR.\nModel\nWN18RR\nFB15k-237\nMRR\nHits@10\nMRR\nHits@10\nNodePiece\n0.403\n0.515\n0.256\n0.420\n+RandomEQ\n0.425\n0.522\n0.263\n0.425\nEARL\n0.440\n0.527\n0.310\n0.501\n+RandomEQ\n0.442\n0.536\n0.308\n0.502\nSSQR\n0.483\n0.578\n0.361\n0.545\n∆(↑)†\n9.28%\n7.84%\n16.45%\n8.57%\nw/o GCN\n0.479\n0.577\n0.309\n0.482\n∆(↓)‡\n0.83%\n0.17%\n14.40%\n11.56%\nw/o sem\n0.447\n0.521\n0.347\n0.528\n∆(↓)‡\n7.45%\n9.86%\n3.88%\n3.12%\nis set to 200 as default. The number of codebook\nM and codes for each entity N is set to 2048 and\n32. The maximum number of training epochs is\n800. For LLM fine-tuning, LLaMA2 (7B) and\nLLaMA3.1 (8B) are utilized using M as 2048 and\nN as 16 for computation efficiency. The query for\nhead prediction (?, r, t) is transformed to the tail\nprediction by adding reverse relation of r. The\nmean reciprocal rank (MRR) and Hits@k values\nare set as evaluation metrics for model performance.\nMoreover, the triple classification task employs ac-\ncuracy, precision, recall and F1-score as metrics.\nMore detailed settings are shown in the Appendix.\n4.1\nSSQR Results\nWe compare the performance of our SSQR\nwith three unsupervised methods, i.e., Node-\nPiece (Galkin et al., 2022), EARL (Chen et al.,\n2023), and random entity quantization (RandomEQ\nfor short) (Li et al., 2023), for KG quantized repre-\nsentations. The results are given in Table 2.\nAs can be observed, SSQR achieves signifi-\ncant performance improvement against baselines,\nwhich has 9.28% and 7.84% improvements com-\npared with the previous optimal performance on the\nWN18RR dataset. When at the FB15k-237 dataset,\nthe improvements are even better, i.e., 16.45% and\n8.57%. Although these unsupervised methods are\nsimple and efficient for implementation, they fail\nto capture the structures of KGs. In contrast, our\nproposed self-supervised strategies would provide\nan effective way for quantized representations for\nKG structure learning.\n4.2\nSSQR Result Analysis\nAblation Studies. We carry out the ablation studies\nto verify the effectiveness of each module in SSQR\nas the bottom part of Table 2. Generally, the per-\nformance of link prediction degrades when GCN\nor semantic distilling is removed, but the extent of\ndegradation varies across different datasets. It can\nbe seen that the GCN encoder is more important\nfor the FB15k-237 dataset (14.40% and 11.56%\ndecline), while semantic information has more im-\npact on WN18RR (7.45% and 9.86%). This may\nbe due to the fact that FB15k-237 contains a rich\nKG structure which requires GCN to capture, while\nthe semantic text is more important for WN18RR\nto make up for the defects caused by the lack of\nrich structural information.\nRelevance among Entity Codes. We also calcu-\nlate the cosine similarity of quantized representa-\ntion in Figure 4, including the original text em-\nbedding, SSQR, SSQR w/o GCN, and SSQR w/o\nsemantics. When using only text embeddings, the\nsimilarities are all small positive values. SSQR w/o\nGCN has similarities that are all close to 1. These\nphenomena indicate that entity representations are\nin a small corner of the space (i.e., anisotropic),\nwhere the representation space is not fully utilized\nfor efficient representation. SSQR solves this prob-\nlem to a certain extent, with a greater range and\nvariety of similarities. Removing semantic infor-\nmation would diminish that advantage.\nImpacts of M and N. The number of codebooks\n\n(a) WN18RR dataset.\n(b) FB15k-237 dataset.\nFigure 5: The effects of codebook length (M) and se-\nquence length (N) for each entity.\nand sequence lengths for codes, i.e., M and N, are\nvital hyper-parameters for SSQR. We explore their\nimpacts in Figure 5. Generally, larger M and N\nwould lead to better performance as they increase\nthe modeling ability of SSQR. In the WN8RR\ndataset, N has a greater impact on M, indicating\nthe necessity of a large N for holistic and distin-\nguishable representations. It may be caused by the\nsparser structure and more entities in the WN18RR.\nDistinguishability of SSQR. Following Ran-\ndomEQ, we calculate the general entropy and Jac-\ncard distance to show codes-level and codewords-\nlevel distinguishability, respectively. For general\nentropy, SSQR has 16.76 and 15.27 on WN18RR\nand FB15k-237 datasets, similar to RandomEQ\n(16.75/15.27) and higher than that of NodePiece\n(15.94/15.26) and EARL (8.20/14.50). It shows\nthat our method has more diverse entity codes and\nbetter entity differentiation ability than other quan-\ntization methods. The Jaccard distances of each\nmodel are shown in Figure 6. RandomEQ and\nSSQR have high values that are far better than those\nof NodePiece and EARL. RandomEQ is superior\non the FB15k-237 dataset but SSQR performs bet-\nter on the WN18RR dataset. In summary, SSQR\nexhibits a robust capacity to distinguish different\nentities and effectively represent KGs.\n4.3\nQuantized Representations with LLMs\nLink Prediction.\nFor fine-tuning, we utilize\nthe pre-trained AdaProp (Zhang et al., 2023) to\ngenerate 20 candidates for each query as it has\nstrong and balanced performance on most KG\ntasks. For comparison, we selected the current\nadvanced embedding models, like TransE (Bordes\net al., 2013), CompGCN (Vashishth et al., 2020),\nAdaProp (Zhang et al., 2023), MA-GNN (Xu\net al., 2023), TCRA (Guo et al., 2024a), and Dif-\nfusionE (Cao et al., 2024). Besides, we include\nfive advanced LLM-based methods for more direct\ncomparison, including KICGPT (Wei et al., 2023),\n200\n400\n600\n800\n1000\n0.2\n0.4\n0.6\n0.8\n1.0\nJaccard distance\nRandomEQ\nNodePiece\nEARL\nSSQR\n(a) WN18RR dataset.\n200\n400\n600\n800\n1000\n0.6\n0.7\n0.8\n0.9\n1.0\nJaccard distance\nRandomEQ\nNodePiece\nEARL\nSSQR\n(b) FB15k-237 dataset.\nFigure 6: The mean Jaccard distance between codes of\na specific entity and its k nearest ones.\nCSProm-KG-CD (Li et al., 2024), ARR (Chen\net al., 2024), KG-FIT (Jiang et al., 2024), and\nMKGL (Guo et al., 2024b).\nThe results of link prediction are shown in Ta-\nble 3. It can be observed that SSQR with LLaMA2\nor LLaMA3.1 is obviously superior in KG link pre-\ndiction against general embedding methods. Com-\npared with the previous state-of-the-art MA-GNN,\nSSQR with LLaMA2 achieves about 4.60%, 8.09%,\n4.39%, -0.88% and 18.47%, 32.62%, 18.31%,\n4.92% improvement in two datasets, respectively.\nCompared with LLM-based methods, SSQR-\nLLaMA2 also shows competitive performance. It\nis better than KICGPT, CSProm-KG-CD, and Chat-\nGPT. Even KICGPT achieves good results on the\nFB15k-237 dataset, it can also be raised by 8.98%,\n14.37%, 9.60%, and 7.76%.\nFor the KG-FIT\n(HAKE), it also has 6.87%, 12.30%, 3.87%, and\n-3.16% improvements on the WN18RR dataset.\nAlthough there is a slight deficiency in terms\nof Hits@10, improvements on other metrics are\nhigh. Meanwhile, SSQR-LLaMA3.1 is better than\nSSQR-LLaMA2, demonstrating that learned quan-\ntized representations can be used for a more pow-\nerful LLM to get better performance. From all the\nresults, our methods generally achieve a greater im-\nprovement in the Hits@1 metric, which is caused\nby the candidate selection and ranking strategies we\nused in LLM fine-tuning. The candidate selection\nmodel may have limited ability, but our method has\na strong ability to select the correct answer from\nall candidates. This demonstrates that our method\nhas good scalability and can be further improved\nwith more accurate candidate selection models.\nTriple Classification.\nBeyond the link prediction\ntask, we conduct experiments on triple classifica-\ntion on the FB15k-237N dataset. The results are\nshown in Table 4, where our method outperforms\ngeneral embedding methods and other LLM-based\nbaselines. For the advanced KoPA (Zhang et al.,\n\nTable 3: The experiment results of general embedding methods and LLM-based methods for KG link prediction.\nModel\nWN18RR\nFB15k237\nMRR\nHits@1\nHits@3\nHits@10\nMRR\nHits@1\nHits@3\nHits@10\nGeneral Embedding Methods\nTransE (Bordes et al., 2013)\n0.223\n0.014\n0.401\n0.529\n0.330\n0.231\n0.369\n0.528\nCompGCN (Vashishth et al., 2020)\n0.479\n0.443\n0.494\n0.546\n0.355\n0.264\n0.390\n0.535\nAdaProp (Zhang et al., 2023)\n0.562\n0.499\n–\n0.671\n0.417\n0.331\n–\n0.585\nMA-GNN (Xu et al., 2023)\n0.565\n0.507\n0.592\n0.679\n0.379\n0.282\n0.415\n0.569\nTCRA (Guo et al., 2024a)\n0.496\n0.457\n0.511\n0.574\n0.367\n0.275\n0.403\n0.554\nDiffusionE (Cao et al., 2024)\n0.557\n0.504\n–\n0.658\n0.376\n0.294\n–\n0.539\nLLM-based Methods\nKICGPT (Wei et al., 2023)\n0.549\n0.474\n0.585\n0.641\n0.412\n0.327\n0.448\n0.554\nCSProm-KG-CD (Li et al., 2024)\n0.559\n0.508\n0.578\n0.660\n–\n–\n–\n–\nARR (Chen et al., 2024)\n0.521\n–\n0.607\n–\n0.398\n–\n0.436\n–\nKG-FIT (Jiang et al., 2024)\n0.553\n0.488\n0.595\n0.695\n0.362\n0.275\n0.485\n0.572\nMKGL (Guo et al., 2024b)\n0.552\n0.500\n0.577\n0.656\n0.415\n0.325\n0.454\n0.591\nSSQR-LLaMA2\n0.591\n0.548\n0.618\n0.673\n0.449\n0.374\n0.491\n0.597\nSSQR-LLaMA3.1\n0.598\n0.559\n0.618\n0.675\n0.459\n0.393\n0.491\n0.597\nTable 4: The experiment results of the triple classifica-\ntion on FB15k-237N dataset. The results of baselines\nare taken from Zhang et al. (2024b).\nModel\nAcc\nP\nR\nF1\nTransE (Bordes et al., 2013)\n0.697\n0.708\n0.671\n0.689\nDistMult (Yang et al., 2015)\n0.587\n0.590\n0.568\n0.579\nRotatE (Sun et al., 2019)\n0.684\n0.692\n0.664\n0.678\nAlpacazero-shot\n0.561\n0.533\n0.974\n0.689\nGPT-3.5zero-shot\n0.602\n0.866\n0.240\n0.376\nKG-LLaMA (Yao et al., 2023)\n0.748\n0.674\n0.962\n0.793\nKG-Alpaca (Yao et al., 2023)\n0.699\n0.627\n0.983\n0.766\nKoPA (Zhang et al., 2024b)\n0.777\n0.708\n0.941\n0.808\nSSQR-LLaMA2\n0.794\n0.757\n0.867\n0.808\nw/o SSQR\n0.754\n0.699\n0.891\n0.783\n∆\n-5.13% -7.71% +2.85% -3.07%\nSSQR-LLaMA3.1\n0.798\n0.759\n0.872\n0.811\nw/o SSQR\n0.767\n0.711\n0.901\n0.795\n∆\n-3.77% -6.34% +3.41% -2.03%\n2024b) model, the performance in the F1-score\nmetric is comparable to that of SSQR. However,\nthe accuracies of SSQR show a significant improve-\nment, i.e., 0.794/0.798 vs. 0.777, demonstrating\nthe effectiveness of integrating SSQR with LLMs.\n4.4\nInsights of LLM Fune-tuning\nAblation Studies. We carry out ablation studies\nto verify the effectiveness of quantized represen-\ntations for LLM tuning. The results are shown\nin Table 5 and the bottom part of Table 4, where\nw/o SSQR means only utilizing the entity’s name\nfor fine-tuning and removing learned entity codes.\nFor the link prediction task, there is a large perfor-\nmance drop, especially in the MRR, Hits@1, and\nHits@3 metrics. A similar pattern is also present in\nTable 5: The ablation results for the link prediction task.\nModel\nMRR\nHits@1\nHits@3\nHits@10\nWN18RR\nSSQR-LLaMA2\n0.591\n0.548\n0.618\n0.673\nw/o SSQR\n0.541\n0.495\n0.603\n0.668\n∆(↓)\n8.46%\n9.67%\n2.43%\n0.74%\nFB15k-237\nSSQR-LLaMA2\n0.449\n0.374\n0.491\n0.597\nw/o SSQR\n0.401\n0.322\n0.441\n0.589\n∆(↓)\n10.69%\n13.90%\n10.18%\n1.34%\nthe triple classification task. We observe that when\nunder the w/o SSQR setting, LLMs have overfitting\nissues, where their performance on training sets is\nvery high but fails to generalize to valid and test\nsets. This demonstrates that the learned discrete\ncodes are distinguishable and representative for dif-\nferent entities, thereby allowing their utilization as\nfeatures to assist KG tasks in LLMs.\nImpacts of M and N for LLM Tuning. We ex-\nplore the impacts of M and N for LLM tuning, the\nresults are shown in Figure 7. First, we present\nthe results of Original, which are the original re-\nsults of AdaProp. It is shown that all other results\nare better than those of AdaProp, showing it is\neffective for LLM fine-tuning with quantized repre-\nsentations. The settings with N=16 and M=2048\nhave better results compared to 16-512 and 8-2048,\nindicating large values are needed to represent en-\ntity structural and semantic information, serving\nbetter features for LLMs. N is more important than\nM, which drops more performance, especially on\nthe FB15k-237 dataset (16-512 even not dropping\n\nFigure 7: The impacts of quantized representation for\nKG link prediction task using LLMs on FB15k-237.\na lot), indicating the long quantized feature is ben-\neficial for LLMs to distinguish entities.\nToken Embeddings in LLMs. To view the repre-\nsentation of codewords and actual language tokens\nin LLMs, we display all 2048 codewords and cor-\nrespondingly sample an equal number of language\ntokens. Further, we reduce the embeddings to 2-\ndimensional space using t-SNE (Van der Maaten\nand Hinton, 2008) and plot the results in Figure 8.\nIt is shown that these two types of tokens are gen-\nerally divided into two categories, indicating they\nhave different representation zones. It is consistent\nwith the intuition and indicates LLM can perceive\nthat they are different types of tokens, showing po-\ntential for further exploration of LLM on KG tasks\nusing SSQR.\n5\nRelated Work\nFor parameter-efficient embeddings on large KGs,\nNodePiece (Galkin et al., 2022) introduces an\nanchor-based method to learn a fixed-size entity\nvocabulary, where unsupervised strategies of Per-\nsonalized PageRank (Page, 1999), node degree, and\nrandom are used for anchor selection. Each entity\ncan be represented through k closest anchors and\ntheir respective distances. Further, EARL (Chen\net al., 2023) randomly samples 10% entities as\nanchors and introduces connected relation informa-\ntion to match anchors’ counterparts. To simplify\nthe whole process, Li et al. (2023) introduces ran-\ndom entity quantization (RandomEQ) to randomly\nset anchor entities and randomly select relations\nfor matching. The results show that RandomEQ\nachieves similar results compared to previous cu-\nrated strategies and has more distinguishable ability.\nIn general, these methods are all in an unsupervised\nlearning manner, which could be efficient for large\nKG embedding but fails to model comprehensive\nstructural and semantic information.\nFigure 8: Token embedding virtualization in LLMs\n(WN18RR dataset), where red and blue dots are real\nword tokens and code tokens, respectively.\nCurrently, numerous research studies are ded-\nicated to incorporating KGs with LLMs to max-\nimize and exploit their respective strengths (Pan\net al., 2024). On one hand, using prompt engineer-\ning or retrieve strategies (Wei et al., 2023; Sun et al.,\n2024; Kau et al., 2024), the information of KGs\nbe sampled and instantiated as tokens like natural\nlanguage to input LLMs. On the other hand, the\ntriple-level or sub-graph structures of KG can be\ninputted to the LLMs to inject knowledge (Hron\net al., 2024). However, because of the natural gap\nbetween the graph structure of KGs and the natural\nlanguage, how to seamlessly and effectively inte-\ngrate the whole structural and semantic information\nof KGs with LLMs is an open problem.\n6\nConclusion and Potential Impacts\nFor seamlessly integrating KGs with LLMs, we\nintroduce a self-supervised quantized representa-\ntion method (SSQR). It compresses the structural\nand semantic information of entities in KGs to a\ndiscrete permutation of codewords, which has a\nsimilar format as the natural language and can be di-\nrectly inputted to the LLMs. By specific instruction\ndata and fine-tuning, LLMs can seamlessly learn\nKG’s knowledge, which can be used in KG applica-\ntions. To verify the effectiveness of our method, we\nimplement experiments on KG link prediction and\ntriple classification tasks, which demonstrate the su-\nperiority of our method. This innovative paradigm\npromises to usher in transformative techniques for\nKGs in the era of LLMs. In the future, we will ex-\nplore more applications and make progress towards\nunified frameworks for multiple KG tasks, e.g.,\nKG-based QA (Luo et al., 2024a), KG-based rec-\nommendation (Huang et al., 2023), and language\nmodeling (Luo et al., 2024b).\n\nLimitations\nDespite our SSQR method’s capacity to facilitate\nthe seamless integration of KGs with LLMs, our\nstudy encounters the generalization limitation due\nto the substantial computational burden associated\nwith LLMs. In most recent and our studies, LLMs\nare fine-tuned for a specific KG and the correspond-\ning task, which can not be applied to various KG\ntasks and largely limits the model generalization\nability. In the future, we will try to construct uni-\nfied LLMs for KGs by implementing quantization\nwithin the same discrete space.\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. In IJC-\nNLP, pages 675–718.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In NIPS, pages 2787–2795.\nZongsheng Cao, Jing Li, Zigan Wang, and Jinliang Li.\n2024. Diffusione: Reasoning on knowledge graphs\nvia diffusion-based graph neural networks. In KDD,\npages 222–230.\nMingyang Chen, Wen Zhang, Zhen Yao, Yushan Zhu,\nYang Gao, Jeff Z. Pan, and Huajun Chen. 2023.\nEntity-agnostic representation learning for parameter-\nefficient knowledge graph embedding.\nIn AAAI,\npages 4182–4190.\nZhongwu Chen, Long Bai, Zixuan Li, Zhen Huang,\nXiaolong Jin, and Yong Dou. 2024. A new pipeline\nfor knowledge graph reasoning enhanced by large\nlanguage models without fine-tuning. In EMNLP,\npages 1366–1381.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and\nLi Yuan. 2023. Chatlaw: Open-source legal large\nlanguage model with integrated external knowledge\nbases. CoRR, abs/2306.16092.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In AAAI, pages 1811–1818.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. 2021.\nTaming transformers for high-resolution image syn-\nthesis. In CVPR, pages 12873–12883.\nMikhail Galkin, Etienne G. Denis, Jiapeng Wu, and\nWilliam L. Hamilton. 2022. Nodepiece: Composi-\ntional and parameter-efficient representations of large\nknowledge graphs. In ICLR.\nJingtao Guo, Chunxia Zhang, Lingxi Li, Xiaojun Xue,\nand Zhendong Niu. 2024a. A unified joint approach\nwith topological context learning and rule augmenta-\ntion for knowledge graph completion. In Findings of\nthe ACL, pages 13686–13696.\nLingbing Guo, Zhongpu Bo, Zhuo Chen, Yichi Zhang,\nJiaoyan Chen, Yarong Lan, Mengshu Sun, Zhiqiang\nZhang, Yangyifei Luo, Qian Li, Qiang Zhang, Wen\nZhang, and Huajun Chen. 2024b. MKGL: mastery\nof a three-word language. In NeurIPS.\nKai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan,\nMengling Feng, and Erik Cambria. 2025. A survey\nof large language models for healthcare: from data,\ntechnology, and applications to accountability and\nethics. Information Fusion, page 102963.\nJiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne\nLiu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet,\nJD Co-Reyes, Noah Fiedel, C Daniel Freeman, et al.\n2024. Training language models on the knowledge\ngraph: Insights on hallucinations and their detectabil-\nity. CoRR, abs/2408.07852.\nQing Huang, Zhenyu Wan, Zhenchang Xing, Changjing\nWang, Jieshan Chen, Xiwei Xu, and Qinghua Lu.\n2023. Let’s chat to find the apis: Connecting human,\nLLM and knowledge graph through AI chain. In\nASE, pages 471–483. IEEE.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):248:1–248:38.\nPengcheng Jiang, Lang Cao, Cao Xiao, Parminder Bha-\ntia, Jimeng Sun, and Jiawei Han. 2024. KG-FIT:\nknowledge graph fine-tuning upon open-world knowl-\nedge. In NeurIPS.\nAmanda Kau, Xuzeng He, Aishwarya Nambissan,\nAland Astudillo, Hui Yin, and Amir Aryani. 2024.\nCombining knowledge graphs and large language\nmodels. CoRR, abs/2407.06564.\nYoumin Ko, Hyemin Yang, Taeuk Kim, and Hyunjoon\nKim. 2024. Subgraph-aware training of text-based\nmethods for knowledge graph completion. CoRR,\nabs/2407.12703.\nDawei Li, Zhen Tan, Tianlong Chen, and Huan Liu.\n2024. Contextualization distillation from large lan-\nguage model for knowledge graph completion. In\nFindings of the EACL, pages 458–477.\nJiaang Li, Quan Wang, Yi Liu, Licheng Zhang, and\nZhendong Mao. 2023.\nRandom entity quantiza-\ntion for parameter-efficient compositional knowledge\ngraph representation. In EMNLP, pages 2917–2928.\nQika Lin, Jun Liu, Fangzhi Xu, Yudai Pan, Yifan Zhu,\nLingling Zhang, and Tianzhe Zhao. 2022. Incorporat-\ning context graph with logical reasoning for inductive\nrelation prediction. In SIGIR, pages 893–903.\n\nQika Lin, Yifan Zhu, Xin Mei, Ling Huang, Jingying\nMa, Kai He, Zhen Peng, Erik Cambria, and Mengling\nFeng. 2025. Has multimodal learning delivered uni-\nversal intelligence in healthcare? a comprehensive\nsurvey. Information Fusion, 116:102795.\nYang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. 2024.\nFinetuning generative large language models with\ndiscrimination instructions for knowledge graph com-\npletion. In ISWC, pages 199–217.\nHaoran Luo, Haihong E, Zichen Tang, Shiyao Peng,\nYikai Guo, Wentai Zhang, Chenghao Ma, Guant-\ning Dong, Meina Song, Wei Lin, Yifan Zhu, and\nAnh Tuan Luu. 2024a. Chatkbqa: A generate-then-\nretrieve framework for knowledge base question an-\nswering with fine-tuned large language models. In\nFindings of the ACL, pages 2039–2056.\nXindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and\nWei Hu. 2024b.\nKnowla: Enhancing parameter-\nefficient finetuning with knowledgeable adaptation.\nIn NAACL, pages 7153–7166.\nXin Lv, Yankai Lin, Yixin Cao, Lei Hou, Juanzi Li,\nZhiyuan Liu, Peng Li, and Jie Zhou. 2022. Do pre-\ntrained models benefit knowledge graph completion?\nA reliable evaluation and a reasonable approach. In\nFindings of the ACL, pages 3570–3581.\nOpenAI. 2023.\nGpt-4 technical report.\nCoRR,\nabs/2303.08774.\nLawrence Page. 1999. The pagerank citation ranking:\nBringing order to the web. Technical report, Techni-\ncal Report.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2024. Unifying large\nlanguage models and knowledge graphs: A roadmap.\nIEEE TKDE, 36(7):3580–3599.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. CoRR, abs/2304.03277.\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\nWang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-\nYeung Shum, and Jian Guo. 2024. Think-on-graph:\nDeep and responsible reasoning of large language\nmodel on knowledge graph. In ICLR.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. In ICLR.\nXingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin\nYuan, and Wenjie Zhang. 2024. Paths-over-graph:\nKnowledge graph empowered large language model\nreasoning. CoRR, abs/2410.14211.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nChameleon Team. 2024.\nChameleon:\nMixed-\nmodal early-fusion foundation models.\nCoRR,\nabs/2405.09818.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd Workshop on\nContinuous Vector Space Models and their Composi-\ntionality (CVSC), pages 57–66.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and efficient foun-\ndation language models. CoRR, abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. Advances in Neural\nInformation Processing Systems, pages 6306–6315.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(11).\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha P. Talukdar. 2020. Composition-based multi-\nrelational graph convolutional networks. In ICLR.\nJianing Wang, Qiushi Sun, Xiang Li, and Ming Gao.\n2024.\nBoosting language models reasoning with\nchain-of-knowledge prompting. In ACL, pages 4958–\n4981.\nYanbin Wei, Qiushi Huang, Yu Zhang, and James T.\nKwok. 2023. KICGPT: large language model with\nknowledge in context for knowledge graph comple-\ntion. In Findings of the EMNLP, pages 8667–8683.\nFangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun\nLiu, and Erik Cambria. 2025. Are large language\nmodels really good logical reasoners? a comprehen-\nsive evaluation and beyond. IEEE Transactions on\nKnowledge and Data Engineering.\nFangzhi Xu, Qika Lin, Tianzhe Zhao, Jiawei Han, and\nJun Liu. 2024a. Pathreasoner: Modeling reasoning\npath with equivalent extension for logical question\nanswering. In ACL, pages 13413–13429.\nFangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei\nYuan, Shuai Yuan, Qika Lin, Yu Qiao, and Jun Liu.\n2024b. Symbol-llm: Towards foundational symbol-\ncentric interface for large language models. In ACL,\npages 13091–13116.\nHongcai Xu, Junpeng Bao, and Wenbo Liu. 2023.\nDouble-branch multi-attention based graph neural\nnetwork for knowledge graph completion. In ACL,\npages 15257–15271.\n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,\nand Li Deng. 2015. Embedding entities and relations\nfor learning and inference in knowledge bases. In\nICLR.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-\ntian Han, Qizhang Feng, Haoming Jiang, Shaochen\nZhong, Bing Yin, and Xia Hu. 2024a. Harnessing\nthe power of llms in practice: A survey on chatgpt\nand beyond. ACM TKDD, 18(6):1–32.\nLing Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda\nHong, Wei Qu, Wentao Zhang, Bin Cui, Muhan\nZhang, and Jure Leskovec. 2024b. Vqgraph: Re-\nthinking graph representation space for bridging gnns\nand mlps. In ICLR.\nLiang Yao, Jiazhen Peng, Chengsheng Mao, and\nYuan Luo. 2023.\nExploring large language mod-\nels for knowledge graph completion.\nCoRR,\nabs/2308.13916.\nHeidi C. Zhang, Sina J. Semnani, Farhad Ghassemi,\nJialiang Xu, Shicheng Liu, and Monica S. Lam.\n2024a. SPAGHETTI: open-domain question answer-\ning from heterogeneous data sources with retrieval\nand semantic parsing. In Findings of the ACL, pages\n1663–1678.\nYichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu,\nWen Zhang, and Huajun Chen. 2024b. Making large\nlanguage models perform better in knowledge graph\ncompletion. In ACM MM, pages 233–242. ACM.\nYongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen\nChu, and Bo Han. 2023. Adaprop: Learning adaptive\npropagation for graph neural network based knowl-\nedge graph reasoning. In KDD, pages 3446–3457.\nA\nStatistics of WN18RR Dataset\nBesides the statistic analysis of FB15k-237 in Fig-\nure 2, we also conduct the statistic analysis of\nWN18RR, which is shown in Figure 9. Specifically,\nwe sample 200 entities from the whole KG and\nthere are two settings (50% neighbor sampling and\n100% neighbors). In the first setting of 50%, the\nmedian and mean of neighbors are 4.0 and 10.37,\nwhile the median and mean number of needed to-\nkens are 61.5 and 185.84, respectively. For the set-\nting of 100%, the median and mean of neighbors\nare 33.5 and 79.05, while the median and mean\nnumber of needed tokens are 623.5 and 1492.74,\nrespectively. Compared to our SSQR, which only\nrequires 16 tokens to represent each entity, both\n50% and 100% settings demand a considerably\nhigher number of tokens.\nFigure 9: The statistics of 2-hop sampled neighbors and\nneeded tokens (by LLaMA2) for entities in WN18RR.\nB\nBaselines\nIn this section, we give detailed descriptions of\nvarious baselines utilized in the paper.\nB.1\nQuantized Representations for KGs\n• NodePiece (Galkin et al., 2022): The selec-\ntion of quantized anchors relies on unsupervised\nstrategies, including Personalized PageRank, node\ndegree, and random approaches.\n• EARL (Chen et al., 2023): It randomly sam-\nples 10% entities as quantized anchors and intro-\nduces connected relation information to match an-\nchors’ counterparts.\n• Random entity quantization (RandomEQ for\nshort) (Li et al., 2023): It randomly sets anchor en-\ntities and randomly selects relations for matching.\nB.2\nKG Link Prediction\n• TransE (Bordes et al., 2013): The strategy of\nincorporating translational distance is utilized for\nlearning representations of entities and relations.\n• CompGCN (Vashishth et al., 2020):\nSev-\neral entity-relation composition operations are pro-\nposed to combine the semantic information of\nneighbor entity-relation pairs in GNNs.\n• AdaProp (Zhang et al., 2023): An adaptive\npropagation path is learned to filter out irrelevant\nentities while preserving promising targets in the\nGNN framework.\n• MA-GNN (Xu et al., 2023): A dual-branch,\nmulti-attention-based GNN model is employed to\ndevelop expressive entity representations.\n• TCRA (Guo et al., 2024a): A neuro-symbolic\nmethod that combines topological context learning\nwith rule augmentation.\n• DiffusionE (Cao et al., 2024): Introducing dif-\nfusion process to KG embedding method.\n\n• KICGPT (Wei et al., 2023): The method uti-\nlizes a model based on embeddings as the retriever,\nwhich generates a ranked list of potential entities.\nAn in-context learning strategy is then designed to\nguide ChatGPT in re-ranking these entities through\nmulti-round interactions.\n• CSProm-KG-CD (Li et al., 2024): It converts\ncompact and structured triples into segments en-\nriched with context by LLMs. Following this, two\ncustom auxiliary tasks (reconstruction and contex-\ntualization) are presented, which enable compact\nKGC models to incorporate insights derived from\nthese enhanced triples.\n• ARR (Chen et al., 2024): A three-step (align-\nment, reasoning, and reranking) process designed\nto support and amplify conventional KG embed-\nding models, without necessitating fine-tuning.\nThe results are taken from the setting of LLAMA3-\n70B and RotatE (Sun et al., 2019) model.\n• KG-FIT (Jiang et al., 2024): It involves the\nautomatic construction of a semantically consis-\ntent entity hierarchy through clustering and LLM-\nguided refinement. It also details a fine-tuning tech-\nnique that incorporates knowledge from the hierar-\nchical structure and pre-trained text embeddings of\nentities, thereby improving KG embeddings. The\nresults are from the HAKE model setting.\n• MKGL (Guo et al., 2024b): A context retriever\nis introduced to help LLMs be aware of the tex-\ntual and relational context of KGs. A score re-\ntriever is also used to provide the score information.\nLLaMA2 (7B) is utilized as the base LLM.\nB.3\nKG Triple Classification\n• TransE (Bordes et al., 2013): The strategy of\nincorporating translational distance is utilized for\nlearning representations of entities and relations.\n• DistMult (Yang et al., 2015): It utilizes the\nsemantic matching strategy, where the validity of\na fact is depicted as the matching degree between\nthe representation of entity and relation.\n• RotatE (Sun et al., 2019): It defines each re-\nlation as a rotation from the source entity to the\ntarget entity in a complex vector space.\n• Alpacazero-shot: It carries out zero-shot reason-\ning with Alpaca (Taori et al., 2023) with textual\nsequences for predicting the validity of a triple.\n• GPT-3.5zero-shot: It carries out zero-shot rea-\nsoning with GPT-3.5 2 with textual sequences for\n2https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-\napi-updates/\npredicting the validity of a triple.\n• KG-LLaMA (Yao et al., 2023): It carries out\ninstruction tuning with LLaMA with textual se-\nquences for predicting the validity of a triple.\n• KG-Alpaca (Yao et al., 2023): It carries out in-\nstruction tuning with Alpaca with textual sequences\nfor predicting the validity of a triple.\n• KoPA (Zhang et al., 2024b): It proposes a\nknowledge prefix adapter to effectively integrate\npre-trained KG structural embeddings with LLMs.\nAlpaca-7B is utilized as the LLM backbone.\nC\nExperimental Details\nTable 6: The statistics of WN18RR, FB15k-237, and\nFB15k-237N datasets. The former two are for link pre-\ndiction. FB15k-237N dataset is for triple classification,\nwhere ‘/’ splits the positive and negative samples.\nDataset\nEnt\nRel\nTrain\nValid\nTest\nWN18RR\n40943 11\n86835\n3034\n3134\nFB15k-237\n14541 237 272115\n17535\n20466\nFB15k-237N 13104 93\n87282 7041/7041 8226/8226\nThe statistics of utilized datasets are shown in\nTable 6. For the SSQR learning, the default embed-\nding dimension is set to 200. The GCN layer and\ndropout rate are 2 and 0.2. The training batch is\n1024. For optimization, the learning rate is 0.0005\nand the L2 regularization weight is 1e-8. For LLM\ntuning, we utilize 4 NVIDIA H100 GPUs and the\nlearning rate is set to 2e-5 with 3% warmup ra-\ntio. In the link prediction experiment, we first tune\nLLMs on the instruction data of CompGCN’s train-\ning split to initialize. Then, inspired by Wei et al.\n(2023) and Liu et al. (2024), we divide the valid\nset into two segments in a 9:1 ratio. The larger\npart is utilized to finetune LLMs to learn the rank-\ning preference, while the smaller part is used for\nvalidation. In the triple classification experiment,\nwe only update the embedding layer and the last\nfour Transformer layers of LLMs for tuning effi-\nciency. Meanwhile, M and N are set to 1024 and\n16. In the training instruction data, we randomly se-\nlect negative samples at a rate 16 times of positive\nones. The instruction format of triple classification\nis shown in Table 7.\nD\nEntropy and Jaccard Distance\nAs presented by Li et al. (2023), it is significant\nfor the ability to distinguish different entities for\n\nInstruction: Given a triple in the knowledge graph,\nyou need to predict its validity based on the triple\nitself and entities’ quantized representations.\nInput: The triple is: (h, r, t)\nThe quantized representation of entity h is: [Code(h)]\nThe quantized representation of entity t is: [Code(t)]\nPlease determine the validity of the triple and re-\nspond True or False.\nOutput: True/False\nTable 7: Instruction format for triple classification.\nquantized representations. We follow this study to\ncalculate the entropy at the overall representation\nlevel and the Jaccard distance at the codeword-\nselection level. The greater entropy and Jaccard\ndistance values denote the greater distinguishable\nability. The entropy is calculated as:\nH = −\nX\np\n\x00Code(e)\n\x01\n· log p\n\x00Code(e)\n\x01\n. (11)\np\n\x00Code(e)\n\x01\nis the relative frequency of quantized\nrepresentation of entity e. Moreover, the Jaccard\ndistance is given by:\nJ =\n1\n|E| · k\nX\nei∈E\nX\nej∈kNN(ei)\nd\n\x00Code(ei), Code(ej)\n\x01\n,\n(12)\nd\n\x00Code(ei), Code(ej)\n\x01\n= |CSet(ei) ∪CSet(ej)| −|CSet(ei) ∩CSet(ej)|\n|CSet(ei) ∪CSet(ej)|\n,\n(13)\nwhere kNN(ei) retrieves k entities. Each possesses\ncodes that exhibit the nearest Jaccard distance to\nthe codes of ei. CSet(e) is the set of Code(e) by\nremoving the order information of codewords of\nthe entity representations.\nE\nAdditional Experimental Analysis\nE.1\nTraining Process of SSQR\nWe display the training process SSQR in Figure 10,\nwhere w/o GCN and w/o sem denote ablations for\nthe structural embedding and semantic distilling,\nrespectively. The findings indicate that both struc-\ntural embedding and semantic distilling contribute\npositively to the overall learning of quantized rep-\nresentation. The influence of semantic information\non the FB15k-237 dataset is less significant when\ncompared to its effect on GCN. Differently, seman-\ntic information is more important on the WN18RR\ndataset. This could be attributed to the varying\nlevels of KGs’ sparsity.\nFigure 10: The training process of SSQR, where the\nHits@1 metric is used to show the model performance.\nE.2\nRelevance among Entity Codes on\nFB15k-237 Dataset\nWe also calculate the cosine similarity of quan-\ntized representation on the FB15k-237 dataset in\nFigure 12, which has the same setting as Figure 4.\nThe contents presented in these two figures are also\nsimilar. When utilizing solely text embeddings, the\ncorresponding similarities yield positive yet modest\nvalues. Moreover, the similarities associated with\nSSQR without the use of GCN are typically close\nto 1. These observations suggest that entity repre-\nsentations occupy a limited portion of the existing\nspace, thus failing to maximize the efficiency of\nrepresentation. SSQR addresses this issue to some\ndegree by providing a broader range and diversity\nof similarities.\nE.3\nImpacts of M and N for LLM Tuning on\nFB15k-237 Dataset\nWe also explore the impacts of M and N for LLM\ntuning on the FB15k-237 dataset, the results are\nshown in Figure 11. It can lead to conclusions\nsimilar to Figure 7.\nFigure 11: The impacts of quantized representation for\nKG link prediction task using LLMs on WN18RR.\n\n1.00\n0.10 1.00\n0.10 0.22 1.00\n0.07 0.21 0.09 1.00\n0.15 0.35 0.14 0.13 1.00\n0.13 0.07 0.14 0.10 0.12 1.00\n0.11 0.09 0.07 0.16 0.05 0.10 1.00\n0.08 0.23 0.15 0.34 0.14 0.14 0.14 1.00\n0.21 0.09 0.15 0.17 0.19 0.17 0.11 0.19 1.00\n0.11 0.28 0.28 0.14 0.33 0.08 0.11 0.14 0.16 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n(a) Original text embedding.\n1.00\n0.23 1.00\n-0.03 -0.03 1.00\n0.11 -0.21 -0.26 1.00\n0.04 0.03 0.09 0.10 1.00\n0.21 0.04 0.11 0.23 -0.08 1.00\n0.63 0.23 0.15 -0.03 -0.02 0.15 1.00\n0.37 -0.09 0.02 0.62 0.10 0.14 0.28 1.00\n0.40 -0.08 -0.12 0.34 0.16 0.15 0.42 0.42 1.00\n0.12 0.28 0.18 0.06 0.22 0.11 0.03 0.31 0.11 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b) SSQR.\n1.00\n1.00 1.00\n1.00 1.00 1.00\n1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(c) SSQR w/o GCN.\n1.00\n0.15 1.00\n-0.27 -0.24 1.00\n0.00 -0.11 -0.10 1.00\n0.01 0.03 0.21 0.04 1.00\n-0.11 0.13 0.22 -0.00 -0.08 1.00\n0.55 0.06 -0.04 0.06 0.16 -0.02 1.00\n0.15 -0.29 -0.04 0.47 -0.00 -0.07 0.14 1.00\n0.35 0.11 -0.11 0.39 0.04 0.20 0.26 0.51 1.00\n-0.11 0.11 -0.19 -0.01 0.26 -0.02 0.04 0.19 -0.03 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(d) SSQR w/o semantics.\nFigure 12: The cosine similarity of quantized representations on the FB15k-237 dataset (sampled 10 entities).\nE.4\nToken Embeddings in LLMs on\nFB15k-237 Dataset\nSimilar to Figure 8, we display the real word tokens\nand learned code tokens using t-SNE in Figure 13.\nThe evidence also suggests that these two types\nof tokens typically fall into distinct categories, im-\nplying they each have unique representation areas.\nE.5\nCase Studies\nTo intuitively show the seamlessly integrating KG\ntasks with LLMs, we carry out case studies in Ta-\nble 8, 9, and 10, covering both link prediction and\ntriple classification tasks. It demonstrates that our\nmethod can effectively address both tasks, indicat-\ning the validity and good generalization ability of\nour proposed SSQR.\nFigure 13: Token embedding virtualization in LLMs\n(FB15k-237 dataset), where red and blue dots are real\nword tokens and code tokens, respectively.\n\nInput: This is a knowledge graph completion task, which needs to predict the tail entity for an incomplete query triplet.\nThe query triplet is (radiotherapy, hypernym, ?).\nThe quantized representation of entity radiotherapy is: [2006] [588] [350] [1486] [214] [929] [328] [1424] [1792] [919]\n[944] [740] [438] [843] [147] [628]\nThe answer candidates and corresponding quantized representations are as follows:\ndisease, [156] [1880] [1777] [185] [121] [720] [783] [1713] [945] [1077] [180] [1576] [1574] [1433] [216] [1280]\ntomography, [182] [597] [657] [1486] [404] [468] [732] [564] [833] [1470] [1756] [626] [1674] [843] [1928] [513]\nmedical care, [422] [68] [1329] [1517] [1251] [431] [1479] [1445] [1666] [407] [952] [406] [1337] [388] [1982] [685]\nstatus, [1721] [1906] [1773] [1811] [12] [892] [1625] [1476] [1561] [176] [534] [1463] [1657] [368] [70] [1618]\nphysiological state, [1721] [718] [267] [394] [120] [1105] [885] [1823] [1496] [23] [952] [406] [1559] [1198] [1149] [1800]\nmedical science, [565] [413] [842] [1517] [350] [873] [575] [595] [721] [935] [1554] [175] [708] [1643] [1820] [1775]\ninfection, [565] [1594] [990] [1066] [974] [40] [434] [874] [1401] [371] [1700] [1118] [1709] [52] [71] [1408]\npicturing, [788] [168] [641] [1797] [927] [711] [1608] [123] [1163] [1460] [952] [406] [1752] [1464] [553] [1158]\nmedicine, [1879] [1216] [691] [296] [1743] [892] [1851] [595] [2039] [1428] [426] [740] [399] [579] [433] [1987]\nunhealthiness, [1389] [644] [570] [258] [635] [647] [732] [1139] [1660] [407] [464] [1020] [1574] [1905] [926] [1971]\ngrounds, [1268] [1053] [803] [780] [1194] [285] [328] [289] [1163] [915] [1921] [1020] [524] [1774] [430] [1572]\ndefense reaction, [1881] [1821] [1620] [1703] [435] [995] [908] [1308] [1596] [1598] [401] [2008] [903] [817] [92] [1158]\nradiology, [1478] [588] [1340] [1797] [1436] [1914] [1894] [1424] [634] [1460] [1756] [740] [673] [843] [108] [1088]\nradioscopy, [1005] [1002] [1441] [137] [1436] [1378] [1479] [1649] [1544] [1470] [534] [626] [902] [272] [904] [1874]\ntreat, [396] [2007] [1935] [1305] [1993] [1030] [1690] [1445] [1203] [1417] [1554] [495] [1752] [1001] [1236] [98]\nspecialize, [1005] [1933] [1976] [780] [927] [1728] [575] [105] [1791] [1598] [616] [1118] [1752] [425] [437] [1847]\ntherapy, [396] [816] [81] [488] [336] [1164] [1690] [1288] [900] [915] [1554] [175] [666] [1622] [765] [685]\nspecialism, [384] [816] [599] [394] [435] [789] [1479] [105] [664] [407] [1554] [103] [1752] [1708] [697] [1130]\nsymptom, [1721] [1913] [772] [858] [120] [1150] [1374] [289] [1666] [1417] [944] [2008] [1454] [958] [1169] [1800]\nmedicine, [156] [350] [1599] [1955] [1368] [508] [1527] [1445] [1561] [1460] [426] [1142] [940] [653] [793] [471]\nPlease generate quantized representations of the top-3 potential answer entities, ranked from highest to lowest:\nLLM Output: 1, [396] [816] [81] [488] [336] [1164] [1690] [1288] [900] [915] [1554] [175] [666] [1622] [765] [685]\n2, [156] [1880] [1777] [185] [121] [720] [783] [1713] [945] [1077] [180] [1576] [1574] [1433] [216] [1280]\n3, [182] [597] [657] [1486] [404] [468] [732] [564] [833] [1470] [1756] [626] [1674] [843] [1928] [513]\nGround Truth: [396] [816] [81] [488] [336] [1164] [1690] [1288] [900] [915] [1554] [175] [666] [1622] [765] [685]\nTable 8: Case study on WN18RR for link prediction using LLaMA2. The code of ground truth therapy is ranked to\nthe first position from 17-th.\n\nInput: This is a knowledge graph completion task, which needs to predict the tail entity for an incomplete query triplet.\nThe query triplet is (Valparaiso University, inverse relation of /location/location/contains, ?).\nThe quantized representation of entity Valparaiso University is [527] [1345] [1849] [1227] [1751] [2038] [818] [515] [1417]\n[333] [29] [721] [1691] [798] [1033] [153]\nThe answer candidates and corresponding quantized representations are as follows:\nMinnesota, [1532] [258] [1837] [357] [923] [1994] [638] [555] [771] [1003] [1736] [1473] [1495] [1436] [1313] [20]\nNew York, [661] [1243] [542] [1741] [1907] [1799] [858] [1794] [1916] [458] [1844] [909] [438] [1737] [686] [963]\nCalifornia, [1059] [1286] [1604] [846] [1086] [451] [1087] [1794] [994] [297] [1463] [159] [556] [1836] [407] [963]\nMassachusetts, [202] [1243] [977] [757] [304] [389] [1172] [1308] [1916] [1858] [1323] [11] [841] [1680] [1798] [1885]\nIllinois, [961] [1025] [1267] [174] [643] [1951] [1742] [1794] [1720] [1481] [543] [1883] [695] [1921] [182] [963]\nNew York City, [1458] [326] [1707] [239] [151] [640] [1366] [1794] [610] [458] [1844] [932] [122] [311] [121] [868]\nUnited Kingdom, [51] [193] [1354] [669] [1867] [881] [480] [1271] [392] [1858] [650] [909] [1503] [1126] [1550] [153]\nPennsylvania, [361] [825] [1052] [1655] [1670] [732] [951] [1569] [275] [1995] [543] [4] [753] [351] [331] [637]\nLos Angeles, [1584] [1231] [1707] [1461] [1867] [1466] [265] [1933] [850] [1533] [805] [1128] [1824] [1823] [307] [963]\nFlorida, [2016] [326] [542] [1614] [462] [1433] [1388] [819] [926] [1289] [1321] [563] [1977] [1144] [1268] [662]\nOhio, [1643] [1889] [1604] [88] [1364] [485] [1819] [1569] [54] [1582] [1500] [411] [438] [125] [1636] [20]\nTexas, [2012] [1845] [1207] [412] [531] [1394] [1004] [688] [653] [1671] [1790] [1690] [1732] [1686] [1721] [1205]\nVirginia, [99] [825] [738] [1859] [1287] [1540] [708] [780] [653] [662] [756] [1873] [1514] [1686] [59] [409]\nEngland, [848] [1220] [1052] [590] [175] [451] [529] [1933] [808] [1598] [1790] [1988] [670] [524] [121] [136]\nConnecticut, [633] [1025] [672] [338] [1694] [1799] [1528] [1177] [1949] [458] [1703] [411] [395] [33] [233] [1013]\nIndiana, [497] [1875] [1849] [377] [1694] [61] [1471] [1445] [392] [1672] [1500] [300] [711] [1839] [331] [136]\nLos Angeles County, [662] [1772] [558] [1623] [304] [1755] [1388] [1794] [1554] [333] [1662] [1258] [1474] [1764] [1021]\n[409]\nLouisiana, [1584] [326] [1309] [50] [1035] [0] [546] [728] [1130] [297] [1284] [11] [1514] [1929] [1248] [136]\nMichigan, [172] [326] [382] [174] [1364] [1774] [1165] [664] [994] [1480] [1923] [431] [816] [1587] [1443] [860]\nHennepin County, [584] [1890] [1263] [1168] [1990] [656] [784] [1735] [102] [1533] [1979] [1005] [1312] [786] [873]\n[1294]\nPlease generate quantized representations of the top-3 potential answer entities, ranked from highest to lowest:\nLLM Output: 1, [497] [1875] [1849] [377] [1694] [61] [1471] [1445] [392] [1672] [1500] [300] [711] [1839] [331] [136]\n2, [1532] [258] [1837] [357] [923] [1994] [638] [555] [771] [1003] [1736] [1473] [1495] [1436] [1313] [20]\n3, [661] [1243] [542] [1741] [1907] [1799] [858] [1794] [1916] [458] [1844] [909] [438] [1737] [686] [963]\nGround Truth: [497] [1875] [1849] [377] [1694] [61] [1471] [1445] [392] [1672] [1500] [300] [711] [1839] [331] [136]\nTable 9: Case study on FB15k-237 for link prediction using LLaMA2. The code of ground truth Indiana is ranked\nto the first position from 16-th.\nInput: Given a triple in the knowledge graph, you need to predict its validity based on the triple itself and entities’ quantized\nrepresentations.\nThe triple is: (Carl Foreman, /people/person/place_of_birth, Chicago)\nThe quantized representation of entity “Carl Foreman” is: [72] [114] [150] [336] [519] [494] [155] [482] [594] [934] [1012]\n[677] [473] [450] [827] [455]\nThe quantized representation of entity “Chicago” is: [603] [413] [385] [937] [731] [735] [836] [92] [400] [372] [563] [225]\n[82] [526] [81] [229]\nPlease determine the validity of the triple and respond True or False.\nLLM Output: True\nGround Truth: True\nInput: Given a triple in the knowledge graph, you need to predict its validity based on the triple itself and enti-\nties’ quantized representations.\nThe triple is: (Jessica Lange, /people/person/profession, Lawyer)\nThe quantized representation of entity “Jessica Lange” is: [23] [712] [484] [202] [289] [211] [772] [667] [870] [903] [213]\n[693] [360] [59] [868] [722]\nThe quantized representation of entity “Lawyer” is: [760] [84] [976] [802] [430] [735] [912] [480] [966] [411] [284] [113]\n[727] [744] [333] [56]\nPlease determine the validity of the triple and respond True or False.\nLLM Output: False\nGround Truth: False\nTable 10: Two cases on FB15k-237N dataset for triple classification using LLaMA2.')],
 '2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=124, thumbnail=None, content='Since the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15'),
                Paper(arxiv_id='2502.01237', authors=['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], published_at=datetime.datetime(2025, 2, 4, 3, 10, 49, 348000, tzinfo=datetime.timezone.utc), title='The Differences Between Direct Alignment Algorithms are a Blur', summary='Direct Alignment Algorithms (DAAs) simplify language model alignment by\nreplacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement\nLearning from Human Feedback (RLHF) with direct policy optimization. DAAs can\nbe classified by their ranking losses (pairwise vs. pointwise), by the rewards\nused in those losses (e.g., likelihood ratios of policy and reference policy,\nor odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that one-stage methods underperform\ntwo-stage methods. To address this, we incorporate an explicit SFT phase and\nintroduce the beta parameter, controlling the strength of preference\noptimization, into single-stage ORPO and ASFT. These modifications improve\ntheir performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT),\nmatching two-stage methods like DPO. Further analysis reveals that the key\nfactor is whether the approach uses pairwise or pointwise objectives, rather\nthan the specific implicit reward or loss function. These results highlight the\nimportance of careful evaluation to avoid premature claims of performance gains\nor overall superiority in alignment algorithms.', upvotes=100, thumbnail=None, content='Large Language Models (LLMs) demonstrate strong text\ngeneration capabilities, yet aligning them with human val-\nues remains challenging due to underspecified objectives,\nlimited training signals, and the complexity of human in-\ntent (Ouyang et al., 2022; Stiennon et al., 2020). Tradi-\ntional alignment pipelines typically involve Supervised Fine-\nTuning (SFT), reward modeling, and reinforcement learning\nto shape model outputs.\nRecently, Direct Alignment Algorithms (DAAs) have\n1T-Tech.\nCorrespondence\nto:\nBoris\nShaposhnikov\n<b.shaposhnikov@tbank.ru>.\nemerged as an alternative, integrating human preferences\ninto policy optimization without explicit reward modeling\nor reinforcement learning (Rafailov et al., 2023; Hong et al.,\n2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024;\nXiao et al., 2024; D’Oosterlinck et al., 2024; Wang et al.,\n2024). These methods differ in theoretical design (pairwise\nvs. pointwise), implementation details (e.g., reference pol-\nicy vs. odds ratio), and whether an SFT phase is required\n(one-stage vs. two-stage). This diversity raises key ques-\ntions about their relationships, comparative advantages, and\nthe role of SFT.\nIn this paper, we show that one-stage methods (e.g., ORPO,\nASFT) can incorporate an explicit SFT phase, improving\nperformance. We introduce a scaling parameter β that uni-\nfies their formulation with other DAAs, revealing shared\noptimization dynamics between methods using either an\nodds ratio or a reference-based reward. Through theoretical\nand empirical analysis, we systematically compare DAAs,\nemphasizing pairwise vs. pointwise preference optimiza-\ntion. We also show that, while SFT is beneficial, using the\nfull dataset is not always necessary, which reduces com-\nputational costs. To structure our analysis, we address the\nfollowing research questions:\nRQ1: Does an explicit SFT stage improve the alignment\nquality of ORPO and ASFT?\nRQ2: Does the tempering factor enhance the alignment\nquality of ASFT and ORPO?\nRQ3: What factors of DAAs affect alignment quality?\nRQ4: How does the final alignment quality depend on the\namount of data used in the SFT stage?\nBy answering these questions, we clarify key trade-offs in\nalignment strategies and provide guidance for optimizing\nLLM training pipelines.\n2. Preliminaries\n2.1. Modeling Sequences\nGiven a sequence y of length |y|, the log-probability can be\nwritten as log p(y) = P|y|\ni=1 log p(yi | y<i), which may also\nbe conditioned on another sequence x. In practice, optimiz-\ning normalized log-probability\n1\n|y| log p(y) = log\n\x00p(y)\n1\n|y| \x01\n1\narXiv:2502.01237v1  [cs.LG]  3 Feb 2025\n\nThe Differences Between Direct Alignment Algorithms are a Blur\noften improves numerical stability and leads to better train-\ning. However, once normalized, the resulting quantity is\nno longer a strict probability measure. Throughout this pa-\nper, whenever we write p(y), we refer to this normalized\nversion p(y)\n1\n|y| . Whenever a method does not apply this\nnormalization, we indicate it explicitly.\nWelleck et al. (2019) introduced a log-unlikelihood term\nthat reduces the probability of certain undesirable tokens:\nlog\n\x001 −p(c | y<i)\n\x01\nfor c ∈C. It can be extended to an\nentire sequence as log\n\x001 −p(y)\n\x01\n.\n2.2. Reinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF)\n(Ouyang et al., 2022; Stiennon et al., 2020) is a prominent\napproach to aligning language models. It generally has three\nstages:\n• Supervised Fine-Tuning (SFT). During the SFT stage,\nthe model πθ is trained to follow instructions by max-\nimizing the probability of correct output y given in-\nput x. For a single training pair (x, y), we define the\nper-sample SFT loss as LSFT(πθ, x, y) = −log πθ(y |\nx). During fine-tuning, we minimize the expectation\nof this per-sample loss over the training dataset D:\nE(x,y) ∼D\nh\nLSFT(πθ, x, y)\ni\n.\n• Reward Modeling (RM). A reward model rψ(x, y) pro-\nduces a satisfaction score. It is trained on preference\npairs using the Bradley-Terry model (Bradley & Terry,\n1952): LRM(rψ) = −E(x,yw,yl)∼D\n\x02\nlog σ\n\x00rψ(x, yw) −\nrψ(x, yl)\n\x01\x03\n, where yw is the preferred response and yl is\nthe less preferred one.\n• Reward\nMaximization.\nThe\nobjective\nis\nto\ngenerate\nresponses\nthat\nmaximize\nthe\nlearned\nreward,\nwith\na\nKL\npenalty\nto\nprevent\nreward\nhacking:\nmaxπθ Ex∼D, y∼πθ(y|x)\n\x02\nrϕ(x, y)\n\x03\n−\nβ DKL\n\x02\nπθ(x, y) ∥πref(x, y)\n\x03\n. Reinforcement learning\n(RL) algorithms are commonly used to optimize this\nobjective (Schulman et al., 2017; Ouyang et al., 2022).\n2.3. Direct Alignment Algorithms\nDirect alignment algorithms replace the reward modeling\nand RL stages (but keep the SFT phase) with a single align-\nment step. Various preference-optimization loss functions\nhave been proposed, employing these core components:\n• rref\nθ (y, x) = log\n\x00 πθ(y|x)\nπref(y|x)\n\x01\nfrom DPO (Rafailov et al.,\n2023), which acts as an implicit reward β rref\nθ . No length\nnormalization is used.\n• rodds\nθ\n(y, x) = log\n\x00πθ(y|x)\n1−πθ(y|x)\n\x01\nproposed in ORPO (Hong\net al., 2024), representing the odds of generating y versus\nnot generating it.\nSeveral Direct Alignment Algorithms use these notations.\nInformation on sequence probability normalization for these\nmethods is presented in Appendix A.1.\n• Direct Preference Optimization (DPO) (Rafailov\net al., 2023):\nLDPO\n=\n−log σ\n\x00β rref\nθ (yw, x) −\nβ rref\nθ (yl, x)\n\x01\n.This method does not normalize probabili-\nties by length.2\n• Identity Preference Optimization (IPO) (Azar et al.,\n2023): LIPO =\n\x00rref\nθ (yw, x) −rref\nθ (yl, x) −\n1\n2β\n\x012.\n• Simple Preference Optimization (SimPO) (Meng\net al., 2024): LSimPO = −log σ\n\x00β log πθ(yw, x) −\nβ log πθ(yl, x) −γ\n\x01\n.\n• Noise\nContrastive\nAlignment\n(NCA)\n(Chen\net al., 2024):\nLNCA\n=\n−log σ\n\x00β rref\nθ (yw, x)\n\x01\n−\n0.5 log σ\n\x00−β rref\nθ (yw, x)\n\x01\n−0.5 log σ\n\x00−β rref\nθ (yl, x)\n\x01\n.\n• Calibrated\nDirect\nPreference\nOptimization\n(Cal-DPO) (Xiao et al., 2024):\nLCal−DPO\n=\n−log σ\n\x00rref\nθ (yw, x) −rref\nθ (yl, x)\n\x01\n+\n\x00rref\nθ (yw, x) −\n1\n2β\n\x012 +\n\x00rref\nθ (yl, x) +\n1\n2β\n\x012.\n• Anchored Preference Optimization Zero (APO-\nZero) (D’Oosterlinck et al., 2024):\nLAPO−Zero\n=\n−σ\n\x00β rref\nθ (yw, x)\n\x01\n+ σ\n\x00β rref\nθ (yl, x)\n\x01\n.\n2.4. Single-Stage Alignment Methods\nSingle-stage alignment (as a subset of DAA methods)\nmerges SFT and direct alignment in one step by adding their\nlosses: LSingle(πθ) = −E(x,yw,yl)∼D\n\x02\nLSFT(πθ, x, yw) +\nλ LAlign(πθ, x, yw, yl)\n\x03\n, where λ is a hyperparameter, and\nno reference policy πref is required.\nIn this paper, we focus on:\n• Odds Ratio Preference Optimization (ORPO)\n(Hong et al., 2024): LORPO = −log πθ(yw|x) −\nλ log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLORPOAlign\n.\n• Aligned Supervised Fine-Tuning (ASFT) (Wang\net al., 2024):\nLASFT\n=\n−log πθ(yw|x) −\nλ\n\x10\nlog σ\n\x00rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLASFTAlign\n\x11\n.\n2Unless otherwise noted, the expectation over (x, yw, yl) ∼D\nis taken.\n2\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n3. Method\nMany DAAs have been proposed, raising questions about\ntheir differences and significance. They can be categorized\nin various ways. For example, one classification separates\nsingle-stage methods, which perform alignment directly\nafter obtaining a base model (ASFT and ORPO), from two-\nstage methods (which perform SFT before alignment), as\nin DPO, IPO, SimPO, etc. Under this scheme, ASFT and\nORPO are single-stage methods.\nAnother classification considers whether rref or rodds is\nused as an implicit reward. ASFT and ORPO also differ\nfrom other losses by using an odds ratio, whereas other\nmethods in Section 2 use normalized policy probabilities.1\nDAAs can also be distinguished by whether their loss func-\ntion is optimized for pairwise or pointwise preferences.\nDPO, for instance, increases the policy’s probability of\nchoosing preferred sequences relative to rejected ones. In\ncontrast, ASFT simply increases or decreases probabilities\nfor chosen or rejected sequences without comparing them\ndirectly.\n3.1. Generalizing ASFT and ORPO\nDespite these classifications, it can still be difficult to pin-\npoint the essential differences among DAAs, especially\nwhen design choices limit generalization. ASFT and ORPO,\nfor example, lack a parameter β, probably because they\nwere conceived as single-stage methods, making the dis-\ntance from a reference policy unnecessary. It might seem\nodd to introduce such a parameter in single-stage methods,\nbut we will show that for both ASFT and ORPO, the single-\nstage design and the absence of β are not strictly required.\n3.1.1. ORPO AND ASFT CAN OPERATE WITHOUT THE\nSFT LOSS TERM AND AS TWO-STAGE METHODS.\nWe begin by inspecting the ASFT objective and demonstrate\nthat it combines both likelihood and unlikelihood terms:\nTheorem 3.1. LASFT is equivalent to the Binary Cross-\nEntropy (BCE) loss, encapsulating both likelihood and un-\nlikelihood components:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nThe proof of Theorem 3.1 is provided in Appendix B. Conse-\nquently,\nLASFTAlign = −\n\x10\nlog πθ(yw|x) + log\n\x001 −πθ(yl|x)\n\x01\x11\n.\nNext, we derive a direct relationship between LORPO and\n1SimPO does not explicitly use a reference policy, but can be\ntreated similarly if a uniform reference policy is assumed.\nLASFT, showing that the latter provides an upper bound on\nthe former:\nTheorem 3.2. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x))\n+ πθ(yl|x)(1 −πθ(yw|x))\n\x01\n,\nwhere the additional term is symmetric in yw and yl.\nThe proof of Theorem 3.2 is provided in Appendix C. As for\nLASFTAlign, the alignment term is then\nLORPOAlign = −log πθ(yw|x) −log(1 −πθ(yl|x))\n+ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nCorollary 3.3. LORPO ≤LASFT and LORPOAlign ≤\nLASFTAlign.\nThis follows from the fact that the additional term in LORPO\nis non-positive when πθ(yw|x) and πθ(yl|x) lie in [0, 1], and\nπθ(yw|x) + πθ(yl|x) ≤1.\nThese findings yield two main observations:\n• LASFT provides an upper bound on LORPO. Minimiz-\ning the former also minimizes the latter.\n• LASFT can be viewed as a minimal form of a DAA\nloss, reflecting the structure of BCE.\nAn essential insight from these formulations is that the SFT\nterm in the ASFT and ORPO losses is already included in\nthe full loss. We hypothesize that this feature may allow us\nto omit the SFT term in the complete loss, first performing\nan SFT phase and then using only the alignment terms for\nmodel alignment. From this perspective, one can experi-\nment with these methods in both single-stage and two-stage\nconfigurations to see which approach is more effective.\n3.1.2. TEMPERING ASFT AND ORPO\nWe now consider the original single-stage methods from Sec-\ntion 2.4 and examine how the alignment terms LORPOAlign\nand LASFTAlign compare. These terms optimize preferences\nand, depending on the coefficient λ, can dominate or have a\nsmaller impact on the final loss.\nLASFTAlign and LORPOAlign strongly resemble the DAA\nlosses discussed in Section 2.3. The single-stage analogue\nof rref\nθ\nis rodds\nθ\n. Inspired by this analogy, we introduce a\ncoefficient β to scale rodds\nθ\n:\nLβ\nASFTAlign\n= −log σ(βrodds\nθ\n(yw, x)) −log σ(−βrodds\nθ\n(yl, x)),\nLβ\nORPOAlign\n= −log σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x)).\n3\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nBoth Lβ\nASFT and Lβ\nORPO generalize their vanilla counter-\nparts (recovering them when β = 1). As in DPO, β can\nbe viewed as a temperature or scaling parameter that regu-\nlates the intensity of the preference for “good” odds. This\nbecomes clearer when looking at the gradients:\n∇θLβ\nASFTAlign = −β\nh\nσ(βrodds\nθ\n(yl, x))∇θrodds\nθ\n(yl, x)\n+\n\x001 −σ(βrodds\nθ\n(yw, x))\n\x01\n∇θrodds\nθ\n(yw, x)\ni\n,\n∇θLβ\nORPOAlign = −β\nh\x00∇θrodds\nθ\n(yw, x) −∇θrodds\nθ\n(yl, x)\n\x01\n×\n\x10\n1 −σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x))\n\x11i\n,\nwhere ∇θrodds\nθ\n(y, x) =\n∇θ log πθ(y|x)\n1−πθ(y|x) .\nWhen β →0,\nσ(β · · · ) ≈1\n2, both methods aggressively improve the odds\nratio (increasing for yw and decreasing for yl). As β in-\ncreases, the updates become bounded by the factor σ(β · · · )\n(similar to a reward threshold in DPO). Hence, once the\nmodel improves, further updates are limited, either individ-\nually for Lβ\nASFTAlign or by pairwise ranking in Lβ\nORPOAlign.\nThis alignment with other DAAs allows for a direct com-\nparison of all methods in different setups, clarifying which\naspects are most critical for successful performance.\n3.2. On the Difference Between Direct Alignment\nAlgorithms\nDifferent methods can be grouped by the type of ”reward”\nfunction used in their loss. In general terms, Lβ\nASFTAlign\nand Lβ\nORPOAlign employ an odds ratio, while DPO, IPO,\nSimPO, NCA, Cal-DPO, and APO-Zero use a ratio between\nthe probability of the policy and that of a reference policy.\nThe following theorems make this classification clearer:\nTheorem 3.4. The gradient of Lβ\nASFTAlign becomes\ncollinear with the gradient of LORPOAlign as β →0. For-\nmally,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nindicating that both gradients point in the same direction.\nThe proof of Theorem 3.4 is provided in Appendix D.1.\nA related property applies to Lβ\nORPOAlign:\nTheorem 3.5. The gradient of Lβ\nORPOAlign is collinear with\nthe gradient of LORPOAlign for any β > 0. Formally,\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nβ > 0.\nThe proof of Theorem 3.5 is provided in Appendix E.1.\nFinally:\nTheorem\n3.6.\nFor\neach\nmethod\nX\n∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n,\nas\nβ →0, the gradient of LX is collinear with the gradient of\nLDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nThe proof of Theorem 3.6 is provided in Appendix F.1.\nThese theorems suggest that for sufficiently small β, these\nloss functions are split into two categories with indistin-\nguishable gradient directions. Although the magnitudes may\ndiffer and they may not be collinear for β ̸→0, one could in-\nfer that their performance should be similar when β is small.\nFrom this perspective, two main distinctions arise among\nthese methods: the use of an odds ratio (rodds\nθ\n) and the use\nof the ratio to a reference policy (rref\nθ ). Both choices might\ninfluence the final performance of these methods. Further-\nmore, it remains an open question whether odds-ratio-based\napproaches outperform reference-policy-based ones (e.g.,\nDPO), and how these distinctions compare to the contrast\nbetween pointwise and pairwise preference formulations.\nFrom traditional learning-to-rank (Liu et al., 2009) research,\npairwise methods often produce more direct and less noisy\nranking signals than pointwise techniques, which could lead\nto superior performance in practice (Burges et al., 2005; Li,\n2011; Melnikov et al., 2016). In the following sections, we\npresent experimental results that provide further insight into\nwhich aspects most strongly influence DAA training.\n4. Experimental Setup\nWe systematically compare and evaluate DAA methods us-\ning a standard training and instruction-following evaluation\nframework (Tunstall et al., 2023; Meng et al., 2024; Gorba-\ntovski et al., 2024). Our main experiments use the Llama\n3.1 8B model (AI@Meta, 2024), trained on the UltraChat\n(Ding et al., 2023) and UltraFeedback (UF) (Cui et al., 2023)\ndatasets, and evaluated on the AlpacaEval 2 (Dubois et al.,\n2024; Li et al., 2023) and ArenaHard (Li et al., 2024) bench-\nmarks. For the Reddit TL;DR (Stiennon et al., 2020) task,\nwe employ the Llama 3.2 3B model, comparing it side by\nside with the “golden” validation split (Rafailov et al., 2023;\n2024) using the prompt in Appendix I.\n4.1. Base vs SFT-Initialized Models.\nTo investigate the impact of SFT and the applicability of\none-stage loss LAlign component, we use the UF dataset for\nSFT (avoiding additional knowledge from UltraChat), and\nfor pairwise preference optimization. We carefully tuned the\nhyperparameters to optimize each method’s performance.\nFor the Base-initialized setup, we perform a grid search over\n4\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nORPO\nASFT\n0\n20\n40\n60\n80\n100\nLlama 3.2 3B TL;DR\n(GPT-4 WinRate, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nLlama 3.2 3B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0\n10\n20\n30\nLlama 3.1 8B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nFigure 1. Impact of the β Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning β (Section 3.1.2) affects\nboth ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2\nLC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each\nmethod’s best configuration at β = 1 as the baseline. See Section 5.2 for more details.\nlearning rates {6 × 10−6, 8 × 10−6, 1 × 10−5}, inspired\nby values suggested in ORPO and ASFT, and explore λ ∈\n{0.1, 0.2, 0.5, 1.0} for 1 and 2 training epochs keeping a\nsimilar budget to compare with the SFT-initialized setup.\nIn the SFT-initialized setup, we experiment with both\nLORPOAlign and LASFTAlign alone, as well as in combina-\ntion with LSFT, following the original methods. We tune\nthe learning rates {5 × 10−7, 7 × 10−7, 1 × 10−6} for one\nepoch, starting from an SFT model trained for 1 epoch at\n6 × 10−6.\n4.2. β Sensitivity.\nBuilding on the theoretical insights from Section 3.2, where\nDAA losses share indistinguishable gradient directions as\nβ →0, we evaluate each method across various β values to\nexamine quality-KL trade-offs. In classical DPO, β regu-\nlates the KL penalty from the reference policy, but setting\nβ too small can induce training instability. Therefore, we\nconduct a thorough sweep of at least six β values per DAA,\nexploring the performance limit of each method. To broaden\nour analysis, we consider three scenarios:\n1. Llama 3.2 3B TL;DR. A relatively simpler Reddit\nTL;DR summarization task, evaluated via GPT side-\nby-side comparison on 500 samples from the “golden”\nvalidation split (Rafailov et al., 2023; 2024).\n2. Llama 3.2 3B UF. The UltraChat and UF datasets\nserve as more challenging alignment settings due to\ntheir coverage of diverse and complex tasks, includ-\ning common sense reasoning, mathematical problem-\nsolving, code generation, logical reasoning, creative\nwriting, and general knowledge.\n3. Llama 3.1 8B UF. A larger, more capable model on the\nsame UltraChat and UF datasets, allowing us to assess\nhow increased model capacity influences β-sensitivity\nin these diverse tasks.\nFor the UF-based experiments, we measure model qual-\nity primarily using the AlpacaEval 2 Length-Controlled\n(LC) Win-Rate and ArenaHard (AH) WR, and then track\nKL divergence from a reference model to construct Pareto\nfronts. For the TL;DR scenario, we rely on GPT-based\npreference judgments using ‘gpt-4o-2024-08-06‘ model.\nConcretely, in each scenario we train models for differ-\nent values β, combining them with four possible learning\nrates {1 × 10−6, 7 × 10−7, 5 × 10−7, 3 × 10−7}. Further\nimplementation details, including training procedures and\ngeneration hyperparameters, are provided in Appendix A.\n4.3. SFT Quality.\nAlthough in principle single-stage methods do not require\na separate SFT phase, in practice an SFT-trained reference\nmodel often improves the final performance of two-stage\npipelines (see Section 5.1). Prior work, such as (Zhou et al.,\n2024), has shown that a small but high-quality dataset can be\nsufficient for instruction tuning. However, beyond response\nquality, it remains unclear how the amount of SFT data in-\nfluences alignment effectiveness. This raises a fundamental\nquestion: how much supervised data is actually needed to\nproduce a reference model that yields high-quality results\nafter the subsequent alignment step?\nTo investigate this, we prepared seven SFT checkpoints by\ntraining Llama 3.1 8B Base on 1%, 3%, 5%, 10%, 25%,\n50%, and 100% of the UltraChat dataset (2,079, 6,236,\n10,393, 20,786, 51,966, 103,932, and 207,865 records, re-\nspectively) using our SFT-initialized procedure. We then\napplied each alignment method – using optimal hyperparam-\neters from our β-sensitivity experiments (Appendix Table 7)\n– to these seven SFT checkpoints and the original base model.\nFinally, we evaluated all resulting aligned models on Al-\npacaEval 2 LC, analyzing their performance relative to the\nfraction of SFT data used.\n5\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nASFT\nCal-DPO\nNCA\nAPO Zero\nORPO\nSimPO\nIPO\nDPO\nSFT\n436\n5\n59\n457\n2\n41\n459\n5\n36\n463\n3\n34\n451\n3\n46\n458\n1\n41\n457\n2\n41\n456\n5\n39\n178\n24\n298\nWin\nTie\nLose\nWin / Tie / Lose Rate %\n35.6 / 4.8 / 59.6\n91.2 / 1.0 / 7.8\n91.4 / 0.4 / 8.2\n91.6 / 0.2 / 8.2\n90.2 / 0.6 / 9.2\n92.6 / 0.6 / 6.8\n91.8 / 1.0 / 7.2\n91.4 / 0.4 / 8.2\n87.2 / 1.0 / 11.8\nFigure 2. GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best\nhyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT\nachieves 87.2%, maintaining robust summarization performance. See Section 5.2 for more details.\n5. Results\n5.1. RQ1: Does an explicit SFT stage improve the\nalignment quality of ORPO and ASFT?\nAs shown in Table 1, the performance of ORPO and ASFT\nmethods improves significantly when the alignment loss\nLAlign is applied after a preceding SFT stage. In particular,\nORPO achieves results comparable to classical DPO in both\nLC Win Rate and AH WR metrics. In contrast, ASFT shows\nnotable gains in AH WR after the SFT stage, although it\nstill underperforms compared to ORPO or DPO.\nInit\nMethod\nLC% (std)\nWR% (std)\nAH% (CI)\nBase\nSFT\n6.7 (0.43)\n4.5 (0.63)\n3.5 (-0.7, 0.8)\nSFT\nORPO\n24.1 (0.84)\n17.8 (1.17)\n15.3 (-1.6, 1.8)\nSFT\nASFT\n16.4 (0.72)\n11.9 (0.99)\n10.6 (-1.2, 1.3)\nBase\nORPO\n14.8 (0.71)\n10.3 (0.95)\n8.4 (-1.3, 1.3)\nBase\nASFT\n14.5 (0.73)\n10.2 (0.94)\n7.5 (-1.1, 1.2)\nSFT\nORPO†\n13.4 (0.69)\n9.3 (0.91)\n7.7 (-0.9, 1.1)\nSFT\nASFT†\n11.4 (0.63)\n7.5 (0.83)\n7.5 (-1.1, 1.1)\nSFT\nDPO\n23.4 (0.85)\n20.0 (1.18)\n17.5 (-1.8, 1.8)\nTable 1. Base and SFT-initialized alignment methods on the\nLlama 3.1 8B model with the UF dataset. SFT-initialized meth-\nods demonstrate better performance compared to their traditional\nformulations without LSFT. Results marked with † correspond to\ntraining with LSFT, using the best hyperparameters: lr = 1×10−6\nfor ORPO and lr = 7 × 10−7 for ASFT. For other setups, the\nbest hyperparameters are: lr = 5 × 10−7 for standard SFT\nORPO/ASFT, and lr = 1 × 10−5/6 × 10−6 for Base ORPO/ASFT.\nFor single-stage methods, the use of λ = 1 provides the best\nresults within the explored grid of λ ∈{0.1, 0.2, 0.5, 1.0},\nespecially after two epochs of training. However, combining\nLSFT and LAlign in a single-stage setup leads to suboptimal\nresults compared to explicitly separating these phases, even\nwhen starting from an SFT-trained model. Incorporating an\nexplicit SFT stage improves overall performance for ORPO\nand ASFT methods. Therefore, all further experiments focus\non applying the LAlign components of ORPO and ASFT on\ntop of an SFT-trained model.\n5.2. RQ2: Does the tempering factor enhance the\nalignment quality of ASFT and ORPO?\nFigure 1 illustrates that introducing the β parameter (as\ndescribed in Section 3.1.2) improves the performance of\nboth ASFT and ORPO LAlign in our tested scenarios. For a\nfair comparison, we used the best-performing learning rate\nfor each baseline — LASFTAlign and LORPOAlign — while\nfixing β = 1. In the Llama 3.2 3B TL;DR experiment,\nthese adjustments led to an improvement of +7.0 for ORPO\nand +43.4 for ASFT in GPT-4 WR. In the Llama 3.1 8B\nUF setup, tuning β provided additional gains of +3.46 for\nORPO and +8.27 for ASFT on the AlpacaEval 2 LC WR.\n5.3. RQ3: What factors of DAAs affect alignment\nquality?\nBased on Section 3, we perform a comprehensive evaluation\nof alignment losses, including DPO, IPO, SimPO, NCA,\nCal-DPO, and APO-Zero, as well as enhanced Lβ\nASFTAlign\nand Lβ\nORPOAlign with the introduced parameter β. Unlike\nclassical methods where β typically regulates KL diver-\ngence against a reference policy πref, β in Lβ\nASFTAlign and\nLβ\nORPOAlign directly modulates the strength of preference\noptimization. To explore the upper limits of each method’s\nperformance, we performed an extensive hyperparameter\nsearch, analyzing both alignment quality and KL divergence.\nFull implementation details, including training setups and\nevaluation criteria, are provided in Appendix A.\nLlama 3.2 3B TL;DR: Figure 2 presents a comparison of\nall methods on the Reddit TL;DR validation subset, using\ntheir best hyperparameters. Most methods achieve a GPT-4\nWin Rate exceeding 90%, indicating robust summarization\nperformance on this relatively straightforward task. ASFT\nis slightly lower at 87.2% Win Rate, but still demonstrates\nstrong overall results.\nLlama 3.2 3B UF and Llama 3.1 8B UF: Table 2 summa-\n6\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nAlpacaEval 2\nArenaHard\nAlpacaEval 2\nArenaHard\nLC% (std)\nWR% (std)\nWR% (CI)\nLC% (std)\nWR% (std)\nWR% (CI)\nSFT\n5.02 (0.34)\n3.21 (0.55)\n1.4 (-0.4, 0.4)\n10.27 (0.54)\n5.44 (0.70)\n2.6 (-0.5, 0.6)\nDPO\n11.43 (0.58)\n11.79 (0.99)\n6.8 (-1.0, 0.9)\n26.82 (0.77)\n23.69 (1.25)\n19.0 (-1.9, 1.8)\nIPO\n11.24 (0.60)\n11.67 (1.01)\n6.8 (-1.0, 1.1)\n28.18 (0.83)\n24.43 (1.26)\n19.1 (-1.6, 1.5)\nSimPO\n10.56 (0.44)\n11.94 (0.95)\n6.4 (-1.0, 1.1)\n27.65 (0.77)\n25.62 (1.29)\n21.5 (-1.9, 1.9)\nORPO\n10.67 (0.50)\n12.23 (0.97)\n6.6 (-1.0, 1.1)\n28.25 (0.71)\n28.59 (1.33)\n20.9 (-2.0, 2.0)\nAPO Zero\n10.36 (0.53)\n11.22 (0.98)\n6.0 (-1.0, 0.9)\n23.15 (0.76)\n19.03 (1.18)\n17.3 (-1.8, 1.8)\nNCA\n10.33 (0.53)\n11.02 (0.97)\n5.1 (-0.7, 0.8)\n23.21 (0.80)\n18.67 (1.17)\n15.1 (-1.5, 1.6)\nCal-DPO\n10.62 (0.57)\n10.15 (0.94)\n4.8 (-0.9, 0.9)\n23.19 (0.82)\n18.85 (1.18)\n15.2 (-1.5, 1.6)\nASFT\n10.63 (0.55)\n9.21 (0.88)\n5.1 (-0.9, 0.9)\n20.82 (0.79)\n16.34 (1.13)\n13.5 (-1.6, 1.5)\nTable 2. AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF. The SFT model was trained on the UltraChat\ndataset. The best hyperparameters for each method were selected according to Section 4.2. Bold values indicate the best performance for\neach benchmark, while underlined values represent the second-best performance. See Section 5.3 for more details.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nKL Divergence with SFT Model\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\nFigure 3. Pareto front for alignment quality and KL divergence.\nResults for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are\ngrouped into pairwise and pointwise categories, with pairwise\nachieving higher LC values while remaining within overlapping\nconfidence intervals. See Section 5.3 for more details.\nrizes the results for both Llama 3.2 3B UF and Llama 3.1\n8B UF setups. For the smaller 3B model, the methods per-\nform similarly on LC WR, with slight differences emerging\non AH. Although these differences align with the pairwise\nvs. pointwise distinction (e.g., DPO, IPO, ORPO, SimPO\nvs. APO-Zero, NCA, Cal-DPO, ASFT), no single approach\nconsistently dominates across metrics. The overlap in con-\nfidence intervals further indicates that the results for these\nmethods are statistically similar in this setup, with no clear\nseparation.\nIn contrast, the 8B model reveals a clearer performance\ndifferentiation. Pairwise methods consistently outperformed\npointwise ones on AlpacaEval 2 and ArenaHard metrics,\nwith ORPO achieving the highest overall alignment quality.\nAs illustrated in Figure 3, pairwise approaches dominated\nthe KL Pareto front for the larger model, demonstrating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy (%)\nPointwise Pairwise\n0.42\n0.44\n0.71\n0.89\n=0.290\n=0.456\nMLP Dim = 1\nMLP Dim = 3\nFigure 4. Pairwise vs. Pointwise Ranking Methods on Toy Ex-\nample. Model capacity impacts ranking accuracy, with pairwise\nmethods outperforming pointwise ones as capacity increases. This\nbehavior is consistent with results observed in Llama experiments\non the UF dataset. See Section 5.3 for more details.\ntheir ability to more effectively balance alignment quality\nand divergence. Pareto fronts for the remaining setups are\nincluded in Appendix G for completeness.\nThese observations suggest that model capacity plays a sig-\nnificant role in amplifying the advantages of pairwise rank-\ning, where LLMs act as rankers (similar to Liu et al. (2024)).\nFor smaller models, such as the 3B setup, limited capacity\nmay hinder the ability to fully exploit pairwise gradient sig-\nnals. This hypothesis is supported by additional evidence\nfrom the toy example experiment (Figure 4), where pairwise\nmethods demonstrated performance similar to pointwise\nmethods with weaker MLPs but achieved better ranking\naccuracy as the model capacity increased. Full details of the\ntoy example setup are provided in Appendix H.\n5.4. RQ4: How does the final alignment quality depend\non the amount of data used in the SFT stage?\nIn Section 5.1, we show that DAAs designed to bypass\nthe SFT phase still underperform compared to models\nthat undergo SFT and are then aligned using a similar\n7\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\n25\nAlpaca Eval 2 LC WR (%)\nDPO\nIPO\nSimPO\nORPO\nSFT\nLine Type\nFraction\nFull\n(a) Pairwise\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\nAlpaca Eval 2 LC WR (%)\nAPO Zero\nNCA\nCal-DPO\nASFT\nSFT\nLine Type\nFraction\nFull\n(b) Pointwise\nFigure 5. Impact of SFT Dataset Size on Alignment Quality. Performance of the pairwise (a) and pointwise (b) alignment methods on\nAlpacaEval 2 (LC WR metric) when the SFT policy is trained on different fractions of the UltraChat dataset. Even a small fraction of SFT\ndata (e.g., 5–10%) yields substantial gains over starting from the raw base model. See Section 5.4 for more details.\npreference-optimization loss function without the SFT term.\nAs discussed in Section 4.3, this raises the question of how\nmuch supervised data is needed to compensate for the ad-\nditional computation and achieve comparable alignment\nperformance.\nTo investigate this, we trained seven SFT models on progres-\nsively larger UltraChat subsets (1% to 100%) and applied\neach alignment algorithm to these models and the non-fine-\ntuned base model, yielding eight initializations per method.\nFigures 5(a) and 5(b) summarize the results for pairwise\nand pointwise alignment methods, respectively. As the plots\nshow, no method starting from the raw base model can\nmatch the final quality of a method trained with the entire\nSFT dataset. However, even a modest size expansion of the\nSFT dataset yields substantial improvements in alignment\nquality: for example, moving from 3% to 5% of the data\nmore than doubles the AlpacaEval 2 LC score for the final\nmodel. Crucially, using only 10% of UltraChat for SFT\nyields nearly the same quality as using the entire dataset.\nAdding an SFT phase requires more overall training, but\nit pays off significantly in the final result. Moreover, one\ndoes not need the entire supervised corpus to realize most\nof these gains; even 5–10% of the data is often enough for\nDAAs to reach most of their potential.\n6. Conclusion\nThis paper presents a comprehensive theoretical and empiri-\ncal analysis of DAAs. Theoretically, we demonstrated that\nwithin each category - odds-based (rodds) and reference-\npolicy-based (rref) – gradient directions of popular methods\nalign as β →0, revealing shared optimization dynamics\nwithin these groups. We also showed that single-stage losses\n(e.g., ASFT, ORPO) can be extended to two-stage pipelines\nwith an explicit SFT step and optional β-scaling, enabling\ngreater flexibility. Experimentally, we addressed four core\nresearch questions (RQ1–4), exploring single- vs. two-stage\ntraining, implicit rewards, objective types, and the impact\nof the SFT phase. Our key findings are:\n• Include an SFT phase.\nAn SFT stage consistently\nimproves alignment performance (RQ1), with ORPO\nachieving +9.3 LC / +6.9 AH and ASFT +1.9 LC / +3.1\nAH in the setup from Section 4.1. Even 5–10% of the\nsupervised dataset often suffices to achieve near-optimal\nresults (RQ4).\n• Pairwise methods outperform pointwise objectives.\nAlignment quality depends more on the choice between\npairwise and pointwise objectives than on the formula-\ntion of implicit reward (e.g., rodds or rref). Pairwise\nmethods generally perform better (e.g., ORPO outper-\nforming ASFT by +7.43 LC / +7.4 AH in the Llama\n3.1 8B UF setup), particularly in larger models (RQ3).\nAmong these, ORPO and SimPO also stand out as prac-\ntical options for memory-constrained scenarios, as they\ndo not rely on a reference policy.\n• Choose hyperparameters carefully. Alignment per-\nformance is highly sensitive to learning rates and the\ncoefficient β. We provide optimal configurations for dif-\nferent methods based on comprehensive grid searches\nin our experimental setups, highlighting the added gains\nfrom tuning β in odds-based methods, where it controls\nthe strength of preference optimization (RQ2).\nLimitations and Future Work. Although our study sys-\ntematically compares DAAs, it has several limitations. We\ntested a limited set of datasets (UltraChat, UltraFeedback,\nReddit TL;DR) and benchmarks (AlpacaEval 2, ArenaHard),\nwhich may affect generalizability to other domains. The re-\nliance on GPT-based evaluators can introduce biases. More-\nover, we evaluated on 3B–8B models, so the observed ad-\nvantages of pairwise over pointwise objectives could shift\nat larger scales.\n8\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nReferences\nAI@Meta.\nLlama 3 model card.\n2024.\nURL\nhttps://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nAzar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,\nD., Valko, M., and Munos, R. A general theoretical\nparadigm to understand learning from human preferences,\n2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,\nT. J., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,\nEl-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez,\nD., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,\nN., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-\nCandlish, S., Olah, C., Mann, B., and Kaplan, J. Train-\ning a helpful and harmless assistant with reinforcement\nlearning from human feedback. ArXiv, abs/2204.05862,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:248118878.\nBradley, R. A. and Terry, M. E. Rank Analysis of Inclom-\nplete Block Design: The Method of Paired Comparisons.\nBiometrika, 39(3-4):324–345, 12 1952.\nISSN 0006-\n3444. doi: 10.1093/biomet/39.3-4.324. URL https:\n//doi.org/10.1093/biomet/39.3-4.324.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,\nHamilton, N., and Hullender, G. Learning to rank using\ngradient descent. In Proceedings of the 22nd international\nconference on Machine learning, pp. 89–96, 2005.\nChen, H., He, G., Yuan, L., Cui, G., Su, H., and Zhu, J.\nNoise contrastive alignment of language models with\nexplicit rewards, 2024. URL https://arxiv.org/\nabs/2402.05369.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback, 2023.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDing, N., Chen, Y., Xu, B., Qin, Y., Hu, S., Liu, Z., Sun, M.,\nand Zhou, B. Enhancing chat language models by scaling\nhigh-quality instructional conversations. In Bouamor,\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\n2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 3029–3051, Singapore, December\n2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.183. URL https://\naclanthology.org/2023.emnlp-main.183.\nD’Oosterlinck, K., Xu, W., Develder, C., Demeester, T.,\nSingh, A., Potts, C., Kiela, D., and Mehri, S. Anchored\npreference optimization and contrastive revisions: Ad-\ndressing underspecification in alignment, 2024. URL\nhttps://arxiv.org/abs/2408.06266.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nGorbatovski, A., Shaposhnikov, B., Malakhov, A., Sur-\nnachev, N., Aksenov, Y., Maksimov, I., Balagansky, N.,\nand Gavrilov, D. Learn your reference model for real\ngood alignment. arXiv preprint arXiv:2404.09656, 2024.\nHong, J., Lee, N., and Thorne, J. Orpo: Monolithic prefer-\nence optimization without reference model, 2024. URL\nhttps://arxiv.org/abs/2403.07691.\nKingma,\nD. P. and Ba,\nJ.\nAdam:\nA method\nfor stochastic optimization.\nCoRR, abs/1412.6980,\n2014.\nURL https://api.semanticscholar.\norg/CorpusID:6628106.\nLi, H. A short introduction to learning to rank. IEICE\nTRANSACTIONS on Information and Systems, 94(10):\n1854–1862, 2011.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline, 2024.\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-\nval: An automatic evaluator of instruction-following\nmodels.\nhttps://github.com/tatsu-lab/\nalpaca_eval, 5 2023.\nLiu, T., Qin, Z., Wu, J., Shen, J., Khalman, M., Joshi,\nR., Zhao, Y., Saleh, M., Baumgartner, S., Liu, J., et al.\nLipo: Listwise preference optimization through learning-\nto-rank. arXiv preprint arXiv:2402.01878, 2024.\nLiu, T.-Y. et al. Learning to rank for information retrieval.\nFoundations and Trends® in Information Retrieval, 3(3):\n225–331, 2009.\nMelnikov, V., H¨ullermeier, E., Kaimann, D., Frick, B., and\nGupta, P. Pairwise versus pointwise ranking: A case\nstudy. Schedae Informaticae, pp. 73–83, 2016.\nMeng, Y., Xia, M., and Chen, D. Simpo: Simple preference\noptimization with a reference-free reward. arXiv preprint\narXiv:2405.14734, 2024.\n9\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P. F., Leike, J.,\nand Lowe, R. Training language models to follow instruc-\ntions with human feedback. In Koyejo, S., Mohamed, S.,\nAgarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\nAdvances in Neural Information Processing Systems, vol-\nume 35, pp. 27730–27744. Curran Associates, Inc., 2022.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023. URL https://arxiv.org/abs/\n2305.18290.\nRafailov, R., Chittepu, Y., Park, R., Sikchi, H., Hejna, J.,\nKnox, B., Finn, C., and Niekum, S. Scaling laws for\nreward model overoptimization in direct alignment algo-\nrithms. arXiv preprint arXiv:2406.02900, 2024.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\nspeed: System optimizations enable training deep learn-\ning models with over 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pp. 3505–3506,\n2020.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O.\nProximal policy optimization al-\ngorithms.\nCoRR, abs/1707.06347, 2017.\nURL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1707.html#SchulmanWDRK17.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\nR., Voss, C., Radford, A., Amodei, D., and Christiano,\nP. Learning to summarize from human feedback. In\nNeurIPS, 2020.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Ra-\nsul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier,\nC., Habib, N., et al. Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944, 2023.\nWang, R., Sun, J., Hua, S., and Fang, Q. Asft: Aligned\nsupervised fine-tuning through absolute likelihood, 2024.\nURL https://arxiv.org/abs/2409.10571.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nand Weston, J. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319, 2019.\nXiao, T., Yuan, Y., Zhu, H., Li, M., and Honavar, V. G. Cal-\ndpo: Calibrated direct preference optimization for lan-\nguage model alignment, 2024. URL https://arxiv.\norg/abs/2412.14516.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\n10\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA. Implementation Details\nA.1. Probability Normalization\nAs discussed in Section 2.1, not all DDAs incorporate length-based probability normalization by default. In this paper,\nhowever, we consistently apply such normalization wherever probabilities are involved. This choice avoids introducing extra\nnotation and reduces the cognitive load on the reader. Table 3 summarizes the methods that originally include length-based\nnormalization.\nMethod\nUse normalization\nDPO (Rafailov et al., 2023)\n✗\nIPO (Azar et al., 2023)\n✗\nSimPO (Meng et al., 2024)\n✓\nNCA (Chen et al., 2024)\n✗\nCal-DPO (Xiao et al., 2024)\n✗\nAPO-Zero (D’Oosterlinck et al., 2024)\n✗\nORPO (Hong et al., 2024)\n✓\nASFT (Wang et al., 2024)\n✓\nTable 3. Methods that include (✓) or omit (✗) length-based probability normalization in their original formulation.\nA.2. Training Details\nOur experiments were conducted using the Llama 3.2 3B and Llama 3.1 8B Base models (AI@Meta, 2024). The training\nsetup, datasets, and hyperparameters were designed to ensure reproducibility and consistency. Unless otherwise noted, the\nhyperparameters in Table 4 were used across all experiments.\nHyperparameter\nValue\nMax Tokens Length\n1024 (TL;DR setup), 4096 (UF setup)\nEpochs\n1 (or 2 when specified)\nLearning Rate (SFT)\n6.0 × 10−6\nLearning Rate (Base Init.)\n{6.0 × 10−6, 8.0 × 10−6, 1.0 × 10−5}\nLearning Rate (Alignment)\n{3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6}\nOptimizer\nAdam (Kingma & Ba, 2014)\nAdam β1\n0.9\nAdam β2\n0.95\nBatch Size\n128\nLearning Schedule\nLinear Decay\nWarm-up Ratio\n0.03\nMax Gradient Norm\n2\nMemory Optimization\nDeepSpeed (Rasley et al., 2020)\nAttention Mechanism\nFlash Attention 2 (Dao, 2023)\nTable 4. Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models.\nTraining was performed on 8 NVIDIA A100 GPUs with 80GB memory each. Depending on the number of epochs, training\nfor each configuration took between 3 to 6 hours.\nA.2.1. DATASETS.\nWe used two primary datasets:\n• Reddit TL;DR (Bai et al., 2022): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B\nmodel.\n11\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n• UltraChat (Ding et al., 2023): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B and\nLlama 3.1 8B models.\n• UltraFeedback (Cui et al., 2023): used for both SFT (in the Base vs. SFT-initialized comparison, where we selected\nchosen subset from preference pairs) and for pairwise preference optimization in all DAA methods.\nThe dataset sizes are summarized in Table 5. For Base vs. SFT-initialized setups, only UltraFeedback was used. For\nβ-sensitivity experiments, the models were first trained on UltraChat for SFT and subsequently fine-tuned on UltraFeedback.\nThe Reddit TL;DR dataset was processed to remove duplicates, retaining only uniquely preferred summaries for SFT.\nDataset\nTraining Examples\nValidation Examples\nUltraChat\n207,865\n23,110\nUltraFeedback\n61,135\n2,000\nReddit TL;DR (SFT)\n41,947\n11,941\nReddit TL;DR (Preference)\n73,396\n21,198\nTable 5. Summary of dataset sizes used for training and validation.\nA.2.2. β-SENSITIVITY EXPERIMENTS.\nWe conducted a comprehensive analysis to evaluate the sensitivity of DAA methods to β, examining its impact on the\ntrade-off between model quality and KL divergence. Each method was trained using six or more distinct β values to identify\na configuration that achieves stable and effective performance. The specific β values tested for each method are as follows:\nMethod\nβ Values Tested\nDPO\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1}\nIPO\n{0.0007, 0.001, 0.005, 0.01, 0.05, 0.1}\nSimPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0}\nORPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nASFT\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nAPO-Zero\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2}\nCal-DPO\n{0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003}\nNCA\n{0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05}\nTable 6. Range of β values tested for each DAA method on all scenarios.\nFor each β, we tested four learning rates (3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6), training on the UltraFeedback\ndataset. All runs began from an SFT-initialized model trained on UltraChat (lr = 6.0 × 10−6, 1 epoch). The best-performing\nlearning rate for each β was selected to construct Pareto fronts, balancing quality (measured via AlpacaEval 2 LC Win-Rate)\nand KL divergence.\nFor SimPO in the Llama 3.1 8B UF setup, the ratio γ\nβ = 0.5 was kept fixed as recommended by Meng et al. (2024).\nAdditionally, a single learning rate (lr = 6.0 × 10−7) was tested across all β values for this method, as the same datasets\nand model scale were used. For Llama 3.2 TL;DR and UF setups, we tested four learning rates similar to other DAAs.\nBeyond the standard β values described in Table 6, additional values were explored for specific configurations to reach\nthe extreme points of the Pareto front. For example: - {0.00001, 0.00003} for Cal-DPO in Llama 3.2 3B TL;DR and UF\nsetups, - {0.00001, 0.00003, 0.00005} for NCA in Llama 3.2 3B TL;DR, - {0.0003, 0.0005} for APO-Zero in Llama 3.2\n3B TL;DR, - {0.0003, 0.0005, 0.001, 0.003, 0.005} for ASFT in Llama 3.2 3B TL;DR.\nThe hyperparameters resulting in the best performance are presented in Table 7.\nA.3. Generation Details\nWe evaluated model performance on AlpacaEval 2 and ArenaHard for UltraFeedback setups, while for the Reddit TL;DR\nsetup, we used side-by-side comparisons with GPT-4o on a curated golden validation subset of 500 samples. Additionally,\n12\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B TL;DR\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nLearning Rate\nβ\nLearning Rate\nβ\nLearning Rate\nβ\nDPO\n7.0 × 10−7\n0.05\n1.0 × 10−6\n0.01\n1.0 × 10−6\n0.003\nIPO\n1.0 × 10−6\n0.005\n7.0 × 10−7\n0.001\n1.0 × 10−6\n0.001\nSimPO\n3.0 × 10−7\n0.5\n7.0 × 10−7\n1.0\n6.0 × 10−7\n1.0\nORPO\n3.0 × 10−7\n0.5\n5.0 × 10−7\n0.2\n5.0 × 10−7\n0.5\nASFT\n3.0 × 10−7\n0.001\n1.0 × 10−6\n0.2\n7.0 × 10−7\n0.1\nAPO Zero\n3.0 × 10−7\n0.001\n3.0 × 10−7\n0.005\n3.0 × 10−7\n0.003\nNCA\n3.0 × 10−7\n0.0001\n3.0 × 10−7\n0.0005\n3.0 × 10−7\n0.0003\nCal-DPO\n3.0 × 10−7\n0.00003\n5.0 × 10−7\n0.0003\n3.0 × 10−7\n0.0003\nTable 7. Best hyperparameters for each DAA method across setups.\nKL divergence was measured on the validation subset for all setups using the generation hyperparameters listed in Table 8.\nFor ArenaHard, the temperature was set to 0 to adhere to the original benchmark configuration.\nHyperparameter\nValue\nTemperature\n0.9\nTop-k\n40\nTop-p\n1.0\nMax New Tokens\n256 (TL;DR setup), 4096 (UF setup)\nTable 8. Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models.\nB. Equivalence of ASFT Loss and Binary Cross-Entropy Loss\nLemma B.1.\nlog σ(rodds\nθ\n(y, x)) = log πθ(y|x)\nProof.\nlog σ(rodds\nθ\n(y, x)) = log σ(log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(1−πθ(y|x))−log(πθ(y|x)) = log\n1\n1 + 1−πθ(y|x)\nπθ(y|x)\n= −log\n\x10\n1 + 1 −πθ(y|x)\nπθ(y|x)\n\x11\n= −log πθ(y|x) + 1 −πθ(y|x)\nπθ(y|x)\n= log πθ(y|x).\nLemma B.2.\nlog σ(−rodds\nθ\n(y, x)) = log\n\x001 −πθ(y|x)\n\x01\nProof.\nlog σ(−rodds\nθ\n(y, x)) = log σ(−log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(πθ(y|x))−log(1−πθ(y|x)) = log\n1\n1 +\nπθ(y|x)\n1−πθ(y|x)\n=\n−log\n\x10\n1 +\nπθ(y|x)\n1 −πθ(y|x)\n\x11\n= −log 1 −πθ(y|x) + πθ(y|x)\n1 −πθ(y|x)\n= log(1 −πθ(y|x)).\n13\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nTheorem B.3. LASFT is equivalent to the binary cross-entropy loss, encompassing both likelihood and unlikelihood\ncomponents:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nProof. To show that LASFT is equivalent to the BCE loss, we start with the definition:\nLASFT = −log πθ(yw|x) −λ log σ(rodds\nθ\n(yw, x)) −λ log σ(−rodds\nθ\n(yl, x)),\nwhere rodds\nθ\n(y, x) =\nπθ(y|x)\n1−πθ(y,x). Applying Lemma B.1 and Lemma B.2 to the expression, we obtain:\nLASFT = −log πθ(yw|x) −λ log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n= −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)).\nC. Relationship Between ORPO and ASFT Loss Functions\nTheorem C.1. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nProof. We start by defining the ORPO loss:\nLORPO = −log πθ(yw|x) −λ log σ\n\x12\nlog\nπ(yw|x)\n1 −π(yw|x) −log\nπ(yl|x)\n1 −π(yl|x)\n\x13\n.\nExpanding the second term using the identity log σ(x) = x −log(ex + 1), we get:\n−log σ\n\x12\nlog\nπθ(yw|x)\n1 −πθ(yw|x) −log\nπθ(yl|x)\n1 −πθ(yl|x)\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x)(1 −πθ(yl|x))\nπθ(yl|x)(1 −πθ(yw|x)) + 1\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\nπθ(yl|x)(1 −πθ(yw|x))\n\x13\n= −log πθ(yw|x) −log(1 −πθ(yl|x)) + log\n\x00πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\n\x01\n|\n{z\n}\nORPOAlign\n.\nCombining all terms, we obtain:\nLORPO = −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)) + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n= LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\nD. Proof of Theorem 3.4\nTheorem D.1 (Collinearity of β-ASFT and ORPO Gradients). Let\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n,\nwhere\nrodds\nθ\n(y, x) = log\n\x10\nπθ(y|x)\n1−πθ(y|x)\n\x11\n.\n14\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nDefine the ORPO alignment loss as\nLORPOAlign = −log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n.\nThen,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n\r\r∇θ Lβ\nASFTAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r,\ni.e., their gradients become collinear in the same direction as β →0.\nProof. Step 1. Gradient of β-ASFT.\nDenote pw = πθ(yw | x), pl = πθ(yl | x). Then\nrodds\nθ\n(yw, x) = log\n\x10\npw\n1−pw\n\x11\n,\nrodds\nθ\n(yl, x) = log\n\x10\npl\n1−pl\n\x11\n.\nBy definition,\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n.\nFor small β, a first-order Taylor expansion of σ(β z) around 0 yields σ(β z) = 1\n2 + β z\n4 +O(β2). Thus, σ(β rodds\nθ\n(yw, x)) ≈\n1\n2 and σ(−β rodds\nθ\n(yl, x)) ≈1\n2. Taking gradients and applying the chain rule gives each term approximately proportional to\n± β ∇θ[rodds\nθ\n(·)]. Concretely,\n∇θ\n\x02\n−log σ(β rodds\nθ\n(yw, x))\n\x03\n≈−β\n2 ∇θ\n\x02\nrodds\nθ\n(yw, x)\n\x03\n,\n∇θ\n\x02\n−log σ(−β rodds\nθ\n(yl, x))\n\x03\n≈+ β\n2 ∇θ\n\x02\nrodds\nθ\n(yl, x)\n\x03\n.\nHence, summing up,\n∇θ Lβ\nASFTAlign ≈β\n2\nh\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\ni\n.\nObserve that β > 0 implies the overall scalar factor β\n2 is strictly positive in front of the difference of gradients.\nStep 2. Gradient of ORPO alignment loss.\nDefine ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n.\nIts gradient (using the chain rule) is proportional to\n∇θ LORPOAlign ∝−∇θ\n\x02\nrodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x03\n= ∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x).\nUp to a strictly positive logistic factor (since σ(·) ∈(0, 1)), the coefficient in front of ∇θ[rodds\nθ\n(·)] remains negative, but we\ntrack the absolute scalar to see it is positive. Indeed, one can write\n−∇θ\n\x00∆rodds\nθ\n(x)\n\x01\n= κORPO ∇θrodds\nθ\n(yl, x) −κORPO ∇θrodds\nθ\n(yw, x),\nκORPO > 0.\nStep 3. Conclusion (positive collinearity).\nComparing the two gradients:\n∇θ Lβ\nASFTAlign ≈\nβ\n2\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n,\n∇θ LORPOAlign ∝\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n.\nThe ratio is thus strictly positive for small β. Consequently,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nestablishing collinearity in the same direction.\n15\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nE. Proof of Theorem 3.5\nTheorem E.1 (Collinearity of β-ORPO and ORPO Gradients). Let\n∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x),\nand consider\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nIts gradient is collinear with the gradient of the standard ORPO alignment loss\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\nfor any fixed β > 0. Formally,\n∇θ Lβ\nORPOAlign\n\r\r∇θ Lβ\nORPOAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r.\nProof. Step 1. Gradient of β-ORPO.\nLet ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nBy the chain rule,\n∇θ Lβ\nORPOAlign = −\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\nβ ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince σ′(z) = σ(z) [1 −σ(z)], we have\n−\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\n= −β\n\x02\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01\x03\n.\nThus,\n∇θ Lβ\nORPOAlign = −β\nh\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01i\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince β > 0 and 1 −σ(·) > 0, the factor multiplying ∇θ[∆rodds\nθ\n(x)] is strictly negative.\nStep 2. Gradient of standard ORPO (i.e. β = 1).\nFor\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n,\nthe gradient is\n∇θ LORPOAlign = −\n\x02\n1 −σ(∆rodds\nθ\n(x))\n\x03\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nThis also has a strictly negative scalar in front of ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nStep 3. Conclusion (exact positive ratio).\nSince ∇θ Lβ\nORPOAlign and ∇θ LORPOAlign both differ from ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\nby a negative coefficient, it follows that these\ntwo gradients coincide up to a strictly positive factor:\n∇θ Lβ\nORPOAlign = κ(β) ∇θ LORPOAlign,\nκ(β) > 0.\nHence\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nproving the claimed collinearity (in the same direction) for every fixed β > 0.\n16\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nF. Proof of Theorem 3.6\nTheorem F.1 (Unified Collinearity of DPO with IPO, SimPO, NCA, Cal-DPO, and APO-Zero). Let\n∆rref\nθ (x) = rref\nθ\n\x00yw, x\n\x01\n−rref\nθ\n\x00yl, x\n\x01\n,\nand define the DPO loss\nLDPO = −log\n\x10\nσ\n\x00β ∆rref\nθ (x)\n\x01\x11\n,\nβ > 0.\nFor each method X ∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n, as β →0, the gradient of LX is asymptotically\ncollinear (i.e., it differs by a positive factor) with the gradient of LDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nProof of Theorem 3.6. Step 1: DPO as the baseline (tracking its sign).\nBy definition,\nLDPO = −log σ\n\x00β ∆rref\nθ (x)\n\x01\n.\nSince σ(u) = 1/(1 + e−u), for β > 0, one computes\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nObserve that β > 0 and σ(·) ∈(0, 1) imply\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01\n> 0.\nHence the factor multiplying ∇θ ∆rref\nθ (x) is negative. To unify directions by a positive multiple, note\n−∇θ LDPO = β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x),\nwhich has a strictly positive scalar in front. Thus, ∇θ LDPO is collinear with ∇θ ∆rref\nθ , and in particular its negative is a\npositive multiple of ∇θ ∆rref\nθ .\nStep 2: IPO.\nThe IPO loss is\nLIPO =\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x112\n.\nIts gradient is\n∇θ LIPO = 2\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x11\n∇θ ∆rref\nθ (x).\nAs β →0, the term\n1\n2β dominates ∆rref\nθ (x). Hence,\n∆rref\nθ (x) −\n1\n2β ≈−1\n2β ,\nso\n∇θ LIPO ≈−1\nβ ∇θ ∆rref\nθ (x).\nWe compare this with\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nBoth gradients are negative multiples of ∇θ ∆rref\nθ (x). Therefore,\n∇θ LIPO = κIPO(β) ∇θ LDPO,\nwith κIPO(β) > 0 as β →0.\nHence they are collinear in the same direction asymptotically.\n17\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nStep 3: SimPO.\nThe SimPO loss is\nLSimPO = −log σ\n\x00β ∆sθ −γ\n\x01\n,\nwhere ∆sθ = log πθ(yw | x) −log πθ(yl | x). Its gradient takes the form\n∇θ LSimPO = −β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\n∇θ ∆sθ.\nAgain, β > 0 and 1 −σ(·) > 0. Also, σ(β ∆sθ −γ) ∈(0, 1). Thus the prefactor\n−β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\nis strictly negative for each β > 0. Therefore, just like DPO, ∇θ LSimPO is in the negative direction of ∇θ ∆sθ. But ∇θ ∆sθ\nis proportionally the same as ∇θ ∆rref\nθ\nfor small-β expansions (both are differences of log-likelihood or reward-like terms).\nSo\n∇θ LSimPO = κSimPO(β) ∇θ LDPO,\nκSimPO(β) > 0 for small β.\nHence they are collinear with a positive factor in the low-β limit.\nStep 4: NCA.\nDefine\nrref\nw\n= rref\nθ\n\x00yw, x\n\x01\n,\nrref\nl\n= rref\nθ\n\x00yl, x\n\x01\n.\nThen NCA is\nLNCA = −log σ\n\x00β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nl\n\x01\n.\nFor small β, expand\nσ(β z) = 1\n2 + β z\n4\n+ O(β2),\nso log σ(β z) = log 1\n2 + log\n\x10\n1 + β z\n2 + O(β2)\n\x11\n. Each gradient term then yields a linear-in-β combination of ∇θ rref\nw and\n∇θ rref\nl . Collecting terms shows that, as β →0,\n∇θ LNCA ∝β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= β ∇θ ∆rref\nθ (x).\nComparing this with ∇θ LDPO = −β\n\x02\n1 −σ(. . . )\n\x03\n∇θ ∆rref\nθ (x) reveals another negative factor on the DPO side. In ratio\nform,\n∇θ LNCA = κNCA(β) ∇θ LDPO\nwith κNCA(β) > 0 for small β.\nHence collinearity follows.\nStep 5: Cal-DPO.\nThe Cal-DPO loss is\nLCal-DPO = −log σ\n\x00∆rref\nθ (x)\n\x01\n+\n\x00rref\nw −\n1\n2β\n\x012 +\n\x00rref\nl\n+\n1\n2β\n\x012.\nFor β near 0, the large constants ± 1\n2β dominate. The gradient w.r.t. θ in these squared terms is effectively\n∝−1\nβ ∇θ rref\nw\n+\n1\nβ ∇θ rref\nl\n= −1\nβ ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −1\nβ ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO has the same negative sign structure in front of ∇θ ∆rref\nθ , their ratio is again positive. Thus\n∇θ LCal-DPO = κCal-DPO(β) ∇θ LDPO\nwith κCal-DPO(β) > 0 as β →0.\nStep 6: APO-Zero.\nAPO-Zero is given by\nLAPO-Zero = −σ\n\x00β rref\nw\n\x01\n+ σ\n\x00β rref\nl\n\x01\n.\n18\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nIts gradient involves terms ∇θ σ(β rref\nw ) and ∇θ σ(β rref\nl ), each proportional to β ∇θ rref\nw and β ∇θ rref\nl . Subtracting these\nyields\n∇θ LAPO-Zero ∝−β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −β ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO also has a negative constant factor, their ratio has a positive limit. Therefore,\n∇θ LAPO-Zero = κAPO-Zero(β) ∇θ LDPO,\nκAPO-Zero(β) > 0 for small β.\nConclusion.\nIn each method X, one sees that ∇θ LX has the same negative-sign structure around ∇θ ∆rref\nθ (x) as does ∇θ LDPO,\nensuring a positive ratio in the limit. Formally,\n∇θ LX = κX(β) ∇θ LDPO,\nκX(β) > 0,\nas β →0.\nThus,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥,\nwhich completes the proof of their alignment in the same direction.\nG. Pareto fronts for Llama 3.2 setups\nThe results presented in this section correspond to the best hyperparameter configurations identified during the hyperparame-\nter search described in Section 4.2, including the optimal learning rate for each method. This ensures that the Pareto fronts\nreflect the upper performance limits for alignment quality.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL Divergence with SFT Model\n40\n50\n60\n70\n80\n90\nGPT-4 Win Rate (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(a) Llama 3.2 3B TL;DR\n0.0\n0.2\n0.4\n0.6\n0.8\nKL Divergence with SFT Model\n5\n6\n7\n8\n9\n10\n11\n12\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(b) Llama 3.2 3B UF\nFigure 6. Pareto front for alignment quality and KL divergence. Results for Llama 3.2 3B TL;DR and UF setups on GPT-4 Win\nRate vs. ”golden” validation subset and AlpacaEval 2 LC respectively with different β values. Methods are grouped into pairwise and\npointwise categories. For the summarization task (Llama 3.2 3B TL;DR), both pointwise and pairwise methods achieve strong overall\nresults. For the UF setup, methods also perform similarly within overlapping confidence intervals, indicating no clear separation.\nH. Toy Example Details\nTo analyze the differences between pairwise and pointwise ranking methods, especially with respect to the ranking nature of\nalignment losses in LLMs, a simplified toy experiment was conducted under a controlled setup. A dataset of 2000 triplets\n(x, yw, yl) was generated, where x, yw, and yl are real-valued scalars satisfying yw > yl. The data was split into 80% for\ntraining and 20% for testing. When the model processes a scalar input x together with a candidate y, these two numbers\nform a vector in R2, which serves as the input of the Multi-Layer Perceptron (MLP) to predict the reward r.\n19\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA single-hidden-layer MLP with ReLU activation was used in two capacity settings: lower (hidden size = 1) and higher\n(hidden size = 3). The model takes x and a candidate y as input, producing a reward r analogous to training a reward model\nfor RLHF (Stiennon et al., 2020).\nTwo losses were evaluated: the pairwise Bradley-Terry loss (Bradley & Terry, 1952),\nLPairwise = −log\n\x00σ(β(rw −rl))\n\x01\n,\nand the pointwise loss,\nLPointwise = −\n\x02\nlog\n\x00σ(βrw)\n\x01\n+ log\n\x00σ(−βrl)\n\x01\x03\n.\nEach configuration was trained over 100 runs, tuning the learning rate from {0.5, 0.3, 0.1, 0.01, 0.03, 0.05} and β from\n{5.0, 2.0, 1.0, 0.2, 0.1, 0.05, 0.01}. Alignment accuracy was defined as the proportion of cases with rw > rl.\nThe results show that both methods yield comparable performance in the low-capacity regime, while pairwise ranking\nachieves higher accuracy as model capacity increases, mirroring the effects observed in larger-scale experiments from the\nSection 5.3.\nI. GPT-4 Side-By-Side Evaluation Prompt\nFor our Side-By-Side evaluations with GPT-4o, we designed a prompt tailored to the Reddit TL;DR dataset to assess\naccuracy, completeness, relevance, and conciseness. The full prompt used in our experiments is detailed below.\nAct as an impartial judge and evaluate the quality of the summaries provided\nby two AI assistants for the text displayed below. Your evaluation should\nconsider accuracy, completeness, relevance, and conciseness.\nYou will be given a text, Assistant A’s summary, and Assistant B’s summary.\nYour job is to evaluate which assistant’s summary is better based on the\ntext provided.\nBegin your evaluation by comparing both assistants’ summaries with the\noriginal text. Identify and correct any inaccuracies.\nEnsure the summaries are complete, capturing all essential information\nfrom the text without introducing fabricated details.\nAssess the relevance of the information each assistant chose to include\nin their summary, ensuring it reflects the core message of the text.\nEvaluate the conciseness of the summaries, favoring those that efficiently\nconvey the necessary information without unnecessary verbosity.\nAvoid any position biases and ensure the order in which the summaries\nwere presented does not influence your decision.\nDo not allow the length of the summaries to influence your evaluation,\nexcept in the context of conciseness and efficiency.\nDo not favor certain names of the assistants.\nBe as objective as possible.\nYou should only evaluate the summaries provided by both assistants\nand NOT the original text itself.\nIf both summaries are irrelevant, contain hallucinations, or are\ninconsistent with the original text, mark the comparison as inconclusive\nand choose option "C".\nAfter providing your explanation, output your final verdict by strictly\nfollowing this format:\n"""\n20\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nComparison: <One-sentence comparison>\nWinner: <A if assistant A is better, B if assistant B is better, and C for a tie.>\n"""\n21')]}
2025-02-06 00:27:48,555 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:30:21,381 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:32:11,337 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:32:37,933 - INFO - Total execution time: 26.00 seconds (0.43 minutes)
2025-02-06 00:32:37,941 - INFO - Papers: {'2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=124, thumbnail=None, content='Since the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15'),
                Paper(arxiv_id='2502.01237', authors=['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], published_at=datetime.datetime(2025, 2, 4, 3, 10, 49, 348000, tzinfo=datetime.timezone.utc), title='The Differences Between Direct Alignment Algorithms are a Blur', summary='Direct Alignment Algorithms (DAAs) simplify language model alignment by\nreplacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement\nLearning from Human Feedback (RLHF) with direct policy optimization. DAAs can\nbe classified by their ranking losses (pairwise vs. pointwise), by the rewards\nused in those losses (e.g., likelihood ratios of policy and reference policy,\nor odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that one-stage methods underperform\ntwo-stage methods. To address this, we incorporate an explicit SFT phase and\nintroduce the beta parameter, controlling the strength of preference\noptimization, into single-stage ORPO and ASFT. These modifications improve\ntheir performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT),\nmatching two-stage methods like DPO. Further analysis reveals that the key\nfactor is whether the approach uses pairwise or pointwise objectives, rather\nthan the specific implicit reward or loss function. These results highlight the\nimportance of careful evaluation to avoid premature claims of performance gains\nor overall superiority in alignment algorithms.', upvotes=100, thumbnail=None, content='Large Language Models (LLMs) demonstrate strong text\ngeneration capabilities, yet aligning them with human val-\nues remains challenging due to underspecified objectives,\nlimited training signals, and the complexity of human in-\ntent (Ouyang et al., 2022; Stiennon et al., 2020). Tradi-\ntional alignment pipelines typically involve Supervised Fine-\nTuning (SFT), reward modeling, and reinforcement learning\nto shape model outputs.\nRecently, Direct Alignment Algorithms (DAAs) have\n1T-Tech.\nCorrespondence\nto:\nBoris\nShaposhnikov\n<b.shaposhnikov@tbank.ru>.\nemerged as an alternative, integrating human preferences\ninto policy optimization without explicit reward modeling\nor reinforcement learning (Rafailov et al., 2023; Hong et al.,\n2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024;\nXiao et al., 2024; D’Oosterlinck et al., 2024; Wang et al.,\n2024). These methods differ in theoretical design (pairwise\nvs. pointwise), implementation details (e.g., reference pol-\nicy vs. odds ratio), and whether an SFT phase is required\n(one-stage vs. two-stage). This diversity raises key ques-\ntions about their relationships, comparative advantages, and\nthe role of SFT.\nIn this paper, we show that one-stage methods (e.g., ORPO,\nASFT) can incorporate an explicit SFT phase, improving\nperformance. We introduce a scaling parameter β that uni-\nfies their formulation with other DAAs, revealing shared\noptimization dynamics between methods using either an\nodds ratio or a reference-based reward. Through theoretical\nand empirical analysis, we systematically compare DAAs,\nemphasizing pairwise vs. pointwise preference optimiza-\ntion. We also show that, while SFT is beneficial, using the\nfull dataset is not always necessary, which reduces com-\nputational costs. To structure our analysis, we address the\nfollowing research questions:\nRQ1: Does an explicit SFT stage improve the alignment\nquality of ORPO and ASFT?\nRQ2: Does the tempering factor enhance the alignment\nquality of ASFT and ORPO?\nRQ3: What factors of DAAs affect alignment quality?\nRQ4: How does the final alignment quality depend on the\namount of data used in the SFT stage?\nBy answering these questions, we clarify key trade-offs in\nalignment strategies and provide guidan'),
                Paper(arxiv_id='2502.01456', authors=['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], published_at=datetime.datetime(2025, 2, 4, 0, 2, 39, 922000, tzinfo=datetime.timezone.utc), title='Process Reinforcement through Implicit Rewards', summary="Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for the reinforcement learning\n(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of training process reward models (PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable to reward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using only policy rollouts and outcome labels through\nimplict process rewards. PRIME combines well with various advantage functions\nand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting from\nQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkey reasoning benchmarks over the SFT model. Notably, our resulting model,\nEurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning\nbenchmarks with 10% of its training data.", upvotes=48, thumbnail=None, content='Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While dense\nrewards also offer an appealing choice for the reinforcement learning (RL) of\nLLMs since their fine-grained rewards have the potential to address some inher-\nent issues of outcome rewards, such as training efficiency and credit assignment,\nthis potential remains largely unrealized. This can be primarily attributed to the\nchallenges of training process reward models (PRMs) online, where collecting\nhigh-quality process labels is prohibitively expensive, making them particularly\nvulnerable to reward hacking. To address these challenges, we propose PRIME\n(Process Reinforcement through IMplicit rEwards), which enables online PRM\nupdates using only policy rollouts and outcome labels through implict process\nrewards. PRIME combines well with various advantage functions and forgoes the\ndedicated reward model training phase that existing approaches require, substan-\ntially reducing the development overhead. We demonstrate PRIME’s effectiveness\non competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME\nachieves a 15.1% average improvement across several key reasoning benchmarks\nover the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses\nQwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its train-\ning data.1\n1\nINTRODUCTION\nDense process rewards, which provide feedback at each intermediate step rather than only the whole\ntrajectory, have proven effective in inference-time scaling of large language models (LLMs) on\nchallenging reasoning tasks (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Yuan\net al., 2024b). On the training side, they also present superiorities in the reinforcement learning\n(RL) of LLMs, particularly in improving training efficiency (Sutton & Barto, 2018) and credit\nassignment (Leike et al., 2018) compared with sparse outcome rewards. However, successful\napplications of dense rewards in RL for LLMs are limited (Setlur et al., 2024), as current industry-\nleading models primarily depend on verifiable outcome rewards and have not yet demonstrated\nmeaningful progress with dense rewards (DeepSeek-AI et al., 2025; Team et al., 2025).\nWe identify the central challenge as how to acquire and utilize high-quality dense rewards at scale,\nwhich enables online process reward model (PRM) update efficiently. The reason is that, optimizing\ntowards a static reward model eventually leads to overoptimization or reward hacking (Gao et al.,\n∗Core Contributors.\n†Project Lead.\n1Models and data are available at: https://github.com/PRIME-RL/PRIME.\n1\narXiv:2502.01456v1  [cs.LG]  3 Feb 2025\n\nPreprint\nAIME 2024\nAMC\nMinerva Math\nOlympiadBench\nMATH-500\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n26.7\n57.8\n38.6\n42.1\n79.2\n48.9\n3.3\n30.1\n32.7\n29.8\n65.1\n32.2\n13.3\n50.6\n34.6\n40.7\n79.8\n43.8\n16.7\n30.1\n35.3\n31.9\n64.6\n35.7\n9.3\n45.8\n36.8\n43.3\n76.4\n43.3\nEurus-2-7B-PRIME\nEurus-2-7B-SFT\nQwen-2.5-Math-7B-Instruct\nLlama-3.1-70B-Instruct\nGPT-4o-2024-08-06\nFigure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics\nbenchmarks, outperforming advanced math models and larger models. Notably, PRIME brings\nsubstantial performance gain (+16.7%) over Eurus-2-7B-SFT.\n2022) due to distribution shift. Ideally, this can be solved by improving the reward model online (Leike\net al., 2018). However, acquiring dense process labels for training is prohibitively more expensive.\nExisting methods either need to build complicated human annotation pipelines (Lightman et al.,\n2023) or rely on estimation-based methods, which require about 10× more rollouts for each step than\nsampling only the response-level trajectories (Wang et al., 2023; Kazemnejad et al., 2024). Neither of\nthem is scalable in online RL. Moreover, to the best of our knowledge, it remains underexplored how\nto incorporate dense rewards into RL for LLMs.\nIn this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable frame-\nwork for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level\nrewards. At its core, the framework employs recently proposed implicit process reward model-\ning (Yuan et al., 2024b) to train dense reward models with only outcome-level labels. This enables\nPRIME to perform online learning of reward signals using only outcome labels on policy rollouts,\nthereby fundamentally mitigating reward hacking while maintaining the same computational cost as\ntraditional outcome reward models (ORMs). Besides scalability, PRIME also (1) serves as a general\nmethod to fuse token-level dense rewards and sparse outcome rewards by calculating their returns\nseparately before summing together, which is compatible with diverse RL algorithms (Williams, 1992;\nKool et al., 2019; Shao et al., 2024; Ahmadian et al., 2024; Schulman et al., 2017); (2) eliminates the\ndedicated reward modeling stage, which is required by existing works, by simply initializing from the\nSFT model or even the base model (§ 5.6). In summary, starting from one single language model, the\nPRIME framework can efficiently accomplish the generation of dense rewards, the initialization and\nupdating of reward models, as well as the reinforcement learning (RL) training of the policy model.\nTable 1: The comparison of resource requirements between Eurus-\n2-7B-PRIME and Qwen2.5-Math-7B-Instruct.\nModel\nEurus-2-7B-PRIME\nQwen2.5-Math-7B-Instruct\nBase Model\nQwen2.5-Math-7B\nQwen2.5-Math-7B\nSFT Data\n230K (open-source)\n2.5M (open-source & in-house)\nRM Data\n0\n618K (in-house)\nRM\nEurus-2-7B-SFT\nQwen2.5-Math-RM (72B)\nRL Data\n150K queries × 4 samples\n66K queries × 32 samples\nIn\nexperiments,\nwe\ntrain\nQwen2.5-Math-7B-Base (Yang\net al., 2024b) with PRIME after\na lightweight SFT warmup stage.\nCompared to RL using outcome\nrewards only, PRIME achieves\na 2.5× sample efficiency gain\nand a 6.9% performance im-\nprovements on challenging math\nproblems. As shown in Figure 1,\nthrough PRIME, we successfully\nachieve substantial improvement on key mathematical reasoning benchmarks over the SFT model,\nleading to 16.7% improvement on average, and over 20% on AMC&AIME competitions. Our\nfinal model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical\nbenchmarks. Notably, this is achieved with only 10% of the data used by Qwen-Math, as in Table 1.\n2\n\nPreprint\nOur analysis shows that updating the PRM online is key to the success of PRIME (§5.1). We also\nshow that PRIME could generally boost various RL algorithms, including RLOO (Ahmadian et al.,\n2024), REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GRPO (Shao et al., 2024)\n(§5.4). In terms of the design choices of advantage estimate, we observe that Implicit PRMs are better\nto be used as reward models than value models (§5.5).\n2\nREINFORCEMENT LEARNING FOR LLMS AND THE CHALLENGES OF\nINCOPORATING DENSE REWARDS\nReinforcement Learning (RL) aims to learn an optimal policy πθ that maximizes the expected\ncumulative discounted reward, namely return, when interacting with an environment. In the context\nof autoregressive language modeling, state at step t is the concatenation of prompt x and current\nresponse y<t, and the action is the t-th token or step yt.\n2.1\nRL PRELIMINARIES FOR LLMS\nPolicy Gradient. Policy gradient is a fundamental algorithm that directly optimizes this objective.\nCentral to this approach is the advantage function At, which quantifies how much better an action is\ncompared to alternatives in a given state:\n∇θJ(θ) = Ex∼D,y∼πθ\n" T\nX\nt=0\n∇θ log πθ(yt|y<t)At\n#\n(1)\nwhere (x, y) represents a pair of input and output. x is omitted for brevity. In practice, the advantage\nfunction is implemented as cumulative discounted rewards subtracting a baseline:\nAt =\nT\nX\ns=t\nγs−tr(ys) −b\n(2)\nγ ∈[0, 1] is a discount factor that optionally decays future rewards, and r(ys) is the reward provided\nby the environment at time step s with x and y<s being omitted in conditions. Eq. 2 is the general\nformula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and\ndense reward at each step is crucial for RL. Different choices of b include, e.g. directly using values\nWilliams (1992), group average of rewards (Shao et al., 2024), and leave-one-out average of rewards\nAhmadian et al. (2024); Kool et al. (2019).\nValue Models. Though the MC estimate is unbiased, it suffers from high variance because of the\nreliance on all future actions and rewards, which can be random and noisy. Value models, which\npredict expected accumulated rewards starting from a state, are adopted to help reduce the variance\nin advantage estimation, such as Generalized Advantage Estimation (GAE; Schulman et al., 2016):\nAGAE(γ,λ)\nt\n= P∞\ns=0(γλ)sδt+s, where δt = r(yt) + γV (y<t+1) −V (y<t) is the temporal difference\n(TD) error (Sutton, 1988), V is a value model, and λ controls the bias-variance tradeoff in advantage\nestimation. PPO (Schulman et al., 2017) is a representative of such actor-critic algorithms that\nexplicitly train a value model along with the policy.\nReward Sparsity. Although dense rewards can be naturally integrated into the advantage function\nthrough Eq. 2, unfortunately, only outcome reward models (ORMs) are available in most practices\nof LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive\nno rewards (Rafailov et al., 2023; Shao et al., 2024; DeepSeek-AI et al., 2025). In this bandit\nsetting, r(yt) = 0 for t < T while r(yT ) can be non-zero, and Eq. 2 becomes A = r(yT ) −b. This\nformulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback\nonly at the end of the entire generation. This may (1) encourage spurious solutions with incorrect\nprocesses but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter\nthe credit assignment problem (Sutton & Barto, 2018). These drawbacks could be further amplified\non complicated tasks, which require more thinking and execution steps, urging the need of dense\nrewards (Uesato et al., 2022; Lightman et al., 2023). Some may consider employing a value model\nto mitigate the problem, as it predicts values at every step t. However, previous work showed that\nvalue models may not be able to solve the reward sparsity issue effectively due to training challenges,\ndespite the additional computation overhead (Shao et al., 2024; Ahmadian et al., 2024). We will\nalso empirically validate this claim in §5.5.\n3\n\nPreprint\n2.2\nKEY CHALLENGES IN SCALABLE DENSE REWARDS\nThe way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs,\nwhich score model responses over each token or step. However, it is usually infeasible in practice to\nincorporate dense rewards into online RL because of three critical challenges in implementation.\nC1. Process rewards are hard to define. It is difficult to collect step-level labels since reasoning\nsteps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating\nlabels for each token is too costly. Moreover, defining the absolute correctness of intermediate\nprocesses as dense rewards can be ambiguous, as some incorrect steps can also positively contribute\nto the final answer by pruning searching branches (OpenAI, 2024; DeepSeek-AI et al., 2025).\nC2. PRM online updates are not scalable. It is crucial to prevent reward overoptimization or reward\nhacking, which requires the reward model or value model to be updated online along with the policy\nmodel (Schulman et al., 2017; Gao et al., 2022). However, training PRMs often requires extensive\nnuanced step-level annotation, which is infeasible in online RL training. Therefore, this brings about\nconsiderable scalability and generalization concerns in dense rewards for RL.\nC3. Explicit reward modeling brings extra cost. Training reward models requires extensive\nannotation and broad data coverage to ensure a good balance between adaptability to the policy\ndistribution and generalization to distribution shifts. Hence, the explicit training stage introduces a\nvery costly data collection and an additional training overhead, especially for PRMs which typically\nrequire stepwise labels.\nNotably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMs\ninto their large-scale RL training (DeepSeek-AI et al., 2025).\n3\nPRIME\nTo address the above challenges, we propose PRIME, a scalable online RL method with dense\nrewards. The key insight of PRIME is to apply implicit process rewards, which are derivable from the\nImplicit PRM that is trained with only outcome labels (Yuan et al., 2024b). This property enables us to\nupdate the PRMs online to avoid reward hacking. We then design a flexible framework to incorporate\nimplicit process rewards with outcome rewards into any kind of MC advantage estimate. PRIME is\nillustrated in Figure 2 and Algorithm 1. Next, we will detail the implicit process rewards (§3.1) and\nhow we leverage them to calculate advantages (§3.2), and introduce other techniques we used (§3.3).\n3.1\nENABLING SCALABLE REWARD UPDATE WITH IMPLICIT REWARD MODELING\nWe consider dense rewards from the Implicit PRM because of the scalability. In short, Implicit PRM\nenables training an ORM with outcome labels only while repurposing it as a PRM at inference. The\ntraining stage is the same as standard ORM pipelines, with the only difference being representing\nthe reward as rϕ(y) := β log πϕ(y)\nπref(y), where πϕ is the RM and πref is the reference model, both of\nwhich are causal LMs. At inference, the process rewards are obtained by:\nrϕ(yt) := β log πϕ(yt|y<t)\nπref(yt|y<t)\n(3)\nIn PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we\nupdate the Implicit PRM online with on-policy rollouts and outcome supervision and then\ncalculate token-level dense rewards to estimate advantages, which solves C1 and C2 mentioned in\n§2.2 respectively: (1) To prevent overoptimization and reward hacking, it is crucial to update reward\nmodels online. However, updating previous PRMs (Lightman et al., 2023) requires annotating step\nlabels on the latest policy rollouts, which is neither efficient nor scalable during online RL. In contrast,\nthe Implicit PRM only demands outcome labels to train due to its special reward representation,\nand thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which\nhave already been collected to update the policy model. (2) Unlike common PRMs that produce only\nstep-level rewards, the Implicit PRM provides more fine-grained token-level rewards at no additional\ncost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra\noverhead, making it easy to combine with any RL algorithms for advantage estimation.\n4\n\nPreprint\nAlgorithm 1 Process Reinforcement through Implicit Rewards (PRIME)\nInput Language model πθinit; outcome reward verifier ro; dataset D; sample number K; total iteration\nN.\n1: Initialize policy model πθ ←πθinit, πθold ←πθinit, implicit PRM πϕ ←πθinit, reference model\nπref ←πθinit\n2: for iteration = 1, ..., N do\n3:\nSample batch of prompts B ∼D\n4:\nGenerate K responses: {y1, ..., yK} ∼πθ(·|x) for x ∈B\n5:\nCompute outcome rewards: ro\n\x00y1:K\x01\n6:\nApply accuracy filter (§3.3) on all prompts: T ←Filter(x, y1:K, ro\n\x00y1:K\x01\n) for x ∈B\n7:\nForward pass πϕ, πref on each (x, y) ∈T to obatin implicit process reward rϕ(yt) with Eq. 3\n8:\nUpdate Implicit PRM πϕ by CE loss on (x, y, ro (y)) ∈T :\nLCE(ϕ) = −E(x,y,ro(y))∼T [ro (y) · log σ (rϕ (y)) + (1 −ro (y)) · log (1 −σ (rϕ (y)))]\n9:\nCompute advantages A with Eq. 5\n10:\nUpdate policy πθ by PPO loss in Eq. 6\n11:\nUpdate old parameters: θold ←θ\n12: end for\nOutput Optimized policy model πθ\n3.2\nADVANTAGE ESTIMATION AND POLICY UPDATE\nSFT \nModel\nImplicit \nPRM\nPolicy \nModel\nImplicit \nPRM\nPolicy \nModel\nPrompt\nResponse\nOutcome \nVerifier\n𝒓𝒐\n𝒓𝒑\nUpdate\nUpdate\n𝝅𝒓𝒆𝒇\n𝝅𝒓𝒆𝒇\nSFT \nModel\nFigure 2: Illustration of PRIME. PRIME follows\nthat (1) initialize policy model and the Implicit\nPRM both with the reference model; (2) sample\nmultiple responses for each prompt and filter with\noutput accuracy; (3) obtain implicit process re-\nwards by the Implicit PRM and update it using\ncross-entropy (CE) loss; (4) compute advantage\nand policy loss then update the policy model.\nEstimating advantages using Monte Carlo es-\ntimator with a leave-one-out baseline. After\nobtaining token-level dense rewards, we calcu-\nlate advantages based on either MC estimators\nor GAE. To determine the advantage function\nin PRIME, we compare GAE with several MC\nestimators, including REINFORCE (Williams,\n1992), RLOO (Ahmadian et al., 2024), and\nGRPO (Shao et al., 2024). Experimental details\nand results can be found in §5.4.\nWe find that MC estimators, despite being sim-\npler, are strong enough to produce stable results.\nTherefore, we choose MC estimate as our ad-\nvantage function and despite PRIME being com-\npatible with any baseline estimation approaches,\nwe instantiate it with a leave-one-out baseline\nfrom K samples (Ahmadian et al., 2024) in this\npaper, as it performs better in the experiments:\nAi = r(yi\nT ) −\n1\nK −1\nX\nj̸=i\nr(yj\nT )\n(4)\nwhere r(yi\nT ) denotes the reward of i-th response at final step T, K is the number of samples for one\nprompt. The leave-one-out (LOO) baseline helps reduce variances.\nMore specifically, we use an Implicit PRM πϕ and an outcome verifier or reward model ro. We\ncalculate the return of implicit process rewards and outcome rewards separately if both are available,\nsince directly mixing their values may lead to numerical instability (Shao et al., 2024). For implicit\nprocess rewards, we perform a three-step process to calculate return: (1) Use the averaged implicit\nprocess rewards to calculate the leave-one-out baseline; (2) Normalize the process reward at step t by\nsubtracting the baseline; (3) Calculate the discounted return for each response. For outcome rewards,\nwe directly adopt LOO without any modification. Finally, the advantage is set to the combination of\n5\n\nPreprint\nboth returns:\nAi\nt =\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8f0rϕ(yi\ns) −\n1\nK −1\nX\nj̸=i\nrϕ\n\x00yj\x01\n\uf8f9\n\uf8fb\n|\n{z\n}\nRLOO with implicit process rewards\n+ ro\n\x00yi\x01\n−\n1\nK −1\nX\nj̸=i\nro\n\x00yj\x01\n|\n{z\n}\nRLOO with outcome rewards\n(5)\nUpdating policy with PPO clip surrogate loss. We adopt PPO clip surrogate loss for more stable\npolicy updates:\nLCLIP(θ) =Et\n"\nmin\n\x12 πθ(yt|y<t)\nπθold(yt|y<t)At, clip\n\x10 πθ(yt|y<t)\nπθold(yt|y<t), 1 −ϵ, 1 + ϵ\n\x11\nAt\n\x13#\n(6)\nwhere ϵ is a clipping parameter. The loss prevents the updated policy from deviating too far from the\noriginal distribution, which is the prerequisite of importance sampling. The legitimacy of importance\nsampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling\nefficiency.\n3.3\nOTHER TECHNIQUES\nInitializing PRM with SFT/base model. In practice, we find that the starting policy model itself\nserves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in §2.2\nand even outperforms a dedicatedly trained PRM, as shown in § 5.1.\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nw/ filter\nw/o filter\nFigure 3: Impact of online prompt filtering on\ntraining rewards.\nOnline Prompt Filtering. As we sample mul-\ntiple trajectories for each prompt, we introduce\nonline prompt filtering which filters prompts\nwithin a certain accuracy range. This (1) pre-\nserves only the prompts within a certain median-\nlevel difficulty range (Yang et al., 2024b) and (2)\nbalances data distribution for the Implicit PRM\nonline training.\nWe present the ablation study results in Figure 3\nusing RLOO with outcome rewards only, from\nwhich we can see that the online prompt filter\nlargely lowers the variance of RL training.\nHow PRIME addresses challenges in §2.2. In\nsummary, as illustrated in Figure 2 and Algo-\nrithm 1, PRIME adopts implicit process rewards\nfor efficient PRM online update (C2), then inte-\ngrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs\nare directly initialized from SFT or base models, which foregoes explicit reward modeling (C3).\n4\nEXPERIMENTS\n4.1\nIMITATION WARMUP\nWe focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-\nMath-7B-Base (Yang et al., 2024b) for its great mathematical capabilities. We first performed\nsupervised finetuning for RL preparation.\nData Construction. To construct the SFT dataset, we collect reasoning instructions from several open-\nsource datasets. For completion, we employed LLaMA-3.1-70B-Instruct (Meta, 2024) to answer the\ninstructions, with a system prompt requesting the model to perform action-centric chain-of-thought.\nWe finally obtained 230K SFT data, the detailed sources and statistics can be found in § A.\nSFT Results. After finetuning, the performance of our SFT model is reported in Figure 1. Compared\nto baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks.\n6\n\nPreprint\nTable 2: Detailed results of PRIME and RLOO w/ outcome verifier (OV). At the same 240 steps, the\nmodel trained by PRIME is generally better than the model trained by outcome rewards.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nGPT-4o\n-\n9.3\n45.8\n76.4\n36.8\n43.3\n58.9\n48.8\n45.6\nLlama-3.1-70B-Inst.\n-\n20.0\n37.3\n65.0\n37.1\n30.5\n35.0\n34.4\n37.0\nQwen2.5-Math-7B-Inst.\n-\n13.3\n50.6\n79.8\n34.6\n40.7\n11.7\n11.3\n34.6\nEurus-2-7B-SFT\n0\n3.3\n30.1\n66.2\n32.7\n29.8\n21.7\n17.8\n28.8\nRLOO w/ OV Only\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\n80\n20.0\n41.0\n68.2\n38.2\n37.0\n26.7\n26.6\n36.8\n160\n13.3\n42.2\n72.0\n37.1\n38.7\n26.7\n25.6\n36.5\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\n320\n16.7\n51.8\n77.8\n39.7\n41.5\n36.1\n28.5\n41.7\nEurus-2-7B-PRIME\n592\n26.7\n57.8\n79.2\n38.6\n42.1\n33.3\n28.6\n43.9\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\n2.5x Efficient\n6.9% Higher\nPRIME\nRLOO w/ OV Only\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\n256\n288\n320\nSteps\n30\n32\n34\n36\n38\n40\n42\nAvg. Test Acc\nPRIME\nRLOO w/ OV Only\n(b) Test accuracy across different gradient steps.\nFigure 4: The effect of dense reward. We compare PRIME and RLOO with outcome verifier\n(OV). Dense rewards in PRIME lead to 2.5× sample efficiency and 6.9% performance improvement.\nPRIME also substantially outperforms RLOO on downstream tasks.\n4.2\nRL SETTINGS\nRule-based Outcome Verifier. Consistent with recent research that adopts exact match with ground\ntruth as unhackable rewards (Gao et al., 2024; Lambert et al., 2024; DeepSeek-AI et al., 2025), we\ndefine the rule-based ground truth outcome verifiers (OV) for math and coding as follows:\nrmath\no\n(y) =\n\x1a1,\nmatched\n0,\notherwise\nrcode\no\n(y) =\nP #passes\nP #test cases\nHyperparameters. We use veRL (Sheng et al., 2024) to conduct experiments. By default, we\ninitialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For\nhyperparameters, we use a constant 5 × 10−7 learning rate together with AdamW optimizer for\npolicy model, and use a 10−6 learning rate for PRMs. Both policy and PRMs use a batch size of 256\nand micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each\nprompt. We set β = 0.05 for PRM training. We set KL coefficient to 0 in all experiments.\nEvaluation Benchmarks. We evaluate on 7 reasoning benchmarks, focusing on competition-level\nmathematics and programming tasks, including AIME 2024 (Li et al., 2024), AMC (Li et al., 2024),\nMATH-500 (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He\net al., 2024), LeetCode (Guo et al., 2024), and LiveCodeBench (v2) (Jain et al., 2024).\n4.3\nMAIN RESULTS\nAs shown in Figure 1 and Table 2, Eurus-2-7B-PRIME achieves substantial improvements on key\nreasoning benchmarks over the SFT version of the model, leading to 15.1% improvement on average,\nand over 20% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7%\npass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct,\ndemonstrating its excellent reasoning ability.\n7\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n30\n32\n34\n36\n38\n40\nAvg. Test Acc\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(b) Test accuracy across different gradient steps.\nFigure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the\nbest results. Surprisingly, using PRMs trained on extra rollouts hurts the performance in both online\nand offline settings.\n4.4\nDENSE REWARDS V.S. SPARSE REWARDS\nWe first validate the effect of dense rewards compared to RLOO with outcome rewards only. We\ntrain this model for 240 steps. For PRIME, we use the same setting and train the model for 592\nsteps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure 4.\nCompared with sparse reward, PRIME takes 40% of the training steps to achieve the same\ntraining rewards as RLOO and improves the final rewards by 6.9%, with lower variances. On\ndownstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in\nTable 2.\n5\nANALYSIS\n5.1\nDESIGN CHOICES FOR THE IMPLICIT PRM\nThe Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this\nsection, we explore two major factors: (1) the initialization model and (2) the update mechanism.\nSFT model initializes a good PRM. Conventionally, we need to collect data to train RMs\nand PRMs, and then we can use them in RL. However, the Implicit PRM is a language\nmodel, so we can initialize it from any language model with the same tokenizer as the pol-\nicy model.\nTo investigate whether it is still necessary to train a PRM in advance, we con-\nduct experiments with different PRM initialization strategies: with the SFT model itself and\nwith a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT\nwith additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in § B.5).\n0\n50\n100\n150\n200\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nPRM Accuracy\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\nFigure 6: Impact of PRM online update. The\noffline PRM is gradully been overoptimized while\nonline PRMs achieve higher accuracy throughout\ntraining.\nWe report the experiment results in Figure 5.\nSurprisingly, directly using Eurus-2-7B-SFT\nto initialize the PRM greatly outperforms Eu-\nrusPRM which was trained on more samples.\nWe conjecture that initializing policy model and\nPRM from the same model largely alleviates\nthe distribution shift issue, as the PRM is only\ntrained on the online rollouts from the policy\nmodel.\nOnline PRM update is essential. To verify the\neffect of online PRM update, we pair the correct\nand wrong samples and calculate the PRM\nprediction accuracy using rϕ(y).\nWe report\nthe PRM classification accuracy in Figure 6.\nThe figure clearly shows that, online update\nmitigates\noveroptimization\nand\nreward\n8\n\nPreprint\n(a) Policy ref: We use the policy logprob as πref\nfor PRM.\n(b) SFT ref: We retain the initial policy to provide πref for\nPRM and KL.\nFigure 7: Comparison of different reference policy implementations. One uses the running policy’s\nold logprobs as reference (policy ref) while the other uses the initial SFT model as the reference\nmodel (SFT ref).\nhacking. The offline PRM, though starting with\nhigh accuracy, gradually drops during RL training procedure due to distribution shift. In contrast,\nonline PRMs that are trained on policy rollouts show the reverse curve.\nThis is further validated with training rewards and downstream performance. To breakdown, Eurus-2-\n7B-SFT is both used as PRM initialization and the reference model in the main experiment, so the\nPRM is totally trained from scratch, which means the initial PRM outputs zero reward for all tokens.\nTherefore, Figure 4 also demonstrates the effect of online PRM update. For EurusPRM initialization,\nthe online run outperforms the offline run as well in Figure 5.\n5.2\nREFERENCE MODEL CHOICE IS FLEXIBLE\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPolicy ref\nSFT ref\nFigure 8: Different reference model for PRM.\nWe compare two reference model selection strate-\ngies for PRIME. Using the policy model as refer-\nence and using the initial SFT model as reference.\nTheir rewards are similar.\nWe implement two variants of our algorithms to\nexplore the effect of reference model of implicit\nPRM, one using the initial SFT model as the\nreference model (SFT ref) while the other using\nthe running policy’s old logprobs as reference\n(policy ref), as shown in Figure 7a. The policy\nref simply adopts the old logprob of the policy\nmodel as πref, while the SFT ref remains the ini-\ntial SFT model for an additional πref calculation.\nWe compare their performance in this section.\nFrom the training rewards in Figure 8, we find\nthe two strategies are close and have pros and\ncons in different aspects: The Q value calculated\nby implicit PRM is the expectation under the dis-\ntribution of the reference model. So the updat-\ning policy could natrually serve as the reference.\nOn the other hand, KL divergence calculation\nis only allowed when the initial SFT model is\nretained.\n5.3\nSINGLE-FORWARD V.S. DOUBLE-FORWARD\nSince our implicit PRM is concurrently updated in training, for each rollout stage, we can update the\nPRM before the policy model and use the updated PRM to re-calculate the process rewards, which\n9\n\nPreprint\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nPRM Accuracy\nBefore-double forward\nBefore-single forward\nAfter-double forward\n(a) PRM classification accuracy on training samples.\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\nOutcome Training Rewards\nDouble forward\nSingle forward\n(b) Training outcome rewards.\nFigure 9: Single and double forward. While double forward methods obtain higher accuracy after\nonline update, the two variants achieve similar rewards during training.\nwe call the double-forward setting. We investigate the impact of double-forward in both the training\nand test phases. Our default setting applies single-forward, which uses process rewards from old\nPRMs. We plot PRM accuracy on rollouts and training rewards in Figure 9.\nAccordingly, we find that double-forward could increase PRM accuracy, but the training rewards\nremain close between the two methods.\n5.4\nPRIME WITH OTHER RL ALGORITHMS\nAs we stated before, PRIME is equally applicable to other RL algorithms beyond RLOO. In this\nsection, we implement PRIME with REINFORCE (Williams, 1992), GRPO (Shao et al., 2024), and\nPPO (Schulman et al., 2017). Similarly to RLOO, we only modify the advantage estimation functions\nand leave the clip surrogate loss unchanged.\nFirst of all, We compare different REINFORCE-like advantage estimators including REINFORCE,\nGRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms\ncompatible with the compound of outcome verifier reward and process reward, we accordingly make\nadaptions similar to Eq. 5. For GRPO, we have\nAi\nt = ro\n\x00yi\x01\n−mean(ro\n\x00yj\x01\n)\nstd(ro (yj))\n|\n{z\n}\nGRPO with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nrϕ(yi\ns) −mean\n\x12\nrϕ(yj)\n|yj|\n\x13\nstd\n\x10\nrϕ(yj)\n|yj|\n\x11\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n|\n{z\n}\nGRPO with implicit process rewards\n.\n(7)\nFor REINFORCE, we have\nAi\nt =\nro\n\x00yi\x01\n| {z }\nREINFORCE with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t · rϕ(yi\ns)\n|\n{z\n}\nREINFORCE with implicit process rewards\n.\n(8)\nFrom Figure 10 and Table 3, We show that PRIME boosts these algorithms on both efficiency and\nperformance as it does with RLOO. PRIME contributes consistently regardless of the policy update\nmethod, making it a generic algorithm. It indicates that PRIME is a general plug-in for almost any\nRL algorithm for LLM., which largely extends the use cases of PRIME.\nMoreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional\ncomputation cost from the critic model is redundant. This makes it possible to compensate for the\nexpense of the process reward model by using REINFORCE-like algorithms with simpler advantage\nestimators. Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm.\n10\n\nPreprint\nTable 3: Testset results of different RL algorithms.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nRLOO\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\nRLOO w/ PRIME\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\nREINFORCE\n240\n6.7\n47.0\n72.6\n36.0\n37.2\n27.2\n25.0\n36.0\nREINFORCE w/ PRIME\n240\n6.7\n50.0\n76.4\n36.8\n39.1\n27.8\n27.5\n37.8\nGRPO\n240\n10.0\n44.6\n73.2\n37.5\n36.6\n25.0\n25.8\n36.1\nGRPO w/ PRIME\n240\n16.7\n47.0\n75.0\n34.9\n38.2\n28.9\n23.9\n37.8\nPPO\n240\n10.0\n41.0\n73.6\n36.0\n36.3\n28.3\n25.7\n35.8\nPRIME as Value Model\n240\n16.7\n44.6\n72.6\n34.6\n35.7\n27.8\n24.6\n36.6\nPPO w/ PRIME\n240\n13.3\n50.6\n77.4\n37.1\n40.6\n30.0\n26.7\n39.4\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\nREINFORCE w/ PRIME\nGRPO\nGRPO w/ PRIME\nPPO\nPPO w/ PRIME\nFigure 10: PRIME also benefits REINFORCE,\nGRPO, and PPO, achieving similar improve-\nment as RLOO.\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\n+ linear-head value model\n+ Implicit PRM as value\n+ Implicit PRM as reward\nFigure 11: Comparison of value models and reward\nmodels. We show that value models, either the\noriginal PPO one or Implicit PRM, is substaintially\nworse than reward models.\n5.5\nVALUE OR REWARD, HOW TO USE THE IMPLICIT PRM?\nBesides using process rewards to estimate returns, we can also employ the Implicit PRM to predict\nvalues for advantage estimation in Eq. 2. Therefore, we compare four variants of MC estimate\nto determine the best way to incorporate dense supervision. Recall that the Implicit PRM has\nvϕ(y<t+1) = Pt\ni=1 β log πϕ(yi|y<i)\nπref(yi|y<i) with the process reward being rϕ(yt) = vϕ(y<t+1)−vϕ(y<t),\nand we assume a ground-truth outcome verifier ro, γ = 1, then we represent the variants as follows:\n(1) REINFORCE: At = ro(y).\n(2) On top of (1), using a linear-head value model V to calculate the baseline: At = ro(y)−V (y<t).\nThis is the original PPO in Figure 10 as we set γ = 1 and λ = 1.\n(3) On top of (1), using values from the Implicit PRM to serve as the baseline: At = ro(y) −\nvϕ(y<t). This is equivalent to PPO with its value model being replaced by values from the Implicit\nPRM when γ = 1 and λ = 1.\n(4) On top of (1), using process rewards from the Implicit PRM to calculate the return: At =\nro(y) + PT\ns=t rϕ(ys). This is the REINFORCE w/ PRIME in Figure 10.\nFigure 11 reports the results. Comparing PPO and REINFORCE, we find that an additional value\nmodel does not benefit policy performance. Notably, using rewards from the Implicit PRM to\ncalculate returns, which is the default setting in PRIME, greatly outperforms all three baselines,\nregardless of where the values come from. This indicates that PRMs work better than value models\nin RL for LLMs.\n5.6\n“ZERO” EXPERIMENTS\nDeepSeek-AI et al. (2025) proposed DeepSeek-R1-Zero, which is directly trained from a base model\nwith reinforcement learning. To further investigate the “Zero” setting, we also perform RL from\n11\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500\n0.525\nOutcome Training Rewards\nPRIME-Zero\nPRIME\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n32\n34\n36\n38\n40\n42\n44\n46\n48\nAvg. Test Acc\nPRIME-Zero\nPRIME\nQwen2.5-Math-7B-Instruct\n(b) Math test accuracy across different gradient steps.\nFigure 12: “Zero” RL from Qwen2.5-Math-7B. RL from the base model converges way faster than\nthe SFT model, surpassing the instruct version within 32 steps.\n0\n20\n40\n60\n80\nSteps\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nOutcome Training Rewards\nPRIME-Zero\n(a) Outcome training rewards (10-step moving).\n0\n16\n32\n48\n64\n80\n96\nSteps\n42\n44\n46\n48\n50\n52\nAvg. Test Acc\nQwen2.5-32B-Instruct\nPRIME-Zero\n(b) Math test accuracy across different gradient steps.\nFigure 13: “Zero” RL from Qwen2.5-32B-Base. RL from a 32B base model shows more promising\ngain, surpassing the instruct version within 16 steps.\nQwen2.5-Math-7B-Base and Qwen2.5-32B-Base (Yang et al., 2024a), skipping the SFT phase. We\npresent the experimental results in Figure 12 and Figure 13. The observations are as follows:\n(1) RL from base model is suprisingly efficient and effective. Comparing PRIME from Qwen2.5-\nMath-7B and Eurus-2-7B-SFT, the “Zero” setting converges much faster. This indicates that directly\nperforming RL from a base model might be a strong alternative to the conventional SFT-RL pipeline.\n(2) Larger models benefit more. Comparing 7B and 32B models, we see that the 32B model\ngains more on both training rewards and test performance. This is aligned with the conclusion in\nDeepSeek-AI et al. (2025).\n(3) Saturation could be a potential issue. Although PRIME-Zero obtains impressive performance\ngain, we find it quickly saturated at a very early stage (about 50 steps), which hinders further\nimprovement like in DeepSeek-AI et al. (2025). This is possibly attributed to the decrease of response\ndiversity, and we leave this as future work.\n6\nRELATED WORK\nRL for LLM Reasoning. In the context of LLMs, reinforcement learning has been widely used for\naligning human preferences (Christiano et al., 2017; Ouyang et al., 2022; Cui et al., 2024), but the\nopen-source community mostly adopt the data-driven imitation learning methods (Yuan et al., 2024a;\nYue et al., 2024; Wei et al., 2024; Liu et al., 2024) to enhance the reasoning capabities of LLMs. Over\nthe past few months, the paradigm gradually shifted. OpenAI o1 (Jaech et al., 2024) first showed\nthe tremendous potential of large-sacle RL for reasoning LLMs, and recent works have verified the\nscaling effect of the simple RL recipe with merely outcome rewards (DeepSeek-AI et al., 2025; Team\n12\n\nPreprint\net al., 2025). Meanwhile, the role of dense rewards in RL remains underexplored, which is the main\nfocus of PRIME.\nImplicit Rewards. Implicit rewards are broadly adopted in LLM alignment (Rafailov et al., 2023;\nChen et al., 2024b; Azar et al., 2024; Ethayarajh et al., 2024; Rosset et al., 2024; Chen et al., 2024a).\nRafailov et al. (2024) first showed that optimizing DPO objective learns a Q function implicitly. Zhou\net al. (2024) utilized implicit rewards in PPO, and showed that dense implicit rewards are better than\nsparse ones. Yuan et al. (2024b) further extended the conclusion to any loss funtion optimizing\nEq. 3.\n7\nCONCLUSION\nAs the fuel of LLMs, data, will be depleted in the near future, we are entering a new era of\nsearch and exploration, which is exemplified by reinforcement learning (Sutton, 2019). This work\ndevelops PRIME, which produces and leverages dense rewards in online RL for LLM reasoning.\nThroughout the experiments, we validate that PRIME (1) greatly benefits sample efficiency and policy\nperformance, (2) is easy to use with minimum cost, and (3) is a general method that works with broad\nRL algorithms together.\nREFERENCES\nArash Ahmadian, Chris Cremer, Matthias Gall´e, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,\nAhmet ¨Ust¨un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning\nfrom human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal\nValko, and R´emi Munos. A general theoretical paradigm to understand learning from human\npreferences. International Conference on Artificial Intelligence and Statistics, abs/2310.12036,\n2024.\nChangyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakan-\ntham, and Min Lin. Bootstrapping language models with dpo implicit rewards. arXiv preprint\narXiv:2406.09760, 2024a.\nHuayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment\nof language models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024b.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie,\nRuobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language\nmodels with scaled ai feedback. In ICML, 2024.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,\nQihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,\nZhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao\nWu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong,\nKai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao,\nLitong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang,\nMeng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang,\nQinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang,\nShuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng\nYe, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng\n13\n\nPreprint\nLiang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan\nWang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang,\nXinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen,\nXiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li,\nY. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang,\nYi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan,\nYiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia\nHe, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang,\nZhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen\nZhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\nURL https://arxiv.org/abs/2501.12948.\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\nalignment as prospect theoretic optimization. ICML, 2024.\nJiaxuan Gao, Shusheng Xu, Wenjie Ye, Weiling Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju\nWang, and Yi Wu. On designing effective rl reward at training time for llm reasoning. ArXiv,\nabs/2410.15115, 2024.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\nInternational Conference on Machine Learning, 2022.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,\nYu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming–the\nrise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han,\nYujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: A\nchallenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific\nproblems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n3828–3850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.\n211/.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence\nwith apps. arXiv preprint arXiv:2105.09938, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021b.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nAmirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,\nAaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through\nrefined credit assignment. arXiv preprint arXiv:2410.01679, 2024.\nWouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for\nfree! In DeepRLStructPred@ICLR, 2019. URL https://api.semanticscholar.org/\nCorpusID:198489118.\n14\n\nPreprint\nNathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\nBrahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya\nMalik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris\nWilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hanna Hajishirzi.\nT¨ulu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124, 2024.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent\nalignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems,\n35:3843–3857, 2022.\nJia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif\nRasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in\nai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository,\n13:9, 2024.\nRongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and\nGe Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852,\n2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\nde Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal,\nAlexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,\nNando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. ArXiv,\nabs/2305.20050, 2023.\nZihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing\nfrontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084,\n2024.\nMeta. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nOpenAI. Openai o1 system card. ArXiv, abs/2412.16720, 2024.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems, 35:27730–\n27744, 2022.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\nin Neural Information Processing Systems, 36, 2023.\nRafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q∗: Your language model is\nsecretly a q-function. arXiv preprint arXiv:2404.12358, 2024.\nCorby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and\nTengyang Xie. Direct nash optimization: Teaching language models to self-improve with general\npreferences. ArXiv, abs/2404.03715, 2024.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel.\nHigh-\ndimensional continuous control using generalized advantage estimation. In 4th International\nConference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings, 2016.\n15\n\nPreprint\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal,\nAlekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated\nprocess verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/\n2402.03300.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,\nHaibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint\narXiv: 2409.19256, 2024.\nSkunkworksAI. reasoning-0.01, 2024.\nRichard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019.\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:\n9–44, 1988.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with\nllms. arXiv preprint arXiv:2501.12599, 2025.\nQwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data.\narXiv preprint arXiv:2410.01560, 2024.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang\nSui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ArXiv,\nabs/2312.08935, 2023.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering\ncode generation with oss-instruct. In Forty-first International Conference on Machine Learning,\n2024.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229–256, 1992.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,\nLe Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu\nCui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115,\n2024a.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,\nJianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,\nXingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert\nmodel via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122.\n16\n\nPreprint\nLifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,\nRuobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.\nAdvancing llm reasoning generalists with preference trees. ArXiv, 2024a.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels, 2024b. URL https://arxiv.\norg/abs/2412.01981.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nXiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the\nweb. ArXiv, abs/2405.03548, 2024.\nKaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li,\nGanqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou.\nUltramedical: Building specialized generalists in biomedicine, 2024.\nTianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and\nXiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv\npreprint arXiv:2402.14658, 2024.\nZhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. Weak-to-strong\nsearch: Align large language models via searching over small language models. arXiv preprint\narXiv:2405.19262, 2024.\n17\n\nPreprint\nTable 4: Actions in action-centric chain-of-thought reasoning framework.\nAction Name\nDescription\nASSESS\nAnalyze current situation, identify key elements and goals\nADVANCE\nMove forward with reasoning - calculate, conclude, or form hypothesis\nVERIFY\nCheck accuracy of current approach, look for errors\nSIMPLIFY\nBreak complex problems into simpler parts\nSYNTHESIZE\nCombine multiple pieces of information into complete solution\nPIVOT\nChange strategy when current approach isn’t working\nOUTPUT\nSummarize thought process and present final answer\nTable 5: Data statistics of SFT data.\nTask\nDataset\nSize\nAvg. Response Length\nSource\nMath\nMathInstruct-MATH (Yue et al., 2023)\n12715\n964.01\nhttps://huggingface.co/datasets/TIGER-Lab/MathInstruct\nOpenMathIns-2-Aug Math (Toshniwal et al., 2024)\n15086\n1202.25\nhttps://huggingface.co/datasets/nvidia/OpenMathInstruct-2\nNumina (Li et al., 2024)\n55845\n1331.61\nhttps://huggingface.co/datasets/AI-MO/NuminaMath-CoT\nReasoning-001 (SkunkworksAI, 2024)\n29831\n1316.49\nhttps://huggingface.co/datasets/SkunkworksAI/reasoning-0.01\nCoding\nCode-Feedback (Zheng et al., 2024)\n27663\n1805.16\nhttps://huggingface.co/datasets/m-a-p/Code-Feedback\nMagicoder (Wei et al., 2024)\n24480\n1828.72\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K\nMagicoder-OSS (Wei et al., 2024)\n28980\n1850.05\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\nBiomedicine\nUltraMedical mc (Zhang et al., 2024)\n35163\n891.06\nhttps://huggingface.co/datasets/TsinghuaC3I/UltraMedical\nTotal / Avg.\n-\n229763\n1390.75\n-\nA\nSFT DATA & TRAINING DETAILS\nWe first perform supervised finetuning on the base model to get a starter model for RL.\nAction-centric chain-of-thought reasoning. We apply imitation learning (supervised finetuning) as\na warmup stage to teach models to learn certain reasoning patterns. To this end, we first design an\naction-centric chain-of-thought reasoning framework. Table 4 shows the actions in the action-centric\nchain-of-thought reasoning framework. When the model generates answers, it conducts multi-step\nreasoning and chooses one of the 7 actions at each step. The response begins with the ASSESS action\nand ends with the OUTPUT action.\nConstruction of the SFT dataset. To construct the SFT dataset, we collect reasoning instructions\nfrom several open-source datasets. It is noteworthy that we did not include many datasets with\nground-truth answers in SFT, even though they are of higher quality. However, we reserve them for\nlater RL training. The reason is that we aim to use different datasets for SFT and RL to diversify the\nexploration in RL, and we consider ground-truth more essential in RL than in SFT. For completion,\nwe employ LLaMA-3.1-70B-Instruct to answer the instructions, with a system prompt requesting the\nmodel to perform an action-centric chain-of-thought. Table 5 summarizes the key statistics of the\ndatasets used for SFT. The datasets span mathematics, coding, and biomedicine. We finally obtain\n230K SFT data and the average response length is 1390 tokens.\nSFT Training. During the SFT phase, we conduct full parameter fine-tuning with a learning rate\nof 1e-05, utilizing the AdamW optimizer alongside a cosine annealing learning rate schedule and a\nwarmup ratio of 0.1. The batch size was set to 96, with a fixed random seed of 42. The model was\ntrained on 230K datasets for 3 epochs.\nB\nRL DATA PREPROCESSING\nB.1\nRL DATA COLLECTION AND PREPROCESSING\nWe curate a high-quality RL training dataset of mathematics and coding problems with outcome\nverifiers (LaTeX answers for math and test cases for coding). For math, we source from NuminaMath-\nCoT (Li et al., 2024), which contains about 860K math problems. The problems span from Chinese\nhigh school mathematics to International Mathematical Olympiad competition questions. For coding,\nwe source from APPS (Hendrycks et al., 2021a), CodeContests (Li et al., 2022), TACO (Li et al.,\n2023), and Codeforces2. To further increase data quality, we conduct detailed cleaning and filtering.\nFinally, we retain 457k math problems and 27k coding problems.\n2https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions\n18\n\nPreprint\nB.2\nDATA FILTERING AND QUESTION-TYPE CLASSIFICATION\nThe preprocessing pipeline employs a systematic rule-based approach to filter and classify mathemati-\ncal problems to create a high-quality dataset with solvable problems, appropriate difficulty levels, and\ncorrect solutions. We exclude problems containing figures or diagrams since they require visual pro-\ncessing capabilities. We also remove proof questions due to difficulties in answer verification. Based\non specific patterns, the remaining problems are classified into question-answering, multiple-choice,\nor fill-in-the-blank questions. Since fill-in-the-blank questions comprise less than 400 examples\ncompared to the much larger set of multiple-choice questions, we focus solely on multiple-choice\nquestions for further processing.\nB.3\nCONVERTING TO DIRECT QUESTION-ANSWER FORMAT\nWe transform multiple-choice questions into a direct question-answer format through three sequential\nstages: rule-based filtering, LLM-based filtering, and LLM-based formatting.\nWe first identify and remove questions that inherently require multiple-choice options - specifically,\nthose where comparing specific statements or properties is essential to the problem-solving process.\nThese questions cannot be meaningfully converted to a direct question-answer format. The initial\nfiltering employs simple rule-based pattern matching, searching for keywords like ”following” and\n”statement” that typically indicate option-dependent problems.\nFollowing the rule-based filtering, we employ Llama-3.1-8B-Instruct to perform a more nuanced\nclassification of the remaining questions. Our pilot study revealed that while the LLM occasionally\nmisclassifies questions, it tends to err on the conservative side - marking potentially convertible\nquestions as requiring options rather than the reverse. Given our large dataset, we accepted this\nconservative approach to maintain quality.\nFor questions classified as convertible, we implement a two-phase reformatting process: 1) Question\nReformatting: Removing choice indicators and restructuring the question to elicit direct answers. 2)\nSolution Reformatting: Converting multiple-choice solutions into step-by-step derivations, ensuring\nall final answers are presented in standard LaTeX boxed format. This systematic approach maintains\nmathematical rigor while creating a standardized format suitable for downstream applications.\nB.4\nPROBLEM AND SOLUTION VALIDATION\nThe final stage involves merging all question-answer pairs and performing LLM-based comprehensive\nvalidation. We identify two key aspects in validation: solvability and correctness.\nWe leverage state-of-the-art mathematical reasoning models, including QwQ-32B-Preview (Team,\n2024) and Qwen2.5-Math-72B-Instruct (Yang et al., 2024b), employing a self-consistency approach\nto determine problem solvability, and if solvable, verify the correctness of solutions provided in the\noriginal dataset.\nTo enhance validation accuracy, we first analyzed sample problems to identify characteristics of\nsolvable and unsolvable cases and created synthetic unsolvable problems featuring missing conditions\nor logical contradictions. Based on these samples, we developed specialized prompts to improve\nthe models’ ability to distinguish solvability. Each problem undergoes five independent validation\nattempts, where the LLM: 1) Provides step-by-step solutions using LaTeX formatting. 2) Identifies\nunsolvability due to missing conditions or logical contradictions. 3) Generates complete reason-\ning traces for solvable problems. 4) Presents final answers in standardized LaTeX boxed format\n(\\boxed{...}). 5) Document any impediments to solution completion.\nWe evaluate two key consistency measures across multiple validation attempts: 1) Status Consistency:\nagreement on problem solvability. 2) Answer Consistency: consistency of solutions across different\nattempts and agreement between generated solutions and ground truth. The final dataset retains only\nproblems that demonstrate consistent solvability across validation attempts, agreement in solutions\nacross multiple attempts, and alignment with ground truth answers. This rigorous validation process\nensures the resulting dataset comprises well-defined, solvable problems with verified, accurate\nsolutions.\n19\n\nPreprint\nTable 6: Data statistics of EurusPRM training dataset.\nDataset\nGenerator Model\nNum. Inst\nResp/Inst\nStep-level/Response-level\nUltraInteract\nLlama-3.1-8B-Inst\n20177\n8\nResponse-level\nLlama-3.1-8B-Base\n13570\n8\nResponse-level\nQwen2.5-72B-Inst\n4758\n8\nResponse-level\nQwen2.5-Math-7B-Base\n25713\n8\nResponse-level\nNumina-SynMath\nLlama-3.1-8B-Inst\n4783\n8\nResponse-level\nQwen2.5-Math-7B-Base\n5806\n8\nResponse-level\nNumina-Olympiads\nLlama-3.1-8B-Inst\n2909\n8\nResponse-level\nQwen2.5-Math-7B-Base\n4739\n8\nResponse-level\nB.5\nPRM DATA\nThe dataset statistics of training EurusPRM are shown in Table 6.\n20'),
                Paper(arxiv_id='2502.01341', authors=['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], published_at=datetime.datetime(2025, 2, 4, 10, 51, 54, 103000, tzinfo=datetime.timezone.utc), title='AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding', summary='Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.', upvotes=29, thumbnail=None, content='Vision-Language Models (VLMs) have gained significant\ntraction in recent years as a powerful framework for multi-\nmodal document understanding tasks that involve interpret-\n1ServiceNow 2York University 3Mila 4 ´Ecole de Technolo-\ngie Sup´erieure\n5Universit´e de Montr´eal\n6McGill University\n7University of Waterloo\n8CIFAR AI Chair\n9Polytechnique\nMontr´eal 10University of British Columbia. Correspondence to:\nAhmed Masry <ahmed.masry@servicenow.com>, Sai Rajeswar\n<sai.mudumba@servicenow.com>.\nLlama-3.2-3B-Perciever R.\nLlama-3.2-3B-MLP\nLlama-3.2-3B-Ovis\nLlama-3.2-3B-Align (ours)\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\nDocVQA\nInfoVQA\nDeepForm\nKLC\nWTQ\nTabFact\nChartQA\nTextVQA\nTableVQA\nFigure 1: Performance of Different VLM Connectors.\nThe proposed ALIGN connector outperforms other methods\nacross benchmarks using the same training configuration.\nRadial distance is proportion of maximal score, truncated at\n0.7 (black dot).\ning both the visual and textual contents of scanned docu-\nments (Kim et al., 2022; Lee et al., 2023; Liu et al., 2023a;\n2024; Hu et al., 2024; Wang et al., 2023a; Rodriguez et al.,\n2024b). Such tasks are common in real-world commercial\napplications, including invoice parsing (Park et al., 2019),\nform reading (Jaume et al., 2019), and document question\nanswering (Mathew et al., 2021b). VLM architectures typ-\nically consist of three components: (i) a vision encoder to\nprocess raw images, (ii) a Large Language Model (LLM)\npre-trained on text, and (iii) a connector module that maps\nthe visual features from the vision encoder into the LLM’s\nsemantic space.\nA central challenge in this pipeline is to effectively map the\ncontinuous feature embeddings of the vision encoder into\nthe latent space of the LLM while preserving the semantic\nproperties of visual concepts. Existing approaches can be\nbroadly categorized into deep fusion and shallow fusion\n1\narXiv:2502.01341v1  [cs.CL]  3 Feb 2025\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nmethods. Deep fusion methods, such as NVLM (Dai et al.,\n2024), Flamingo (Alayrac et al., 2022), CogVLM (Wang\net al., 2023b), and LLama 3.2-Vision (Grattafiori et al.,\n2024), integrate visual and textual features by introducing\nadditional cross-attention and feed-forward layers at each\nlayer of the LLM. While effective at enhancing cross-modal\ninteraction, these methods substantially increase the param-\neter count of the VLM compared to the base LLM, resulting\nin high computational overhead and reduced efficiency.\nIn contrast, shallow fusion methods project visual features\nfrom the vision encoder into the LLM input embedding\nspace using either multilayer perceptrons (MLPs) (Liu et al.,\n2023b; 2024) or attention-based mechanisms such as the\nPerceiver Resampler (Li et al., 2023; Laurenc¸on et al., 2024;\nAlayrac et al., 2022), before concatenating them with the\ntextual prompt’s input embeddings. This approach is more\nparameter-efficient and computationally lighter than deep\nfusion methods, but it lacks a mechanism to ensure the pro-\njected embeddings remain within the region spanned by\nthe LLM’s text embeddings – i.e. regions the LLM was\npretrained to understand. As a result, unconstrained vi-\nsual features can produce out-of-distribution (OOD) and\nnoisy inputs, leading to misalignment between modalities\nand often degrading overall performance. Recent methods\nlike Ovis (Lu et al., 2024) attempt to alleviate these issues\nby introducing separate visual embeddings indexed from\nthe vision encoder outputs and combined together to con-\nstruct the visual inputs to the LLM. However, this approach\nsignificantly increases parameter count due to the massive\nembedding matrix and requires extensive training to learn a\nnew embedding space without guaranteeing alignment with\nthe LLM’s input latent space.\nTo address these limitations, this paper introduces ALIGN-\nVLM, a novel framework that sidesteps direct projection\nof visual features into the LLM embedding space. Instead,\nour proposed connector, ALIGN, maps visual features into\nprobability distributions over the LLM’s existing pretrained\nvocabulary embeddings, which are then combined into a\nweighted representation of the text embeddings. By con-\nstraining each visual feature as a convex combination of the\nLLM text embeddings, our approach leverages the linguistic\npriors already encoded in the LLM’s text space. This en-\nsures that the resulting visual features lie within the convex\nhull of the LLM’s embedding space, reducing the risk of\nnoisy or out-of-distribution inputs and improving alignment\nbetween modalities. Our experimental results show that\nthis approach improves performance on various document\nunderstanding tasks, outperforming prior connector meth-\nods by effectively fusing visual and linguistic content. We\nsummarize our main contributions as follows:\n• We propose a novel connector, ALIGN, to bridge the\nrepresentation gap between vision and text modalities.\n• We introduce a family of Vision-Language Models,\nALIGNVLM, that achieves state-of-the-art perfor-\nmance on multimodal document understanding tasks\nby leveraging ALIGN.\n• We conduct extensive experiments demonstrating the\nrobustness and effectiveness of ALIGN across different\nmodel sizes ranging from 1B to 8B parameters.\nOur code and models will be public upon acceptance.\n2. Related Work\n2.1. Vision-Language Models\nOver the past few years, Vision-Language Models (VLMs)\nhave achieved remarkable progress, largely due to advances\nin Large Language Models (LLMs). Initially demonstrating\nbreakthroughs in text understanding and generation (Brown\net al., 2020; Raffel et al., 2023; Achiam et al., 2023;\nGrattafiori et al., 2024; Qwen et al., 2025; Team, 2024),\nLLMs are now increasingly used to effectively interpret vi-\nsual inputs (Liu et al., 2023b; Li et al., 2024; Wang et al.,\n2024; Chen et al., 2024b; Dai et al., 2024; Drouin et al.,\n2024; Rodriguez et al., 2022). This progress has enabled\nreal-world applications across diverse domains, particularly\nin multimodal document understanding for tasks like form\nreading (Svetlichnaya, 2020), document question answer-\ning (Mathew et al., 2021b), and chart question answer-\ning (Masry et al., 2022). VLMs commonly adopt a three-\ncomponent architecture: a pretrained vision encoder (Zhai\net al., 2023; Radford et al., 2021), a LLM, and a connector\nmodule. A key challenge for VLMs is effectively aligning\nvisual features with the LLM’s semantic space to enable\naccurate and meaningful multimodal interpretation.\n2.2. Vision-Language Alignment for Multimodal Models\nExisting vision-language alignment approaches can be clas-\nsified into deep fusion and shallow fusion. Deep fusion\nmethods integrate visual and textual features by modifying\nthe LLM’s architecture, adding cross-attention and feed-\nforward layers. For example, Flamingo (Alayrac et al.,\n2022) employs the Perceiver Resampler, which uses fixed\nlatent embeddings to attend to vision features and fuses\nthem into the LLM via gated cross-attention layers. Simi-\nlarly, NVLM (Dai et al., 2024) adopts cross-gated attention\nwhile replacing the Perceiver Resampler with a simpler\nMLP. CogVLM (Wang et al., 2023b) extends this approach\nby incorporating new feed-forward (FFN) and QKV lay-\ners for the vision modality within every layer of the LLM.\nWhile these methods improve cross-modal alignment, they\nsignificantly increase parameter counts and computational\noverhead, making them less efficient.\nOn the other hand, shallow fusion methods are more compu-\ntationally efficient, mapping visual features into the LLM’s\n2\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nembedding space without altering its architecture. These\nmethods can be categorized into three main types: (1) MLP-\nbased mapping, such as LLaVA (Liu et al., 2023b) and\nPaliGemma (Beyer et al., 2024), which use multilayer per-\nceptrons (MLP) to project visual features but often pro-\nduce misaligned or noisy features due to a lack of con-\nstraints (Rodriguez et al., 2024b); (2) cross-attention mech-\nanisms, BLIP-2 (Li et al., 2023) uses Q-Former, which\nutilizes a fixed set of latent embeddings to cross-attend to\nvisual features, but that may still produce noisy or OOD\nvisual features; and (3) visual embeddings, such as those\nintroduced by Ovis (Lu et al., 2024), which use embeddings\nindexed by the vision encoder’s outputs to produce the vi-\nsual inputs. While this regularizes feature mapping, it adds\nsubstantial parameter overhead and creates a new vision em-\nbedding space, risking misalignment with the LLM’s text\nembedding space. Encoder-free VLMs, like Fuyu-8B 1 and\nEVE (Diao et al., 2024), eliminate dedicated vision encoders\nbut show degraded performance (Beyer et al., 2024).\nIn contrast, ALIGNVLM maps visual features from the vi-\nsion encoder into probability distributions over the LLM’s\ntext embeddings, using them to compute a convex combi-\nnation. By leveraging the linguistic priors encoded in the\nLLM’s vocabulary, ALIGNVLM ensures that visual features\nremain within the convex hull of the text embeddings, mit-\nigating noisy or out-of-distribution inputs and enhancing\nalignment, particularly for tasks that require joint modalities\nrepresentation like multimodal document understanding.\n3. Methodology\n3.1. Model Architecture\nThe overall model architecture, shown in Figure 2, consists\nof three main components:\n(1) Vision Encoder.\nTo handle high-resolution images of\ndifferent aspect ratios, we divide each input image into mul-\ntiple tiles according to one of the predefined aspect ratios\n(e.g., 1:1, 1:2, . . . , 9:1) chosen via a coverage ratio (Lu\net al., 2024; Chen et al., 2024a). Due to limited compu-\ntational resources, we set the maximum number of tiles\nto 9. Each tile is further partitioned into 14 × 14 patches,\nprojected into vectors, and processed by a SigLip-400M vi-\nsion encoder (Zhai et al., 2023) to extract contextual visual\nfeatures.\nEach tile t ∈{1, · · · , T} is divided into Nt patches\nPt = {pt,1, · · · , pt,Nt},\nwhere pt,i is the i-th patch of tile t. The vision encoder\n1https://www.adept.ai/blog/fuyu-8b\nmaps these patches to a set of visual feature vectors\nFt = VisionEncoder(Pt)\nFt = {ft,1, · · · , ft,Nt},\nft,i ∈Rd.\nFinally, we concatenate the feature sets across all tiles into\na single output\nF = concat\n\x10\nF1, F2, · · · , FT\n\x11\n.\n(2) ALIGN Module.\nThis module aligns the visual fea-\ntures with the LLM. A linear layer W1 ∈RD×d first\nprojects the visual features F ∈RT ·Nt×d to the LLM’s\ntoken embedding space: one RD vector per token. A sec-\nond linear layer W2 ∈RV ×D (initialized from the LLM’s\nlanguage-model head) followed by a softmax, produces a\nprobability simplex Pvocab over the LLM’s vocabulary (V\ntokens)\nPvocab =\n(1)\nsoftmax(LayerNorm(W2 LayerNorm(W1F)))\nWe then use the LLM text embeddings Etext ∈RV ×D to\ncompute a weighted sum\nF′\nalign = P⊤\nvocabEtext.\n(2)\nFinally, we concatenate F′\nalign with the tokenized text em-\nbeddings to form the LLM input\nHinput = concat\n\x00F′\nalign, Etext(x)\n\x01\n,\nwhere Etext(x) is obtained by tokenizing the input text x =\n(x1, · · · , xM) and selecting the corresponding embeddings\nfrom Etext such that\nEtext(x) =\n\x02\nEtext(x1), · · · , Etext(xM)\n\x03\n.\n(3)\n(3) Large Language Model.\nWe feed the concatenated\nvision and text vectors, Hinput, into the LLM, which then\ngenerates output text auto-regressively. To demonstrate\nthe effectiveness of our alignment technique, we experi-\nment with the Llama 3.1 model family (Grattafiori et al.,\n2024). These models offer state-of-the-art performance and\npermissive licenses, making them suitable for commercial\napplications. In particular, we utilize Llama 3.2-1B, Llama\n3.2-3B, and Llama 3.1-8B.\n3.2. Motivation and relation with existing methods\nBy construction, each RD representation in F′\nalign is con-\nstrained to the convex hull of the points Etext, thus concen-\ntrating the visual features in the part of latent space that\nthe LLM can effectively interpret. Moreover, we argue that\n3\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nVision \nEncoder\nLinear\nLayer Norm\nLM Head (LLM)\nLayer Norm\nSoftmax\nWeighted \nAverage Sum\nVision Inputs\nLLM Embedding Matrix\nText Inputs\nFull Embedding\nMatrix\nSelected Text \nEmbeddings\nText \nTokenizer\nAlign Module\nLLM\nQuestion: What percentage of \nAmericans are online?\nResponse: 90%\nFigure 2: ALIGNVLM Model Architecture. The vision encoder extracts image features, which are processed to produce\nprobabilities over the LLM embeddings. A weighted average combines these probabilities with embeddings to generate\nvision input vectors. Text inputs are tokenized, and the corresponding embeddings are selected from the embedding matrix,\nwhich is then used as input to the LLM. We display the vision layers in blue , and the text layers in purple .\nour initialization of W2 to the language model head is an\ninductive bias toward recycling some of the semantics of\nthese text tokens into visual tokens. This contrasts with\npast methods that have been proposed to adapt the vision\nencoder outputs F ∈RT ·Nt×d to an F′ ∈RT ·Nt×D to be\nfed to the LLM. Here, we consider two examples in more\ndetail, highlighting these contrasts.\n(1) MLP Connector (Liu et al., 2023b) applies a linear pro-\njection with parameters WMLP ∈RD×d and bMLP ∈RD,\nfollowed by an activation function σ (e.g., ReLU)\nF′\nMLP = σ(WMLPF + bMLP).\nThese parameters are all learned from scratch, with no par-\nticular bias aligning them to text embeddings.\n(2) Visual Embedding Table (Lu et al., 2024) introduces an\nentire new set of visual embeddings EVET ∈RK×D which,\ntogether with the weights WVET ∈RK×d, specifies\nF′\nVET = softmax(WVETF)⊤EVET.\nWhen D < d, our W2W1 amounts to a low-rank version\nof WVET. There is thus much more to learn to obtain F′\nVET,\nand there is again no explicit pressure to align it with the\ntext embeddings.\n3.3. Training Datasets & Stages\nWe train our model in three stages:\nStage 1.\nThis stage focuses on training the ALIGN Mod-\nule to map visual features to the LLM’s text embeddings\neffectively. We use the CC-12M dataset (Changpinyo et al.,\n2021), a large-scale web dataset commonly used for VLM\npretraining (Liu et al., 2023b), which contains 12M image-\ntext pairs. However, due to broken or unavailable links,\nwe retrieved 8.1M pairs. This dataset facilitates the align-\nment of visual features with the text embedding space of\nthe LLM. During this stage, we train the full model, as this\napproach improves performance and stabilizes the training\nof the ALIGN Module.\nStage 2.\nThe goal is to enhance the model’s document\nunderstanding capabilities, such as OCR, document struc-\nture comprehension, in-depth reasoning, and instruction-\nfollowing. We leverage the BigDocs-7.5M dataset (Ro-\ndriguez et al., 2024a), a curated collection of license-\npermissive datasets designed for multimodal document un-\nderstanding. This dataset aligns with the Accountability,\nResponsibility, and Transparency (ART) principles (Bom-\nmasani et al., 2023; Vogus & Llans´oe, 2021), ensuring com-\npliance for commercial applications. As in Stage 1, we train\nthe full model during this stage.\nStage 3.\nTo enhance the model’s instruction-tuning ca-\npabilities, particularly for downstream tasks like question\nanswering, we further train it on the DocDownstream (Ro-\ndriguez et al., 2024a; Hu et al., 2024) instruction tuning\ndataset. In this stage, the vision encoder is frozen, focusing\ntraining exclusively on the LLM and ALIGN module.\n4. Experimental Setup\nSetup.\nWe conduct all experiments using 8 nodes of H100\nGPUs, totaling 64 GPUs. For model training, we leverage\n4\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nTable 1: Main Results on General Document Benchmarks. We compare ALIGNVLM (ours) with state-of-the-art\n(SOTA) open and closed-source instructed models, and with base models that we trained using the process described in\nSection 3.3. ALIGNVLM models outperform all Base VLM models trained in the same data regime. Our models also\nperform competitively across document benchmarks even compared with SOTA models, in which the data regime is more\ntargeted and optimized. Color coding for comparison: closed-source models , open-source models below 7B parameters ,\nopen-source models between 7-12B parameters .\nModel\nDocVQA\nVAL\nInfoVQA\nVAL\nDeepForm\nTEST\nKLC\nTEST\nWTQ\nTEST\nTabFact\nTEST\nChartQA\nTEST\nTextVQA\nVAL\nTableVQA\nTEST\nAvg. Score\nClosed-Source VLMs\n(Opaque Training Data)\nClaude-3.5 Sonnet\n88.48\n59.05\n31.41\n24.82\n47.13\n53.48\n51.84\n71.42\n81.27\n56.54\nGeminiPro-1.5\n91.23\n73.94\n32.16\n24.07\n50.29\n71.22\n34.68\n68.16\n80.43\n58.46\nGPT-4o 20240806\n92.80\n66.37\n38.39\n29.92\n46.63\n81.10\n85.70\n70.46\n72.87\n64.91\nOpen-Source Instruct VLMs\n(Semi-Opaque Training Data)\nJanus-1.3B (Wu et al., 2024a)\n30.15\n17.09\n0.62\n15.06\n9.30\n51.34\n57.20\n51.97\n18.67\n27.93\nQwen2-VL-2B (Wang et al., 2024)\n89.16\n64.11\n32.38\n25.18\n38.20\n57.21\n73.40\n79.90\n43.07\n55.84\nInternVL-2.5-2B (Chen et al., 2024b)\n87.70\n61.85\n13.14\n16.58\n36.33\n57.26\n74.96\n76.85\n42.20\n51.87\nDeepSeek-VL2-Tiny-3.4B (Wu et al., 2024b)\n88.57\n63.88\n25.11\n19.04\n35.07\n52.15\n80.92\n80.48\n56.30\n55.72\nPhi3.5-Vision-4B (Abdin et al., 2024)\n86.00\n56.20\n10.47\n7.49\n17.18\n30.43\n82.16\n73.12\n70.70\n48.19\nQwen2-VL-7B (Wang et al., 2024)\n93.83\n76.12\n34.55\n23.37\n52.52\n74.68\n83.16\n84.48\n53.97\n64.08\nLLaVA-NeXT-7B (Xu et al., 2024)\n63.51\n30.90\n1.30\n5.35\n20.06\n52.83\n52.12\n65.10\n32.87\n36.00\nDocOwl1.5-8B (Hu et al., 2024)\n80.73\n49.94\n68.84\n37.99\n38.87\n79.67\n68.56\n68.91\n52.60\n60.68\nInternVL-2.5-8B (Chen et al., 2024b)\n91.98\n75.36\n34.55\n22.31\n50.33\n74.75\n82.84\n79.00\n52.10\n62.58\nOvis-1.6-Gemma2-9B (Lu et al., 2024)\n88.84\n73.97\n45.16\n23.91\n50.72\n76.66\n81.40\n77.73\n48.33\n62.96\nLlama3.2-11B (Grattafiori et al., 2024)\n82.71\n36.62\n1.78\n3.47\n23.03\n58.33\n23.80\n54.28\n22.40\n34.04\nPixtral-12B (Agrawal et al., 2024)\n87.67\n49.45\n27.37\n24.07\n45.18\n73.53\n71.80\n76.09\n67.13\n58.03\nDocument Understanding Instructed Models\n(Instruction Tuned on BigDocs-7.5M + DocDownStream (Rodriguez et al., 2024a; Hu et al., 2024))\nQwen2-VL-2B (base+) (Qwen et al., 2025)\n57.23\n31.88\n49.31\n34.39\n31.61\n64.75\n68.60\n61.01\n47.53\n49.59\nALIGNVLM-Llama-3.2-1B (ours)\n72.42\n38.16\n60.47\n33.71\n28.66\n71.31\n65.44\n48.81\n50.29\n52.14\nALIGNVLM-Llama-3.2-3B (ours)\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.10\n58.81\nDocOwl1.5-8B (base+) (Hu et al., 2024)\n78.70\n47.62\n64.39\n36.93\n35.69\n72.65\n65.80\n67.30\n49.03\n57.56\nLlama3.2-11B (base+) (Grattafiori et al., 2024)\n78.99\n44.27\n67.05\n37.22\n40.18\n78.04\n71.40\n68.46\n56.73\n60.26\nALIGNVLM-Llama-3.1-8B (ours)\n81.18\n53.75\n63.25\n35.50\n45.31\n83.04\n75.00\n64.60\n64.33\n62.88\nthe MS-Swift framework (Zhao et al., 2024) for its flexibil-\nity. Additionally, we utilize the DeepSpeed framework (Am-\ninabadi et al., 2022), specifically the ZeRO-3 configuration,\nto optimize efficient parallel training across multiple nodes.\nDetailed hyperparameters are outlined in Appendix A.1.\nBaselines.\nOur work focuses on architectural innovations,\nso we ensure that all baselines are trained on the same\ndatasets. To enable fair comparisons, we evaluate our mod-\nels against a set of Base VLMs fine-tuned on the same\ninstruction-tuning tasks (Stages 2 and 3) as our models,\nusing the BigDocs-7.5M and BigDocs-DocDownstream\ndatasets. This approach ensures consistent training data,\navoiding biases introduced by the Instruct versions of\nVLMs, which are often trained on undisclosed instruction-\ntuning datasets. Due to the scarcity of recently released\npublicly available Base VLMs, we primarily compare our\nmodel against the following Base VLMs of varying sizes:\nQwen2-VL-2B (Wang et al., 2024), DocOwl1.5-8B (Hu\net al., 2024), and LLama 3.2-11B (Grattafiori et al., 2024).\nFor additional context, we also include results from\nthe Instruct versions of recent VLMs of different sizes:\nPhi3.5-Vision-4B (Abdin et al., 2024), Qwen2-VL-2B and\n7B (Wang et al., 2024), LLaVA-NeXT-7B (Liu et al.,\n2024), InternVL2.5-2B and 8B (Chen et al., 2024b), Janus-\n1.3B (Wu et al., 2024a), DeepSeek-VL2-Tiny (Wu et al.,\n2024b), Ovis1.6-Gemma-9B (Lu et al., 2024), Llama3.2-\n11B (Grattafiori et al., 2024), DocOwl1.5-8B (Hu et al.,\n2024), and Pixtral-12B (Agrawal et al., 2024).\nEvaluation Benchmarks.\nWe evaluate our models on a\ndiverse range of document understanding benchmarks that\nassess the model’s capabilities in OCR, chart reasoning,\ntable processing, or form comprehension. In particular, we\nemploy the VLMEvalKit (Duan et al., 2024) framework\nand report the results on the following popular benchmarks:\n5\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nTable 2: Impact of Connector Designs on VLM Performance: We present the results of experiments evaluating different\nconnector designs for conditioning LLMs on visual features. Our proposed ALIGN connector is compared against a basic\nMulti-Layer Perceptron (MLP), the Perceiver Resampler, and Ovis. The results demonstrate that ALIGN consistently\noutperforms these alternatives across all benchmarks.\nModel\nDocVQA\nVAL\nInfoVQA\nVAL\nDeepForm\nTEST\nKLC\nTEST\nWTQ\nTEST\nTabFact\nTEST\nChartQA\nTEST\nTextVQA\nVAL\nTableVQA\nTEST\nAvg. Score\nLlama-3.2-3B-MLP\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n53.06\nLlama-3.2-3B-Perciever R.\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n50.68\nLlama-3.2-3B-Ovis\n74.68\n42.11\n58.02\n33.50\n33.13\n76.67\n67.92\n52.60\n53.93\n54.72\nLlama-3.2-3B-ALIGN (ours)\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.10\n58.81\nDocVQA (Mathew et al., 2021b), InfoVQA (Mathew et al.,\n2021a), DeepForm (Svetlichnaya, 2020), KLC (Stanisławek\net al., 2021), WTQ (Pasupat & Liang, 2015), TabFact (Chen\net al., 2020), ChartQA (Masry et al., 2022), TextVQA (Singh\net al., 2019), and TableVQA (Kim et al., 2024).\n5. Results\n5.1. Main Results\nTable 1 presents the performance of ALIGNVLM com-\npared to state-of-the-art (SOTA) open- and closed-source\ninstructed models, as well as baseline Base VLMs fine-tuned\nin the same instruction-tuning setup. The results demon-\nstrate that ALIGNVLM consistently outperforms all Base\nVLMs within the same size category and achieves com-\npetitive performance against SOTA Instruct VLMs despite\nbeing trained on a more limited data regime. Below, we\nprovide a detailed analysis.\nALIGNVLM vs. Base VLMs.\nOur ALIGNVLM mod-\nels, based on Llama 3.2-1B and Llama 3.2-3B, significantly\noutperform the corresponding Base VLM, Qwen2-VL-2B,\nby up to 9.22%. Notably, ALIGNVLM-Llama-3.2-3B sur-\npasses DocOwl1.5-8B, which has 4B more parameters,\ndemonstrating the effectiveness of ALIGN in enhancing mul-\ntimodal capabilities compared to traditional shallow fusion\nmethods (e.g., MLPs). Furthermore, our 8B model achieves\na 2.62% improvement over Llama3.2-11B despite sharing\nthe same Base LLM, Llama3.1-8B. Since all models in this\ncomparison were trained on the same instruction-tuning\nsetup, this experiment provides a controlled evaluation, iso-\nlating the impact of architectural differences rather than\ndataset biases. Consequently, these results suggest that\nALIGNVLM outperforms VLMs with shallow fusion tech-\nniques and surpasses parameter-heavy deep fusion VLMs,\nsuch as Llama3.2-11B, while maintaining a more efficient\narchitecture.\nALIGNVLM vs. Instruct VLMs.\nEven as open-source\nInstruct models are trained on significantly larger, of-\nten undisclosed instruction-tuning datasets, ALIGNVLM\nachieves superior performance. For instance, ALIGNVLM-\nLlama-3.2-3B (58.81%) outperforms all instructed VLMs in\nits size category, surpassing its closest competitor, Qwen2-\nVL-2B (55.84%), by 2.97%. Additionally, our 8B model\noutperforms significantly larger models such as Llama 3.2-\n11B and PixTral-12B by substantial margins. It also sur-\npasses InternVL-2.5-8B and performs competitively with\nQwen2-VL-7B, though a direct comparison may not be\nentirely fair since Qwen2-VL-7B was trained on an undis-\nclosed instruction-tuning dataset. Finally, ALIGNVLM also\nexhibits comparable performance to closed-source models\nlike GeminiPro-1.5 and GPT4o.\nOverall, these results validate the effectiveness of ALIGN\nand establish ALIGNVLM as a state-of-the-art model for\nmultimodal document understanding.\n5.2. Impact of Connector Designs on VLM Performance\nTo assess the effectiveness of our ALIGN module, we com-\npare it against three different and widely used shallow fusion\nVLM connectors: MLP, Perceiver Resampler, and Ovis. The\nresults in Table 2 show that ALIGN consistently outperforms\nall alternatives, demonstrating its superiority both in align-\ning visual and textual modalities and in multimodal docu-\nment understanding. MLP and Perceiver Resampler achieve\nthe lowest performance, 53.06% and 50.68%, respectively,\ndue to their direct feature projection, which lacks an explicit\nmechanism to align visual features with the LLM’s text\nspace, leading to misalignment. Ovis introduces a separate\nvisual embedding table, but this additional complexity does\nnot significantly improve alignment, yielding only 54.72%\naccuracy. In contrast, ALIGN ensures that visual features\nremain within the convex hull of the LLM’s text latent space,\nleveraging the linguistic priors of the LLM to enhance align-\nment and mitigate noisy embeddings. This design leads to\nthe highest performance (58.81%), establishing ALIGN as\nthe most effective connector for integrating vision and lan-\nguage in multimodal document understanding. We provide\nsome example outputs of the Llama-3.2-3B models with\ndifferent connector designs in Appendix A.3.\n6\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nFigure 3: Probability distribution over the LLM text\ntokens, showing dense probabilities and higher values for\ntokens associated with white space in document images.\n5.3. Probability Distribution over Text Tokens Analysis\nTo better understand the behavior of ALIGN, we examine\nthe probability distribution, Pvocab in Eq (1), over the LLM’s\ntext vocabulary generated from visual features. Specifically,\nwe process 100 document images through the vision en-\ncoder and ALIGN, then average the resulting probability\ndistributions across all image patches. The final distribution\nis shown in Figure 3. As illustrated, the distribution is dense\n(rather than sparse), with the highest probability assigned to\na single token being 0.0118. This can be explained by the\nvision feature space being continuous and of much higher\ncardinality than the discrete text space. Indeed, while the\nLLM has 128K distinct vocabulary tokens, an image patch\n(e.g., 14×14 pixels) contains continuous, high-dimensional\ninformation that cannot be effectively mapped to a single or\na few discrete tokens.\nFurthermore, we observe that tokens on the left side of the\ndistribution in Figure 3 have higher probabilities than the\nrest. Upon investigation, we found that these tokens corre-\nspond to patches that are predominantly white – a common\nfeature in document images. Further analysis of the associ-\nated text tokens reveals that they predominantly consist of\npunctuation marks, as illustrated further in Appendix A.2.\nThis suggests that the model repurposes punctuation marks\nto represent whitespaces. This may be attributed to the fact\nthat both punctuation and whitespaces act as structural cues\nand separators. Other possibilities include whitespaces be-\ning rarely directly-required to perform a task, and LLMs\nmay pay less specific attention to common tokens such as\npunctuation.\n5.4. Pixel-Level Tasks Analysis\nTo rigorously evaluate the ability of vision-language mod-\nels to integrate fine-grained visual and textual pixel-level\ncues, we test our model on the VCR benchmark (Zhang\n0\n20\n40\n60\nExact Match (%)\nVCR EN Hard\nVCR EN Easy\n48.07\n65.84\n37.89\n51.43\nLlama-3.2-3B-Align (Ours)\nLlama-3.2-3B-MLP\nFigure 4: Comparison of Llama-3.2-3b-ALIGN and Llama-\n3.2-3B-MLP on the Easy and Hard VCR tasks.\net al., 2024), which requires the model to recover partially\noccluded texts with pixel-level hints from the revealed parts\nof the text. This task challenges VLM’s alignment of text\nand image in extreme situations. Current state-of-the-art\nmodels like GPT-4V (OpenAI et al., 2023), Claude 3.5 Son-\nnet (Anthropic, 2024), and Llama-3.2 (Dubey et al., 2024)\nsignificantly underperform humans on hard VCR task due to\ntheir inability to process subtle pixel-level cues in occluded\ntext regions. These models frequently discard critical vi-\nsual tokens during image tokenization on semantic priors,\noverlooking the interplay between partial character strokes\nand contextual visual scenes. To evaluate performance on\nVCR, we modify our Stage 3 SFT dataset composition by\nreplacing the exclusive use of DocDownstream with a 5:1\nblended ratio of DocDownstream and VCR training data.\nThis adjustment enables direct evaluation of our architecture\nALIGN’s ability to leverage pixel-level character cues.\nFrom the experimental outcomes, it is evident that ALIGN-\nVLM consistently outperforms the MLP Connector Model\nacross both easy and hard settings of the pixel-level VCR\ntask (see Figure 4), with improvements ranging from 10.18%\non the hard setting to 14.41% on the easy setting.\nWe provide a case study on VCR in Figure 5, featuring four\nrepresentative examples. In Figure 5a, it is evident that the\nMLP connector model fails to capture semantic consistency\nas effectively as ALIGNVLM. The phrase “The commune\nfirst census in written history in” (where the words in italics\nare generated by the model while the rest are in the image)\nis not as semantically coherent as the phrase generated by\nALIGN “The commune first appears in written history in”.\nBeyond the issue of semantic fluency, in Figure 5b we also\nobserve that ALIGNVLM successfully identifies the uncov-\nered portion of the letter “g” in “accounting” and uses it as\na pixel-level hint to infer the correct word. In contrast, the\nMLP model fails to effectively attend to this crucial detail.\n7\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nGT:\n(appears in written\nhistory in)\nMLP:\n(census in written his-\ntory in) ✗\nALIGN\n(appears in written\nhistory in) ✓\n(a) Positive Example 1\nGT:\n(the system used for\nassigning)\nMLP:\n(the system used for\naccounting) ✗\nALIGN\n(the system used for\nassigning) ✓\n(b) Positive Example 2\nGT:\n(mines situated near\nLlanengan on)\nMLP:\n(mines situated near\nLlanengan on) ✓\nALIGN (mines situated near\nLlanongan on) ✗\n(c) Negative Example 1\nGT:\n(Gorden County\nis home to)\nMLP:\n(Gorden County\nis home to) ✓\nALIGN\n(Garden County\nis home to) ✗\n(d) Negative Example 2\nFigure 5: Case Study for Pixel-Level Tasks. We provide examples of our proposed ALIGN connector compared with a the\nMulti-Layer Perceptron (MLP) connector. The ALIGN connector tends to better map visual elements to common words. GT\nis the ground truth.\nFigures 5c and 5d show examples where ALIGNVLM fails\non the VCR task. These carefully picked instances show\nthat our method mistakes names of landmarks with common\nwords when the two are very similar. As seen in the exam-\nples, ALIGNVLM mistakes “Llanengan” for “Llanongan”\nand “Gorden” for “Garden”. In both instances, the pairs\ndiffer by one character, indicating perhaps that ALIGNVLM\ntends to align vision representations to more common to-\nkens in the vocabulary. One approach that would potentially\nmitigate such errors would be to train ALIGNVLM with\nmore contextually-relevant data.\n5.5. Robustness to Noise Analysis\nTo evaluate the robustness of our ALIGN connector to noisy\nvisual features, we conduct an experiment where random\nGaussian noise is added to the visual features produced by\nthe vision encoder before passing them into the connector.\nSpecifically, given the visual features F ∈RN×d output\nby the vision encoder (where N is the number of feature\nvectors and d is their dimensionality), we perturbed them as\neF = F + N,\nN ∼N(0, σ = 3).\nTable 3: Robustness to Noise. Comparison of Avg. Scores\nwith and without Gaussian noise (σ = 3), including perfor-\nmance drop (∆).\nModel\nWithout Noise\nWith Noise\nDrop (∆)\nLlama-3.2-3B-MLP\n53.06\n27.52\n↓25.54\nLlama-3.2-3B-ALIGN (ours)\n58.81\n57.14\n↓1.67\nAs shown in Table 3, our ALIGN connector demonstrates\nhigh robustness to noise, with only a 1.67% average drop in\nperformance. In contrast, the widely adopted MLP connec-\ntor suffers a significant performance degradation of 25.54%,\nhighlighting its vulnerability to noisy inputs. These em-\npirical results support our hypothesis that leveraging the\nknowledge encoded in the LLM’s text embeddings and con-\nstraining the visual features within the convex hull of the\ntext latent space act as a regularization mechanism, reducing\nthe model’s sensitivity to noisy visual features.\n6. Conclusion\nWe introduce ALIGN, a novel connector designed to align\nvision and language latent spaces in vision-language mod-\nels (VLMs), specifically enhancing multimodal document\nunderstanding. By improving cross-modal alignment and\nminimizing noisy embeddings, our models, ALIGNVLM,\nwhich leverage ALIGN, achieve state-of-the-art performance\nacross diverse document understanding tasks. This includes\noutperforming base VLMs trained on the same datasets and\nopen-source instruct models trained on undisclosed data.\nExtensive experiments and ablations validate the robustness\nand effectiveness of ALIGN compared to existing connec-\ntor designs, establishing it as a significant contribution to\nvision-language modeling. Future work will explore train'),
                Paper(arxiv_id='2502.01534', authors=['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], published_at=datetime.datetime(2025, 2, 4, 1, 4, 33, 630000, tzinfo=datetime.timezone.utc), title='Preference Leakage: A Contamination Problem in LLM-as-a-judge', summary='Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between data\ngenerator LLM and judge LLM: being the same model, having an inheritance\nrelationship, and belonging to the same model family. Through extensive\nexperiments, we empirically confirm the bias of judges towards their related\nstudent models caused by preference leakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that preference leakage is a pervasive\nissue that is harder to detect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings imply that preference leakage\nis a widespread and challenging problem in the area of LLM-as-a-judge. We\nrelease all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage.', upvotes=28, thumbnail=None, content='Preference Leakage: A Contamination Problem in LLM-as-a-judge\nDawei Li * 1 Renliang Sun * 2 Yue Huang 3 Ming Zhong 4 Bohan Jiang 1\nJiawei Han 4 Xiangliang Zhang 3 Wei Wang 2 Huan Liu 1\nAbstract\nLarge Language Models (LLMs) as judges and\nLLM-based data synthesis have emerged as\ntwo fundamental LLM-driven data annotation\nmethods in model development. While their com-\nbination significantly enhances the efficiency of\nmodel training and evaluation, little attention has\nbeen given to the potential contamination brought\nby this new model development paradigm. In this\nwork, we expose preference leakage, a contami-\nnation problem in LLM-as-a-judge caused by the\nrelatedness between the synthetic data generators\nand LLM-based evaluators. To study this issue,\nwe first define three common relatednesses\nbetween data generator LLM and judge LLM:\nbeing the same model, having an inheritance\nrelationship, and belonging to the same model\nfamily.\nThrough extensive experiments, we\nempirically confirm the bias of judges towards\ntheir related student models caused by preference\nleakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that prefer-\nence leakage is a pervasive issue that is harder to\ndetect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings\nimply that preference leakage is a widespread\nand challenging problem in the area of LLM-\nas-a-judge.\nWe release all codes and data at:\nhttps://github.com/David-Li0406/\nPreference-Leakage1.\n1. Introduction\nRecent\nadvancements\nin\nLarge\nLanguage\nModels\n(LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong\net al., 2024; Zhang et al., 2024a) have empowered various\n*Equal contribution 1Arizona State University 2University of\nCalifornia, Los Angeles 3University of Notre Dame 4University\nof Illinois Urbana Champaign. Correspondence to: Dawei Li\n<daweili5@asu.edu>.\n1More resources on LLM-as-a-judge are on the website:\nhttps://llm-as-a-judge.github.io/\ndownstream tasks and applications. However, this also\nposes substantial challenges to the automatic evaluation\nof these models. Representatively, LLM-based AI agents’\nfocus transfer from traditional natural language processing\ntasks (Yang et al., 2023; Zhang et al., 2023) to real-world\n(Liu et al., 2023b; Huang et al., 2023), open-ended response\ngeneration (Wu et al., 2024), which greatly limits the\napplicability of traditional n-gram matching methods (e.g.,\nBLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu\net al., 2016; Reiter, 2018) or model-based evaluators (Zhang\net al., 2020; Zhong et al., 2022) for evaluation.\nTo address these challenges, the paradigm of LLM-as-a-\njudge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a;\nZhong et al., 2024; Li et al., 2025) has been proposed, de-\nsigned to leverage LLM as evaluators to assess response\nquality. By combining powerful LLMs with well-designed\nprompting strategies, LLM-as-a-judge enables human-like\nevaluation of long-form and open-ended generation in a\nmore cost-efficient and scalable manner. However, recent\nstudies point out some weaknesses of such assessment. For\ninstance, Ye et al. (2024) explores various biases and vulner-\nabilities of LLM-as-a-judge, highlighting the importance of\ndeveloping a reliable and fair LLM-based evaluation system.\nIn this work, we aim to introduce another concern in LLM-\nas-a-Judge–Preference Leakage. This issue arises when the\nLLMs used for data generation and evaluation are closely re-\nlated, as illustrated in Figure 1. Synthetic data generated by\nLLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c)\nhas become a cornerstone of model training (Lee et al.,\n2025). When combined with LLM-as-a-Judge, they offer\nsignificant efficiency gains in model development. However,\nlimited attention has been given to the potential contami-\nnation that occurs when the generator and evaluator LLMs\nshare a close relationship. During our preliminary study,\nwe find this issue is particularly pervasive in popular LLM-\nas-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al.,\n2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant\nstudies (more details can be found in Appendix A), due to\nthe common reliance on the most advanced LLMs, such\nas GPT-4 (Achiam et al., 2023), for both data synthesis\nand evaluation to ensure the highest quality outputs. In our\nwork, we reveal this relatedness—akin to the overlap be-\ntween training data and evaluation sets in traditional data\n1\narXiv:2502.01534v1  [cs.LG]  3 Feb 2025\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ncontamination—would introduce a systematic bias of judge\nLLMs towards their related student models (i.e., the model\ndistilled by the data generator which is related to the judge).\nCompared to other biases in LLM-as-a-Judge, such as length\nbias or egocentric bias (Ye et al., 2024; Panickssery et al.,\n2024), preference leakage is subtler and more challenging\nto detect, especially given that most LLMs do not disclose\ntheir training data.\nTo investigate and reveal the preference leakage problem,\nwe first define three relatednesses between data generator\nLLM and judge LLM: being the same model, having an\ninheritance relationship, and belonging to the same model\nfamily. Each of these scenarios is commonly encountered\nin real-world applications. Then, we pose and answer three\ncore research questions about preference leakage:\n• RQ1: Does preference leakage introduce systematic\nbiases in LLM-based evaluation? To answer it, we\nconduct experiments with various LLM baselines in two\nwidely recognized LLM-as-a-judge benchmarks, also in-\ntroduce the preference leakage score to quantify the bias\ncaused by preference leakage. The analysis results sug-\ngest an obvious bias of judging LLMs toward their related\nstudent models.\n• RQ2: What is the severity of preference leakage under\nvarious scenarios? We conduct experiments under vari-\nous relatedness settings, tuning techniques, and data mix-\ning strategies to address it, finding that preference leakage\nconsistently affects judge LLMs. Moreover, the severity\nof preference leakage correlates with the degree of relat-\nedness between the data generator and LLM judges, as\nwell as the proportion of synthetic data.\n• RQ3: What are the underlying mechanisms causing\npreference leakage? For this question, we analyze LLMs’\nrecognition capabilities on their related student models’\ngeneration as well as the distribution of bias across differ-\nent question types and judgment dimensions. The analysis\nreveals that preference leakage is a subtle, hard-to-detect\nissue, particularly affecting subjective questions and judg-\nment dimensions.\nTo summarize, our contributions in this work are as follows:\n• We introduce preference leakage, a contamination issue\narising from the relatedness between the data generator\nand judge LLMs.\n• We conduct extensive experiments across various LLMs\nand benchmarks to study how and to what extent the\npotential bias brought by preference leakage influences\njudgment.\n• Our further analysis reveals that preference leakage is\nprevalent in diverse scenarios and difficult for judge LLMs\nto detect, providing valuable insights for future research\non this challenging issue.\n2. Related Work\n2.1. LLM-as-a-Judge\nLLM-as-a-Judge, introduced by Zheng et al. (2023), lever-\nages LLMs to automatically evaluate responses and assign\nrewards. This approach has gained widespread adoption\nin areas such as model alignment (Zhang et al., 2024d)\nand benchmarking (Liu et al., 2023a; Zhang et al., 2024b;\nGao et al., 2023; Zhong et al., 2024), driving significant\nprogress in the field. Building on this concept, Zhuge et al.\n(2024) proposed Agent-as-a-Judge, where agentic systems\nare employed to evaluate other agentic systems. Addition-\nally, Prometheus, a series of open-source LLMs tailored for\nLLM-as-a-Judge (Kim et al., 2023; 2024), addresses the\nprohibitive costs associated with proprietary models, further\ndemocratizing the technology.\nDespite its promising potential, recent studies have high-\nlighted the vulnerabilities and limitations of LLM-as-a-\nJudge. Notable concerns include biases during evaluation.\nFor example, Zheng et al. (2023) identify position bias,\nwhere LLMs may favor responses based on their order in\nthe input, thereby compromising fairness. Other studies (Ye\net al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al.,\n2023; Huang et al., 2024) further emphasize the risks of\nevaluation biases. Thakur et al. (2024) assessed the judg-\nment capabilities of LLM judges, finding that only the most\nadvanced models align reasonably well with human evalu-\nators. Moreover, a recent study (Shi et al., 2024) revealed\nthe susceptibility of LLM-as-a-Judge to adversarial attacks,\nleading to incorrect judgments. In this paper, we explore an-\nother critical vulnerability of LLM-as-a-Judge—preference\nleakage—which poses additional risks to the reliability of\nthis evaluation paradigm.\n2.2. Data Leakage\nThe possible overlap between training data and evaluation\nbenchmarks has become a central issue, since LLMs are usu-\nally trained on extensive web corpora (Dodge et al., 2021).\nThis phenomenon, known as data leakage, can artificially\nimprove the performance of LLMs and undermine the re-\nliability of the assessment (Deng et al., 2024a; Jiang et al.,\n2024b).\nSeveral researchers have proposed methods to detect and\nmitigate data contamination. Deng et al. (2024b) proposed\na retrieval-based approach to assess the degree of overlap\nbetween pre-training text and benchmark data. Golchin &\nSurdeanu (2023) have developed “guided instruction” to\nflag contaminated instances. Dong et al. (2024b) proposed\nthe CDD method to identify peaks in the output distribution\nto detect data contamination. Several studies analyze data\nleakage for specific LLMs (Balloccu et al., 2024) and report\ncontamination such as cross-language contamination (Yao\n2\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nEvaluation\nTestset\nTraining\nCorpus\nData leakage\nTrain\nTraining\nCorpus\nEvaluation \nTestset\nEvaluate\nData Leakage!\nSynthetic \nData\nData \nGenerator\nTrained\nModel\nTrained\nModel\nTrained\nModel\nJudge\nJudge\nModel\nPreference Leakage!\nRelatedness \nOverlap\nLLM for Data \nSynthesis\nLLM-as-\na-Judge\nPreference leakage\nTrain\n(1). Same model\n(2). Inheritance\nSynthetic \ndata\n(3). Within the \nsame model family\nSynthesize\nFigure 1. Overview of preference leakage. We make a comparison between data leakage and preference leakage and present three types of\nrelatedness: being the same model, having an inheritance relationship and belonging to the same model family.\net al., 2024) and task contamination (Li & Flanigan, 2024)\nthat can evade traditional detection methods. To address data\ncontamination issues, Ni et al. (2024) have used web user\nquery detection and benchmark mixture. White et al. (2024)\nuse the most recent information to update the problem.\n3. Preference Leakage\nIn this section, we first provide the formal definition of data\ncontamination as the preliminary (Section 3.1). Based on\nthe concept, we demonstrate how LLM-based data synthesis\nand evaluation can lead to the evolving preference leakage\nproblem (Section 3.2).\n3.1. Preliminary: Data Leakage\nData leakage, also known as data contamination, refers to\nthe inadvertent inclusion of information from the evalua-\ntion benchmarks into the training corpus thus creating an\noverlap between training and testing sets (Kaufman et al.,\n2012). This overlap would significantly influence the eval-\nuation fairness by inflating the models’ performance since\nthe model has prior exposure to and memorized information\nthat it’s expected to generalize during testing (Elangovan\net al., 2021).\nFormally, let T represent the training corpus and E be the\nevaluation set during test time. Data contamination occurs\nif:\nT ∩E ̸= ∅,\n(1)\nwhere ∩denotes the intersection between the two sets. Such\noverlap violates the fundamental assumption that training\nand testing datasets should be disjoint to ensure an unbiased\nassessment of the model’s generalization ability.\n3.2. From Data Leakage to Preference Leakage\nWith the advent of LLMs, synthetic data generated by these\nmodels (Tan et al., 2024) has been widely adopted in var-\nious stages of model training, including pre-training, rein-\nforcement learning with AI feedback (RLAIF) and super-\nvised fine-tuning. Concurrently, the concept of LLM-as-\na-judge has emerged, where LLMs are employed to auto-\nmate the evaluation process. While these LLM-as-an-oracle\napproaches reduce human effort in data annotation, signif-\nicantly enhancing the efficiency and scalability of model\ntraining and evaluation, they also blur the lines between\nmodels and data, introducing evolving challenges (Shu-\nmailov et al., 2024; Dai et al., 2024).\nIn this work, we examine the evolving contamination prob-\nlem brought by LLM-as-a-oracle and formally propose the\nconcept of preference leakage. This refers to a situation\nin which the LLMs used for synthetic data generation and\nevaluation are related. Formally, we define this as:\nLLMG ∩LLMJ ̸= ∅,\n(2)\nwhere LLMG and LLMJ denote the LLMs used for train-\ning data generation and evaluation. ∩represents the related-\nness between the two (sets of) LLMs. This relatedness may\ninvolve:\n• Being the same model: the data generator and evaluator\nare the same model:\nLLMG = LLMJ.\n(3)\n• Inheritance relationship: one model is trained on syn-\nthetic data generated by the other:\nLLMG = Inherit(LLMJ),\n(4)\nLLMJ = Inherit(LLMG).\n(5)\n3\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n• Within the same model family: the data generator and\nevaluator belong to the same model family (e.g., GPT\nfamily (Achiam et al., 2023) and Gemini family (Team\net al., 2024)):\nLLMG, LLMJ ∈FX.\n(6)\nDue to this relatedness, the preference of the judge models\n(e.g., format, style and wording) can be leaked to the student\nmodels through the synthetic data, resulting in non-trivial\nbias from the judge LLMs during the test time.\n4. Main Experiment\n4.1. Experiment Setup\nModels. We choose three powerful LLMs as data generator/\njudge models. They are GPT-4o-2024-11-20 (Achiam et al.,\n2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA-\n3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student\nmodel, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and\nQwen-2.5-14B (Yang et al., 2024). To avoid potential prefer-\nence leakage due to distilling data from other LLMs during\nthe instruction-tuning process, we choose to use the -PRE-\nTRAINED version rather than the -INSTRUCT version of\nthese student models.\nEvaluation Datasets. We choose two representative pair-\nwise evaluation datasets, Arena-Hard (Li et al., 2024e)\nand AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the\ntrained student models. Arena-Hard includes 500 challeng-\ning questions in English. Additionally, the evaluation agree-\nment between Arena-Hard and Chatbot Arena (Zheng et al.,\n2023)’s hard prompts achieved a 96.7% Spearman corre-\nlation, demonstrating the consistency of Arena-Hard with\nhuman preferences (Li et al., 2024e). AlpacaEval 2.0 is an\nimproved evaluation method based on AlpacaEval (Li et al.,\n2023) and contains 805 questions. Compared to version 1.0,\nAlpacaEval 2.0 significantly reduces the effect of text length\non the evaluation results.\nImplementation Details. In our main experiment, we ex-\namine the preference leakage introduced by using the same\ndata generator and evaluator in supervised fine-tuning (SFT).\nWe will discuss other relatedness and learning methods in\nSection 5. To obtain synthetic datasets, We first randomly\nsample 30,000 prompts from the Ultrafeedback dataset (Cui\net al., 2024). The Ultrafeedback dataset includes instruc-\ntions from several publicly available high-quality datasets\nsuch as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al.,\n2023), and Evol-Instruct (Xu et al., 2023). For each data gen-\nerator model, we provide these prompts for them to produce\nsynthetic responses, resulting in three synthetic instruction\ndatasets. We then use each dataset to supervised fine-tune\nthe student model, obtaining three different versions for each\nbaseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-Gemini-\n1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each\ntwo student models and obtain three model pairs. For each\nmodel pair, we perform the pairwise comparison using the\nthree judge models respectively.\nMetrics & Annotation Based on our hypothesis, preference\nleakage would lead to bias of judge LLMs towards their\nrelated student models. Following this principle, we design\nthe preference leakage score PLS(i, j) to measure the bias\nin model pair (i, j) caused by preference leakage:\nPLS(i, j) =\n\x10\nWR(i,i)−AVG(i,j)\nAVG(i,j)\n\x11\n+\n\x10\nWR(j,j)−AVG(j,i)\nAVG(j,i)\n\x11\n2\n,\n(7)\nAVG(i, j) = WR(i, i) + WR(i, j)\n2\n.\n(8)\nHere WR(i, j) represents the win-rate score from judge\nmodel i to student model j. Intuitively, a large preference\nleakage score indicates that the two judge models demon-\nstrate strong bias toward their related student models, sug-\ngesting a significant preference leakage phenomenon.\nWhile our proposed preference leakage score quantifies the\ndegree of preference leakage in each model pair, we also\nperform manual annotation to assess the preference leakage\nin each individual model. We randomly select 100 questions\nfrom AlpacaEval 2.0 and have three well-trained annota-\ntors perform pairwise comparisons independently for each\nresponse pair. After the annotation, the majority voting is\napplied to each response pair to get the final annotation\nresults.\nMore details about model training, metric explanation, and\nannotation process can be found in Appendix B.\nModel\nData Generator/ Judge Pair\nArena-Hard\nAlpacaEval 2.0\nAvg.\nGPT-4o & Gemini-1.5\n28.7%\n18.4%\n23.6%\nGPT-4o & LLaMA-3.3\n-6.7%\n1.4%\n-2.7%\nMistral-7B\nLLaMA-3.3 & Gemini-1.5\n13.1%\n19.8%\n16.4%\nGPT-4o & Gemini-1.5\n37.1%\n18.6%\n27.9%\nGPT-4o & LLaMA-3.3\n1.0%\n2.3%\n1.7%\nQwen-2.5-14B\nLLaMA-3.3 & Gemini-1.5\n25.4%\n18.4%\n21.9%\nTable 1. Preference leakage score result on Arena-Hard and Al-\npacaEval 2.0. The blue background indicates a negative prefer-\nence leakage score value and the purple background indicates a\npositive value. The deeper the color, the larger the absolute value.\n4.2. Main Results\nIn our main experiment, we aim to provide insights into\nRQ1.\nPreference leakage exists in most model pairs. The origi-\nnal judgment results from Arena-Hard and AlpacaEval 2.0,\nalong with the calculated preference leakage scores, are\nshown in Figure 2, Figure 3, and Table 1. As the results\ndemonstrate, in most model pairs (except Mistral-GPT-4o vs\n4\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n18.2%\n39.8%\n42.0%\n27.4%\n43.8%\n28.8%\n38.4%\n34.6%\n27.0%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n46.2%\n42.7%\n11.1%\n50.4%\n35.0%\n14.6%\n55.8%\n27.0%\n17.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n9.2%\n31.4%\n59.4%\n14.6%\n30.0%\n55.4%\n22.2%\n30.8%\n47.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n22.0%\n33.5%\n44.5%\n28.8%\n50.2%\n21.6%\n49.8%\n29.0%\n21.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n52.1%\n40.7%\n7.2%\n39.0%\n51.8%\n9.2%\n57.4%\n29.6%\n13.0%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n10.0%\n29.4%\n60.6%\n16.4%\n48.4%\n35.2%\n24.6%\n30.0%\n44.4%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nTie\nModel B Wins\nFigure 2. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n36.8%\n63.2%\n49.5%\n50.5%\n55.1%\n44.9%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n65.8%\n34.2%\n60.3%\n39.7%\n61.6%\n38.4%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n22.6%\n77.4%\n39.5%\n60.5%\n43.1%\n56.9%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n39.3%\n60.7%\n52.4%\n47.6%\n57.8%\n42.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n63.3%\n36.7%\n59.3%\n40.7%\n61.5%\n38.5%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n26.2%\n73.8%\n42.9%\n57.1%\n50.1%\n49.9%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nModel B Wins\nFigure 3. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in\nAlpacaEval 2.0.\nMistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-\n3.3), the judge LLMs exhibit a strong preference toward\ntheir related student models, leading to large positive val-\nues in the preference leakage scores. This finding suggests\nthat preference leakage, along with the resulting bias, is\nwidespread in SFT when the data generator and evaluator\nare the same.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n73.6%\n8.8%\n17.6%\n79.5%\n1.7%18.8%\nLLaMA-2 vs Others\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n76.2%\n17.9% 6.0%\n79.8%\n20.2%0.0%\nJudge Model\nLLaMA-2 vs Claude-v1\nModel A Wins\nTie\nModel B Wins\nFigure 4. Comparison between GPT-4 and human’s judgment for\nLLaMA-2 from MTBench.\nEvaluators’ bias towards certain LLMs can be inherited\nby its student models. From Figure 2 and Figure 3, we find\nan obvious preference of GPT-4o towards Mistral/ Qwen-\nLLaMA-3.3 and this leads to the low preference leakage\nscore in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and\nQwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate\nthe source of this preference, we examine whether the GPT-\n4 evaluator has a bias toward LLaMA series models. Using\nthe MTBench (Zheng et al., 2023) dataset, which includes\npairwise comparison judgments from both humans and GPT-\n4, we compare GPT-4’s and human evaluators’ judgments\non LLaMA-2 vs other models (including Vicuna, Alpaca,\nGPT-3.5, and GPT-4, which are preferred by GPT-4 due\nto preference leakage or egocentric bias) and LLaMA-2 vs\nClaude. The results, shown in Figure 4, reveal a clear pref-\nerence for LLaMA-2 by GPT-4. Consequently, we conclude\nthat evaluators’ bias can be inherited. In this case, GPT-4’s\nbias toward LLaMA has been passed on to LLaMA’s stu-\ndent models. This inheritance, combined with the opaque\ntraining data of LLMs, makes preference leakage a more\ncomplex and challenging problem.\nModel pairs with similar performance tend to have more\n5\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n53.0%\n47.0%\n40.2%\n59.8%\n49.4%\n50.6%\n58.4%\n41.6%\nJudge Model\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n62.0%\n38.0%\n76.2%\n23.8%\n72.1%\n27.9%\n67.8%\n32.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n36.0%\n64.0%\n17.1%\n82.9%\n39.0%\n61.0%\n46.0%\n54.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\nModel A Wins\nModel B Wins\nFigure 5. Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0.\nobvious preference leakage. As shown in Table 1, we ob-\nserve that the preference leakage scores for Mistral-GPT-4o\nvs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini-\n1.5 (23.6% and 27.9% respectively) are consistently higher\nthan that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9%\nrespectively). We think that this is largely due to the more\ncomparable performance between the student models of\nGPT-4o and Gemini-1.5. Intuitively, when the quality of the\ntwo responses is similar, the evaluator may rely more heav-\nily on its inherent preferences to make a judgment, thereby\nexacerbating the preference leakage issue.\nLarger student models cause more bias from judge\nLLMs. Another observation from Table 1 is that the over-\nall preference leakage score for Qwen-2.5-14B is higher\nthan that for Mistral-7B. Drawing on insights from previous\nstudies on data leakage, which suggest that larger and more\npowerful LLMs are more capable of memorizing extensive\ninformation and are thus more susceptible to data contamina-\ntion (Bordt et al., 2024; Duan et al., 2024), we attribute this\ndifference in preference leakage to the size and capabilities\nof the student LLMs. We assume that larger student models,\ndue to their better performance and generalization abilities,\nare more capable of learning and memorizing the hidden\npreference pattern from the synthetic data, thus leading to a\nmore serious preference leakage.\nDifferent data generator/ judge LLMs result in varying\ndegrees of bias under preference leakage. While we have\nconcluded that student model pairs with similar performance\nor more powerful student models tend to exhibit greater\npreference leakage, we also examine whether different data\ngenerator and judge LLMs contribute to varying degrees\nof preference leakage. Analyzing the manual annotation\nresults presented in Table 5, we observe that Gemini-1.5\nshows a strong bias toward its students, followed by GPT-4o,\nwith LLaMA-3.3 displaying the least bias. This variation in\npreference leakage may stem from differences in the level\nof leaked preference in the synthetic responses generated\nby the data generator LLMs. For instance, an LLM with a\ndistinctive style or format in its responses offers more op-\nportunities for student models to learn these characteristics,\npotentially leading to more pronounced preference leakage\nduring evaluation. Future work could further quantify the\nextent of leaked preference for each data generator model.\n5. Further Analysis\nIn this section, we conduct relatedness analysis, learning\nmethod analysis and data mixing analysis (Section 5.1 - 5.3)\nto answer RQ2. Due to the cost consideration, we conduct\nthese analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5.\nMoreover, we perform recognition analysis and category\nanalysis to answer RQ3.\nArena-Hard AlpacaEval 2.0\nAvg.\nSame Model\n28.7%\n18.4%\n23.6%\nInheritance\nw/ same ins.\n17.8%\n20.7%\n19.3%\nInheritance\nw/ different ins.\n18.3%\n26.3%\n22.3%\nSame Family\nw/ same series\n10.1%\n7.6%\n8.9%\nSame Family\nw/ different series\n3.3%\n2.2%\n2.8%\nTable 2. Preference leakage score in different relatedness between\nthe data generator and the judging LLM.\n5.1. Relatedness Analysis\nWe demonstrate the impact of different relatedness condi-\ntions between the data generator and the judge LLM on the\npreference leakage problem, as shown in Table 2.\nPreference leakage under inheritance settings causes ob-\nvious bias of judges towards their related students. For\nthe inheritance relationship, we consider the situation where\nthe data generator is inherited from the judge model. We\nconducted the following two experiments: (1). we give the\nsame instructions again as in the SFT stage (Inheritance w/\nsame ins.), or (2). we sample the same number of different\ninstructions from the Ultrafeedback (Inherence w/ different\nins.). Then, we let the fine-tuned Mistral model generate\nthe answers and use these generated data to fine-tune a new\nMistral student model. From the results, with the same in-\nstructions, the average preference leakage score is 19.3%. In\ncomparison, the score with different instructions is 22.3%.\n6\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFirstly, in an inheritance setting, data generators can inherit\njudges’ preferences, which are then passed on to new stu-\ndent models, thereby compromising the fairness of their\nevaluation. Second, even when different instructions are\nused, judges’ preferences leaked to data generators can still\nbe transferred to the new student model through synthetic\ndata, leading to a high preference leakage score.\nModels within the same series tend to cause more sig-\nnificant bias. For two models within the same family, we\nconsider two settings: (1) Same series, where training data\nis generated by GPT-4o and Gemini-1.5-flash, and judged\nby GPT-4-turbo and Gemini-1.5-pro; (2) Different series,\nwhere training data is still generated by GPT-4o and Gemini-\n1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro.\nIn the same series setting, the average preference leakage\nscore is 8.9%, indicating that despite using different mod-\nels for data generation and judgment, their relatedness in\nterms of model family leads to some preference leakage.\nIn contrast, the different series setting yields a significantly\nlower leakage score of 2.8%, likely due to differences in\narchitecture, training data, and other factors, reducing the\ninfluence of model-related biases in evaluation.\nArena-Hard\nAlpacaEval 2.0\nAvg.\nSFT\n28.7%\n18.4%\n23.6%\nDPO\n7.7%\n2.7%\n5.2%\nICL\n-4.2%\n-1.1%\n-2.7%\nTable 3. Preference leakage score in different learning methods.\n5.2. Learning Method Analysis\nWe also compare three learning methods, supervised\nfine-tuning (SFT), direct preference optimization (DPO)\n(Rafailov et al., 2024), and in-context learning (ICL) (Dong\net al., 2024a), to explore the different influences to them un-\nder preference leakage. We first build a data pool based on\nhuman-written instruction-tuning data from OASST (K¨opf\net al., 2024), LIMA (Zhou et al., 2024), and MOSS (Sun\net al., 2024b) to supervised fine-tune the pre-trained model.\nFor DPO, we sample 2 responses for each instruction from\nsampled UltraFeedback instruction and prompt each data\ngenerator to produce the pairwise feedback. Then we use\nthe DPO loss to further train the fine-tuned policy on each\nsynthetic pairwise dataset. Appendix C shows the prompt\nwe use to craft synthetic pairwise feedback. For ICL, we\nsample 4 instruction-response pairs from each LLMs’ syn-\nthetic dataset as the demonstration during inference.\nTuning approaches would leak judges’ preference to the\nstudent models. Various learning methods show significant\ndifferences in preference leakage scores across learning\nmethods. SFT exhibits the highest average leakage score at\n23.6%. In contrast, DPO achieves a much lower score of\n5.2%, likely because its focus on preferences helps minimize\nthe unintended transfer of judge model biases. Meanwhile,\nICL, which relies on contextual examples without updating\nmodel parameters, is least affected by the data generator’s\npreferences, resulting in the lowest leakage scores.\n20\n40\n60\n80\n100\nContamination Ratio (%)\n0\n5\n10\n15\n20\n25\n30\nPreference Leakage Score (%)\nAlpacaEval2.0 - Manual\nArenaHard - Manual\nAlpacaEval2.0 - Synthetic\nArenaHard - Synthetic\nFigure 6. Experiment results on data mixing. ‘Manual’ represents\nthe original synthetic data mixed with manually-written data. ‘Syn-\nthetic’ represents the original data mixed with other synthetic data.\n5.3. Data Mixing Analysis\nIn real-world applications, synthetic data from a single LLM\nis often mixed with manually-written data or other multi-\nsource synthetic data to train student models. To mimic\nthese scenarios and explore how much synthetic data could\nlead to preference leakage, we conduct a data mixing anal-\nysis. Specifically, we randomly sample 10%, 30%, 50%,\nand 70% from the original synthetic dataset and mix it with\nmanually-written data and multi-source synthetic data, re-\nspectively, in order to maintain a consistent total volume of\ntraining data (30,000). For the manually-written data, we\nsample from the data pool collected in Section 5.2. For the\nmulti-source synthetic data, we use the original synthetic\ndata from Ultrafeedback, which includes responses gener-\nated by various LLMs (e.g., WizardLM, Flcon, etc.). After\nobtaining the mixing training data, we train the student mod-\nels using SFT and calculate their preference leakage scores\nbased on the judgment results. Figure 6 presents the results\nwith two mixing strategies across two benchmarks.\nThe degree of preference leakage is directly proportional\nto the amount of synthetic data. We observe a strong\ncorrelation between the proportion of synthetic data in the\nmixture and the preference leakage score, with no clear\nthreshold separating cases with preference leakage from\nthose without. This suggests that preference leakage can\noccur even with a small amount of leaked synthetic data,\nposing significant challenges for its detection.\n5.4. Can Judges Recognize Student Models?\nPrevious studies demonstrate the LLM judges can recog-\nnize and thus prefer their own generation (Panickssery et al.,\n2024). In this work, we pose a similar question: Does prefer-\nence leakage also source from the LLM judges’ recognition\n7\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nMathematics\nBusiness\nDaily Life\nScience\nWriting\nOthers\nProgramming\n0\n10\n20\n30\nPreference Leakage Score (%)\n7.7\n16.5\n17.2\n17.3\n21.0\n23.8\n31.4\n(a) Question Type\nCompleteness\nClarity\nRichness\nSatisfaction\nFactuality\nLogical\nOthers\nCreativity\nFairness\n20\n24\n28\n32\nPreference Leakage Score (%)\n27.9\n28.6\n28.8\n29.0\n29.2\n30.2\n30.4\n30.7\n32.4\n(b) Judgment dimension\nFigure 7. Category analysis results on question type and judgment dimension.\nTask\nModel\nAccuracy\nStudent Recognition\nGPT-4o\n60.0%\nGemini-1.5\n25.4%\nLLaMA-3.3\n54.2%\nResponse Classification\nBERT\n82.4%\nTable 4. Student recognition (binary classification) and response\nclassification results (three-class classification).\nof their related student models’ generation? To study this,\nwe follow Panickssery et al. (2024) to prompt the three\njudge LLMs and test whether they could recognize their\nrelated student models’ generation. Additionally, we split\nthree student models’ generation into training and testing\nsets, and train a BERT classifier to perform a three-class\nclassification inspired by the previous study on detecting\nhuman-AI text (Zhang et al., 2024c). Detailed instruction\nand training settings can be found in Appendix D.\nJudge LLMs do not show good performance in recogniz-\ning the generation of their student models. As the result\npresented in Table 4, we find that the recognition perfor-\nmance of each judge LLM in the content of related students\nis poor, with accuracy around the performance of random\nguess. Moreover, we observe no correlation between recog-\nnition performance and the preference leakage degree for\njudge LLMs. For instance, while Gemini-1.5 leads to the\nmost preference leakage (as shown in Section 4.2), it per-\nforms the worst in recognition tasks. These suggest that\npreference leakage is subtler and harder-to-detect for judge\nLLMs, in contrast to the more obvious egocentric bias.\nCertain features embedded in student models through\nsynthetic data. Although judge LLMs do not perform\nwell in related student recognition, we notice the fine-tuned\nBERT classification demonstrates a high accuracy score in\nclassifier response generated by each student model. This\nsuggests that certain characteristics—such as style and for-\nmat—are embedded in the student models through the syn-\nthetic responses. This finding further supports the existence\nof preference leakage and lays the groundwork for future\nresearch aimed at detecting and preventing it.\n5.5. Impact on Question Type & Judgment Dimension\nIn this section, we explore the impact of preference leakage\nacross various question types and judgment dimensions. For\nthe question type analysis, we first propose several general\nquestion types based on the question clusters introduced by\nArena-Hard. Then, we prompt GPT-4o to map each question\nin Arena-Hard and AlpacaEval to one of the question types\nand calculate the preference leakage score for each question\ncategory. For the judgment dimension analysis, we follow\nthe judgment dimensions introduced by Liu et al. (2023a)\nand also utilize GPT-4o to map the rationale generated by\njudge LLMs to one or multiple judgment dimensions. More\ndetailed prompt can be found in Appendix E. The analysis\nresults are presented in Figure 7.\nSubjective question and judgment dimension tend to\nlead to more bias. For question type analysis, we find ob-\njective questions with a definitive answer, like mathematical\nones, demonstrate the least preference leakage. By contrast,\nsubjective questions that have more than one standard an-\nswer, such as programming and writing, usually lead to a\nmore obvious preference leakage. This observation is also\napplied to judgment dimension analysis, as objective di-\nmensions (like completeness) have an overall lower leakage\ndegree compared with subjective ones (like fairness). This\nsuggests that preference leakage tends to be more significant\nin objective questions and dimensions, where the contami-\nnated model is more likely to receive biased preference.\n6. Conclusion\nIn this work, we formally highlight the preference leakage\nproblem in LLM-as-a-judge systems. The results of our\nmain experiment, measured using the proposed preference\nleakage score, reveal a clear bias in each judge toward its\nrespective student model. We also observe that this bias\nis more pronounced in comparable model pairs and larger\nstudent models. Furthermore, we conduct additional anal-\nysis on various factors, including the relationship between\nthe data generator and judge LLMs, model tuning tech-\n8\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nniques, and data mixing strategies. Our findings suggest\nthat preference leakage can cause significant bias across\ndiverse scenarios. Finally, through recognition and category\nanalyses, we investigate the underlying mechanisms of pref-\nerence leakage, demonstrating that it is a challenging and\nhard-to-detect issue, especially in subjective questions and\njudgment dimensions. In the future, we aim to explore meth-\nods for detecting, preventing, and mitigating this evolving\nchallenge in LLM-as-a-judge systems.\nImpact Statements\nBy revealing preference leakage, this work could help build\nmore trustworthy and ethically grounded AI systems. The\nrelatedness between data generators and evaluators can sys-\ntematically bias evaluations, potentially compromising the\nfairness and reliability of the automatic evaluation paradigm.\nThese biased evaluations may indirectly affect downstream\ntasks such as AI alignment and decision-making systems,\nleading to unintended ethical risks. To mitigate preference\nleakage, we hope that researchers will propose more reli-\nable evaluation methods, diversify training data sources, and\ndevelop contamination-resistant benchmarks in the future.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. ArXiv preprint,\nabs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nBalloccu, S., Schmidtov´a, P., Lango, M., and Duˇsek, O.\nLeak, cheat, repeat: Data contamination and evaluation\nmalpractices in closed-source llms. In Proceedings of the\n18th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pp. 67–93, 2024.\nBordt, S., Nori, H., and Caruana, R. Elephants never forget:\nTesting language models for memorization of tabular data.\nIn NeurIPS 2023 Second Table Representation Learning\nWorkshop, 2024.\nChen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B.\nHumans or llms as the judge? a study on judgement\nbiases. arXiv preprint arXiv:2402.10669, 2024.\nCui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y.,\nXie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting\nlanguage models with scaled ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nDai, S., Xu, C., Xu, S., Pang, L., Dong, Z., and Xu, J. Bias\nand unfairness in information retrieval systems: New\nchallenges in the llm era. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 6437–6447, 2024.\nDeng, C., Zhao, Y., Heng, Y., Li, Y., Cao, J., Tang, X.,\nand Cohan, A. Unveiling the spectrum of data contami-\nnation in language models: A survey from detection to\nremediation. arXiv preprint arXiv:2406.14644, 2024a.\nDeng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A.\nInvestigating data contamination in modern benchmarks\nfor large language models. In Proceedings of the 2024\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pp. 8698–8711,\n2024b.\nDodge, J., Sap, M., Marasovi´c, A., Agnew, W., Ilharco, G.,\nGroeneveld, D., Mitchell, M., and Gardner, M. Docu-\nmenting large webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1286–1305, 2021.\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia,\nH., Xu, J., Wu, Z., Chang, B., et al. A survey on in-\ncontext learning. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing,\npp. 1107–1128, 2024a.\nDong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and\nLi, G. Generalization or memorization: Data contamina-\ntion and trustworthy evaluation for large language models.\narXiv preprint arXiv:2402.15938, 2024b.\nDuan, S., Khona, M., Iyer, A., Schaeffer, R., and Fiete, I. R.\nUncovering latent memories: Assessing data leakage and\nmemorization patterns in large language models. arXiv\npreprint arXiv:2406.14549, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nElangovan, A., He, J., and Verspoor, K. Memorization vs.\ngeneralization: Quantifying data leakage in nlp perfor-\nmance evaluation. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pp. 1325–1335, 2021.\nGan, R., Wu, Z., Sun, R., Lu, J., Wu, X., Zhang, D.,\nPan, K., Yang, P., Yang, Q., Zhang, J., et al. Ziya2:\nData-centric learning is all llms need. arXiv preprint\narXiv:2311.03301, 2023.\n9\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nGao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan,\nX. Human-like summarization evaluation with chatgpt.\narXiv preprint arXiv:2304.02554, 2023.\nGolchin, S. and Surdeanu, M. Time travel in llms: Trac-\ning data contamination in large language models. arXiv\npreprint arXiv:2308.08493, 2023.\nHu, S., Luo, Y., Wang, H., Cheng, X., Liu, Z., and Sun, M.\nWon’t get fooled again: Answering questions with false\npremises. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 5626–5643, 2023.\nHuang, Y., Shi, J., Li, Y., Fan, C., Wu, S., Zhang, Q., Liu, Y.,\nZhou, P., Wan, Y., Gong, N. Z., et al. Metatool benchmark\nfor large language models: Deciding whether to use tools\nand which to use. arXiv preprint arXiv:2310.03128, 2023.\nHuang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y.,\nGao, C., Huang, Y., Lyu, W., Zhang, Y., et al. Posi-\ntion: Trustllm: Trustworthiness in large language models.\nIn International Conference on Machine Learning, pp.\n20166–20270. PMLR, 2024.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Car-\nney, A., et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiang, B., Li, D., Tan, Z., Zhou, X., Rao, A., Lerman, K.,\nBernard, H. R., and Liu, H. Assessing the impact of\nconspiracy theories using large language models. arXiv\npreprint arXiv:2412.07019, 2024a.\nJiang, M., Liu, K. Z., Zhong, M., Schaeffer, R., Ouyang,\nS., Han, J., and Koyejo, S. Investigating data contami-\nnation for pre-training language models. arXiv preprint\narXiv:2401.06059, 2024b.\nKaufman, S., Rosset, S., Perlich, C., and Stitelman, O. Leak-\nage in data mining: Formulation, detection, and avoid-\nance. ACM Transactions on Knowledge Discovery from\nData (TKDD), 6(4):1–21, 2012.\nKim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun,\nS., Shin, S., Kim, S., Thorne, J., et al. Prometheus: Induc-\ning fine-grained evaluation capability in language models.\nIn The Twelfth International Conference on Learning\nRepresentations, 2023.\nKim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J.,\nWelleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M.\nPrometheus 2: An open source language model special-\nized in evaluating other language models. arXiv preprint\narXiv:2405.01535, 2024.\nKoo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M.,\nand Kang, D.\nBenchmarking cognitive biases in\nlarge language models as evaluators.\narXiv preprint\narXiv:2309.17012, 2023.\nK¨opf, A., Kilcher, Y., von R¨utte, D., Anagnostidis, S.,\nTam, Z. R., Stevens, K., Barhoum, A., Nguyen, D., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations-\ndemocratizing large language model alignment. Advances\nin Neural Information Processing Systems, 36, 2024.\nLee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J.,\nLu, K. R., Bishop, C., Hall, E., Carbune, V., Rastogi,\nA., et al. Rlaif vs. rlhf: Scaling reinforcement learning\nfrom human feedback with ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nLee, S., Zhou, J., Ao, C., Li, K., Du, X., He, S., Liu, J., Yang,\nM., Wen, Z., and Ni, S. Distillation quantification for\nlarge language models. arXiv preprint arXiv:2501.12619,\n2025.\nLi, C. and Flanigan, J. Task contamination: Language mod-\nels may not be few-shot anymore. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 38,\npp. 18471–18480, 2024.\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z.,\nBhattacharjee, A., Jiang, Y., Chen, C., Wu, T., et al. From\ngeneration to judgment: Opportunities and challenges of\nllm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024a.\nLi, D., Tan, Z., Chen, T., and Liu, H. Contextualization dis-\ntillation from large language model for knowledge graph\ncompletion. arXiv preprint arXiv:2402.01729, 2024b.\nLi, D., Yang, S., Tan, Z., Baik, J. Y., Yun, S., Lee, J.,\nChacko, A., Hou, B., Duong-Tran, D., Ding, Y., et al.\nDalk: Dynamic co-augmentation of llms and kg to an-\nswer alzheimer’s disease questions with scientific litera-\nture. arXiv preprint arXiv:2405.04819, 2024c.\nLi, D., Tan, Z., and Liu, H. Exploring large language models\nfor feature selection: A data-centric perspective. ACM\nSIGKDD Explorations Newsletter, 26(2):44–53, 2025.\nLi, M., Chen, L., Chen, J., He, S., Gu, J., and Zhou, T. Selec-\ntive reflection-tuning: Student-selected data recycling for\nllm instruction-tuning. arXiv preprint arXiv:2402.10110,\n2024d.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline. arXiv preprint arXiv:2406.11939, 2024e.\n10\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval:\nAn automatic evaluator of instruction-following models,\n2023.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74–81, 2004.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. In Proceedings of\nthe 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 3214–\n3252, 2022.\nLiu, C.-W., Lowe, R., Serban, I., Noseworthy, M., Charlin,\nL., and Pineau, J. How NOT to evaluate your dialogue sys-\ntem: An empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In Su, J., Duh, K.,\nand Carreras, X. (eds.), Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Process-\ning, pp. 2122–2132, Austin, Texas, 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1230.\nURL https://aclanthology.org/D16-1230.\nLiu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes\ngood data for alignment? a comprehensive study of auto-\nmatic data selection in instruction tuning. In The Twelfth\nInternational Conference on Learning Representations,\n2024.\nLiu, X., Lei, X., Wang, S., Huang, Y., Feng, Z., Wen, B.,\nCheng, J., Ke, P., Xu, Y., Tam, W. L., et al. Alignbench:\nBenchmarking chinese alignment of large language mod-\nels. arXiv preprint arXiv:2311.18743, 2023a.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nDing, H., Men, K., Yang, K., et al. Agentbench: Evalu-\nating llms as agents. arXiv preprint arXiv:2308.03688,\n2023b.\nNi, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neu-\nbig, G., and You, Y. Mixeval: Deriving wisdom of the\ncrowd from llm benchmark mixtures. arXiv preprint\narXiv:2406.06565, 2024.\nPanickssery, A., Bowman, S. R., and Feng, S. Llm evalu-\nators recognize and favor their own generations. arXiv\npreprint arXiv:2404.13076, 2024.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, pp. 311–318,\n2002.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nReiter, E. A structured review of the validity of BLEU.\nComputational Linguistics, 44(3):393–401, 2018. doi: 10.\n1162/coli a 00322. URL https://aclanthology.\norg/J18-3002.\nShi, J., Yuan, Z., Liu, Y., Huang, Y., Zhou, P., Sun,\nL., and Gong, N. Z.\nOptimization-based prompt in-\njection attack to llm-as-a-judge.\nIn Proceedings of\nthe 2024 on ACM SIGSAC Conference on Computer\nand Communications Security, CCS ’24, pp. 660–674,\nNew York, NY, USA, 2024. Association for Comput-\ning Machinery. ISBN 9798400706363. doi: 10.1145/\n3658644.3690291.\nURL https://doi.org/10.\n1145/3658644.3690291.\nShumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Ander-\nson, R., and Gal, Y. Ai models collapse when trained on\nrecursively generated data. Nature, 631(8022):755–759,\n2024.\nSun, R., Liu, M., Yang, S., Wang, R., He, J., and Zhang, J.\nFostering natural conversation in large language models\nwith nico: a natural interactive conversation dataset. arXiv\npreprint arXiv:2408.09330, 2024a.\nSun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Liu, X.,\nYan, H., Shao, Y., Tang, Q., Zhang, S., Zhao, X., Chen,\nK., Zheng, Y., Zhou, Z., Li, R., Zhan, J., Zhou, Y.,\nLi, L., Yang, X., Wu, L., Yin, Z., Huang, X., Jiang,\nY.-G., and Qiu, X.\nMoss: An open conversational\nlarge language model. Machine Intelligence Research,\n2024b. ISSN 2731-5398. URL https://github.\ncom/OpenMOSS/MOSS.\nTan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee,\nA., Karami, M., Li, J., Cheng, L., and Liu, H. Large\nlanguage models for data annotation and synthesis: A sur-\nvey. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 930–957,\n2024.\nTeam, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L.,\nGulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S.,\net al. Gemini 1.5: Unlocking multimodal understand-\ning across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nThakur, A. S., Choudhary, K., Ramayapally, V. S.,\nVaidyanathan, S., and Hupkes, D. Judging the judges:\nEvaluating alignment and vulnerabilities in llms-as-\njudges. arXiv preprint arXiv:2406.12624, 2024.\n11\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nTong, Y., Li, D., Wang, S., Wang, Y., Teng, F., and Shang,\nJ. Can llms learn from previous mistakes? investigat-\ning llms’ errors to boost for reasoning. arXiv preprint\narXiv:2403.20046, 2024.\nWang, S., Tong, Y., Zhang, H., Li, D., Zhang, X., and Chen,\nT. Bpo: Towards balanced preference optimization be-\ntween knowledge breadth and depth in alignment. arXiv\npreprint arXiv:2411.10914, 2024.\nWhite, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain,\nS., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S.,\net al. Livebench: A challenging, contamination-free llm\nbenchmark. arXiv preprint arXiv:2406.19314, 2024.\nWu, S., Huang, Y., Gao, C., Chen, D., Zhang, Q., Wan, Y.,\nZhou, T., Zhang, X., Gao, J., Xiao, C., et al. Unigen: A\nunified framework for textual dataset generation using\nlarge language models. arXiv preprint arXiv:2406.18966,\n2024.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115, 2024.\nYang, S., Sun, R., and Wan, X. A new dataset and empirical\nstudy for sentence simplification in chinese. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n8306–8321, 2023.\nYao, F., Zhuang, Y., Sun, Z., Xu, S., Kumar, A., and Shang,\nJ. Data contamination can cross language barriers. arXiv\npreprint arXiv:2406.13236, 2024.\nYe, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N.,\nGao, T., Geyer, W., Huang, C., Chen, P.-Y., et al. Justice\nor prejudice? quantifying biases in llm-as-a-judge. arXiv\npreprint arXiv:2410.02736, 2024.\nZhang, H., Li, D., Li, Y., Shang, C., Shi, C., and Jiang, Y.\nAssisting language learners: Automated trans-lingual def-\ninition generation via contrastive prompt learning. arXiv\npreprint arXiv:2306.06058, 2023.\nZhang, H., Shang, C., Wang, S., Zhang, D., Yao, F., Sun,\nR., Yu, Y., Yang, Y., and Wei, F. Shifcon: Enhancing\nnon-dominant language capabilities with a shift-based\ncontrastive framework. arXiv preprint arXiv:2410.19453,\n2024a.\nZhang, H., Wu, Y., Li, D., Yang, Z., Zhao, R., Jiang, Y., and\nTan, F. Balancing speciality and versatility: a coarse to\nfine framework for supervised fine-tuning large language\nmodel. arXiv preprint arXiv:2404.10306, 2024b.\nZhang, Q., Gao, C., Chen, D., Huang, Y., Huang, Y.,\nSun, Z., Zhang, S., Li, W., Fu, Z., Wan, Y., and Sun,\nL. LLM-as-a-coauthor: Can mixed human-written and\nmachine-generated text be detected? In Duh, K., Gomez,\nH., and Bethard, S. (eds.), Findings of the Association\nfor Computational Linguistics: NAACL 2024, pp. 409–\n436, Mexico City, Mexico, June 2024c. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-naacl.29. URL https://aclanthology.\norg/2024.findings-naacl.29/.\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi,\nY. Bertscore: Evaluating text generation with bert. In\nInternational Conference on Learning Representations,\n2020.\nZhang, X., Peng, B., Tian, Y., Zhou, J., Jin, L., Song,\nL., Mi, H., and Meng, H.\nSelf-alignment for fac-\ntuality:\nMitigating hallucinations in LLMs via self-\nevaluation. In Ku, L.-W., Martins, A., and Srikumar,\nV. (eds.), Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pp. 1946–1965, Bangkok, Thailand,\nAugust 2024d. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.acl-long.107. URL https:\n//aclanthology.org/2024.acl-long.107/.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nZheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z.,\nand Ma, Y. Llamafactory: Unified efficient fine-tuning of\n100+ language models. arXiv preprint arXiv:2403.13372,\n2024.\nZhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P.,\nZhu, C., Ji, H., and Han, J. Towards a unified multi-\ndimensional evaluator for text generation.\nIn Gold-\nberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npp. 2023–2038. Association for Computational Linguis-\ntics, 2022.\ndoi: 10.18653/V1/2022.EMNLP-MAIN.\n131.\nURL https://doi.org/10.18653/v1/\n2022.emnlp-main.131.\nZhong, M., Zhang, A., Wang, X., Hou, R., Xiong, W., Zhu,\nC., Chen, Z., Tan, L., Bi, C., Lewis, M., et al. Law of the\nweakest link: Cross capabilities of large language models.\narXiv preprint arXiv:2409.19951, 2024.\n12\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\nZhuge, M., Zhao, C., Ashley, D., Wang, W., Khizbullin, D.,\nXiong, Y., Liu, Z., Chang, E., Krishnamoorthi, R., Tian,\nY., et al. Agent-as-a-judge: Evaluate agents with agents.\narXiv preprint arXiv:2410.10934, 2024.\n13\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nA. Preliminary Study of Preference Leakage in Real World\nIn our preliminary study, we investigate whether preference leakage is a real-world issue in mainstream leaderboards and\nbenchmarks. To this end, we examine two widely used LLM-as-a-judge leaderboards (AlpacaEval 2.0 and Arena-Hard) and\na well-known benchmark (MTBench). All three rely on GPT-4 as the judge model and report pairwise judgment results for\nvarious LLMs. Our analysis reveals that several candidate models distilled from GPT-4 or other GPT-series models (e.g.,\nVicuna and Alpaca) appear across all these leaderboards and benchmarks, suggesting that preference leakage is a pervasive\nissue in these datasets. Besides, we also examine if preference leakage exists in LLM-relevant research studies and also find\na bunch of work utilizing the same or related model(s) to do distillation/ data synthesis and evaluation (Yang et al., 2023;\nLiu et al., 2024; Lee et al., 2024; Li et al., 2024d; Wang et al., 2024; Sun et al., 2024a). All of these suggest preference\nleakage to be a widespread problem in both LLM-as-a-judge datasets and LLM-relevant research.\nB. Experiment Details\nB.1. Training Details\nWe use LLaMA-Factory (Zheng et al., 2024), an efficient LLM tuning library for our experiment. The maximum sequence\nlength is set to 1024 tokens, and a cutoff length of 1024 tokens is enforced to prevent excessive tokenization. The data\npreprocessing will be done in parallel with 16 workers to speed up the preparation process. The training use a per-device\nbatch size of 2, with gradient accumulation over 2 steps to simulate a larger batch size for SFT and a per-device batch size of\n1, with gradient accumulation over 4 steps to simulate a larger batch size for DPO. The learning rate is set to 1.0e-5 and each\nmodel will be trained for 3 epochs. A cosine learning rate scheduler is used with a warmup ratio of 0.1 to gradually increase\nthe learning rate during the initial steps. All of the experiments use BF16 precision to speed up training while maintaining\nnumerical stability. All the experiments are conducted in an 8 Nvidia A100 GPU cluster with CUDA version 11.8.\nJudge Model\nMistral-GPT-4o vs Mistral-Gemini-1.5\nMistral-GPT-4o Wins\nMistral-Gemini-1.5 Wins\nGPT-4o\n55.1%\n44.9%\nGemini-1.5\n36.8%\n63.2%\nPreference Leakage Score\n18.4%\nTable 5. A case on AlpacaEval 2.0 with the model pair Mistral-GPT-4o vs Mistral-Gemini-1.5 to demonstrate how the preference leakage\nscore is calculated.\nB.2. Detailed Explanation for Preference Leakage Score\nWe present a case in Table B.1 to show how we calculate the preference leakage score for the Mistral-GPT-4o vs Mistral-\nGemini-1.5 pair on AlpacaEval 2.0. Based on the definition of preference leakage score, we first calculate:\nAVG(Mistral-GPT-4o, Mistral-Gemini-1.5) = 55.1 + 36.8\n2\n= 45.95%\n(9)\nAVG(Mistral-Gemini-1.5, Mistral-GPT-4o) = 63.2 + 44.9\n2\n= 54.05%\n(10)\nAfter that, we calculate the preference leakage score:\nPLS(Mistral-GPT-4o, Mistral-Gemini-1.5) =\n\x00 55.1−45.95\n45.95\n\x01\n+\n\x00 63.2−54.05\n54.05\n\x01\n2\n= 18.4%\n(11)\n.\nB.3. Manual Annotation Details\nWe randomly sample 100 questions from AlpacaEval 2.0 and ask three well-trained annotators to conduct pairwise\ncomparisons of the responses from each model pair for these questions. For annotation efficiency, we also develop an\nannotation tool that involves the function of uploading multiple model responses, jumping to specific problems, and\n14\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ndownloading annotation results (Figure 8). After annotation, we adopt the majority voting to get the final label for each\nresponse pair. We also calculate the average agreement of three annotators and find it to be 78.6, indicating a relatively\nconsistent annotation result.\nC. Learning Method Analysis Details\nThe table below presents the prompt we use to generate synthetic pairwise feedback from each model.\nPairwise Feedback Prompt\nPlease act as an impartial judge and evaluate the quality of the responses provided\nby two AI assistants to the user question displayed below.\nYour evaluation should\nconsider correctness and helpfulness.\nYou will be given assistant A’s answer,\nand assistant B’s answer.\nYour job is to evaluate which assistant’s answer is\nbetter.\nYou should independently solve the user question step-by-step first.\nThen compare both assistants’ answers with your answer.\nIdentify and correct\nany mistakes.\nAvoid any position biases and ensure that the order in which the\nresponses were presented does not influence your decision.\nDo not allow the length\nof the responses to influence your evaluation.\nDo not favor certain names of the\nassistants.\nBe as objective as possible.\nAfter providing your explanation, output\nyour final verdict by strictly following this format:\n"[[A]]" if assistant A is\nbetter, "[[B]]" if assistant B is better.\n## Instruction:\n[The Start of Assistant A’s Answer]\n[RESPONSE A]\n[The End of Assistant A’s Answer]\n[The Start of Assistant B’s Answer]\n[RESPONSE B]\n[The End of Assistant B’s Answer]\nPlease output the generated content in a json format, for example:\n{ "reason":\n//\nstring, reasons behind the chosen preferred answer "prefered answer":\n// string,\nthe prefered answer you selected, [[A]] or [[B]] }\nFormatted the abovementioned schema and produce the reason and preferred answer:\nD. Recogniton Analysis Details\nThe table below presents the prompt we use for the recognition analysis.\nRecognition Prompt\nGiven an instruction and a response, your task is to judge whether this response\nis generated by a model that is trained on a synthetic dataset you produced (your\nstudent model).\n## Instruction:\n[INSTRUCTION]\n## Response:\n[Response]\nPlease output the generated content in a json format, for example:\n"reason":\n//\nstring, reasons behind the judgment "judgment":\n// string, whether the answer is\ngenerated by your student model, choose from yes or no\nFormatted the abovementioned schema and produce the reason and judgment:\nFor response classification, we split all the response from three student models into training (80%) and testing (20%) subsets.\n15\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFigure 8. The annotation tool we develop for annotation efficiency.\nThen, we finetune a BERT-base-uncased model in the training set. The model is trained for 3 epochs with a learning rate of\n2e-5, a batch size of 16 for both training and evaluation, and a weight decay of 0.01, with evaluations conducted at the end\nof each epoch.\nE. Category Analysis Details\nThe tables below present the prompt we use for question type and judgment dimension cateogory analysis.\nQuestion Type Categorization Prompt\nGiven a question, please categorize it to one of the following categories:\n1.\nComputer Science & Programming\n2.\nMathematics & Statistics\n3.\nScience & Engineering\n4.\nBusiness & Finance\n5.\nWriting & Communication\n6.\nSocial & Daily Life\n7.\nOthers\n## Question:\n[QUESTION]\nPlease output the generated content in a json format, for example:\n{ "question\ncategory":\n// string, specific category name, such as "Computer Science &\nProgramming" }\nFormatted the abovementioned schema and categorize the given question:\n16\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nJudgment Dimension Categorization Prompt\nGiven a pairwise comparison judgment made by an AI, please categorize each\nconsidered aspect in the rationale to one of the following categories:\n{\n"Factuality":\n"Whether the information provided in the response is accurate, based\non reliable facts and data.",\n"User Satisfaction":\n"Whether the response meets the user’s question and needs, and\nprovides a comprehensive and appropriate answer to the question.",\n"Logical Coherence":\n"Whether the response maintains overall consistency and\nlogical coherence between different sections, avoiding self-contradiction.",\n"Richness":\n"Whether the response includes rich info, depth, context, diversity,\ndetailed explanations and examples to meet user needs and provide a comprehensive\nunderstanding.",\n"Creativity":\n"Whether the response is innovative or unique, providing novel\ninsights or solutions.",\n"Fairness and Responsibility":\n"Whether the advice or information provided in the\nresponse is feasible, carries acertain degree of responsibility, and considers\npotential risks and consequences.",\n"Completeness":\n"Whether the response provides sufficient information and details\nto meet the user’s needs, and whether it avoids omitting important aspects.",\n"Clarity":\n"Whether the response is clear and understandable, and whether it uses\nconcise language and structure so that the user can easily understand it.",\n"Others":\n"Other aspects which is not listed above."\n}\n## Judgment:\n[JUDGMENT]\nPlease output the generated content in a json format, for example:\n{ "Factuality":\n// list, all aspects that belong to this category, such as ["correctness",\n"mistakes"] ...\n}\nFormatted the abovementioned schema and categorize aspects in the judgment:\n17')]}
2025-02-06 00:35:47,518 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:36:12,031 - INFO - Total execution time: 23.53 seconds (0.39 minutes)
2025-02-06 00:36:12,038 - INFO - Papers: {'2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=124, thumbnail=None, content='Since the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15'),
                Paper(arxiv_id='2502.01237', authors=['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], published_at=datetime.datetime(2025, 2, 4, 3, 10, 49, 348000, tzinfo=datetime.timezone.utc), title='The Differences Between Direct Alignment Algorithms are a Blur', summary='Direct Alignment Algorithms (DAAs) simplify language model alignment by\nreplacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement\nLearning from Human Feedback (RLHF) with direct policy optimization. DAAs can\nbe classified by their ranking losses (pairwise vs. pointwise), by the rewards\nused in those losses (e.g., likelihood ratios of policy and reference policy,\nor odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that one-stage methods underperform\ntwo-stage methods. To address this, we incorporate an explicit SFT phase and\nintroduce the beta parameter, controlling the strength of preference\noptimization, into single-stage ORPO and ASFT. These modifications improve\ntheir performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT),\nmatching two-stage methods like DPO. Further analysis reveals that the key\nfactor is whether the approach uses pairwise or pointwise objectives, rather\nthan the specific implicit reward or loss function. These results highlight the\nimportance of careful evaluation to avoid premature claims of performance gains\nor overall superiority in alignment algorithms.', upvotes=100, thumbnail=None, content='Large Language Models (LLMs) demonstrate strong text\ngeneration capabilities, yet aligning them with human val-\nues remains challenging due to underspecified objectives,\nlimited training signals, and the complexity of human in-\ntent (Ouyang et al., 2022; Stiennon et al., 2020). Tradi-\ntional alignment pipelines typically involve Supervised Fine-\nTuning (SFT), reward modeling, and reinforcement learning\nto shape model outputs.\nRecently, Direct Alignment Algorithms (DAAs) have\n1T-Tech.\nCorrespondence\nto:\nBoris\nShaposhnikov\n<b.shaposhnikov@tbank.ru>.\nemerged as an alternative, integrating human preferences\ninto policy optimization without explicit reward modeling\nor reinforcement learning (Rafailov et al., 2023; Hong et al.,\n2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024;\nXiao et al., 2024; D’Oosterlinck et al., 2024; Wang et al.,\n2024). These methods differ in theoretical design (pairwise\nvs. pointwise), implementation details (e.g., reference pol-\nicy vs. odds ratio), and whether an SFT phase is required\n(one-stage vs. two-stage). This diversity raises key ques-\ntions about their relationships, comparative advantages, and\nthe role of SFT.\nIn this paper, we show that one-stage methods (e.g., ORPO,\nASFT) can incorporate an explicit SFT phase, improving\nperformance. We introduce a scaling parameter β that uni-\nfies their formulation with other DAAs, revealing shared\noptimization dynamics between methods using either an\nodds ratio or a reference-based reward. Through theoretical\nand empirical analysis, we systematically compare DAAs,\nemphasizing pairwise vs. pointwise preference optimiza-\ntion. We also show that, while SFT is beneficial, using the\nfull dataset is not always necessary, which reduces com-\nputational costs. To structure our analysis, we address the\nfollowing research questions:\nRQ1: Does an explicit SFT stage improve the alignment\nquality of ORPO and ASFT?\nRQ2: Does the tempering factor enhance the alignment\nquality of ASFT and ORPO?\nRQ3: What factors of DAAs affect alignment quality?\nRQ4: How does the final alignment quality depend on the\namount of data used in the SFT stage?\nBy answering these questions, we clarify key trade-offs in\nalignment strategies and provide g'),
                Paper(arxiv_id='2502.01456', authors=['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], published_at=datetime.datetime(2025, 2, 4, 0, 2, 39, 922000, tzinfo=datetime.timezone.utc), title='Process Reinforcement through Implicit Rewards', summary="Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for the reinforcement learning\n(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of training process reward models (PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable to reward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using only policy rollouts and outcome labels through\nimplict process rewards. PRIME combines well with various advantage functions\nand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting from\nQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkey reasoning benchmarks over the SFT model. Notably, our resulting model,\nEurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning\nbenchmarks with 10% of its training data.", upvotes=48, thumbnail=None, content='Dense process rewards, which provide feedback at each intermediate step rather than only the whole\ntrajectory, have proven effective in inference-time scaling of large language models (LLMs) on\nchallenging reasoning tasks (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Yuan\net al., 2024b). On the training side, they also present superiorities in the reinforcement learning\n(RL) of LLMs, particularly in improving training efficiency (Sutton & Barto, 2018) and credit\nassignment (Leike et al., 2018) compared with sparse outcome rewards. However, successful\napplications of dense rewards in RL for LLMs are limited (Setlur et al., 2024), as current industry-\nleading models primarily depend on verifiable outcome rewards and have not yet demonstrated\nmeaningful progress with dense rewards (DeepSeek-AI et al., 2025; Team et al., 2025).\nWe identify the central challenge as how to acquire and utilize high-quality dense rewards at scale,\nwhich enables online process reward model (PRM) update efficiently. The reason is that, optimizing\ntowards a static reward model eventually leads to overoptimization or reward hacking (Gao et al.,\n∗Core Contributors.\n†Project Lead.\n1Models and data are available at: https://github.com/PRIME-RL/PRIME.\n1\narXiv:2502.01456v1  [cs.LG]  3 Feb 2025\n\nPreprint\nAIME 2024\nAMC\nMinerva Math\nOlympiadBench\nMATH-500\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n26.7\n57.8\n38.6\n42.1\n79.2\n48.9\n3.3\n30.1\n32.7\n29.8\n65.1\n32.2\n13.3\n50.6\n34.6\n40.7\n79.8\n43.8\n16.7\n30.1\n35.3\n31.9\n64.6\n35.7\n9.3\n45.8\n36.8\n43.3\n76.4\n43.3\nEurus-2-7B-PRIME\nEurus-2-7B-SFT\nQwen-2.5-Math-7B-Instruct\nLlama-3.1-70B-Instruct\nGPT-4o-2024-08-06\nFigure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics\nbenchmarks, outperforming advanced math models and larger models. Notably, PRIME brings\nsubstantial performance gain (+16.7%) over Eurus-2-7B-SFT.\n2022) due to distribution shift. Ideally, this can be solved by improving the reward model online (Leike\net al., 2018). However, acquiring dense process labels for training is prohibitively more expensive.\nExisting methods either need to build complicated human annotation pipelines (Lightman et al.,\n2023) or rely on estimation-based methods, which require about 10× more rollouts for each step than\nsampling only the response-level trajectories (Wang et al., 2023; Kazemnejad et al., 2024). Neither of\nthem is scalable in online RL. Moreover, to the best of our knowledge, it remains underexplored how\nto incorporate dense rewards into RL for LLMs.\nIn this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable frame-\nwork for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level\nrewards. At its core, the framework employs recently proposed implicit process reward model-\ning (Yuan et al., 2024b) to train dense reward models with only outcome-level labels. This enables\nPRIME to perform online learning of reward signals using only outcome labels on policy rollouts,\nthereby fundamentally mitigating reward hacking while maintaining the same computational cost as\ntraditional outcome reward models (ORMs). Besides scalability, PRIME also (1) serves as a general\nmethod to fuse token-level dense rewards and sparse outcome rewards by calculating their returns\nseparately before summing together, which is compatible with diverse RL algorithms (Williams, 1992;\nKool et al., 2019; Shao et al., 2024; Ahmadian et al., 2024; Schulman et al., 2017); (2) eliminates the\ndedicated reward modeling stage, which is required by existing works, by simply initializing from the\nSFT model or even the base model (§ 5.6). In summary, starting from one single language model, the\nPRIME framework can efficiently accomplish the generation of dense rewards, the initialization and\nupdating of reward models, as well as the reinforcement learning (RL) training of the policy model.\nTable 1: The comparison of resource requirements between Eurus-\n2-7B-PRIME and Qwen2.5-Math-7B-Instruct.\nModel\nEurus-2-7B-PRIME\nQwen2.5-Math-7B-Instruct\nBase Model\nQwen2.5-Math-7B\nQwen2.5-Math-7B\nSFT Data\n230K (open-source)\n2.5M (open-source & in-house)\nRM Data\n0\n618K (in-house)\nRM\nEurus-2-7B-SFT\nQwen2.5-Math-RM (72B)\nRL Data\n150K queries × 4 samples\n66K queries × 32 samples\nIn\nexperiments,\nwe\ntrain\nQwen2.5-Math-7B-Base (Yang\net al., 2024b) with PRIME after\na lightweight SFT warmup stage.\nCompared to RL using outcome\nrewards only, PRIME achieves\na 2.5× sample efficiency gain\nand a 6.9% performance im-\nprovements on challenging math\nproblems. As shown in Figure 1,\nthrough PRIME, we successfully\nachieve substantial improvement on key mathematical reasoning benchmarks over the SFT model,\nleading to 16.7% improvement on average, and over 20% on AMC&AIME competitions. Our\nfinal model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical\nbenchmarks. Notably, this is achieved with only 10% of the data used by Qwen-Math, as in Table 1.\n2\n\nPreprint\nOur analysis shows that updating the PRM online is key to the success of PRIME (§5.1). We also\nshow that PRIME could generally boost various RL algorithms, including RLOO (Ahmadian et al.,\n2024), REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GRPO (Shao et al., 2024)\n(§5.4). In terms of the design choices of advantage estimate, we observe that Implicit PRMs are better\nto be used as reward models than value models (§5.5).\n2\nREINFORCEMENT LEARNING FOR LLMS AND THE CHALLENGES OF\nINCOPORATING DENSE REWARDS\nReinforcement Learning (RL) aims to learn an optimal policy πθ that maximizes the expected\ncumulative discounted reward, namely return, when interacting with an environment. In the context\nof autoregressive language modeling, state at step t is the concatenation of prompt x and current\nresponse y<t, and the action is the t-th token or step yt.\n2.1\nRL PRELIMINARIES FOR LLMS\nPolicy Gradient. Policy gradient is a fundamental algorithm that directly optimizes this objective.\nCentral to this approach is the advantage function At, which quantifies how much better an action is\ncompared to alternatives in a given state:\n∇θJ(θ) = Ex∼D,y∼πθ\n" T\nX\nt=0\n∇θ log πθ(yt|y<t)At\n#\n(1)\nwhere (x, y) represents a pair of input and output. x is omitted for brevity. In practice, the advantage\nfunction is implemented as cumulative discounted rewards subtracting a baseline:\nAt =\nT\nX\ns=t\nγs−tr(ys) −b\n(2)\nγ ∈[0, 1] is a discount factor that optionally decays future rewards, and r(ys) is the reward provided\nby the environment at time step s with x and y<s being omitted in conditions. Eq. 2 is the general\nformula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and\ndense reward at each step is crucial for RL. Different choices of b include, e.g. directly using values\nWilliams (1992), group average of rewards (Shao et al., 2024), and leave-one-out average of rewards\nAhmadian et al. (2024); Kool et al. (2019).\nValue Models. Though the MC estimate is unbiased, it suffers from high variance because of the\nreliance on all future actions and rewards, which can be random and noisy. Value models, which\npredict expected accumulated rewards starting from a state, are adopted to help reduce the variance\nin advantage estimation, such as Generalized Advantage Estimation (GAE; Schulman et al., 2016):\nAGAE(γ,λ)\nt\n= P∞\ns=0(γλ)sδt+s, where δt = r(yt) + γV (y<t+1) −V (y<t) is the temporal difference\n(TD) error (Sutton, 1988), V is a value model, and λ controls the bias-variance tradeoff in advantage\nestimation. PPO (Schulman et al., 2017) is a representative of such actor-critic algorithms that\nexplicitly train a value model along with the policy.\nReward Sparsity. Although dense rewards can be naturally integrated into the advantage function\nthrough Eq. 2, unfortunately, only outcome reward models (ORMs) are available in most practices\nof LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive\nno rewards (Rafailov et al., 2023; Shao et al., 2024; DeepSeek-AI et al., 2025). In this bandit\nsetting, r(yt) = 0 for t < T while r(yT ) can be non-zero, and Eq. 2 becomes A = r(yT ) −b. This\nformulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback\nonly at the end of the entire generation. This may (1) encourage spurious solutions with incorrect\nprocesses but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter\nthe credit assignment problem (Sutton & Barto, 2018). These drawbacks could be further amplified\non complicated tasks, which require more thinking and execution steps, urging the need of dense\nrewards (Uesato et al., 2022; Lightman et al., 2023). Some may consider employing a value model\nto mitigate the problem, as it predicts values at every step t. However, previous work showed that\nvalue models may not be able to solve the reward sparsity issue effectively due to training challenges,\ndespite the additional computation overhead (Shao et al., 2024; Ahmadian et al., 2024). We will\nalso empirically validate this claim in §5.5.\n3\n\nPreprint\n2.2\nKEY CHALLENGES IN SCALABLE DENSE REWARDS\nThe way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs,\nwhich score model responses over each token or step. However, it is usually infeasible in practice to\nincorporate dense rewards into online RL because of three critical challenges in implementation.\nC1. Process rewards are hard to define. It is difficult to collect step-level labels since reasoning\nsteps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating\nlabels for each token is too costly. Moreover, defining the absolute correctness of intermediate\nprocesses as dense rewards can be ambiguous, as some incorrect steps can also positively contribute\nto the final answer by pruning searching branches (OpenAI, 2024; DeepSeek-AI et al., 2025).\nC2. PRM online updates are not scalable. It is crucial to prevent reward overoptimization or reward\nhacking, which requires the reward model or value model to be updated online along with the policy\nmodel (Schulman et al., 2017; Gao et al., 2022). However, training PRMs often requires extensive\nnuanced step-level annotation, which is infeasible in online RL training. Therefore, this brings about\nconsiderable scalability and generalization concerns in dense rewards for RL.\nC3. Explicit reward modeling brings extra cost. Training reward models requires extensive\nannotation and broad data coverage to ensure a good balance between adaptability to the policy\ndistribution and generalization to distribution shifts. Hence, the explicit training stage introduces a\nvery costly data collection and an additional training overhead, especially for PRMs which typically\nrequire stepwise labels.\nNotably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMs\ninto their large-scale RL training (DeepSeek-AI et al., 2025).\n3\nPRIME\nTo address the above challenges, we propose PRIME, a scalable online RL method with dense\nrewards. The key insight of PRIME is to apply implicit process rewards, which are derivable from the\nImplicit PRM that is trained with only outcome labels (Yuan et al., 2024b). This property enables us to\nupdate the PRMs online to avoid reward hacking. We then design a flexible framework to incorporate\nimplicit process rewards with outcome rewards into any kind of MC advantage estimate. PRIME is\nillustrated in Figure 2 and Algorithm 1. Next, we will detail the implicit process rewards (§3.1) and\nhow we leverage them to calculate advantages (§3.2), and introduce other techniques we used (§3.3).\n3.1\nENABLING SCALABLE REWARD UPDATE WITH IMPLICIT REWARD MODELING\nWe consider dense rewards from the Implicit PRM because of the scalability. In short, Implicit PRM\nenables training an ORM with outcome labels only while repurposing it as a PRM at inference. The\ntraining stage is the same as standard ORM pipelines, with the only difference being representing\nthe reward as rϕ(y) := β log πϕ(y)\nπref(y), where πϕ is the RM and πref is the reference model, both of\nwhich are causal LMs. At inference, the process rewards are obtained by:\nrϕ(yt) := β log πϕ(yt|y<t)\nπref(yt|y<t)\n(3)\nIn PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we\nupdate the Implicit PRM online with on-policy rollouts and outcome supervision and then\ncalculate token-level dense rewards to estimate advantages, which solves C1 and C2 mentioned in\n§2.2 respectively: (1) To prevent overoptimization and reward hacking, it is crucial to update reward\nmodels online. However, updating previous PRMs (Lightman et al., 2023) requires annotating step\nlabels on the latest policy rollouts, which is neither efficient nor scalable during online RL. In contrast,\nthe Implicit PRM only demands outcome labels to train due to its special reward representation,\nand thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which\nhave already been collected to update the policy model. (2) Unlike common PRMs that produce only\nstep-level rewards, the Implicit PRM provides more fine-grained token-level rewards at no additional\ncost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra\noverhead, making it easy to combine with any RL algorithms for advantage estimation.\n4\n\nPreprint\nAlgorithm 1 Process Reinforcement through Implicit Rewards (PRIME)\nInput Language model πθinit; outcome reward verifier ro; dataset D; sample number K; total iteration\nN.\n1: Initialize policy model πθ ←πθinit, πθold ←πθinit, implicit PRM πϕ ←πθinit, reference model\nπref ←πθinit\n2: for iteration = 1, ..., N do\n3:\nSample batch of prompts B ∼D\n4:\nGenerate K responses: {y1, ..., yK} ∼πθ(·|x) for x ∈B\n5:\nCompute outcome rewards: ro\n\x00y1:K\x01\n6:\nApply accuracy filter (§3.3) on all prompts: T ←Filter(x, y1:K, ro\n\x00y1:K\x01\n) for x ∈B\n7:\nForward pass πϕ, πref on each (x, y) ∈T to obatin implicit process reward rϕ(yt) with Eq. 3\n8:\nUpdate Implicit PRM πϕ by CE loss on (x, y, ro (y)) ∈T :\nLCE(ϕ) = −E(x,y,ro(y))∼T [ro (y) · log σ (rϕ (y)) + (1 −ro (y)) · log (1 −σ (rϕ (y)))]\n9:\nCompute advantages A with Eq. 5\n10:\nUpdate policy πθ by PPO loss in Eq. 6\n11:\nUpdate old parameters: θold ←θ\n12: end for\nOutput Optimized policy model πθ\n3.2\nADVANTAGE ESTIMATION AND POLICY UPDATE\nSFT \nModel\nImplicit \nPRM\nPolicy \nModel\nImplicit \nPRM\nPolicy \nModel\nPrompt\nResponse\nOutcome \nVerifier\n𝒓𝒐\n𝒓𝒑\nUpdate\nUpdate\n𝝅𝒓𝒆𝒇\n𝝅𝒓𝒆𝒇\nSFT \nModel\nFigure 2: Illustration of PRIME. PRIME follows\nthat (1) initialize policy model and the Implicit\nPRM both with the reference model; (2) sample\nmultiple responses for each prompt and filter with\noutput accuracy; (3) obtain implicit process re-\nwards by the Implicit PRM and update it using\ncross-entropy (CE) loss; (4) compute advantage\nand policy loss then update the policy model.\nEstimating advantages using Monte Carlo es-\ntimator with a leave-one-out baseline. After\nobtaining token-level dense rewards, we calcu-\nlate advantages based on either MC estimators\nor GAE. To determine the advantage function\nin PRIME, we compare GAE with several MC\nestimators, including REINFORCE (Williams,\n1992), RLOO (Ahmadian et al., 2024), and\nGRPO (Shao et al., 2024). Experimental details\nand results can be found in §5.4.\nWe find that MC estimators, despite being sim-\npler, are strong enough to produce stable results.\nTherefore, we choose MC estimate as our ad-\nvantage function and despite PRIME being com-\npatible with any baseline estimation approaches,\nwe instantiate it with a leave-one-out baseline\nfrom K samples (Ahmadian et al., 2024) in this\npaper, as it performs better in the experiments:\nAi = r(yi\nT ) −\n1\nK −1\nX\nj̸=i\nr(yj\nT )\n(4)\nwhere r(yi\nT ) denotes the reward of i-th response at final step T, K is the number of samples for one\nprompt. The leave-one-out (LOO) baseline helps reduce variances.\nMore specifically, we use an Implicit PRM πϕ and an outcome verifier or reward model ro. We\ncalculate the return of implicit process rewards and outcome rewards separately if both are available,\nsince directly mixing their values may lead to numerical instability (Shao et al., 2024). For implicit\nprocess rewards, we perform a three-step process to calculate return: (1) Use the averaged implicit\nprocess rewards to calculate the leave-one-out baseline; (2) Normalize the process reward at step t by\nsubtracting the baseline; (3) Calculate the discounted return for each response. For outcome rewards,\nwe directly adopt LOO without any modification. Finally, the advantage is set to the combination of\n5\n\nPreprint\nboth returns:\nAi\nt =\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8f0rϕ(yi\ns) −\n1\nK −1\nX\nj̸=i\nrϕ\n\x00yj\x01\n\uf8f9\n\uf8fb\n|\n{z\n}\nRLOO with implicit process rewards\n+ ro\n\x00yi\x01\n−\n1\nK −1\nX\nj̸=i\nro\n\x00yj\x01\n|\n{z\n}\nRLOO with outcome rewards\n(5)\nUpdating policy with PPO clip surrogate loss. We adopt PPO clip surrogate loss for more stable\npolicy updates:\nLCLIP(θ) =Et\n"\nmin\n\x12 πθ(yt|y<t)\nπθold(yt|y<t)At, clip\n\x10 πθ(yt|y<t)\nπθold(yt|y<t), 1 −ϵ, 1 + ϵ\n\x11\nAt\n\x13#\n(6)\nwhere ϵ is a clipping parameter. The loss prevents the updated policy from deviating too far from the\noriginal distribution, which is the prerequisite of importance sampling. The legitimacy of importance\nsampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling\nefficiency.\n3.3\nOTHER TECHNIQUES\nInitializing PRM with SFT/base model. In practice, we find that the starting policy model itself\nserves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in §2.2\nand even outperforms a dedicatedly trained PRM, as shown in § 5.1.\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nw/ filter\nw/o filter\nFigure 3: Impact of online prompt filtering on\ntraining rewards.\nOnline Prompt Filtering. As we sample mul-\ntiple trajectories for each prompt, we introduce\nonline prompt filtering which filters prompts\nwithin a certain accuracy range. This (1) pre-\nserves only the prompts within a certain median-\nlevel difficulty range (Yang et al., 2024b) and (2)\nbalances data distribution for the Implicit PRM\nonline training.\nWe present the ablation study results in Figure 3\nusing RLOO with outcome rewards only, from\nwhich we can see that the online prompt filter\nlargely lowers the variance of RL training.\nHow PRIME addresses challenges in §2.2. In\nsummary, as illustrated in Figure 2 and Algo-\nrithm 1, PRIME adopts implicit process rewards\nfor efficient PRM online update (C2), then inte-\ngrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs\nare directly initialized from SFT or base models, which foregoes explicit reward modeling (C3).\n4\nEXPERIMENTS\n4.1\nIMITATION WARMUP\nWe focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-\nMath-7B-Base (Yang et al., 2024b) for its great mathematical capabilities. We first performed\nsupervised finetuning for RL preparation.\nData Construction. To construct the SFT dataset, we collect reasoning instructions from several open-\nsource datasets. For completion, we employed LLaMA-3.1-70B-Instruct (Meta, 2024) to answer the\ninstructions, with a system prompt requesting the model to perform action-centric chain-of-thought.\nWe finally obtained 230K SFT data, the detailed sources and statistics can be found in § A.\nSFT Results. After finetuning, the performance of our SFT model is reported in Figure 1. Compared\nto baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks.\n6\n\nPreprint\nTable 2: Detailed results of PRIME and RLOO w/ outcome verifier (OV). At the same 240 steps, the\nmodel trained by PRIME is generally better than the model trained by outcome rewards.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nGPT-4o\n-\n9.3\n45.8\n76.4\n36.8\n43.3\n58.9\n48.8\n45.6\nLlama-3.1-70B-Inst.\n-\n20.0\n37.3\n65.0\n37.1\n30.5\n35.0\n34.4\n37.0\nQwen2.5-Math-7B-Inst.\n-\n13.3\n50.6\n79.8\n34.6\n40.7\n11.7\n11.3\n34.6\nEurus-2-7B-SFT\n0\n3.3\n30.1\n66.2\n32.7\n29.8\n21.7\n17.8\n28.8\nRLOO w/ OV Only\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\n80\n20.0\n41.0\n68.2\n38.2\n37.0\n26.7\n26.6\n36.8\n160\n13.3\n42.2\n72.0\n37.1\n38.7\n26.7\n25.6\n36.5\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\n320\n16.7\n51.8\n77.8\n39.7\n41.5\n36.1\n28.5\n41.7\nEurus-2-7B-PRIME\n592\n26.7\n57.8\n79.2\n38.6\n42.1\n33.3\n28.6\n43.9\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\n2.5x Efficient\n6.9% Higher\nPRIME\nRLOO w/ OV Only\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\n256\n288\n320\nSteps\n30\n32\n34\n36\n38\n40\n42\nAvg. Test Acc\nPRIME\nRLOO w/ OV Only\n(b) Test accuracy across different gradient steps.\nFigure 4: The effect of dense reward. We compare PRIME and RLOO with outcome verifier\n(OV). Dense rewards in PRIME lead to 2.5× sample efficiency and 6.9% performance improvement.\nPRIME also substantially outperforms RLOO on downstream tasks.\n4.2\nRL SETTINGS\nRule-based Outcome Verifier. Consistent with recent research that adopts exact match with ground\ntruth as unhackable rewards (Gao et al., 2024; Lambert et al., 2024; DeepSeek-AI et al., 2025), we\ndefine the rule-based ground truth outcome verifiers (OV) for math and coding as follows:\nrmath\no\n(y) =\n\x1a1,\nmatched\n0,\notherwise\nrcode\no\n(y) =\nP #passes\nP #test cases\nHyperparameters. We use veRL (Sheng et al., 2024) to conduct experiments. By default, we\ninitialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For\nhyperparameters, we use a constant 5 × 10−7 learning rate together with AdamW optimizer for\npolicy model, and use a 10−6 learning rate for PRMs. Both policy and PRMs use a batch size of 256\nand micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each\nprompt. We set β = 0.05 for PRM training. We set KL coefficient to 0 in all experiments.\nEvaluation Benchmarks. We evaluate on 7 reasoning benchmarks, focusing on competition-level\nmathematics and programming tasks, including AIME 2024 (Li et al., 2024), AMC (Li et al., 2024),\nMATH-500 (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He\net al., 2024), LeetCode (Guo et al., 2024), and LiveCodeBench (v2) (Jain et al., 2024).\n4.3\nMAIN RESULTS\nAs shown in Figure 1 and Table 2, Eurus-2-7B-PRIME achieves substantial improvements on key\nreasoning benchmarks over the SFT version of the model, leading to 15.1% improvement on average,\nand over 20% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7%\npass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct,\ndemonstrating its excellent reasoning ability.\n7\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n30\n32\n34\n36\n38\n40\nAvg. Test Acc\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(b) Test accuracy across different gradient steps.\nFigure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the\nbest results. Surprisingly, using PRMs trained on extra rollouts hurts the performance in both online\nand offline settings.\n4.4\nDENSE REWARDS V.S. SPARSE REWARDS\nWe first validate the effect of dense rewards compared to RLOO with outcome rewards only. We\ntrain this model for 240 steps. For PRIME, we use the same setting and train the model for 592\nsteps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure 4.\nCompared with sparse reward, PRIME takes 40% of the training steps to achieve the same\ntraining rewards as RLOO and improves the final rewards by 6.9%, with lower variances. On\ndownstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in\nTable 2.\n5\nANALYSIS\n5.1\nDESIGN CHOICES FOR THE IMPLICIT PRM\nThe Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this\nsection, we explore two major factors: (1) the initialization model and (2) the update mechanism.\nSFT model initializes a good PRM. Conventionally, we need to collect data to train RMs\nand PRMs, and then we can use them in RL. However, the Implicit PRM is a language\nmodel, so we can initialize it from any language model with the same tokenizer as the pol-\nicy model.\nTo investigate whether it is still necessary to train a PRM in advance, we con-\nduct experiments with different PRM initialization strategies: with the SFT model itself and\nwith a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT\nwith additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in § B.5).\n0\n50\n100\n150\n200\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nPRM Accuracy\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\nFigure 6: Impact of PRM online update. The\noffline PRM is gradully been overoptimized while\nonline PRMs achieve higher accuracy throughout\ntraining.\nWe report the experiment results in Figure 5.\nSurprisingly, directly using Eurus-2-7B-SFT\nto initialize the PRM greatly outperforms Eu-\nrusPRM which was trained on more samples.\nWe conjecture that initializing policy model and\nPRM from the same model largely alleviates\nthe distribution shift issue, as the PRM is only\ntrained on the online rollouts from the policy\nmodel.\nOnline PRM update is essential. To verify the\neffect of online PRM update, we pair the correct\nand wrong samples and calculate the PRM\nprediction accuracy using rϕ(y).\nWe report\nthe PRM classification accuracy in Figure 6.\nThe figure clearly shows that, online update\nmitigates\noveroptimization\nand\nreward\n8\n\nPreprint\n(a) Policy ref: We use the policy logprob as πref\nfor PRM.\n(b) SFT ref: We retain the initial policy to provide πref for\nPRM and KL.\nFigure 7: Comparison of different reference policy implementations. One uses the running policy’s\nold logprobs as reference (policy ref) while the other uses the initial SFT model as the reference\nmodel (SFT ref).\nhacking. The offline PRM, though starting with\nhigh accuracy, gradually drops during RL training procedure due to distribution shift. In contrast,\nonline PRMs that are trained on policy rollouts show the reverse curve.\nThis is further validated with training rewards and downstream performance. To breakdown, Eurus-2-\n7B-SFT is both used as PRM initialization and the reference model in the main experiment, so the\nPRM is totally trained from scratch, which means the initial PRM outputs zero reward for all tokens.\nTherefore, Figure 4 also demonstrates the effect of online PRM update. For EurusPRM initialization,\nthe online run outperforms the offline run as well in Figure 5.\n5.2\nREFERENCE MODEL CHOICE IS FLEXIBLE\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPolicy ref\nSFT ref\nFigure 8: Different reference model for PRM.\nWe compare two reference model selection strate-\ngies for PRIME. Using the policy model as refer-\nence and using the initial SFT model as reference.\nTheir rewards are similar.\nWe implement two variants of our algorithms to\nexplore the effect of reference model of implicit\nPRM, one using the initial SFT model as the\nreference model (SFT ref) while the other using\nthe running policy’s old logprobs as reference\n(policy ref), as shown in Figure 7a. The policy\nref simply adopts the old logprob of the policy\nmodel as πref, while the SFT ref remains the ini-\ntial SFT model for an additional πref calculation.\nWe compare their performance in this section.\nFrom the training rewards in Figure 8, we find\nthe two strategies are close and have pros and\ncons in different aspects: The Q value calculated\nby implicit PRM is the expectation under the dis-\ntribution of the reference model. So the updat-\ning policy could natrually serve as the reference.\nOn the other hand, KL divergence calculation\nis only allowed when the initial SFT model is\nretained.\n5.3\nSINGLE-FORWARD V.S. DOUBLE-FORWARD\nSince our implicit PRM is concurrently updated in training, for each rollout stage, we can update the\nPRM before the policy model and use the updated PRM to re-calculate the process rewards, which\n9\n\nPreprint\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nPRM Accuracy\nBefore-double forward\nBefore-single forward\nAfter-double forward\n(a) PRM classification accuracy on training samples.\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\nOutcome Training Rewards\nDouble forward\nSingle forward\n(b) Training outcome rewards.\nFigure 9: Single and double forward. While double forward methods obtain higher accuracy after\nonline update, the two variants achieve similar rewards during training.\nwe call the double-forward setting. We investigate the impact of double-forward in both the training\nand test phases. Our default setting applies single-forward, which uses process rewards from old\nPRMs. We plot PRM accuracy on rollouts and training rewards in Figure 9.\nAccordingly, we find that double-forward could increase PRM accuracy, but the training rewards\nremain close between the two methods.\n5.4\nPRIME WITH OTHER RL ALGORITHMS\nAs we stated before, PRIME is equally applicable to other RL algorithms beyond RLOO. In this\nsection, we implement PRIME with REINFORCE (Williams, 1992), GRPO (Shao et al., 2024), and\nPPO (Schulman et al., 2017). Similarly to RLOO, we only modify the advantage estimation functions\nand leave the clip surrogate loss unchanged.\nFirst of all, We compare different REINFORCE-like advantage estimators including REINFORCE,\nGRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms\ncompatible with the compound of outcome verifier reward and process reward, we accordingly make\nadaptions similar to Eq. 5. For GRPO, we have\nAi\nt = ro\n\x00yi\x01\n−mean(ro\n\x00yj\x01\n)\nstd(ro (yj))\n|\n{z\n}\nGRPO with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nrϕ(yi\ns) −mean\n\x12\nrϕ(yj)\n|yj|\n\x13\nstd\n\x10\nrϕ(yj)\n|yj|\n\x11\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n|\n{z\n}\nGRPO with implicit process rewards\n.\n(7)\nFor REINFORCE, we have\nAi\nt =\nro\n\x00yi\x01\n| {z }\nREINFORCE with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t · rϕ(yi\ns)\n|\n{z\n}\nREINFORCE with implicit process rewards\n.\n(8)\nFrom Figure 10 and Table 3, We show that PRIME boosts these algorithms on both efficiency and\nperformance as it does with RLOO. PRIME contributes consistently regardless of the policy update\nmethod, making it a generic algorithm. It indicates that PRIME is a general plug-in for almost any\nRL algorithm for LLM., which largely extends the use cases of PRIME.\nMoreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional\ncomputation cost from the critic model is redundant. This makes it possible to compensate for the\nexpense of the process reward model by using REINFORCE-like algorithms with simpler advantage\nestimators. Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm.\n10\n\nPreprint\nTable 3: Testset results of different RL algorithms.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nRLOO\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\nRLOO w/ PRIME\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\nREINFORCE\n240\n6.7\n47.0\n72.6\n36.0\n37.2\n27.2\n25.0\n36.0\nREINFORCE w/ PRIME\n240\n6.7\n50.0\n76.4\n36.8\n39.1\n27.8\n27.5\n37.8\nGRPO\n240\n10.0\n44.6\n73.2\n37.5\n36.6\n25.0\n25.8\n36.1\nGRPO w/ PRIME\n240\n16.7\n47.0\n75.0\n34.9\n38.2\n28.9\n23.9\n37.8\nPPO\n240\n10.0\n41.0\n73.6\n36.0\n36.3\n28.3\n25.7\n35.8\nPRIME as Value Model\n240\n16.7\n44.6\n72.6\n34.6\n35.7\n27.8\n24.6\n36.6\nPPO w/ PRIME\n240\n13.3\n50.6\n77.4\n37.1\n40.6\n30.0\n26.7\n39.4\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\nREINFORCE w/ PRIME\nGRPO\nGRPO w/ PRIME\nPPO\nPPO w/ PRIME\nFigure 10: PRIME also benefits REINFORCE,\nGRPO, and PPO, achieving similar improve-\nment as RLOO.\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\n+ linear-head value model\n+ Implicit PRM as value\n+ Implicit PRM as reward\nFigure 11: Comparison of value models and reward\nmodels. We show that value models, either the\noriginal PPO one or Implicit PRM, is substaintially\nworse than reward models.\n5.5\nVALUE OR REWARD, HOW TO USE THE IMPLICIT PRM?\nBesides using process rewards to estimate returns, we can also employ the Implicit PRM to predict\nvalues for advantage estimation in Eq. 2. Therefore, we compare four variants of MC estimate\nto determine the best way to incorporate dense supervision. Recall that the Implicit PRM has\nvϕ(y<t+1) = Pt\ni=1 β log πϕ(yi|y<i)\nπref(yi|y<i) with the process reward being rϕ(yt) = vϕ(y<t+1)−vϕ(y<t),\nand we assume a ground-truth outcome verifier ro, γ = 1, then we represent the variants as follows:\n(1) REINFORCE: At = ro(y).\n(2) On top of (1), using a linear-head value model V to calculate the baseline: At = ro(y)−V (y<t).\nThis is the original PPO in Figure 10 as we set γ = 1 and λ = 1.\n(3) On top of (1), using values from the Implicit PRM to serve as the baseline: At = ro(y) −\nvϕ(y<t). This is equivalent to PPO with its value model being replaced by values from the Implicit\nPRM when γ = 1 and λ = 1.\n(4) On top of (1), using process rewards from the Implicit PRM to calculate the return: At =\nro(y) + PT\ns=t rϕ(ys). This is the REINFORCE w/ PRIME in Figure 10.\nFigure 11 reports the results. Comparing PPO and REINFORCE, we find that an additional value\nmodel does not benefit policy performance. Notably, using rewards from the Implicit PRM to\ncalculate returns, which is the default setting in PRIME, greatly outperforms all three baselines,\nregardless of where the values come from. This indicates that PRMs work better than value models\nin RL for LLMs.\n5.6\n“ZERO” EXPERIMENTS\nDeepSeek-AI et al. (2025) proposed DeepSeek-R1-Zero, which is directly trained from a base model\nwith reinforcement learning. To further investigate the “Zero” setting, we also perform RL from\n11\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500\n0.525\nOutcome Training Rewards\nPRIME-Zero\nPRIME\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n32\n34\n36\n38\n40\n42\n44\n46\n48\nAvg. Test Acc\nPRIME-Zero\nPRIME\nQwen2.5-Math-7B-Instruct\n(b) Math test accuracy across different gradient steps.\nFigure 12: “Zero” RL from Qwen2.5-Math-7B. RL from the base model converges way faster than\nthe SFT model, surpassing the instruct version within 32 steps.\n0\n20\n40\n60\n80\nSteps\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nOutcome Training Rewards\nPRIME-Zero\n(a) Outcome training rewards (10-step moving).\n0\n16\n32\n48\n64\n80\n96\nSteps\n42\n44\n46\n48\n50\n52\nAvg. Test Acc\nQwen2.5-32B-Instruct\nPRIME-Zero\n(b) Math test accuracy across different gradient steps.\nFigure 13: “Zero” RL from Qwen2.5-32B-Base. RL from a 32B base model shows more promising\ngain, surpassing the instruct version within 16 steps.\nQwen2.5-Math-7B-Base and Qwen2.5-32B-Base (Yang et al., 2024a), skipping the SFT phase. We\npresent the experimental results in Figure 12 and Figure 13. The observations are as follows:\n(1) RL from base model is suprisingly efficient and effective. Comparing PRIME from Qwen2.5-\nMath-7B and Eurus-2-7B-SFT, the “Zero” setting converges much faster. This indicates that directly\nperforming RL from a base model might be a strong alternative to the conventional SFT-RL pipeline.\n(2) Larger models benefit more. Comparing 7B and 32B models, we see that the 32B model\ngains more on both training rewards and test performance. This is aligned with the conclusion in\nDeepSeek-AI et al. (2025).\n(3) Saturation could be a potential issue. Although PRIME-Zero obtains impressive performance\ngain, we find it quickly saturated at a very early stage (about 50 steps), which hinders further\nimprovement like in DeepSeek-AI et al. (2025). This is possibly attributed to the decrease of response\ndiversity, and we leave this as future work.\n6\nRELATED WORK\nRL for LLM Reasoning. In the context of LLMs, reinforcement learning has been widely used for\naligning human preferences (Christiano et al., 2017; Ouyang et al., 2022; Cui et al., 2024), but the\nopen-source community mostly adopt the data-driven imitation learning methods (Yuan et al., 2024a;\nYue et al., 2024; Wei et al., 2024; Liu et al., 2024) to enhance the reasoning capabities of LLMs. Over\nthe past few months, the paradigm gradually shifted. OpenAI o1 (Jaech et al., 2024) first showed\nthe tremendous potential of large-sacle RL for reasoning LLMs, and recent works have verified the\nscaling effect of the simple RL recipe with merely outcome rewards (DeepSeek-AI et al., 2025; Team\n12\n\nPreprint\net al., 2025). Meanwhile, the role of dense rewards in RL remains underexplored, which is the main\nfocus of PRIME.\nImplicit Rewards. Implicit rewards are broadly adopted in LLM alignment (Rafailov et al., 2023;\nChen et al., 2024b; Azar et al., 2024; Ethayarajh et al., 2024; Rosset et al., 2024; Chen et al., 2024a).\nRafailov et al. (2024) first showed that optimizing DPO objective learns a Q function implicitly. Zhou\net al. (2024) utilized implicit rewards in PPO, and showed that dense implicit rewards are better than\nsparse ones. Yuan et al. (2024b) further extended the conclusion to any loss funtion optimizing\nEq. 3.\n7\nCONCLUSION\nAs the fuel of LLMs, data, will be depleted in the near future, we are entering a new era of\nsearch and exploration, which is exemplified by reinforcement learning (Sutton, 2019). This work\ndevelops PRIME, which produces and leverages dense rewards in online RL for LLM reasoning.\nThroughout the experiments, we validate that PRIME (1) greatly benefits sample efficiency and policy\nperformance, (2) is easy to use with minimum cost, and (3) is a general method that works with broad\nRL algorithms together.\nREFERENCES\nArash Ahmadian, Chris Cremer, Matthias Gall´e, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,\nAhmet ¨Ust¨un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning\nfrom human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal\nValko, and R´emi Munos. A general theoretical paradigm to understand learning from human\npreferences. International Conference on Artificial Intelligence and Statistics, abs/2310.12036,\n2024.\nChangyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakan-\ntham, and Min Lin. Bootstrapping language models with dpo implicit rewards. arXiv preprint\narXiv:2406.09760, 2024a.\nHuayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment\nof language models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024b.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie,\nRuobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language\nmodels with scaled ai feedback. In ICML, 2024.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,\nQihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,\nZhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao\nWu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong,\nKai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao,\nLitong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang,\nMeng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang,\nQinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang,\nShuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng\nYe, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng\n13\n\nPreprint\nLiang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan\nWang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang,\nXinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen,\nXiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li,\nY. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang,\nYi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan,\nYiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia\nHe, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang,\nZhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen\nZhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\nURL https://arxiv.org/abs/2501.12948.\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\nalignment as prospect theoretic optimization. ICML, 2024.\nJiaxuan Gao, Shusheng Xu, Wenjie Ye, Weiling Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju\nWang, and Yi Wu. On designing effective rl reward at training time for llm reasoning. ArXiv,\nabs/2410.15115, 2024.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\nInternational Conference on Machine Learning, 2022.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,\nYu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming–the\nrise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han,\nYujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: A\nchallenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific\nproblems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n3828–3850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.\n211/.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence\nwith apps. arXiv preprint arXiv:2105.09938, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021b.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nAmirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,\nAaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through\nrefined credit assignment. arXiv preprint arXiv:2410.01679, 2024.\nWouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for\nfree! In DeepRLStructPred@ICLR, 2019. URL https://api.semanticscholar.org/\nCorpusID:198489118.\n14\n\nPreprint\nNathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\nBrahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya\nMalik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris\nWilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hanna Hajishirzi.\nT¨ulu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124, 2024.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent\nalignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems,\n35:3843–3857, 2022.\nJia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif\nRasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in\nai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository,\n13:9, 2024.\nRongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and\nGe Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852,\n2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\nde Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal,\nAlexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,\nNando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. ArXiv,\nabs/2305.20050, 2023.\nZihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing\nfrontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084,\n2024.\nMeta. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nOpenAI. Openai o1 system card. ArXiv, abs/2412.16720, 2024.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems, 35:27730–\n27744, 2022.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\nin Neural Information Processing Systems, 36, 2023.\nRafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q∗: Your language model is\nsecretly a q-function. arXiv preprint arXiv:2404.12358, 2024.\nCorby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and\nTengyang Xie. Direct nash optimization: Teaching language models to self-improve with general\npreferences. ArXiv, abs/2404.03715, 2024.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel.\nHigh-\ndimensional continuous control using generalized advantage estimation. In 4th International\nConference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings, 2016.\n15\n\nPreprint\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal,\nAlekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated\nprocess verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/\n2402.03300.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,\nHaibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint\narXiv: 2409.19256, 2024.\nSkunkworksAI. reasoning-0.01, 2024.\nRichard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019.\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:\n9–44, 1988.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with\nllms. arXiv preprint arXiv:2501.12599, 2025.\nQwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data.\narXiv preprint arXiv:2410.01560, 2024.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang\nSui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ArXiv,\nabs/2312.08935, 2023.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering\ncode generation with oss-instruct. In Forty-first International Conference on Machine Learning,\n2024.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229–256, 1992.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,\nLe Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu\nCui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115,\n2024a.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,\nJianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,\nXingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert\nmodel via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122.\n16\n\nPreprint\nLifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,\nRuobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.\nAdvancing llm reasoning generalists with preference trees. ArXiv, 2024a.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels, 2024b. URL https://arxiv.\norg/abs/2412.01981.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nXiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the\nweb. ArXiv, abs/2405.03548, 2024.\nKaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li,\nGanqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou.\nUltramedical: Building specialized generalists in biomedicine, 2024.\nTianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and\nXiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv\npreprint arXiv:2402.14658, 2024.\nZhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. Weak-to-strong\nsearch: Align large language models via searching over small language models. arXiv preprint\narXiv:2405.19262, 2024.\n17\n\nPreprint\nTable 4: Actions in action-centric chain-of-thought reasoning framework.\nAction Name\nDescription\nASSESS\nAnalyze current situation, identify key elements and goals\nADVANCE\nMove forward with reasoning - calculate, conclude, or form hypothesis\nVERIFY\nCheck accuracy of current approach, look for errors\nSIMPLIFY\nBreak complex problems into simpler parts\nSYNTHESIZE\nCombine multiple pieces of information into complete solution\nPIVOT\nChange strategy when current approach isn’t working\nOUTPUT\nSummarize thought process and present final answer\nTable 5: Data statistics of SFT data.\nTask\nDataset\nSize\nAvg. Response Length\nSource\nMath\nMathInstruct-MATH (Yue et al., 2023)\n12715\n964.01\nhttps://huggingface.co/datasets/TIGER-Lab/MathInstruct\nOpenMathIns-2-Aug Math (Toshniwal et al., 2024)\n15086\n1202.25\nhttps://huggingface.co/datasets/nvidia/OpenMathInstruct-2\nNumina (Li et al., 2024)\n55845\n1331.61\nhttps://huggingface.co/datasets/AI-MO/NuminaMath-CoT\nReasoning-001 (SkunkworksAI, 2024)\n29831\n1316.49\nhttps://huggingface.co/datasets/SkunkworksAI/reasoning-0.01\nCoding\nCode-Feedback (Zheng et al., 2024)\n27663\n1805.16\nhttps://huggingface.co/datasets/m-a-p/Code-Feedback\nMagicoder (Wei et al., 2024)\n24480\n1828.72\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K\nMagicoder-OSS (Wei et al., 2024)\n28980\n1850.05\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\nBiomedicine\nUltraMedical mc (Zhang et al., 2024)\n35163\n891.06\nhttps://huggingface.co/datasets/TsinghuaC3I/UltraMedical\nTotal / Avg.\n-\n229763\n1390.75\n-\nA\nSFT DATA & TRAINING DETAILS\nWe first perform supervised finetuning on the base model to get a starter model for RL.\nAction-centric chain-of-thought reasoning. We apply imitation learning (supervised finetuning) as\na warmup stage to teach models to learn certain reasoning patterns. To this end, we first design an\naction-centric chain-of-thought reasoning framework. Table 4 shows the actions in the action-centric\nchain-of-thought reasoning framework. When the model generates answers, it conducts multi-step\nreasoning and chooses one of the 7 actions at each step. The response begins with the ASSESS action\nand ends with the OUTPUT action.\nConstruction of the SFT dataset. To construct the SFT dataset, we collect reasoning instructions\nfrom several open-source datasets. It is noteworthy that we did not include many datasets with\nground-truth answers in SFT, even though they are of higher quality. However, we reserve them for\nlater RL training. The reason is that we aim to use different datasets for SFT and RL to diversify the\nexploration in RL, and we consider ground-truth more essential in RL than in SFT. For completion,\nwe employ LLaMA-3.1-70B-Instruct to answer the instructions, with a system prompt requesting the\nmodel to perform an action-centric chain-of-thought. Table 5 summarizes the key statistics of the\ndatasets used for SFT. The datasets span mathematics, coding, and biomedicine. We finally obtain\n230K SFT data and the average response length is 1390 tokens.\nSFT Training. During the SFT phase, we conduct full parameter fine-tuning with a learning rate\nof 1e-05, utilizing the AdamW optimizer alongside a cosine annealing learning rate schedule and a\nwarmup ratio of 0.1. The batch size was set to 96, with a fixed random seed of 42. The model was\ntrained on 230K datasets for 3 epochs.\nB\nRL DATA PREPROCESSING\nB.1\nRL DATA COLLECTION AND PREPROCESSING\nWe curate a high-quality RL training dataset of mathematics and coding problems with outcome\nverifiers (LaTeX answers for math and test cases for coding). For math, we source from NuminaMath-\nCoT (Li et al., 2024), which contains about 860K math problems. The problems span from Chinese\nhigh school mathematics to International Mathematical Olympiad competition questions. For coding,\nwe source from APPS (Hendrycks et al., 2021a), CodeContests (Li et al., 2022), TACO (Li et al.,\n2023), and Codeforces2. To further increase data quality, we conduct detailed cleaning and filtering.\nFinally, we retain 457k math problems and 27k coding problems.\n2https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions\n18\n\nPreprint\nB.2\nDATA FILTERING AND QUESTION-TYPE CLASSIFICATION\nThe preprocessing pipeline employs a systematic rule-based approach to filter and classify mathemati-\ncal problems to create a high-quality dataset with solvable problems, appropriate difficulty levels, and\ncorrect solutions. We exclude problems containing figures or diagrams since they require visual pro-\ncessing capabilities. We also remove proof questions due to difficulties in answer verification. Based\non specific patterns, the remaining problems are classified into question-answering, multiple-choice,\nor fill-in-the-blank questions. Since fill-in-the-blank questions comprise less than 400 examples\ncompared to the much larger set of multiple-choice questions, we focus solely on multiple-choice\nquestions for further processing.\nB.3\nCONVERTING TO DIRECT QUESTION-ANSWER FORMAT\nWe transform multiple-choice questions into a direct question-answer format through three sequential\nstages: rule-based filtering, LLM-based filtering, and LLM-based formatting.\nWe first identify and remove questions that inherently require multiple-choice options - specifically,\nthose where comparing specific statements or properties is essential to the problem-solving process.\nThese questions cannot be meaningfully converted to a direct question-answer format. The initial\nfiltering employs simple rule-based pattern matching, searching for keywords like ”following” and\n”statement” that typically indicate option-dependent problems.\nFollowing the rule-based filtering, we employ Llama-3.1-8B-Instruct to perform a more nuanced\nclassification of the remaining questions. Our pilot study revealed that while the LLM occasionally\nmisclassifies questions, it tends to err on the conservative side - marking potentially convertible\nquestions as requiring options rather than the reverse. Given our large dataset, we accepted this\nconservative approach to maintain quality.\nFor questions classified as convertible, we implement a two-phase reformatting process: 1) Question\nReformatting: Removing choice indicators and restructuring the question to elicit direct answers. 2)\nSolution Reformatting: Converting multiple-choice solutions into step-by-step derivations, ensuring\nall final answers are presented in standard LaTeX boxed format. This systematic approach maintains\nmathematical rigor while creating a standardized format suitable for downstream applications.\nB.4\nPROBLEM AND SOLUTION VALIDATION\nThe final stage involves merging all question-answer pairs and performing LLM-based comprehensive\nvalidation. We identify two key aspects in validation: solvability and correctness.\nWe leverage state-of-the-art mathematical reasoning models, including QwQ-32B-Preview (Team,\n2024) and Qwen2.5-Math-72B-Instruct (Yang et al., 2024b), employing a self-consistency approach\nto determine problem solvability, and if solvable, verify the correctness of solutions provided in the\noriginal dataset.\nTo enhance validation accuracy, we first analyzed sample problems to identify characteristics of\nsolvable and unsolvable cases and created synthetic unsolvable problems featuring missing conditions\nor logical contradictions. Based on these samples, we developed specialized prompts to improve\nthe models’ ability to distinguish solvability. Each problem undergoes five independent validation\nattempts, where the LLM: 1) Provides step-by-step solutions using LaTeX formatting. 2) Identifies\nunsolvability due to missing conditions or logical contradictions. 3) Generates complete reason-\ning traces for solvable problems. 4) Presents final answers in standardized LaTeX boxed format\n(\\boxed{...}). 5) Document any impediments to solution completion.\nWe evaluate two key consistency measures across multiple validation attempts: 1) Status Consistency:\nagreement on problem solvability. 2) Answer Consistency: consistency of solutions across different\nattempts and agreement between generated solutions and ground truth. The final dataset retains only\nproblems that demonstrate consistent solvability across validation attempts, agreement in solutions\nacross multiple attempts, and alignment with ground truth answers. This rigorous validation process\nensures the resulting dataset comprises well-defined, solvable problems with verified, accurate\nsolutions.\n19\n\nPreprint\nTable 6: Data statistics of EurusPRM training dataset.\nDataset\nGenerator Model\nNum. Inst\nResp/Inst\nStep-level/Response-level\nUltraInteract\nLlama-3.1-8B-Inst\n20177\n8\nResponse-level\nLlama-3.1-8B-Base\n13570\n8\nResponse-level\nQwen2.5-72B-Inst\n4758\n8\nResponse-level\nQwen2.5-Math-7B-Base\n25713\n8\nResponse-level\nNumina-SynMath\nLlama-3.1-8B-Inst\n4783\n8\nResponse-level\nQwen2.5-Math-7B-Base\n5806\n8\nResponse-level\nNumina-Olympiads\nLlama-3.1-8B-Inst\n2909\n8\nResponse-level\nQwen2.5-Math-7B-Base\n4739\n8\nResponse-level\nB.5\nPRM DATA\nThe dataset statistics of training EurusPRM are shown in Table 6.\n20'),
                Paper(arxiv_id='2502.01341', authors=['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], published_at=datetime.datetime(2025, 2, 4, 10, 51, 54, 103000, tzinfo=datetime.timezone.utc), title='AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding', summary='Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.', upvotes=29, thumbnail=None, content='1. Introduction\nVision-Language Models (VLMs) have gained significant\ntraction in recent years as a powerful framework for multi-\nmodal document understanding tasks that involve interpret-\n1ServiceNow 2York University 3Mila 4 ´Ecole de Technolo-\ngie Sup´erieure\n5Universit´e de Montr´eal\n6McGill University\n7University of Waterloo\n8CIFAR AI Chair\n9Polytechnique\nMontr´eal 10University of British Columbia. Correspondence to:\nAhmed Masry <ahmed.masry@servicenow.com>, Sai Rajeswar\n<sai.mudumba@servicenow.com>.\nLlama-3.2-3B-Perciever R.\nLlama-3.2-3B-MLP\nLlama-3.2-3B-Ovis\nLlama-3.2-3B-Align (ours)\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\nDocVQA\nInfoVQA\nDeepForm\nKLC\nWTQ\nTabFact\nChartQA\nTextVQA\nTableVQA\nFigure 1: Performance of Different VLM Connectors.\nThe proposed ALIGN connector outperforms other methods\nacross benchmarks using the same training configuration.\nRadial distance is proportion of maximal score, truncated at\n0.7 (black dot).\ning both the visual and textual contents of scanned docu-\nments (Kim et al., 2022; Lee et al., 2023; Liu et al., 2023a;\n2024; Hu et al., 2024; Wang et al., 2023a; Rodriguez et al.,\n2024b). Such tasks are common in real-world commercial\napplications, including invoice parsing (Park et al., 2019),\nform reading (Jaume et al., 2019), and document question\nanswering (Mathew et al., 2021b). VLM architectures typ-\nically consist of three components: (i) a vision encoder to\nprocess raw images, (ii) a Large Language Model (LLM)\npre-trained on text, and (iii) a connector module that maps\nthe visual features from the vision encoder into the LLM’s\nsemantic space.\nA central challenge in this pipeline is to effectively map the\ncontinuous feature embeddings of the vision encoder into\nthe latent space of the LLM while preserving the semantic\nproperties of visual concepts. Existing approaches can be\nbroadly categorized into deep fusion and shallow fusion\n1\narXiv:2502.01341v1  [cs.CL]  3 Feb 2025\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nmethods. Deep fusion methods, such as NVLM (Dai et al.,\n2024), Flamingo (Alayrac et al., 2022), CogVLM (Wang\net al., 2023b), and LLama 3.2-Vision (Grattafiori et al.,\n2024), integrate visual and textual features by introducing\nadditional cross-attention and feed-forward layers at each\nlayer of the LLM. While effective at enhancing cross-modal\ninteraction, these methods substantially increase the param-\neter count of the VLM compared to the base LLM, resulting\nin high computational overhead and reduced efficiency.\nIn contrast, shallow fusion methods project visual features\nfrom the vision encoder into the LLM input embedding\nspace using either multilayer perceptrons (MLPs) (Liu et al.,\n2023b; 2024) or attention-based mechanisms such as the\nPerceiver Resampler (Li et al., 2023; Laurenc¸on et al., 2024;\nAlayrac et al., 2022), before concatenating them with the\ntextual prompt’s input embeddings. This approach is more\nparameter-efficient and computationally lighter than deep\nfusion methods, but it lacks a mechanism to ensure the pro-\njected embeddings remain within the region spanned by\nthe LLM’s text embeddings – i.e. regions the LLM was\npretrained to understand. As a result, unconstrained vi-\nsual features can produce out-of-distribution (OOD) and\nnoisy inputs, leading to misalignment between modalities\nand often degrading overall performance. Recent methods\nlike Ovis (Lu et al., 2024) attempt to alleviate these issues\nby introducing separate visual embeddings indexed from\nthe vision encoder outputs and combined together to con-\nstruct the visual inputs to the LLM. However, this approach\nsignificantly increases parameter count due to the massive\nembedding matrix and requires extensive training to learn a\nnew embedding space without guaranteeing alignment with\nthe LLM’s input latent space.\nTo address these limitations, this paper introduces ALIGN-\nVLM, a novel framework that sidesteps direct projection\nof visual features into the LLM embedding space. Instead,\nour proposed connector, ALIGN, maps visual features into\nprobability distributions over the LLM’s existing pretrained\nvocabulary embeddings, which are then combined into a\nweighted representation of the text embeddings. By con-\nstraining each visual feature as a convex combination of the\nLLM text embeddings, our approach leverages the linguistic\npriors already encoded in the LLM’s text space. This en-\nsures that the resulting visual features lie within the convex\nhull of the LLM’s embedding space, reducing the risk of\nnoisy or out-of-distribution inputs and improving alignment\nbetween modalities. Our experimental results show that\nthis approach improves performance on various document\nunderstanding tasks, outperforming prior connector meth-\nods by effectively fusing visual and linguistic content. We\nsummarize our main contributions as follows:\n• We propose a novel connector, ALIGN, to bridge the\nrepresentation gap between vision and text modalities.\n• We introduce a family of Vision-Language Models,\nALIGNVLM, that achieves state-of-the-art perfor-\nmance on multimodal document understanding tasks\nby leveraging ALIGN.\n• We conduct extensive experiments demonstrating the\nrobustness and effectiveness of ALIGN across different\nmodel sizes ranging from 1B to 8B parameters.\nOur code and models will be public upon acceptance.\n2. Related Wo'),
                Paper(arxiv_id='2502.01534', authors=['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], published_at=datetime.datetime(2025, 2, 4, 1, 4, 33, 630000, tzinfo=datetime.timezone.utc), title='Preference Leakage: A Contamination Problem in LLM-as-a-judge', summary='Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between data\ngenerator LLM and judge LLM: being the same model, having an inheritance\nrelationship, and belonging to the same model family. Through extensive\nexperiments, we empirically confirm the bias of judges towards their related\nstudent models caused by preference leakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that preference leakage is a pervasive\nissue that is harder to detect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings imply that preference leakage\nis a widespread and challenging problem in the area of LLM-as-a-judge. We\nrelease all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage.', upvotes=28, thumbnail=None, content='Preference Leakage: A Contamination Problem in LLM-as-a-judge\nDawei Li * 1 Renliang Sun * 2 Yue Huang 3 Ming Zhong 4 Bohan Jiang 1\nJiawei Han 4 Xiangliang Zhang 3 Wei Wang 2 Huan Liu 1\nAbstract\nLarge Language Models (LLMs) as judges and\nLLM-based data synthesis have emerged as\ntwo fundamental LLM-driven data annotation\nmethods in model development. While their com-\nbination significantly enhances the efficiency of\nmodel training and evaluation, little attention has\nbeen given to the potential contamination brought\nby this new model development paradigm. In this\nwork, we expose preference leakage, a contami-\nnation problem in LLM-as-a-judge caused by the\nrelatedness between the synthetic data generators\nand LLM-based evaluators. To study this issue,\nwe first define three common relatednesses\nbetween data generator LLM and judge LLM:\nbeing the same model, having an inheritance\nrelationship, and belonging to the same model\nfamily.\nThrough extensive experiments, we\nempirically confirm the bias of judges towards\ntheir related student models caused by preference\nleakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that prefer-\nence leakage is a pervasive issue that is harder to\ndetect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings\nimply that preference leakage is a widespread\nand challenging problem in the area of LLM-\nas-a-judge.\nWe release all codes and data at:\nhttps://github.com/David-Li0406/\nPreference-Leakage1.\n1. Introduction\nRecent\nadvancements\nin\nLarge\nLanguage\nModels\n(LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong\net al., 2024; Zhang et al., 2024a) have empowered various\n*Equal contribution 1Arizona State University 2University of\nCalifornia, Los Angeles 3University of Notre Dame 4University\nof Illinois Urbana Champaign. Correspondence to: Dawei Li\n<daweili5@asu.edu>.\n1More resources on LLM-as-a-judge are on the website:\nhttps://llm-as-a-judge.github.io/\ndownstream tasks and applications. However, this also\nposes substantial challenges to the automatic evaluation\nof these models. Representatively, LLM-based AI agents’\nfocus transfer from traditional natural language processing\ntasks (Yang et al., 2023; Zhang et al., 2023) to real-world\n(Liu et al., 2023b; Huang et al., 2023), open-ended response\ngeneration (Wu et al., 2024), which greatly limits the\napplicability of traditional n-gram matching methods (e.g.,\nBLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu\net al., 2016; Reiter, 2018) or model-based evaluators (Zhang\net al., 2020; Zhong et al., 2022) for evaluation.\nTo address these challenges, the paradigm of LLM-as-a-\njudge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a;\nZhong et al., 2024; Li et al., 2025) has been proposed, de-\nsigned to leverage LLM as evaluators to assess response\nquality. By combining powerful LLMs with well-designed\nprompting strategies, LLM-as-a-judge enables human-like\nevaluation of long-form and open-ended generation in a\nmore cost-efficient and scalable manner. However, recent\nstudies point out some weaknesses of such assessment. For\ninstance, Ye et al. (2024) explores various biases and vulner-\nabilities of LLM-as-a-judge, highlighting the importance of\ndeveloping a reliable and fair LLM-based evaluation system.\nIn this work, we aim to introduce another concern in LLM-\nas-a-Judge–Preference Leakage. This issue arises when the\nLLMs used for data generation and evaluation are closely re-\nlated, as illustrated in Figure 1. Synthetic data generated by\nLLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c)\nhas become a cornerstone of model training (Lee et al.,\n2025). When combined with LLM-as-a-Judge, they offer\nsignificant efficiency gains in model development. However,\nlimited attention has been given to the potential contami-\nnation that occurs when the generator and evaluator LLMs\nshare a close relationship. During our preliminary study,\nwe find this issue is particularly pervasive in popular LLM-\nas-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al.,\n2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant\nstudies (more details can be found in Appendix A), due to\nthe common reliance on the most advanced LLMs, such\nas GPT-4 (Achiam et al., 2023), for both data synthesis\nand evaluation to ensure the highest quality outputs. In our\nwork, we reveal this relatedness—akin to the overlap be-\ntween training data and evaluation sets in traditional data\n1\narXiv:2502.01534v1  [cs.LG]  3 Feb 2025\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ncontamination—would introduce a systematic bias of judge\nLLMs towards their related student models (i.e., the model\ndistilled by the data generator which is related to the judge).\nCompared to other biases in LLM-as-a-Judge, such as length\nbias or egocentric bias (Ye et al., 2024; Panickssery et al.,\n2024), preference leakage is subtler and more challenging\nto detect, especially given that most LLMs do not disclose\ntheir training data.\nTo investigate and reveal the preference leakage problem,\nwe first define three relatednesses between data generator\nLLM and judge LLM: being the same model, having an\ninheritance relationship, and belonging to the same model\nfamily. Each of these scenarios is commonly encountered\nin real-world applications. Then, we pose and answer three\ncore research questions about preference leakage:\n• RQ1: Does preference leakage introduce systematic\nbiases in LLM-based evaluation? To answer it, we\nconduct experiments with various LLM baselines in two\nwidely recognized LLM-as-a-judge benchmarks, also in-\ntroduce the preference leakage score to quantify the bias\ncaused by preference leakage. The analysis results sug-\ngest an obvious bias of judging LLMs toward their related\nstudent models.\n• RQ2: What is the severity of preference leakage under\nvarious scenarios? We conduct experiments under vari-\nous relatedness settings, tuning techniques, and data mix-\ning strategies to address it, finding that preference leakage\nconsistently affects judge LLMs. Moreover, the severity\nof preference leakage correlates with the degree of relat-\nedness between the data generator and LLM judges, as\nwell as the proportion of synthetic data.\n• RQ3: What are the underlying mechanisms causing\npreference leakage? For this question, we analyze LLMs’\nrecognition capabilities on their related student models’\ngeneration as well as the distribution of bias across differ-\nent question types and judgment dimensions. The analysis\nreveals that preference leakage is a subtle, hard-to-detect\nissue, particularly affecting subjective questions and judg-\nment dimensions.\nTo summarize, our contributions in this work are as follows:\n• We introduce preference leakage, a contamination issue\narising from the relatedness between the data generator\nand judge LLMs.\n• We conduct extensive experiments across various LLMs\nand benchmarks to study how and to what extent the\npotential bias brought by preference leakage influences\njudgment.\n• Our further analysis reveals that preference leakage is\nprevalent in diverse scenarios and difficult for judge LLMs\nto detect, providing valuable insights for future research\non this challenging issue.\n2. Related Work\n2.1. LLM-as-a-Judge\nLLM-as-a-Judge, introduced by Zheng et al. (2023), lever-\nages LLMs to automatically evaluate responses and assign\nrewards. This approach has gained widespread adoption\nin areas such as model alignment (Zhang et al., 2024d)\nand benchmarking (Liu et al., 2023a; Zhang et al., 2024b;\nGao et al., 2023; Zhong et al., 2024), driving significant\nprogress in the field. Building on this concept, Zhuge et al.\n(2024) proposed Agent-as-a-Judge, where agentic systems\nare employed to evaluate other agentic systems. Addition-\nally, Prometheus, a series of open-source LLMs tailored for\nLLM-as-a-Judge (Kim et al., 2023; 2024), addresses the\nprohibitive costs associated with proprietary models, further\ndemocratizing the technology.\nDespite its promising potential, recent studies have high-\nlighted the vulnerabilities and limitations of LLM-as-a-\nJudge. Notable concerns include biases during evaluation.\nFor example, Zheng et al. (2023) identify position bias,\nwhere LLMs may favor responses based on their order in\nthe input, thereby compromising fairness. Other studies (Ye\net al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al.,\n2023; Huang et al., 2024) further emphasize the risks of\nevaluation biases. Thakur et al. (2024) assessed the judg-\nment capabilities of LLM judges, finding that only the most\nadvanced models align reasonably well with human evalu-\nators. Moreover, a recent study (Shi et al., 2024) revealed\nthe susceptibility of LLM-as-a-Judge to adversarial attacks,\nleading to incorrect judgments. In this paper, we explore an-\nother critical vulnerability of LLM-as-a-Judge—preference\nleakage—which poses additional risks to the reliability of\nthis evaluation paradigm.\n2.2. Data Leakage\nThe possible overlap between training data and evaluation\nbenchmarks has become a central issue, since LLMs are usu-\nally trained on extensive web corpora (Dodge et al., 2021).\nThis phenomenon, known as data leakage, can artificially\nimprove the performance of LLMs and undermine the re-\nliability of the assessment (Deng et al., 2024a; Jiang et al.,\n2024b).\nSeveral researchers have proposed methods to detect and\nmitigate data contamination. Deng et al. (2024b) proposed\na retrieval-based approach to assess the degree of overlap\nbetween pre-training text and benchmark data. Golchin &\nSurdeanu (2023) have developed “guided instruction” to\nflag contaminated instances. Dong et al. (2024b) proposed\nthe CDD method to identify peaks in the output distribution\nto detect data contamination. Several studies analyze data\nleakage for specific LLMs (Balloccu et al., 2024) and report\ncontamination such as cross-language contamination (Yao\n2\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nEvaluation\nTestset\nTraining\nCorpus\nData leakage\nTrain\nTraining\nCorpus\nEvaluation \nTestset\nEvaluate\nData Leakage!\nSynthetic \nData\nData \nGenerator\nTrained\nModel\nTrained\nModel\nTrained\nModel\nJudge\nJudge\nModel\nPreference Leakage!\nRelatedness \nOverlap\nLLM for Data \nSynthesis\nLLM-as-\na-Judge\nPreference leakage\nTrain\n(1). Same model\n(2). Inheritance\nSynthetic \ndata\n(3). Within the \nsame model family\nSynthesize\nFigure 1. Overview of preference leakage. We make a comparison between data leakage and preference leakage and present three types of\nrelatedness: being the same model, having an inheritance relationship and belonging to the same model family.\net al., 2024) and task contamination (Li & Flanigan, 2024)\nthat can evade traditional detection methods. To address data\ncontamination issues, Ni et al. (2024) have used web user\nquery detection and benchmark mixture. White et al. (2024)\nuse the most recent information to update the problem.\n3. Preference Leakage\nIn this section, we first provide the formal definition of data\ncontamination as the preliminary (Section 3.1). Based on\nthe concept, we demonstrate how LLM-based data synthesis\nand evaluation can lead to the evolving preference leakage\nproblem (Section 3.2).\n3.1. Preliminary: Data Leakage\nData leakage, also known as data contamination, refers to\nthe inadvertent inclusion of information from the evalua-\ntion benchmarks into the training corpus thus creating an\noverlap between training and testing sets (Kaufman et al.,\n2012). This overlap would significantly influence the eval-\nuation fairness by inflating the models’ performance since\nthe model has prior exposure to and memorized information\nthat it’s expected to generalize during testing (Elangovan\net al., 2021).\nFormally, let T represent the training corpus and E be the\nevaluation set during test time. Data contamination occurs\nif:\nT ∩E ̸= ∅,\n(1)\nwhere ∩denotes the intersection between the two sets. Such\noverlap violates the fundamental assumption that training\nand testing datasets should be disjoint to ensure an unbiased\nassessment of the model’s generalization ability.\n3.2. From Data Leakage to Preference Leakage\nWith the advent of LLMs, synthetic data generated by these\nmodels (Tan et al., 2024) has been widely adopted in var-\nious stages of model training, including pre-training, rein-\nforcement learning with AI feedback (RLAIF) and super-\nvised fine-tuning. Concurrently, the concept of LLM-as-\na-judge has emerged, where LLMs are employed to auto-\nmate the evaluation process. While these LLM-as-an-oracle\napproaches reduce human effort in data annotation, signif-\nicantly enhancing the efficiency and scalability of model\ntraining and evaluation, they also blur the lines between\nmodels and data, introducing evolving challenges (Shu-\nmailov et al., 2024; Dai et al., 2024).\nIn this work, we examine the evolving contamination prob-\nlem brought by LLM-as-a-oracle and formally propose the\nconcept of preference leakage. This refers to a situation\nin which the LLMs used for synthetic data generation and\nevaluation are related. Formally, we define this as:\nLLMG ∩LLMJ ̸= ∅,\n(2)\nwhere LLMG and LLMJ denote the LLMs used for train-\ning data generation and evaluation. ∩represents the related-\nness between the two (sets of) LLMs. This relatedness may\ninvolve:\n• Being the same model: the data generator and evaluator\nare the same model:\nLLMG = LLMJ.\n(3)\n• Inheritance relationship: one model is trained on syn-\nthetic data generated by the other:\nLLMG = Inherit(LLMJ),\n(4)\nLLMJ = Inherit(LLMG).\n(5)\n3\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n• Within the same model family: the data generator and\nevaluator belong to the same model family (e.g., GPT\nfamily (Achiam et al., 2023) and Gemini family (Team\net al., 2024)):\nLLMG, LLMJ ∈FX.\n(6)\nDue to this relatedness, the preference of the judge models\n(e.g., format, style and wording) can be leaked to the student\nmodels through the synthetic data, resulting in non-trivial\nbias from the judge LLMs during the test time.\n4. Main Experiment\n4.1. Experiment Setup\nModels. We choose three powerful LLMs as data generator/\njudge models. They are GPT-4o-2024-11-20 (Achiam et al.,\n2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA-\n3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student\nmodel, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and\nQwen-2.5-14B (Yang et al., 2024). To avoid potential prefer-\nence leakage due to distilling data from other LLMs during\nthe instruction-tuning process, we choose to use the -PRE-\nTRAINED version rather than the -INSTRUCT version of\nthese student models.\nEvaluation Datasets. We choose two representative pair-\nwise evaluation datasets, Arena-Hard (Li et al., 2024e)\nand AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the\ntrained student models. Arena-Hard includes 500 challeng-\ning questions in English. Additionally, the evaluation agree-\nment between Arena-Hard and Chatbot Arena (Zheng et al.,\n2023)’s hard prompts achieved a 96.7% Spearman corre-\nlation, demonstrating the consistency of Arena-Hard with\nhuman preferences (Li et al., 2024e). AlpacaEval 2.0 is an\nimproved evaluation method based on AlpacaEval (Li et al.,\n2023) and contains 805 questions. Compared to version 1.0,\nAlpacaEval 2.0 significantly reduces the effect of text length\non the evaluation results.\nImplementation Details. In our main experiment, we ex-\namine the preference leakage introduced by using the same\ndata generator and evaluator in supervised fine-tuning (SFT).\nWe will discuss other relatedness and learning methods in\nSection 5. To obtain synthetic datasets, We first randomly\nsample 30,000 prompts from the Ultrafeedback dataset (Cui\net al., 2024). The Ultrafeedback dataset includes instruc-\ntions from several publicly available high-quality datasets\nsuch as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al.,\n2023), and Evol-Instruct (Xu et al., 2023). For each data gen-\nerator model, we provide these prompts for them to produce\nsynthetic responses, resulting in three synthetic instruction\ndatasets. We then use each dataset to supervised fine-tune\nthe student model, obtaining three different versions for each\nbaseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-Gemini-\n1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each\ntwo student models and obtain three model pairs. For each\nmodel pair, we perform the pairwise comparison using the\nthree judge models respectively.\nMetrics & Annotation Based on our hypothesis, preference\nleakage would lead to bias of judge LLMs towards their\nrelated student models. Following this principle, we design\nthe preference leakage score PLS(i, j) to measure the bias\nin model pair (i, j) caused by preference leakage:\nPLS(i, j) =\n\x10\nWR(i,i)−AVG(i,j)\nAVG(i,j)\n\x11\n+\n\x10\nWR(j,j)−AVG(j,i)\nAVG(j,i)\n\x11\n2\n,\n(7)\nAVG(i, j) = WR(i, i) + WR(i, j)\n2\n.\n(8)\nHere WR(i, j) represents the win-rate score from judge\nmodel i to student model j. Intuitively, a large preference\nleakage score indicates that the two judge models demon-\nstrate strong bias toward their related student models, sug-\ngesting a significant preference leakage phenomenon.\nWhile our proposed preference leakage score quantifies the\ndegree of preference leakage in each model pair, we also\nperform manual annotation to assess the preference leakage\nin each individual model. We randomly select 100 questions\nfrom AlpacaEval 2.0 and have three well-trained annota-\ntors perform pairwise comparisons independently for each\nresponse pair. After the annotation, the majority voting is\napplied to each response pair to get the final annotation\nresults.\nMore details about model training, metric explanation, and\nannotation process can be found in Appendix B.\nModel\nData Generator/ Judge Pair\nArena-Hard\nAlpacaEval 2.0\nAvg.\nGPT-4o & Gemini-1.5\n28.7%\n18.4%\n23.6%\nGPT-4o & LLaMA-3.3\n-6.7%\n1.4%\n-2.7%\nMistral-7B\nLLaMA-3.3 & Gemini-1.5\n13.1%\n19.8%\n16.4%\nGPT-4o & Gemini-1.5\n37.1%\n18.6%\n27.9%\nGPT-4o & LLaMA-3.3\n1.0%\n2.3%\n1.7%\nQwen-2.5-14B\nLLaMA-3.3 & Gemini-1.5\n25.4%\n18.4%\n21.9%\nTable 1. Preference leakage score result on Arena-Hard and Al-\npacaEval 2.0. The blue background indicates a negative prefer-\nence leakage score value and the purple background indicates a\npositive value. The deeper the color, the larger the absolute value.\n4.2. Main Results\nIn our main experiment, we aim to provide insights into\nRQ1.\nPreference leakage exists in most model pairs. The origi-\nnal judgment results from Arena-Hard and AlpacaEval 2.0,\nalong with the calculated preference leakage scores, are\nshown in Figure 2, Figure 3, and Table 1. As the results\ndemonstrate, in most model pairs (except Mistral-GPT-4o vs\n4\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n18.2%\n39.8%\n42.0%\n27.4%\n43.8%\n28.8%\n38.4%\n34.6%\n27.0%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n46.2%\n42.7%\n11.1%\n50.4%\n35.0%\n14.6%\n55.8%\n27.0%\n17.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n9.2%\n31.4%\n59.4%\n14.6%\n30.0%\n55.4%\n22.2%\n30.8%\n47.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n22.0%\n33.5%\n44.5%\n28.8%\n50.2%\n21.6%\n49.8%\n29.0%\n21.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n52.1%\n40.7%\n7.2%\n39.0%\n51.8%\n9.2%\n57.4%\n29.6%\n13.0%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n10.0%\n29.4%\n60.6%\n16.4%\n48.4%\n35.2%\n24.6%\n30.0%\n44.4%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nTie\nModel B Wins\nFigure 2. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n36.8%\n63.2%\n49.5%\n50.5%\n55.1%\n44.9%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n65.8%\n34.2%\n60.3%\n39.7%\n61.6%\n38.4%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n22.6%\n77.4%\n39.5%\n60.5%\n43.1%\n56.9%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n39.3%\n60.7%\n52.4%\n47.6%\n57.8%\n42.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n63.3%\n36.7%\n59.3%\n40.7%\n61.5%\n38.5%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n26.2%\n73.8%\n42.9%\n57.1%\n50.1%\n49.9%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nModel B Wins\nFigure 3. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in\nAlpacaEval 2.0.\nMistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-\n3.3), the judge LLMs exhibit a strong preference toward\ntheir related student models, leading to large positive val-\nues in the preference leakage scores. This finding suggests\nthat preference leakage, along with the resulting bias, is\nwidespread in SFT when the data generator and evaluator\nare the same.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n73.6%\n8.8%\n17.6%\n79.5%\n1.7%18.8%\nLLaMA-2 vs Others\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n76.2%\n17.9% 6.0%\n79.8%\n20.2%0.0%\nJudge Model\nLLaMA-2 vs Claude-v1\nModel A Wins\nTie\nModel B Wins\nFigure 4. Comparison between GPT-4 and human’s judgment for\nLLaMA-2 from MTBench.\nEvaluators’ bias towards certain LLMs can be inherited\nby its student models. From Figure 2 and Figure 3, we find\nan obvious preference of GPT-4o towards Mistral/ Qwen-\nLLaMA-3.3 and this leads to the low preference leakage\nscore in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and\nQwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate\nthe source of this preference, we examine whether the GPT-\n4 evaluator has a bias toward LLaMA series models. Using\nthe MTBench (Zheng et al., 2023) dataset, which includes\npairwise comparison judgments from both humans and GPT-\n4, we compare GPT-4’s and human evaluators’ judgments\non LLaMA-2 vs other models (including Vicuna, Alpaca,\nGPT-3.5, and GPT-4, which are preferred by GPT-4 due\nto preference leakage or egocentric bias) and LLaMA-2 vs\nClaude. The results, shown in Figure 4, reveal a clear pref-\nerence for LLaMA-2 by GPT-4. Consequently, we conclude\nthat evaluators’ bias can be inherited. In this case, GPT-4’s\nbias toward LLaMA has been passed on to LLaMA’s stu-\ndent models. This inheritance, combined with the opaque\ntraining data of LLMs, makes preference leakage a more\ncomplex and challenging problem.\nModel pairs with similar performance tend to have more\n5\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n53.0%\n47.0%\n40.2%\n59.8%\n49.4%\n50.6%\n58.4%\n41.6%\nJudge Model\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n62.0%\n38.0%\n76.2%\n23.8%\n72.1%\n27.9%\n67.8%\n32.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n36.0%\n64.0%\n17.1%\n82.9%\n39.0%\n61.0%\n46.0%\n54.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\nModel A Wins\nModel B Wins\nFigure 5. Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0.\nobvious preference leakage. As shown in Table 1, we ob-\nserve that the preference leakage scores for Mistral-GPT-4o\nvs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini-\n1.5 (23.6% and 27.9% respectively) are consistently higher\nthan that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9%\nrespectively). We think that this is largely due to the more\ncomparable performance between the student models of\nGPT-4o and Gemini-1.5. Intuitively, when the quality of the\ntwo responses is similar, the evaluator may rely more heav-\nily on its inherent preferences to make a judgment, thereby\nexacerbating the preference leakage issue.\nLarger student models cause more bias from judge\nLLMs. Another observation from Table 1 is that the over-\nall preference leakage score for Qwen-2.5-14B is higher\nthan that for Mistral-7B. Drawing on insights from previous\nstudies on data leakage, which suggest that larger and more\npowerful LLMs are more capable of memorizing extensive\ninformation and are thus more susceptible to data contamina-\ntion (Bordt et al., 2024; Duan et al., 2024), we attribute this\ndifference in preference leakage to the size and capabilities\nof the student LLMs. We assume that larger student models,\ndue to their better performance and generalization abilities,\nare more capable of learning and memorizing the hidden\npreference pattern from the synthetic data, thus leading to a\nmore serious preference leakage.\nDifferent data generator/ judge LLMs result in varying\ndegrees of bias under preference leakage. While we have\nconcluded that student model pairs with similar performance\nor more powerful student models tend to exhibit greater\npreference leakage, we also examine whether different data\ngenerator and judge LLMs contribute to varying degrees\nof preference leakage. Analyzing the manual annotation\nresults presented in Table 5, we observe that Gemini-1.5\nshows a strong bias toward its students, followed by GPT-4o,\nwith LLaMA-3.3 displaying the least bias. This variation in\npreference leakage may stem from differences in the level\nof leaked preference in the synthetic responses generated\nby the data generator LLMs. For instance, an LLM with a\ndistinctive style or format in its responses offers more op-\nportunities for student models to learn these characteristics,\npotentially leading to more pronounced preference leakage\nduring evaluation. Future work could further quantify the\nextent of leaked preference for each data generator model.\n5. Further Analysis\nIn this section, we conduct relatedness analysis, learning\nmethod analysis and data mixing analysis (Section 5.1 - 5.3)\nto answer RQ2. Due to the cost consideration, we conduct\nthese analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5.\nMoreover, we perform recognition analysis and category\nanalysis to answer RQ3.\nArena-Hard AlpacaEval 2.0\nAvg.\nSame Model\n28.7%\n18.4%\n23.6%\nInheritance\nw/ same ins.\n17.8%\n20.7%\n19.3%\nInheritance\nw/ different ins.\n18.3%\n26.3%\n22.3%\nSame Family\nw/ same series\n10.1%\n7.6%\n8.9%\nSame Family\nw/ different series\n3.3%\n2.2%\n2.8%\nTable 2. Preference leakage score in different relatedness between\nthe data generator and the judging LLM.\n5.1. Relatedness Analysis\nWe demonstrate the impact of different relatedness condi-\ntions between the data generator and the judge LLM on the\npreference leakage problem, as shown in Table 2.\nPreference leakage under inheritance settings causes ob-\nvious bias of judges towards their related students. For\nthe inheritance relationship, we consider the situation where\nthe data generator is inherited from the judge model. We\nconducted the following two experiments: (1). we give the\nsame instructions again as in the SFT stage (Inheritance w/\nsame ins.), or (2). we sample the same number of different\ninstructions from the Ultrafeedback (Inherence w/ different\nins.). Then, we let the fine-tuned Mistral model generate\nthe answers and use these generated data to fine-tune a new\nMistral student model. From the results, with the same in-\nstructions, the average preference leakage score is 19.3%. In\ncomparison, the score with different instructions is 22.3%.\n6\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFirstly, in an inheritance setting, data generators can inherit\njudges’ preferences, which are then passed on to new stu-\ndent models, thereby compromising the fairness of their\nevaluation. Second, even when different instructions are\nused, judges’ preferences leaked to data generators can still\nbe transferred to the new student model through synthetic\ndata, leading to a high preference leakage score.\nModels within the same series tend to cause more sig-\nnificant bias. For two models within the same family, we\nconsider two settings: (1) Same series, where training data\nis generated by GPT-4o and Gemini-1.5-flash, and judged\nby GPT-4-turbo and Gemini-1.5-pro; (2) Different series,\nwhere training data is still generated by GPT-4o and Gemini-\n1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro.\nIn the same series setting, the average preference leakage\nscore is 8.9%, indicating that despite using different mod-\nels for data generation and judgment, their relatedness in\nterms of model family leads to some preference leakage.\nIn contrast, the different series setting yields a significantly\nlower leakage score of 2.8%, likely due to differences in\narchitecture, training data, and other factors, reducing the\ninfluence of model-related biases in evaluation.\nArena-Hard\nAlpacaEval 2.0\nAvg.\nSFT\n28.7%\n18.4%\n23.6%\nDPO\n7.7%\n2.7%\n5.2%\nICL\n-4.2%\n-1.1%\n-2.7%\nTable 3. Preference leakage score in different learning methods.\n5.2. Learning Method Analysis\nWe also compare three learning methods, supervised\nfine-tuning (SFT), direct preference optimization (DPO)\n(Rafailov et al., 2024), and in-context learning (ICL) (Dong\net al., 2024a), to explore the different influences to them un-\nder preference leakage. We first build a data pool based on\nhuman-written instruction-tuning data from OASST (K¨opf\net al., 2024), LIMA (Zhou et al., 2024), and MOSS (Sun\net al., 2024b) to supervised fine-tune the pre-trained model.\nFor DPO, we sample 2 responses for each instruction from\nsampled UltraFeedback instruction and prompt each data\ngenerator to produce the pairwise feedback. Then we use\nthe DPO loss to further train the fine-tuned policy on each\nsynthetic pairwise dataset. Appendix C shows the prompt\nwe use to craft synthetic pairwise feedback. For ICL, we\nsample 4 instruction-response pairs from each LLMs’ syn-\nthetic dataset as the demonstration during inference.\nTuning approaches would leak judges’ preference to the\nstudent models. Various learning methods show significant\ndifferences in preference leakage scores across learning\nmethods. SFT exhibits the highest average leakage score at\n23.6%. In contrast, DPO achieves a much lower score of\n5.2%, likely because its focus on preferences helps minimize\nthe unintended transfer of judge model biases. Meanwhile,\nICL, which relies on contextual examples without updating\nmodel parameters, is least affected by the data generator’s\npreferences, resulting in the lowest leakage scores.\n20\n40\n60\n80\n100\nContamination Ratio (%)\n0\n5\n10\n15\n20\n25\n30\nPreference Leakage Score (%)\nAlpacaEval2.0 - Manual\nArenaHard - Manual\nAlpacaEval2.0 - Synthetic\nArenaHard - Synthetic\nFigure 6. Experiment results on data mixing. ‘Manual’ represents\nthe original synthetic data mixed with manually-written data. ‘Syn-\nthetic’ represents the original data mixed with other synthetic data.\n5.3. Data Mixing Analysis\nIn real-world applications, synthetic data from a single LLM\nis often mixed with manually-written data or other multi-\nsource synthetic data to train student models. To mimic\nthese scenarios and explore how much synthetic data could\nlead to preference leakage, we conduct a data mixing anal-\nysis. Specifically, we randomly sample 10%, 30%, 50%,\nand 70% from the original synthetic dataset and mix it with\nmanually-written data and multi-source synthetic data, re-\nspectively, in order to maintain a consistent total volume of\ntraining data (30,000). For the manually-written data, we\nsample from the data pool collected in Section 5.2. For the\nmulti-source synthetic data, we use the original synthetic\ndata from Ultrafeedback, which includes responses gener-\nated by various LLMs (e.g., WizardLM, Flcon, etc.). After\nobtaining the mixing training data, we train the student mod-\nels using SFT and calculate their preference leakage scores\nbased on the judgment results. Figure 6 presents the results\nwith two mixing strategies across two benchmarks.\nThe degree of preference leakage is directly proportional\nto the amount of synthetic data. We observe a strong\ncorrelation between the proportion of synthetic data in the\nmixture and the preference leakage score, with no clear\nthreshold separating cases with preference leakage from\nthose without. This suggests that preference leakage can\noccur even with a small amount of leaked synthetic data,\nposing significant challenges for its detection.\n5.4. Can Judges Recognize Student Models?\nPrevious studies demonstrate the LLM judges can recog-\nnize and thus prefer their own generation (Panickssery et al.,\n2024). In this work, we pose a similar question: Does prefer-\nence leakage also source from the LLM judges’ recognition\n7\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nMathematics\nBusiness\nDaily Life\nScience\nWriting\nOthers\nProgramming\n0\n10\n20\n30\nPreference Leakage Score (%)\n7.7\n16.5\n17.2\n17.3\n21.0\n23.8\n31.4\n(a) Question Type\nCompleteness\nClarity\nRichness\nSatisfaction\nFactuality\nLogical\nOthers\nCreativity\nFairness\n20\n24\n28\n32\nPreference Leakage Score (%)\n27.9\n28.6\n28.8\n29.0\n29.2\n30.2\n30.4\n30.7\n32.4\n(b) Judgment dimension\nFigure 7. Category analysis results on question type and judgment dimension.\nTask\nModel\nAccuracy\nStudent Recognition\nGPT-4o\n60.0%\nGemini-1.5\n25.4%\nLLaMA-3.3\n54.2%\nResponse Classification\nBERT\n82.4%\nTable 4. Student recognition (binary classification) and response\nclassification results (three-class classification).\nof their related student models’ generation? To study this,\nwe follow Panickssery et al. (2024) to prompt the three\njudge LLMs and test whether they could recognize their\nrelated student models’ generation. Additionally, we split\nthree student models’ generation into training and testing\nsets, and train a BERT classifier to perform a three-class\nclassification inspired by the previous study on detecting\nhuman-AI text (Zhang et al., 2024c). Detailed instruction\nand training settings can be found in Appendix D.\nJudge LLMs do not show good performance in recogniz-\ning the generation of their student models. As the result\npresented in Table 4, we find that the recognition perfor-\nmance of each judge LLM in the content of related students\nis poor, with accuracy around the performance of random\nguess. Moreover, we observe no correlation between recog-\nnition performance and the preference leakage degree for\njudge LLMs. For instance, while Gemini-1.5 leads to the\nmost preference leakage (as shown in Section 4.2), it per-\nforms the worst in recognition tasks. These suggest that\npreference leakage is subtler and harder-to-detect for judge\nLLMs, in contrast to the more obvious egocentric bias.\nCertain features embedded in student models through\nsynthetic data. Although judge LLMs do not perform\nwell in related student recognition, we notice the fine-tuned\nBERT classification demonstrates a high accuracy score in\nclassifier response generated by each student model. This\nsuggests that certain characteristics—such as style and for-\nmat—are embedded in the student models through the syn-\nthetic responses. This finding further supports the existence\nof preference leakage and lays the groundwork for future\nresearch aimed at detecting and preventing it.\n5.5. Impact on Question Type & Judgment Dimension\nIn this section, we explore the impact of preference leakage\nacross various question types and judgment dimensions. For\nthe question type analysis, we first propose several general\nquestion types based on the question clusters introduced by\nArena-Hard. Then, we prompt GPT-4o to map each question\nin Arena-Hard and AlpacaEval to one of the question types\nand calculate the preference leakage score for each question\ncategory. For the judgment dimension analysis, we follow\nthe judgment dimensions introduced by Liu et al. (2023a)\nand also utilize GPT-4o to map the rationale generated by\njudge LLMs to one or multiple judgment dimensions. More\ndetailed prompt can be found in Appendix E. The analysis\nresults are presented in Figure 7.\nSubjective question and judgment dimension tend to\nlead to more bias. For question type analysis, we find ob-\njective questions with a definitive answer, like mathematical\nones, demonstrate the least preference leakage. By contrast,\nsubjective questions that have more than one standard an-\nswer, such as programming and writing, usually lead to a\nmore obvious preference leakage. This observation is also\napplied to judgment dimension analysis, as objective di-\nmensions (like completeness) have an overall lower leakage\ndegree compared with subjective ones (like fairness). This\nsuggests that preference leakage tends to be more significant\nin objective questions and dimensions, where the contami-\nnated model is more likely to receive biased preference.\n6. Conclusion\nIn this work, we formally highlight the preference leakage\nproblem in LLM-as-a-judge systems. The results of our\nmain experiment, measured using the proposed preference\nleakage score, reveal a clear bias in each judge toward its\nrespective student model. We also observe that this bias\nis more pronounced in comparable model pairs and larger\nstudent models. Furthermore, we conduct additional anal-\nysis on various factors, including the relationship between\nthe data generator and judge LLMs, model tuning tech-\n8\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nniques, and data mixing strategies. Our findings suggest\nthat preference leakage can cause significant bias across\ndiverse scenarios. Finally, through recognition and category\nanalyses, we investigate the underlying mechanisms of pref-\nerence leakage, demonstrating that it is a challenging and\nhard-to-detect issue, especially in subjective questions and\njudgment dimensions. In the future, we aim to explore meth-\nods for detecting, preventing, and mitigating this evolving\nchallenge in LLM-as-a-judge systems.\nImpact Statements\nBy revealing preference leakage, this work could help build\nmore trustworthy and ethically grounded AI systems. The\nrelatedness between data generators and evaluators can sys-\ntematically bias evaluations, potentially compromising the\nfairness and reliability of the automatic evaluation paradigm.\nThese biased evaluations may indirectly affect downstream\ntasks such as AI alignment and decision-making systems,\nleading to unintended ethical risks. To mitigate preference\nleakage, we hope that researchers will propose more reli-\nable evaluation methods, diversify training data sources, and\ndevelop contamination-resistant benchmarks in the future.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. ArXiv preprint,\nabs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nBalloccu, S., Schmidtov´a, P., Lango, M., and Duˇsek, O.\nLeak, cheat, repeat: Data contamination and evaluation\nmalpractices in closed-source llms. In Proceedings of the\n18th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pp. 67–93, 2024.\nBordt, S., Nori, H., and Caruana, R. Elephants never forget:\nTesting language models for memorization of tabular data.\nIn NeurIPS 2023 Second Table Representation Learning\nWorkshop, 2024.\nChen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B.\nHumans or llms as the judge? a study on judgement\nbiases. arXiv preprint arXiv:2402.10669, 2024.\nCui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y.,\nXie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting\nlanguage models with scaled ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nDai, S., Xu, C., Xu, S., Pang, L., Dong, Z., and Xu, J. Bias\nand unfairness in information retrieval systems: New\nchallenges in the llm era. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 6437–6447, 2024.\nDeng, C., Zhao, Y., Heng, Y., Li, Y., Cao, J., Tang, X.,\nand Cohan, A. Unveiling the spectrum of data contami-\nnation in language models: A survey from detection to\nremediation. arXiv preprint arXiv:2406.14644, 2024a.\nDeng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A.\nInvestigating data contamination in modern benchmarks\nfor large language models. In Proceedings of the 2024\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pp. 8698–8711,\n2024b.\nDodge, J., Sap, M., Marasovi´c, A., Agnew, W., Ilharco, G.,\nGroeneveld, D., Mitchell, M., and Gardner, M. Docu-\nmenting large webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1286–1305, 2021.\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia,\nH., Xu, J., Wu, Z., Chang, B., et al. A survey on in-\ncontext learning. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing,\npp. 1107–1128, 2024a.\nDong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and\nLi, G. Generalization or memorization: Data contamina-\ntion and trustworthy evaluation for large language models.\narXiv preprint arXiv:2402.15938, 2024b.\nDuan, S., Khona, M., Iyer, A., Schaeffer, R., and Fiete, I. R.\nUncovering latent memories: Assessing data leakage and\nmemorization patterns in large language models. arXiv\npreprint arXiv:2406.14549, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nElangovan, A., He, J., and Verspoor, K. Memorization vs.\ngeneralization: Quantifying data leakage in nlp perfor-\nmance evaluation. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pp. 1325–1335, 2021.\nGan, R., Wu, Z., Sun, R., Lu, J., Wu, X., Zhang, D.,\nPan, K., Yang, P., Yang, Q., Zhang, J., et al. Ziya2:\nData-centric learning is all llms need. arXiv preprint\narXiv:2311.03301, 2023.\n9\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nGao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan,\nX. Human-like summarization evaluation with chatgpt.\narXiv preprint arXiv:2304.02554, 2023.\nGolchin, S. and Surdeanu, M. Time travel in llms: Trac-\ning data contamination in large language models. arXiv\npreprint arXiv:2308.08493, 2023.\nHu, S., Luo, Y., Wang, H., Cheng, X., Liu, Z., and Sun, M.\nWon’t get fooled again: Answering questions with false\npremises. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 5626–5643, 2023.\nHuang, Y., Shi, J., Li, Y., Fan, C., Wu, S., Zhang, Q., Liu, Y.,\nZhou, P., Wan, Y., Gong, N. Z., et al. Metatool benchmark\nfor large language models: Deciding whether to use tools\nand which to use. arXiv preprint arXiv:2310.03128, 2023.\nHuang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y.,\nGao, C., Huang, Y., Lyu, W., Zhang, Y., et al. Posi-\ntion: Trustllm: Trustworthiness in large language models.\nIn International Conference on Machine Learning, pp.\n20166–20270. PMLR, 2024.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Car-\nney, A., et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiang, B., Li, D., Tan, Z., Zhou, X., Rao, A., Lerman, K.,\nBernard, H. R., and Liu, H. Assessing the impact of\nconspiracy theories using large language models. arXiv\npreprint arXiv:2412.07019, 2024a.\nJiang, M., Liu, K. Z., Zhong, M., Schaeffer, R., Ouyang,\nS., Han, J., and Koyejo, S. Investigating data contami-\nnation for pre-training language models. arXiv preprint\narXiv:2401.06059, 2024b.\nKaufman, S., Rosset, S., Perlich, C., and Stitelman, O. Leak-\nage in data mining: Formulation, detection, and avoid-\nance. ACM Transactions on Knowledge Discovery from\nData (TKDD), 6(4):1–21, 2012.\nKim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun,\nS., Shin, S., Kim, S., Thorne, J., et al. Prometheus: Induc-\ning fine-grained evaluation capability in language models.\nIn The Twelfth International Conference on Learning\nRepresentations, 2023.\nKim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J.,\nWelleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M.\nPrometheus 2: An open source language model special-\nized in evaluating other language models. arXiv preprint\narXiv:2405.01535, 2024.\nKoo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M.,\nand Kang, D.\nBenchmarking cognitive biases in\nlarge language models as evaluators.\narXiv preprint\narXiv:2309.17012, 2023.\nK¨opf, A., Kilcher, Y., von R¨utte, D., Anagnostidis, S.,\nTam, Z. R., Stevens, K., Barhoum, A., Nguyen, D., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations-\ndemocratizing large language model alignment. Advances\nin Neural Information Processing Systems, 36, 2024.\nLee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J.,\nLu, K. R., Bishop, C., Hall, E., Carbune, V., Rastogi,\nA., et al. Rlaif vs. rlhf: Scaling reinforcement learning\nfrom human feedback with ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nLee, S., Zhou, J., Ao, C., Li, K., Du, X., He, S., Liu, J., Yang,\nM., Wen, Z., and Ni, S. Distillation quantification for\nlarge language models. arXiv preprint arXiv:2501.12619,\n2025.\nLi, C. and Flanigan, J. Task contamination: Language mod-\nels may not be few-shot anymore. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 38,\npp. 18471–18480, 2024.\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z.,\nBhattacharjee, A., Jiang, Y., Chen, C., Wu, T., et al. From\ngeneration to judgment: Opportunities and challenges of\nllm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024a.\nLi, D., Tan, Z., Chen, T., and Liu, H. Contextualization dis-\ntillation from large language model for knowledge graph\ncompletion. arXiv preprint arXiv:2402.01729, 2024b.\nLi, D., Yang, S., Tan, Z., Baik, J. Y., Yun, S., Lee, J.,\nChacko, A., Hou, B., Duong-Tran, D., Ding, Y., et al.\nDalk: Dynamic co-augmentation of llms and kg to an-\nswer alzheimer’s disease questions with scientific litera-\nture. arXiv preprint arXiv:2405.04819, 2024c.\nLi, D., Tan, Z., and Liu, H. Exploring large language models\nfor feature selection: A data-centric perspective. ACM\nSIGKDD Explorations Newsletter, 26(2):44–53, 2025.\nLi, M., Chen, L., Chen, J., He, S., Gu, J., and Zhou, T. Selec-\ntive reflection-tuning: Student-selected data recycling for\nllm instruction-tuning. arXiv preprint arXiv:2402.10110,\n2024d.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline. arXiv preprint arXiv:2406.11939, 2024e.\n10\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval:\nAn automatic evaluator of instruction-following models,\n2023.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74–81, 2004.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. In Proceedings of\nthe 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 3214–\n3252, 2022.\nLiu, C.-W., Lowe, R., Serban, I., Noseworthy, M., Charlin,\nL., and Pineau, J. How NOT to evaluate your dialogue sys-\ntem: An empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In Su, J., Duh, K.,\nand Carreras, X. (eds.), Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Process-\ning, pp. 2122–2132, Austin, Texas, 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1230.\nURL https://aclanthology.org/D16-1230.\nLiu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes\ngood data for alignment? a comprehensive study of auto-\nmatic data selection in instruction tuning. In The Twelfth\nInternational Conference on Learning Representations,\n2024.\nLiu, X., Lei, X., Wang, S., Huang, Y., Feng, Z., Wen, B.,\nCheng, J., Ke, P., Xu, Y., Tam, W. L., et al. Alignbench:\nBenchmarking chinese alignment of large language mod-\nels. arXiv preprint arXiv:2311.18743, 2023a.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nDing, H., Men, K., Yang, K., et al. Agentbench: Evalu-\nating llms as agents. arXiv preprint arXiv:2308.03688,\n2023b.\nNi, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neu-\nbig, G., and You, Y. Mixeval: Deriving wisdom of the\ncrowd from llm benchmark mixtures. arXiv preprint\narXiv:2406.06565, 2024.\nPanickssery, A., Bowman, S. R., and Feng, S. Llm evalu-\nators recognize and favor their own generations. arXiv\npreprint arXiv:2404.13076, 2024.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, pp. 311–318,\n2002.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nReiter, E. A structured review of the validity of BLEU.\nComputational Linguistics, 44(3):393–401, 2018. doi: 10.\n1162/coli a 00322. URL https://aclanthology.\norg/J18-3002.\nShi, J., Yuan, Z., Liu, Y., Huang, Y., Zhou, P., Sun,\nL., and Gong, N. Z.\nOptimization-based prompt in-\njection attack to llm-as-a-judge.\nIn Proceedings of\nthe 2024 on ACM SIGSAC Conference on Computer\nand Communications Security, CCS ’24, pp. 660–674,\nNew York, NY, USA, 2024. Association for Comput-\ning Machinery. ISBN 9798400706363. doi: 10.1145/\n3658644.3690291.\nURL https://doi.org/10.\n1145/3658644.3690291.\nShumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Ander-\nson, R., and Gal, Y. Ai models collapse when trained on\nrecursively generated data. Nature, 631(8022):755–759,\n2024.\nSun, R., Liu, M., Yang, S., Wang, R., He, J., and Zhang, J.\nFostering natural conversation in large language models\nwith nico: a natural interactive conversation dataset. arXiv\npreprint arXiv:2408.09330, 2024a.\nSun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Liu, X.,\nYan, H., Shao, Y., Tang, Q., Zhang, S., Zhao, X., Chen,\nK., Zheng, Y., Zhou, Z., Li, R., Zhan, J., Zhou, Y.,\nLi, L., Yang, X., Wu, L., Yin, Z., Huang, X., Jiang,\nY.-G., and Qiu, X.\nMoss: An open conversational\nlarge language model. Machine Intelligence Research,\n2024b. ISSN 2731-5398. URL https://github.\ncom/OpenMOSS/MOSS.\nTan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee,\nA., Karami, M., Li, J., Cheng, L., and Liu, H. Large\nlanguage models for data annotation and synthesis: A sur-\nvey. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 930–957,\n2024.\nTeam, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L.,\nGulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S.,\net al. Gemini 1.5: Unlocking multimodal understand-\ning across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nThakur, A. S., Choudhary, K., Ramayapally, V. S.,\nVaidyanathan, S., and Hupkes, D. Judging the judges:\nEvaluating alignment and vulnerabilities in llms-as-\njudges. arXiv preprint arXiv:2406.12624, 2024.\n11\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nTong, Y., Li, D., Wang, S., Wang, Y., Teng, F., and Shang,\nJ. Can llms learn from previous mistakes? investigat-\ning llms’ errors to boost for reasoning. arXiv preprint\narXiv:2403.20046, 2024.\nWang, S., Tong, Y., Zhang, H., Li, D., Zhang, X., and Chen,\nT. Bpo: Towards balanced preference optimization be-\ntween knowledge breadth and depth in alignment. arXiv\npreprint arXiv:2411.10914, 2024.\nWhite, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain,\nS., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S.,\net al. Livebench: A challenging, contamination-free llm\nbenchmark. arXiv preprint arXiv:2406.19314, 2024.\nWu, S., Huang, Y., Gao, C., Chen, D., Zhang, Q., Wan, Y.,\nZhou, T., Zhang, X., Gao, J., Xiao, C., et al. Unigen: A\nunified framework for textual dataset generation using\nlarge language models. arXiv preprint arXiv:2406.18966,\n2024.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115, 2024.\nYang, S., Sun, R., and Wan, X. A new dataset and empirical\nstudy for sentence simplification in chinese. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n8306–8321, 2023.\nYao, F., Zhuang, Y., Sun, Z., Xu, S., Kumar, A., and Shang,\nJ. Data contamination can cross language barriers. arXiv\npreprint arXiv:2406.13236, 2024.\nYe, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N.,\nGao, T., Geyer, W., Huang, C., Chen, P.-Y., et al. Justice\nor prejudice? quantifying biases in llm-as-a-judge. arXiv\npreprint arXiv:2410.02736, 2024.\nZhang, H., Li, D., Li, Y., Shang, C., Shi, C., and Jiang, Y.\nAssisting language learners: Automated trans-lingual def-\ninition generation via contrastive prompt learning. arXiv\npreprint arXiv:2306.06058, 2023.\nZhang, H., Shang, C., Wang, S., Zhang, D., Yao, F., Sun,\nR., Yu, Y., Yang, Y., and Wei, F. Shifcon: Enhancing\nnon-dominant language capabilities with a shift-based\ncontrastive framework. arXiv preprint arXiv:2410.19453,\n2024a.\nZhang, H., Wu, Y., Li, D., Yang, Z., Zhao, R., Jiang, Y., and\nTan, F. Balancing speciality and versatility: a coarse to\nfine framework for supervised fine-tuning large language\nmodel. arXiv preprint arXiv:2404.10306, 2024b.\nZhang, Q., Gao, C., Chen, D., Huang, Y., Huang, Y.,\nSun, Z., Zhang, S., Li, W., Fu, Z., Wan, Y., and Sun,\nL. LLM-as-a-coauthor: Can mixed human-written and\nmachine-generated text be detected? In Duh, K., Gomez,\nH., and Bethard, S. (eds.), Findings of the Association\nfor Computational Linguistics: NAACL 2024, pp. 409–\n436, Mexico City, Mexico, June 2024c. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-naacl.29. URL https://aclanthology.\norg/2024.findings-naacl.29/.\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi,\nY. Bertscore: Evaluating text generation with bert. In\nInternational Conference on Learning Representations,\n2020.\nZhang, X., Peng, B., Tian, Y., Zhou, J., Jin, L., Song,\nL., Mi, H., and Meng, H.\nSelf-alignment for fac-\ntuality:\nMitigating hallucinations in LLMs via self-\nevaluation. In Ku, L.-W., Martins, A., and Srikumar,\nV. (eds.), Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pp. 1946–1965, Bangkok, Thailand,\nAugust 2024d. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.acl-long.107. URL https:\n//aclanthology.org/2024.acl-long.107/.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nZheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z.,\nand Ma, Y. Llamafactory: Unified efficient fine-tuning of\n100+ language models. arXiv preprint arXiv:2403.13372,\n2024.\nZhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P.,\nZhu, C., Ji, H., and Han, J. Towards a unified multi-\ndimensional evaluator for text generation.\nIn Gold-\nberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npp. 2023–2038. Association for Computational Linguis-\ntics, 2022.\ndoi: 10.18653/V1/2022.EMNLP-MAIN.\n131.\nURL https://doi.org/10.18653/v1/\n2022.emnlp-main.131.\nZhong, M., Zhang, A., Wang, X., Hou, R., Xiong, W., Zhu,\nC., Chen, Z., Tan, L., Bi, C., Lewis, M., et al. Law of the\nweakest link: Cross capabilities of large language models.\narXiv preprint arXiv:2409.19951, 2024.\n12\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\nZhuge, M., Zhao, C., Ashley, D., Wang, W., Khizbullin, D.,\nXiong, Y., Liu, Z., Chang, E., Krishnamoorthi, R., Tian,\nY., et al. Agent-as-a-judge: Evaluate agents with agents.\narXiv preprint arXiv:2410.10934, 2024.\n13\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nA. Preliminary Study of Preference Leakage in Real World\nIn our preliminary study, we investigate whether preference leakage is a real-world issue in mainstream leaderboards and\nbenchmarks. To this end, we examine two widely used LLM-as-a-judge leaderboards (AlpacaEval 2.0 and Arena-Hard) and\na well-known benchmark (MTBench). All three rely on GPT-4 as the judge model and report pairwise judgment results for\nvarious LLMs. Our analysis reveals that several candidate models distilled from GPT-4 or other GPT-series models (e.g.,\nVicuna and Alpaca) appear across all these leaderboards and benchmarks, suggesting that preference leakage is a pervasive\nissue in these datasets. Besides, we also examine if preference leakage exists in LLM-relevant research studies and also find\na bunch of work utilizing the same or related model(s) to do distillation/ data synthesis and evaluation (Yang et al., 2023;\nLiu et al., 2024; Lee et al., 2024; Li et al., 2024d; Wang et al., 2024; Sun et al., 2024a). All of these suggest preference\nleakage to be a widespread problem in both LLM-as-a-judge datasets and LLM-relevant research.\nB. Experiment Details\nB.1. Training Details\nWe use LLaMA-Factory (Zheng et al., 2024), an efficient LLM tuning library for our experiment. The maximum sequence\nlength is set to 1024 tokens, and a cutoff length of 1024 tokens is enforced to prevent excessive tokenization. The data\npreprocessing will be done in parallel with 16 workers to speed up the preparation process. The training use a per-device\nbatch size of 2, with gradient accumulation over 2 steps to simulate a larger batch size for SFT and a per-device batch size of\n1, with gradient accumulation over 4 steps to simulate a larger batch size for DPO. The learning rate is set to 1.0e-5 and each\nmodel will be trained for 3 epochs. A cosine learning rate scheduler is used with a warmup ratio of 0.1 to gradually increase\nthe learning rate during the initial steps. All of the experiments use BF16 precision to speed up training while maintaining\nnumerical stability. All the experiments are conducted in an 8 Nvidia A100 GPU cluster with CUDA version 11.8.\nJudge Model\nMistral-GPT-4o vs Mistral-Gemini-1.5\nMistral-GPT-4o Wins\nMistral-Gemini-1.5 Wins\nGPT-4o\n55.1%\n44.9%\nGemini-1.5\n36.8%\n63.2%\nPreference Leakage Score\n18.4%\nTable 5. A case on AlpacaEval 2.0 with the model pair Mistral-GPT-4o vs Mistral-Gemini-1.5 to demonstrate how the preference leakage\nscore is calculated.\nB.2. Detailed Explanation for Preference Leakage Score\nWe present a case in Table B.1 to show how we calculate the preference leakage score for the Mistral-GPT-4o vs Mistral-\nGemini-1.5 pair on AlpacaEval 2.0. Based on the definition of preference leakage score, we first calculate:\nAVG(Mistral-GPT-4o, Mistral-Gemini-1.5) = 55.1 + 36.8\n2\n= 45.95%\n(9)\nAVG(Mistral-Gemini-1.5, Mistral-GPT-4o) = 63.2 + 44.9\n2\n= 54.05%\n(10)\nAfter that, we calculate the preference leakage score:\nPLS(Mistral-GPT-4o, Mistral-Gemini-1.5) =\n\x00 55.1−45.95\n45.95\n\x01\n+\n\x00 63.2−54.05\n54.05\n\x01\n2\n= 18.4%\n(11)\n.\nB.3. Manual Annotation Details\nWe randomly sample 100 questions from AlpacaEval 2.0 and ask three well-trained annotators to conduct pairwise\ncomparisons of the responses from each model pair for these questions. For annotation efficiency, we also develop an\nannotation tool that involves the function of uploading multiple model responses, jumping to specific problems, and\n14\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ndownloading annotation results (Figure 8). After annotation, we adopt the majority voting to get the final label for each\nresponse pair. We also calculate the average agreement of three annotators and find it to be 78.6, indicating a relatively\nconsistent annotation result.\nC. Learning Method Analysis Details\nThe table below presents the prompt we use to generate synthetic pairwise feedback from each model.\nPairwise Feedback Prompt\nPlease act as an impartial judge and evaluate the quality of the responses provided\nby two AI assistants to the user question displayed below.\nYour evaluation should\nconsider correctness and helpfulness.\nYou will be given assistant A’s answer,\nand assistant B’s answer.\nYour job is to evaluate which assistant’s answer is\nbetter.\nYou should independently solve the user question step-by-step first.\nThen compare both assistants’ answers with your answer.\nIdentify and correct\nany mistakes.\nAvoid any position biases and ensure that the order in which the\nresponses were presented does not influence your decision.\nDo not allow the length\nof the responses to influence your evaluation.\nDo not favor certain names of the\nassistants.\nBe as objective as possible.\nAfter providing your explanation, output\nyour final verdict by strictly following this format:\n"[[A]]" if assistant A is\nbetter, "[[B]]" if assistant B is better.\n## Instruction:\n[The Start of Assistant A’s Answer]\n[RESPONSE A]\n[The End of Assistant A’s Answer]\n[The Start of Assistant B’s Answer]\n[RESPONSE B]\n[The End of Assistant B’s Answer]\nPlease output the generated content in a json format, for example:\n{ "reason":\n//\nstring, reasons behind the chosen preferred answer "prefered answer":\n// string,\nthe prefered answer you selected, [[A]] or [[B]] }\nFormatted the abovementioned schema and produce the reason and preferred answer:\nD. Recogniton Analysis Details\nThe table below presents the prompt we use for the recognition analysis.\nRecognition Prompt\nGiven an instruction and a response, your task is to judge whether this response\nis generated by a model that is trained on a synthetic dataset you produced (your\nstudent model).\n## Instruction:\n[INSTRUCTION]\n## Response:\n[Response]\nPlease output the generated content in a json format, for example:\n"reason":\n//\nstring, reasons behind the judgment "judgment":\n// string, whether the answer is\ngenerated by your student model, choose from yes or no\nFormatted the abovementioned schema and produce the reason and judgment:\nFor response classification, we split all the response from three student models into training (80%) and testing (20%) subsets.\n15\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFigure 8. The annotation tool we develop for annotation efficiency.\nThen, we finetune a BERT-base-uncased model in the training set. The model is trained for 3 epochs with a learning rate of\n2e-5, a batch size of 16 for both training and evaluation, and a weight decay of 0.01, with evaluations conducted at the end\nof each epoch.\nE. Category Analysis Details\nThe tables below present the prompt we use for question type and judgment dimension cateogory analysis.\nQuestion Type Categorization Prompt\nGiven a question, please categorize it to one of the following categories:\n1.\nComputer Science & Programming\n2.\nMathematics & Statistics\n3.\nScience & Engineering\n4.\nBusiness & Finance\n5.\nWriting & Communication\n6.\nSocial & Daily Life\n7.\nOthers\n## Question:\n[QUESTION]\nPlease output the generated content in a json format, for example:\n{ "question\ncategory":\n// string, specific category name, such as "Computer Science &\nProgramming" }\nFormatted the abovementioned schema and categorize the given question:\n16\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nJudgment Dimension Categorization Prompt\nGiven a pairwise comparison judgment made by an AI, please categorize each\nconsidered aspect in the rationale to one of the following categories:\n{\n"Factuality":\n"Whether the information provided in the response is accurate, based\non reliable facts and data.",\n"User Satisfaction":\n"Whether the response meets the user’s question and needs, and\nprovides a comprehensive and appropriate answer to the question.",\n"Logical Coherence":\n"Whether the response maintains overall consistency and\nlogical coherence between different sections, avoiding self-contradiction.",\n"Richness":\n"Whether the response includes rich info, depth, context, diversity,\ndetailed explanations and examples to meet user needs and provide a comprehensive\nunderstanding.",\n"Creativity":\n"Whether the response is innovative or unique, providing novel\ninsights or solutions.",\n"Fairness and Responsibility":\n"Whether the advice or information provided in the\nresponse is feasible, carries acertain degree of responsibility, and considers\npotential risks and consequences.",\n"Completeness":\n"Whether the response provides sufficient information and details\nto meet the user’s needs, and whether it avoids omitting important aspects.",\n"Clarity":\n"Whether the response is clear and understandable, and whether it uses\nconcise language and structure so that the user can easily understand it.",\n"Others":\n"Other aspects which is not listed above."\n}\n## Judgment:\n[JUDGMENT]\nPlease output the generated content in a json format, for example:\n{ "Factuality":\n// list, all aspects that belong to this category, such as ["correctness",\n"mistakes"] ...\n}\nFormatted the abovementioned schema and categorize aspects in the judgment:\n17')]}
2025-02-06 00:40:24,297 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:40:53,237 - INFO - Total execution time: 28.30 seconds (0.47 minutes)
2025-02-06 00:40:53,246 - INFO - Papers: {'2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=124, thumbnail=None, content='Since the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15'),
                Paper(arxiv_id='2502.01237', authors=['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], published_at=datetime.datetime(2025, 2, 4, 3, 10, 49, 348000, tzinfo=datetime.timezone.utc), title='The Differences Between Direct Alignment Algorithms are a Blur', summary='Direct Alignment Algorithms (DAAs) simplify language model alignment by\nreplacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement\nLearning from Human Feedback (RLHF) with direct policy optimization. DAAs can\nbe classified by their ranking losses (pairwise vs. pointwise), by the rewards\nused in those losses (e.g., likelihood ratios of policy and reference policy,\nor odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that one-stage methods underperform\ntwo-stage methods. To address this, we incorporate an explicit SFT phase and\nintroduce the beta parameter, controlling the strength of preference\noptimization, into single-stage ORPO and ASFT. These modifications improve\ntheir performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT),\nmatching two-stage methods like DPO. Further analysis reveals that the key\nfactor is whether the approach uses pairwise or pointwise objectives, rather\nthan the specific implicit reward or loss function. These results highlight the\nimportance of careful evaluation to avoid premature claims of performance gains\nor overall superiority in alignment algorithms.', upvotes=100, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) demonstrate strong text\ngeneration capabilities, yet aligning them with human val-\nues remains challenging due to underspecified objectives,\nlimited training signals, and the complexity of human in-\ntent (Ouyang et al., 2022; Stiennon et al., 2020). Tradi-\ntional alignment pipelines typically involve Supervised Fine-\nTuning (SFT), reward modeling, and reinforcement learning\nto shape model outputs.\nRecently, Direct Alignment Algorithms (DAAs) have\n1T-Tech.\nCorrespondence\nto:\nBoris\nShaposhnikov\n<b.shaposhnikov@tbank.ru>.\nemerged as an alternative, integrating human preferences\ninto policy optimization without explicit reward modeling\nor reinforcement learning (Rafailov et al., 2023; Hong et al.,\n2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024;\nXiao et al., 2024; D’Oosterlinck et al., 2024; Wang et al.,\n2024). These methods differ in theoretical design (pairwise\nvs. pointwise), implementation details (e.g., reference pol-\nicy vs. odds ratio), and whether an SFT phase is required\n(one-stage vs. two-stage). This diversity raises key ques-\ntions about their relationships, comparative advantages, and\nthe role of SFT.\nIn this paper, we show that one-stage methods (e.g., ORPO,\nASFT) can incorporate an explicit SFT phase, improving\nperformance. We introduce a scaling parameter β that uni-\nfies their formulation with other DAAs, revealing shared\noptimization dynamics between methods using either an\nodds ratio or a reference-based reward. Through theoretical\nand empirical analysis, we systematically compare DAAs,\nemphasizing pairwise vs. pointwise preference optimiza-\ntion. We also show that, while SFT is beneficial, using the\nfull dataset is not always necessary, which reduces com-\nputational costs. To structure our analysis, we address the\nfollowing research questions:\nRQ1: Does an explicit SFT stage improve the alignment\nquality of ORPO and ASFT?\nRQ2: Does the tempering factor enhance the alignment\nquality of ASFT and ORPO?\nRQ3: What factors of DAAs affect alignment quality?\nRQ4: How does the final alignment quality depend on the\namount of data used in the SFT stage?\nBy answering these questions, we clarify key trade-offs in\nalignment strategies and provide guidance for optimizing\nLLM training pipelines.\n2. Preliminaries\n2.1. Modeling Sequences\nGiven a sequence y of length |y|, the log-probability can be\nwritten as log p(y) = P|y|\ni=1 log p(yi | y<i), which may also\nbe conditioned on another sequence x. In practice, optimiz-\ning normalized log-probability\n1\n|y| log p(y) = log\n\x00p(y)\n1\n|y| \x01\n1\narXiv:2502.01237v1  [cs.LG]  3 Feb 2025\n\nThe Differences Between Direct Alignment Algorithms are a Blur\noften improves numerical stability and leads to better train-\ning. However, once normalized, the resulting quantity is\nno longer a strict probability measure. Throughout this pa-\nper, whenever we write p(y), we refer to this normalized\nversion p(y)\n1\n|y| . Whenever a method does not apply this\nnormalization, we indicate it explicitly.\nWelleck et al. (2019) introduced a log-unlikelihood term\nthat reduces the probability of certain undesirable tokens:\nlog\n\x001 −p(c | y<i)\n\x01\nfor c ∈C. It can be extended to an\nentire sequence as log\n\x001 −p(y)\n\x01\n.\n2.2. Reinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF)\n(Ouyang et al., 2022; Stiennon et al., 2020) is a prominent\napproach to aligning language models. It generally has three\nstages:\n• Supervised Fine-Tuning (SFT). During the SFT stage,\nthe model πθ is trained to follow instructions by max-\nimizing the probability of correct output y given in-\nput x. For a single training pair (x, y), we define the\nper-sample SFT loss as LSFT(πθ, x, y) = −log πθ(y |\nx). During fine-tuning, we minimize the expectation\nof this per-sample loss over the training dataset D:\nE(x,y) ∼D\nh\nLSFT(πθ, x, y)\ni\n.\n• Reward Modeling (RM). A reward model rψ(x, y) pro-\nduces a satisfaction score. It is trained on preference\npairs using the Bradley-Terry model (Bradley & Terry,\n1952): LRM(rψ) = −E(x,yw,yl)∼D\n\x02\nlog σ\n\x00rψ(x, yw) −\nrψ(x, yl)\n\x01\x03\n, where yw is the preferred response and yl is\nthe less preferred one.\n• Reward\nMaximization.\nThe\nobjective\nis\nto\ngenerate\nresponses\nthat\nmaximize\nthe\nlearned\nreward,\nwith\na\nKL\npenalty\nto\nprevent\nreward\nhacking:\nmaxπθ Ex∼D, y∼πθ(y|x)\n\x02\nrϕ(x, y)\n\x03\n−\nβ DKL\n\x02\nπθ(x, y) ∥πref(x, y)\n\x03\n. Reinforcement learning\n(RL) algorithms are commonly used to optimize this\nobjective (Schulman et al., 2017; Ouyang et al., 2022).\n2.3. Direct Alignment Algorithms\nDirect alignment algorithms replace the reward modeling\nand RL stages (but keep the SFT phase) with a single align-\nment step. Various preference-optimization loss functions\nhave been proposed, employing these core components:\n• rref\nθ (y, x) = log\n\x00 πθ(y|x)\nπref(y|x)\n\x01\nfrom DPO (Rafailov et al.,\n2023), which acts as an implicit reward β rref\nθ . No length\nnormalization is used.\n• rodds\nθ\n(y, x) = log\n\x00πθ(y|x)\n1−πθ(y|x)\n\x01\nproposed in ORPO (Hong\net al., 2024), representing the odds of generating y versus\nnot generating it.\nSeveral Direct Alignment Algorithms use these notations.\nInformation on sequence probability normalization for these\nmethods is presented in Appendix A.1.\n• Direct Preference Optimization (DPO) (Rafailov\net al., 2023):\nLDPO\n=\n−log σ\n\x00β rref\nθ (yw, x) −\nβ rref\nθ (yl, x)\n\x01\n.This method does not normalize probabili-\nties by length.2\n• Identity Preference Optimization (IPO) (Azar et al.,\n2023): LIPO =\n\x00rref\nθ (yw, x) −rref\nθ (yl, x) −\n1\n2β\n\x012.\n• Simple Preference Optimization (SimPO) (Meng\net al., 2024): LSimPO = −log σ\n\x00β log πθ(yw, x) −\nβ log πθ(yl, x) −γ\n\x01\n.\n• Noise\nContrastive\nAlignment\n(NCA)\n(Chen\net al., 2024):\nLNCA\n=\n−log σ\n\x00β rref\nθ (yw, x)\n\x01\n−\n0.5 log σ\n\x00−β rref\nθ (yw, x)\n\x01\n−0.5 log σ\n\x00−β rref\nθ (yl, x)\n\x01\n.\n• Calibrated\nDirect\nPreference\nOptimization\n(Cal-DPO) (Xiao et al., 2024):\nLCal−DPO\n=\n−log σ\n\x00rref\nθ (yw, x) −rref\nθ (yl, x)\n\x01\n+\n\x00rref\nθ (yw, x) −\n1\n2β\n\x012 +\n\x00rref\nθ (yl, x) +\n1\n2β\n\x012.\n• Anchored Preference Optimization Zero (APO-\nZero) (D’Oosterlinck et al., 2024):\nLAPO−Zero\n=\n−σ\n\x00β rref\nθ (yw, x)\n\x01\n+ σ\n\x00β rref\nθ (yl, x)\n\x01\n.\n2.4. Single-Stage Alignment Methods\nSingle-stage alignment (as a subset of DAA methods)\nmerges SFT and direct alignment in one step by adding their\nlosses: LSingle(πθ) = −E(x,yw,yl)∼D\n\x02\nLSFT(πθ, x, yw) +\nλ LAlign(πθ, x, yw, yl)\n\x03\n, where λ is a hyperparameter, and\nno reference policy πref is required.\nIn this paper, we focus on:\n• Odds Ratio Preference Optimization (ORPO)\n(Hong et al., 2024): LORPO = −log πθ(yw|x) −\nλ log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLORPOAlign\n.\n• Aligned Supervised Fine-Tuning (ASFT) (Wang\net al., 2024):\nLASFT\n=\n−log πθ(yw|x) −\nλ\n\x10\nlog σ\n\x00rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLASFTAlign\n\x11\n.\n2Unless otherwise noted, the expectation over (x, yw, yl) ∼D\nis taken.\n2\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n3. Method\nMany DAAs have been proposed, raising questions about\ntheir differences and significance. They can be categorized\nin various ways. For example, one classification separates\nsingle-stage methods, which perform alignment directly\nafter obtaining a base model (ASFT and ORPO), from two-\nstage methods (which perform SFT before alignment), as\nin DPO, IPO, SimPO, etc. Under this scheme, ASFT and\nORPO are single-stage methods.\nAnother classification considers whether rref or rodds is\nused as an implicit reward. ASFT and ORPO also differ\nfrom other losses by using an odds ratio, whereas other\nmethods in Section 2 use normalized policy probabilities.1\nDAAs can also be distinguished by whether their loss func-\ntion is optimized for pairwise or pointwise preferences.\nDPO, for instance, increases the policy’s probability of\nchoosing preferred sequences relative to rejected ones. In\ncontrast, ASFT simply increases or decreases probabilities\nfor chosen or rejected sequences without comparing them\ndirectly.\n3.1. Generalizing ASFT and ORPO\nDespite these classifications, it can still be difficult to pin-\npoint the essential differences among DAAs, especially\nwhen design choices limit generalization. ASFT and ORPO,\nfor example, lack a parameter β, probably because they\nwere conceived as single-stage methods, making the dis-\ntance from a reference policy unnecessary. It might seem\nodd to introduce such a parameter in single-stage methods,\nbut we will show that for both ASFT and ORPO, the single-\nstage design and the absence of β are not strictly required.\n3.1.1. ORPO AND ASFT CAN OPERATE WITHOUT THE\nSFT LOSS TERM AND AS TWO-STAGE METHODS.\nWe begin by inspecting the ASFT objective and demonstrate\nthat it combines both likelihood and unlikelihood terms:\nTheorem 3.1. LASFT is equivalent to the Binary Cross-\nEntropy (BCE) loss, encapsulating both likelihood and un-\nlikelihood components:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nThe proof of Theorem 3.1 is provided in Appendix B. Conse-\nquently,\nLASFTAlign = −\n\x10\nlog πθ(yw|x) + log\n\x001 −πθ(yl|x)\n\x01\x11\n.\nNext, we derive a direct relationship between LORPO and\n1SimPO does not explicitly use a reference policy, but can be\ntreated similarly if a uniform reference policy is assumed.\nLASFT, showing that the latter provides an upper bound on\nthe former:\nTheorem 3.2. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x))\n+ πθ(yl|x)(1 −πθ(yw|x))\n\x01\n,\nwhere the additional term is symmetric in yw and yl.\nThe proof of Theorem 3.2 is provided in Appendix C. As for\nLASFTAlign, the alignment term is then\nLORPOAlign = −log πθ(yw|x) −log(1 −πθ(yl|x))\n+ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nCorollary 3.3. LORPO ≤LASFT and LORPOAlign ≤\nLASFTAlign.\nThis follows from the fact that the additional term in LORPO\nis non-positive when πθ(yw|x) and πθ(yl|x) lie in [0, 1], and\nπθ(yw|x) + πθ(yl|x) ≤1.\nThese findings yield two main observations:\n• LASFT provides an upper bound on LORPO. Minimiz-\ning the former also minimizes the latter.\n• LASFT can be viewed as a minimal form of a DAA\nloss, reflecting the structure of BCE.\nAn essential insight from these formulations is that the SFT\nterm in the ASFT and ORPO losses is already included in\nthe full loss. We hypothesize that this feature may allow us\nto omit the SFT term in the complete loss, first performing\nan SFT phase and then using only the alignment terms for\nmodel alignment. From this perspective, one can experi-\nment with these methods in both single-stage and two-stage\nconfigurations to see which approach is more effective.\n3.1.2. TEMPERING ASFT AND ORPO\nWe now consider the original single-stage methods from Sec-\ntion 2.4 and examine how the alignment terms LORPOAlign\nand LASFTAlign compare. These terms optimize preferences\nand, depending on the coefficient λ, can dominate or have a\nsmaller impact on the final loss.\nLASFTAlign and LORPOAlign strongly resemble the DAA\nlosses discussed in Section 2.3. The single-stage analogue\nof rref\nθ\nis rodds\nθ\n. Inspired by this analogy, we introduce a\ncoefficient β to scale rodds\nθ\n:\nLβ\nASFTAlign\n= −log σ(βrodds\nθ\n(yw, x)) −log σ(−βrodds\nθ\n(yl, x)),\nLβ\nORPOAlign\n= −log σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x)).\n3\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nBoth Lβ\nASFT and Lβ\nORPO generalize their vanilla counter-\nparts (recovering them when β = 1). As in DPO, β can\nbe viewed as a temperature or scaling parameter that regu-\nlates the intensity of the preference for “good” odds. This\nbecomes clearer when looking at the gradients:\n∇θLβ\nASFTAlign = −β\nh\nσ(βrodds\nθ\n(yl, x))∇θrodds\nθ\n(yl, x)\n+\n\x001 −σ(βrodds\nθ\n(yw, x))\n\x01\n∇θrodds\nθ\n(yw, x)\ni\n,\n∇θLβ\nORPOAlign = −β\nh\x00∇θrodds\nθ\n(yw, x) −∇θrodds\nθ\n(yl, x)\n\x01\n×\n\x10\n1 −σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x))\n\x11i\n,\nwhere ∇θrodds\nθ\n(y, x) =\n∇θ log πθ(y|x)\n1−πθ(y|x) .\nWhen β →0,\nσ(β · · · ) ≈1\n2, both methods aggressively improve the odds\nratio (increasing for yw and decreasing for yl). As β in-\ncreases, the updates become bounded by the factor σ(β · · · )\n(similar to a reward threshold in DPO). Hence, once the\nmodel improves, further updates are limited, either individ-\nually for Lβ\nASFTAlign or by pairwise ranking in Lβ\nORPOAlign.\nThis alignment with other DAAs allows for a direct com-\nparison of all methods in different setups, clarifying which\naspects are most critical for successful performance.\n3.2. On the Difference Between Direct Alignment\nAlgorithms\nDifferent methods can be grouped by the type of ”reward”\nfunction used in their loss. In general terms, Lβ\nASFTAlign\nand Lβ\nORPOAlign employ an odds ratio, while DPO, IPO,\nSimPO, NCA, Cal-DPO, and APO-Zero use a ratio between\nthe probability of the policy and that of a reference policy.\nThe following theorems make this classification clearer:\nTheorem 3.4. The gradient of Lβ\nASFTAlign becomes\ncollinear with the gradient of LORPOAlign as β →0. For-\nmally,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nindicating that both gradients point in the same direction.\nThe proof of Theorem 3.4 is provided in Appendix D.1.\nA related property applies to Lβ\nORPOAlign:\nTheorem 3.5. The gradient of Lβ\nORPOAlign is collinear with\nthe gradient of LORPOAlign for any β > 0. Formally,\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nβ > 0.\nThe proof of Theorem 3.5 is provided in Appendix E.1.\nFinally:\nTheorem\n3.6.\nFor\neach\nmethod\nX\n∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n,\nas\nβ →0, the gradient of LX is collinear with the gradient of\nLDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nThe proof of Theorem 3.6 is provided in Appendix F.1.\nThese theorems suggest that for sufficiently small β, these\nloss functions are split into two categories with indistin-\nguishable gradient directions. Although the magnitudes may\ndiffer and they may not be collinear for β ̸→0, one could in-\nfer that their performance should be similar when β is small.\nFrom this perspective, two main distinctions arise among\nthese methods: the use of an odds ratio (rodds\nθ\n) and the use\nof the ratio to a reference policy (rref\nθ ). Both choices might\ninfluence the final performance of these methods. Further-\nmore, it remains an open question whether odds-ratio-based\napproaches outperform reference-policy-based ones (e.g.,\nDPO), and how these distinctions compare to the contrast\nbetween pointwise and pairwise preference formulations.\nFrom traditional learning-to-rank (Liu et al., 2009) research,\npairwise methods often produce more direct and less noisy\nranking signals than pointwise techniques, which could lead\nto superior performance in practice (Burges et al., 2005; Li,\n2011; Melnikov et al., 2016). In the following sections, we\npresent experimental results that provide further insight into\nwhich aspects most strongly influence DAA training.\n4. Experimental Setup\nWe systematically compare and evaluate DAA methods us-\ning a standard training and instruction-following evaluation\nframework (Tunstall et al., 2023; Meng et al., 2024; Gorba-\ntovski et al., 2024). Our main experiments use the Llama\n3.1 8B model (AI@Meta, 2024), trained on the UltraChat\n(Ding et al., 2023) and UltraFeedback (UF) (Cui et al., 2023)\ndatasets, and evaluated on the AlpacaEval 2 (Dubois et al.,\n2024; Li et al., 2023) and ArenaHard (Li et al., 2024) bench-\nmarks. For the Reddit TL;DR (Stiennon et al., 2020) task,\nwe employ the Llama 3.2 3B model, comparing it side by\nside with the “golden” validation split (Rafailov et al., 2023;\n2024) using the prompt in Appendix I.\n4.1. Base vs SFT-Initialized Models.\nTo investigate the impact of SFT and the applicability of\none-stage loss LAlign component, we use the UF dataset for\nSFT (avoiding additional knowledge from UltraChat), and\nfor pairwise preference optimization. We carefully tuned the\nhyperparameters to optimize each method’s performance.\nFor the Base-initialized setup, we perform a grid search over\n4\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nORPO\nASFT\n0\n20\n40\n60\n80\n100\nLlama 3.2 3B TL;DR\n(GPT-4 WinRate, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nLlama 3.2 3B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0\n10\n20\n30\nLlama 3.1 8B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nFigure 1. Impact of the β Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning β (Section 3.1.2) affects\nboth ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2\nLC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each\nmethod’s best configuration at β = 1 as the baseline. See Section 5.2 for more details.\nlearning rates {6 × 10−6, 8 × 10−6, 1 × 10−5}, inspired\nby values suggested in ORPO and ASFT, and explore λ ∈\n{0.1, 0.2, 0.5, 1.0} for 1 and 2 training epochs keeping a\nsimilar budget to compare with the SFT-initialized setup.\nIn the SFT-initialized setup, we experiment with both\nLORPOAlign and LASFTAlign alone, as well as in combina-\ntion with LSFT, following the original methods. We tune\nthe learning rates {5 × 10−7, 7 × 10−7, 1 × 10−6} for one\nepoch, starting from an SFT model trained for 1 epoch at\n6 × 10−6.\n4.2. β Sensitivity.\nBuilding on the theoretical insights from Section 3.2, where\nDAA losses share indistinguishable gradient directions as\nβ →0, we evaluate each method across various β values to\nexamine quality-KL trade-offs. In classical DPO, β regu-\nlates the KL penalty from the reference policy, but setting\nβ too small can induce training instability. Therefore, we\nconduct a thorough sweep of at least six β values per DAA,\nexploring the performance limit of each method. To broaden\nour analysis, we consider three scenarios:\n1. Llama 3.2 3B TL;DR. A relatively simpler Reddit\nTL;DR summarization task, evaluated via GPT side-\nby-side comparison on 500 samples from the “golden”\nvalidation split (Rafailov et al., 2023; 2024).\n2. Llama 3.2 3B UF. The UltraChat and UF datasets\nserve as more challenging alignment settings due to\ntheir coverage of diverse and complex tasks, includ-\ning common sense reasoning, mathematical problem-\nsolving, code generation, logical reasoning, creative\nwriting, and general knowledge.\n3. Llama 3.1 8B UF. A larger, more capable model on the\nsame UltraChat and UF datasets, allowing us to assess\nhow increased model capacity influences β-sensitivity\nin these diverse tasks.\nFor the UF-based experiments, we measure model qual-\nity primarily using the AlpacaEval 2 Length-Controlled\n(LC) Win-Rate and ArenaHard (AH) WR, and then track\nKL divergence from a reference model to construct Pareto\nfronts. For the TL;DR scenario, we rely on GPT-based\npreference judgments using ‘gpt-4o-2024-08-06‘ model.\nConcretely, in each scenario we train models for differ-\nent values β, combining them with four possible learning\nrates {1 × 10−6, 7 × 10−7, 5 × 10−7, 3 × 10−7}. Further\nimplementation details, including training procedures and\ngeneration hyperparameters, are provided in Appendix A.\n4.3. SFT Quality.\nAlthough in principle single-stage methods do not require\na separate SFT phase, in practice an SFT-trained reference\nmodel often improves the final performance of two-stage\npipelines (see Section 5.1). Prior work, such as (Zhou et al.,\n2024), has shown that a small but high-quality dataset can be\nsufficient for instruction tuning. However, beyond response\nquality, it remains unclear how the amount of SFT data in-\nfluences alignment effectiveness. This raises a fundamental\nquestion: how much supervised data is actually needed to\nproduce a reference model that yields high-quality results\nafter the subsequent alignment step?\nTo investigate this, we prepared seven SFT checkpoints by\ntraining Llama 3.1 8B Base on 1%, 3%, 5%, 10%, 25%,\n50%, and 100% of the UltraChat dataset (2,079, 6,236,\n10,393, 20,786, 51,966, 103,932, and 207,865 records, re-\nspectively) using our SFT-initialized procedure. We then\napplied each alignment method – using optimal hyperparam-\neters from our β-sensitivity experiments (Appendix Table 7)\n– to these seven SFT checkpoints and the original base model.\nFinally, we evaluated all resulting aligned models on Al-\npacaEval 2 LC, analyzing their performance relative to the\nfraction of SFT data used.\n5\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nASFT\nCal-DPO\nNCA\nAPO Zero\nORPO\nSimPO\nIPO\nDPO\nSFT\n436\n5\n59\n457\n2\n41\n459\n5\n36\n463\n3\n34\n451\n3\n46\n458\n1\n41\n457\n2\n41\n456\n5\n39\n178\n24\n298\nWin\nTie\nLose\nWin / Tie / Lose Rate %\n35.6 / 4.8 / 59.6\n91.2 / 1.0 / 7.8\n91.4 / 0.4 / 8.2\n91.6 / 0.2 / 8.2\n90.2 / 0.6 / 9.2\n92.6 / 0.6 / 6.8\n91.8 / 1.0 / 7.2\n91.4 / 0.4 / 8.2\n87.2 / 1.0 / 11.8\nFigure 2. GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best\nhyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT\nachieves 87.2%, maintaining robust summarization performance. See Section 5.2 for more details.\n5. Results\n5.1. RQ1: Does an explicit SFT stage improve the\nalignment quality of ORPO and ASFT?\nAs shown in Table 1, the performance of ORPO and ASFT\nmethods improves significantly when the alignment loss\nLAlign is applied after a preceding SFT stage. In particular,\nORPO achieves results comparable to classical DPO in both\nLC Win Rate and AH WR metrics. In contrast, ASFT shows\nnotable gains in AH WR after the SFT stage, although it\nstill underperforms compared to ORPO or DPO.\nInit\nMethod\nLC% (std)\nWR% (std)\nAH% (CI)\nBase\nSFT\n6.7 (0.43)\n4.5 (0.63)\n3.5 (-0.7, 0.8)\nSFT\nORPO\n24.1 (0.84)\n17.8 (1.17)\n15.3 (-1.6, 1.8)\nSFT\nASFT\n16.4 (0.72)\n11.9 (0.99)\n10.6 (-1.2, 1.3)\nBase\nORPO\n14.8 (0.71)\n10.3 (0.95)\n8.4 (-1.3, 1.3)\nBase\nASFT\n14.5 (0.73)\n10.2 (0.94)\n7.5 (-1.1, 1.2)\nSFT\nORPO†\n13.4 (0.69)\n9.3 (0.91)\n7.7 (-0.9, 1.1)\nSFT\nASFT†\n11.4 (0.63)\n7.5 (0.83)\n7.5 (-1.1, 1.1)\nSFT\nDPO\n23.4 (0.85)\n20.0 (1.18)\n17.5 (-1.8, 1.8)\nTable 1. Base and SFT-initialized alignment methods on the\nLlama 3.1 8B model with the UF dataset. SFT-initialized meth-\nods demonstrate better performance compared to their traditional\nformulations without LSFT. Results marked with † correspond to\ntraining with LSFT, using the best hyperparameters: lr = 1×10−6\nfor ORPO and lr = 7 × 10−7 for ASFT. For other setups, the\nbest hyperparameters are: lr = 5 × 10−7 for standard SFT\nORPO/ASFT, and lr = 1 × 10−5/6 × 10−6 for Base ORPO/ASFT.\nFor single-stage methods, the use of λ = 1 provides the best\nresults within the explored grid of λ ∈{0.1, 0.2, 0.5, 1.0},\nespecially after two epochs of training. However, combining\nLSFT and LAlign in a single-stage setup leads to suboptimal\nresults compared to explicitly separating these phases, even\nwhen starting from an SFT-trained model. Incorporating an\nexplicit SFT stage improves overall performance for ORPO\nand ASFT methods. Therefore, all further experiments focus\non applying the LAlign components of ORPO and ASFT on\ntop of an SFT-trained model.\n5.2. RQ2: Does the tempering factor enhance the\nalignment quality of ASFT and ORPO?\nFigure 1 illustrates that introducing the β parameter (as\ndescribed in Section 3.1.2) improves the performance of\nboth ASFT and ORPO LAlign in our tested scenarios. For a\nfair comparison, we used the best-performing learning rate\nfor each baseline — LASFTAlign and LORPOAlign — while\nfixing β = 1. In the Llama 3.2 3B TL;DR experiment,\nthese adjustments led to an improvement of +7.0 for ORPO\nand +43.4 for ASFT in GPT-4 WR. In the Llama 3.1 8B\nUF setup, tuning β provided additional gains of +3.46 for\nORPO and +8.27 for ASFT on the AlpacaEval 2 LC WR.\n5.3. RQ3: What factors of DAAs affect alignment\nquality?\nBased on Section 3, we perform a comprehensive evaluation\nof alignment losses, including DPO, IPO, SimPO, NCA,\nCal-DPO, and APO-Zero, as well as enhanced Lβ\nASFTAlign\nand Lβ\nORPOAlign with the introduced parameter β. Unlike\nclassical methods where β typically regulates KL diver-\ngence against a reference policy πref, β in Lβ\nASFTAlign and\nLβ\nORPOAlign directly modulates the strength of preference\noptimization. To explore the upper limits of each method’s\nperformance, we performed an extensive hyperparameter\nsearch, analyzing both alignment quality and KL divergence.\nFull implementation details, including training setups and\nevaluation criteria, are provided in Appendix A.\nLlama 3.2 3B TL;DR: Figure 2 presents a comparison of\nall methods on the Reddit TL;DR validation subset, using\ntheir best hyperparameters. Most methods achieve a GPT-4\nWin Rate exceeding 90%, indicating robust summarization\nperformance on this relatively straightforward task. ASFT\nis slightly lower at 87.2% Win Rate, but still demonstrates\nstrong overall results.\nLlama 3.2 3B UF and Llama 3.1 8B UF: Table 2 summa-\n6\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nAlpacaEval 2\nArenaHard\nAlpacaEval 2\nArenaHard\nLC% (std)\nWR% (std)\nWR% (CI)\nLC% (std)\nWR% (std)\nWR% (CI)\nSFT\n5.02 (0.34)\n3.21 (0.55)\n1.4 (-0.4, 0.4)\n10.27 (0.54)\n5.44 (0.70)\n2.6 (-0.5, 0.6)\nDPO\n11.43 (0.58)\n11.79 (0.99)\n6.8 (-1.0, 0.9)\n26.82 (0.77)\n23.69 (1.25)\n19.0 (-1.9, 1.8)\nIPO\n11.24 (0.60)\n11.67 (1.01)\n6.8 (-1.0, 1.1)\n28.18 (0.83)\n24.43 (1.26)\n19.1 (-1.6, 1.5)\nSimPO\n10.56 (0.44)\n11.94 (0.95)\n6.4 (-1.0, 1.1)\n27.65 (0.77)\n25.62 (1.29)\n21.5 (-1.9, 1.9)\nORPO\n10.67 (0.50)\n12.23 (0.97)\n6.6 (-1.0, 1.1)\n28.25 (0.71)\n28.59 (1.33)\n20.9 (-2.0, 2.0)\nAPO Zero\n10.36 (0.53)\n11.22 (0.98)\n6.0 (-1.0, 0.9)\n23.15 (0.76)\n19.03 (1.18)\n17.3 (-1.8, 1.8)\nNCA\n10.33 (0.53)\n11.02 (0.97)\n5.1 (-0.7, 0.8)\n23.21 (0.80)\n18.67 (1.17)\n15.1 (-1.5, 1.6)\nCal-DPO\n10.62 (0.57)\n10.15 (0.94)\n4.8 (-0.9, 0.9)\n23.19 (0.82)\n18.85 (1.18)\n15.2 (-1.5, 1.6)\nASFT\n10.63 (0.55)\n9.21 (0.88)\n5.1 (-0.9, 0.9)\n20.82 (0.79)\n16.34 (1.13)\n13.5 (-1.6, 1.5)\nTable 2. AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF. The SFT model was trained on the UltraChat\ndataset. The best hyperparameters for each method were selected according to Section 4.2. Bold values indicate the best performance for\neach benchmark, while underlined values represent the second-best performance. See Section 5.3 for more details.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nKL Divergence with SFT Model\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\nFigure 3. Pareto front for alignment quality and KL divergence.\nResults for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are\ngrouped into pairwise and pointwise categories, with pairwise\nachieving higher LC values while remaining within overlapping\nconfidence intervals. See Section 5.3 for more details.\nrizes the results for both Llama 3.2 3B UF and Llama 3.1\n8B UF setups. For the smaller 3B model, the methods per-\nform similarly on LC WR, with slight differences emerging\non AH. Although these differences align with the pairwise\nvs. pointwise distinction (e.g., DPO, IPO, ORPO, SimPO\nvs. APO-Zero, NCA, Cal-DPO, ASFT), no single approach\nconsistently dominates across metrics. The overlap in con-\nfidence intervals further indicates that the results for these\nmethods are statistically similar in this setup, with no clear\nseparation.\nIn contrast, the 8B model reveals a clearer performance\ndifferentiation. Pairwise methods consistently outperformed\npointwise ones on AlpacaEval 2 and ArenaHard metrics,\nwith ORPO achieving the highest overall alignment quality.\nAs illustrated in Figure 3, pairwise approaches dominated\nthe KL Pareto front for the larger model, demonstrating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy (%)\nPointwise Pairwise\n0.42\n0.44\n0.71\n0.89\n=0.290\n=0.456\nMLP Dim = 1\nMLP Dim = 3\nFigure 4. Pairwise vs. Pointwise Ranking Methods on Toy Ex-\nample. Model capacity impacts ranking accuracy, with pairwise\nmethods outperforming pointwise ones as capacity increases. This\nbehavior is consistent with results observed in Llama experiments\non the UF dataset. See Section 5.3 for more details.\ntheir ability to more effectively balance alignment quality\nand divergence. Pareto fronts for the remaining setups are\nincluded in Appendix G for completeness.\nThese observations suggest that model capacity plays a sig-\nnificant role in amplifying the advantages of pairwise rank-\ning, where LLMs act as rankers (similar to Liu et al. (2024)).\nFor smaller models, such as the 3B setup, limited capacity\nmay hinder the ability to fully exploit pairwise gradient sig-\nnals. This hypothesis is supported by additional evidence\nfrom the toy example experiment (Figure 4), where pairwise\nmethods demonstrated performance similar to pointwise\nmethods with weaker MLPs but achieved better ranking\naccuracy as the model capacity increased. Full details of the\ntoy example setup are provided in Appendix H.\n5.4. RQ4: How does the final alignment quality depend\non the amount of data used in the SFT stage?\nIn Section 5.1, we show that DAAs designed to bypass\nthe SFT phase still underperform compared to models\nthat undergo SFT and are then aligned using a similar\n7\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\n25\nAlpaca Eval 2 LC WR (%)\nDPO\nIPO\nSimPO\nORPO\nSFT\nLine Type\nFraction\nFull\n(a) Pairwise\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\nAlpaca Eval 2 LC WR (%)\nAPO Zero\nNCA\nCal-DPO\nASFT\nSFT\nLine Type\nFraction\nFull\n(b) Pointwise\nFigure 5. Impact of SFT Dataset Size on Alignment Quality. Performance of the pairwise (a) and pointwise (b) alignment methods on\nAlpacaEval 2 (LC WR metric) when the SFT policy is trained on different fractions of the UltraChat dataset. Even a small fraction of SFT\ndata (e.g., 5–10%) yields substantial gains over starting from the raw base model. See Section 5.4 for more details.\npreference-optimization loss function without the SFT term.\nAs discussed in Section 4.3, this raises the question of how\nmuch supervised data is needed to compensate for the ad-\nditional computation and achieve comparable alignment\nperformance.\nTo investigate this, we trained seven SFT models on progres-\nsively larger UltraChat subsets (1% to 100%) and applied\neach alignment algorithm to these models and the non-fine-\ntuned base model, yielding eight initializations per method.\nFigures 5(a) and 5(b) summarize the results for pairwise\nand pointwise alignment methods, respectively. As the plots\nshow, no method starting from the raw base model can\nmatch the final quality of a method trained with the entire\nSFT dataset. However, even a modest size expansion of the\nSFT dataset yields substantial improvements in alignment\nquality: for example, moving from 3% to 5% of the data\nmore than doubles the AlpacaEval 2 LC score for the final\nmodel. Crucially, using only 10% of UltraChat for SFT\nyields nearly the same quality as using the entire dataset.\nAdding an SFT phase requires more overall training, but\nit pays off significantly in the final result. Moreover, one\ndoes not need the entire supervised corpus to realize most\nof these gains; even 5–10% of the data is often enough for\nDAAs to reach most of their potential.\n6. Conclusion\nThis paper presents a comprehensive theoretical and empiri-\ncal analysis of DAAs. Theoretically, we demonstrated that\nwithin each category - odds-based (rodds) and reference-\npolicy-based (rref) – gradient directions of popular methods\nalign as β →0, revealing shared optimization dynamics\nwithin these groups. We also showed that single-stage losses\n(e.g., ASFT, ORPO) can be extended to two-stage pipelines\nwith an explicit SFT step and optional β-scaling, enabling\ngreater flexibility. Experimentally, we addressed four core\nresearch questions (RQ1–4), exploring single- vs. two-stage\ntraining, implicit rewards, objective types, and the impact\nof the SFT phase. Our key findings are:\n• Include an SFT phase.\nAn SFT stage consistently\nimproves alignment performance (RQ1), with ORPO\nachieving +9.3 LC / +6.9 AH and ASFT +1.9 LC / +3.1\nAH in the setup from Section 4.1. Even 5–10% of the\nsupervised dataset often suffices to achieve near-optimal\nresults (RQ4).\n• Pairwise methods outperform pointwise objectives.\nAlignment quality depends more on the choice between\npairwise and pointwise objectives than on the formula-\ntion of implicit reward (e.g., rodds or rref). Pairwise\nmethods generally perform better (e.g., ORPO outper-\nforming ASFT by +7.43 LC / +7.4 AH in the Llama\n3.1 8B UF setup), particularly in larger models (RQ3).\nAmong these, ORPO and SimPO also stand out as prac-\ntical options for memory-constrained scenarios, as they\ndo not rely on a reference policy.\n• Choose hyperparameters carefully. Alignment per-\nformance is highly sensitive to learning rates and the\ncoefficient β. We provide optimal configurations for dif-\nferent methods based on comprehensive grid searches\nin our experimental setups, highlighting the added gains\nfrom tuning β in odds-based methods, where it controls\nthe strength of preference optimization (RQ2).\nLimitations and Future Work. Although our study sys-\ntematically compares DAAs, it has several limitations. We\ntested a limited set of datasets (UltraChat, UltraFeedback,\nReddit TL;DR) and benchmarks (AlpacaEval 2, ArenaHard),\nwhich may affect generalizability to other domains. The re-\nliance on GPT-based evaluators can introduce biases. More-\nover, we evaluated on 3B–8B models, so the observed ad-\nvantages of pairwise over pointwise objectives could shift\nat larger scales.\n8\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nReferences\nAI@Meta.\nLlama 3 model card.\n2024.\nURL\nhttps://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nAzar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,\nD., Valko, M., and Munos, R. A general theoretical\nparadigm to understand learning from human preferences,\n2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,\nT. J., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,\nEl-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez,\nD., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,\nN., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-\nCandlish, S., Olah, C., Mann, B., and Kaplan, J. Train-\ning a helpful and harmless assistant with reinforcement\nlearning from human feedback. ArXiv, abs/2204.05862,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:248118878.\nBradley, R. A. and Terry, M. E. Rank Analysis of Inclom-\nplete Block Design: The Method of Paired Comparisons.\nBiometrika, 39(3-4):324–345, 12 1952.\nISSN 0006-\n3444. doi: 10.1093/biomet/39.3-4.324. URL https:\n//doi.org/10.1093/biomet/39.3-4.324.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,\nHamilton, N., and Hullender, G. Learning to rank using\ngradient descent. In Proceedings of the 22nd international\nconference on Machine learning, pp. 89–96, 2005.\nChen, H., He, G., Yuan, L., Cui, G., Su, H., and Zhu, J.\nNoise contrastive alignment of language models with\nexplicit rewards, 2024. URL https://arxiv.org/\nabs/2402.05369.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback, 2023.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDing, N., Chen, Y., Xu, B., Qin, Y., Hu, S., Liu, Z., Sun, M.,\nand Zhou, B. Enhancing chat language models by scaling\nhigh-quality instructional conversations. In Bouamor,\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\n2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 3029–3051, Singapore, December\n2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.183. URL https://\naclanthology.org/2023.emnlp-main.183.\nD’Oosterlinck, K., Xu, W., Develder, C., Demeester, T.,\nSingh, A., Potts, C., Kiela, D., and Mehri, S. Anchored\npreference optimization and contrastive revisions: Ad-\ndressing underspecification in alignment, 2024. URL\nhttps://arxiv.org/abs/2408.06266.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nGorbatovski, A., Shaposhnikov, B., Malakhov, A., Sur-\nnachev, N., Aksenov, Y., Maksimov, I., Balagansky, N.,\nand Gavrilov, D. Learn your reference model for real\ngood alignment. arXiv preprint arXiv:2404.09656, 2024.\nHong, J., Lee, N., and Thorne, J. Orpo: Monolithic prefer-\nence optimization without reference model, 2024. URL\nhttps://arxiv.org/abs/2403.07691.\nKingma,\nD. P. and Ba,\nJ.\nAdam:\nA method\nfor stochastic optimization.\nCoRR, abs/1412.6980,\n2014.\nURL https://api.semanticscholar.\norg/CorpusID:6628106.\nLi, H. A short introduction to learning to rank. IEICE\nTRANSACTIONS on Information and Systems, 94(10):\n1854–1862, 2011.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline, 2024.\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-\nval: An automatic evaluator of instruction-following\nmodels.\nhttps://github.com/tatsu-lab/\nalpaca_eval, 5 2023.\nLiu, T., Qin, Z., Wu, J., Shen, J., Khalman, M., Joshi,\nR., Zhao, Y., Saleh, M., Baumgartner, S., Liu, J., et al.\nLipo: Listwise preference optimization through learning-\nto-rank. arXiv preprint arXiv:2402.01878, 2024.\nLiu, T.-Y. et al. Learning to rank for information retrieval.\nFoundations and Trends® in Information Retrieval, 3(3):\n225–331, 2009.\nMelnikov, V., H¨ullermeier, E., Kaimann, D., Frick, B., and\nGupta, P. Pairwise versus pointwise ranking: A case\nstudy. Schedae Informaticae, pp. 73–83, 2016.\nMeng, Y., Xia, M., and Chen, D. Simpo: Simple preference\noptimization with a reference-free reward. arXiv preprint\narXiv:2405.14734, 2024.\n9\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P. F., Leike, J.,\nand Lowe, R. Training language models to follow instruc-\ntions with human feedback. In Koyejo, S., Mohamed, S.,\nAgarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\nAdvances in Neural Information Processing Systems, vol-\nume 35, pp. 27730–27744. Curran Associates, Inc., 2022.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023. URL https://arxiv.org/abs/\n2305.18290.\nRafailov, R., Chittepu, Y., Park, R., Sikchi, H., Hejna, J.,\nKnox, B., Finn, C., and Niekum, S. Scaling laws for\nreward model overoptimization in direct alignment algo-\nrithms. arXiv preprint arXiv:2406.02900, 2024.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\nspeed: System optimizations enable training deep learn-\ning models with over 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pp. 3505–3506,\n2020.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O.\nProximal policy optimization al-\ngorithms.\nCoRR, abs/1707.06347, 2017.\nURL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1707.html#SchulmanWDRK17.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\nR., Voss, C., Radford, A., Amodei, D., and Christiano,\nP. Learning to summarize from human feedback. In\nNeurIPS, 2020.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Ra-\nsul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier,\nC., Habib, N., et al. Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944, 2023.\nWang, R., Sun, J., Hua, S., and Fang, Q. Asft: Aligned\nsupervised fine-tuning through absolute likelihood, 2024.\nURL https://arxiv.org/abs/2409.10571.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nand Weston, J. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319, 2019.\nXiao, T., Yuan, Y., Zhu, H., Li, M., and Honavar, V. G. Cal-\ndpo: Calibrated direct preference optimization for lan-\nguage model alignment, 2024. URL https://arxiv.\norg/abs/2412.14516.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\n10\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA. Implementation Details\nA.1. Probability Normalization\nAs discussed in Section 2.1, not all DDAs incorporate length-based probability normalization by default. In this paper,\nhowever, we consistently apply such normalization wherever probabilities are involved. This choice avoids introducing extra\nnotation and reduces the cognitive load on the reader. Table 3 summarizes the methods that originally include length-based\nnormalization.\nMethod\nUse normalization\nDPO (Rafailov et al., 2023)\n✗\nIPO (Azar et al., 2023)\n✗\nSimPO (Meng et al., 2024)\n✓\nNCA (Chen et al., 2024)\n✗\nCal-DPO (Xiao et al., 2024)\n✗\nAPO-Zero (D’Oosterlinck et al., 2024)\n✗\nORPO (Hong et al., 2024)\n✓\nASFT (Wang et al., 2024)\n✓\nTable 3. Methods that include (✓) or omit (✗) length-based probability normalization in their original formulation.\nA.2. Training Details\nOur experiments were conducted using the Llama 3.2 3B and Llama 3.1 8B Base models (AI@Meta, 2024). The training\nsetup, datasets, and hyperparameters were designed to ensure reproducibility and consistency. Unless otherwise noted, the\nhyperparameters in Table 4 were used across all experiments.\nHyperparameter\nValue\nMax Tokens Length\n1024 (TL;DR setup), 4096 (UF setup)\nEpochs\n1 (or 2 when specified)\nLearning Rate (SFT)\n6.0 × 10−6\nLearning Rate (Base Init.)\n{6.0 × 10−6, 8.0 × 10−6, 1.0 × 10−5}\nLearning Rate (Alignment)\n{3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6}\nOptimizer\nAdam (Kingma & Ba, 2014)\nAdam β1\n0.9\nAdam β2\n0.95\nBatch Size\n128\nLearning Schedule\nLinear Decay\nWarm-up Ratio\n0.03\nMax Gradient Norm\n2\nMemory Optimization\nDeepSpeed (Rasley et al., 2020)\nAttention Mechanism\nFlash Attention 2 (Dao, 2023)\nTable 4. Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models.\nTraining was performed on 8 NVIDIA A100 GPUs with 80GB memory each. Depending on the number of epochs, training\nfor each configuration took between 3 to 6 hours.\nA.2.1. DATASETS.\nWe used two primary datasets:\n• Reddit TL;DR (Bai et al., 2022): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B\nmodel.\n11\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n• UltraChat (Ding et al., 2023): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B and\nLlama 3.1 8B models.\n• UltraFeedback (Cui et al., 2023): used for both SFT (in the Base vs. SFT-initialized comparison, where we selected\nchosen subset from preference pairs) and for pairwise preference optimization in all DAA methods.\nThe dataset sizes are summarized in Table 5. For Base vs. SFT-initialized setups, only UltraFeedback was used. For\nβ-sensitivity experiments, the models were first trained on UltraChat for SFT and subsequently fine-tuned on UltraFeedback.\nThe Reddit TL;DR dataset was processed to remove duplicates, retaining only uniquely preferred summaries for SFT.\nDataset\nTraining Examples\nValidation Examples\nUltraChat\n207,865\n23,110\nUltraFeedback\n61,135\n2,000\nReddit TL;DR (SFT)\n41,947\n11,941\nReddit TL;DR (Preference)\n73,396\n21,198\nTable 5. Summary of dataset sizes used for training and validation.\nA.2.2. β-SENSITIVITY EXPERIMENTS.\nWe conducted a comprehensive analysis to evaluate the sensitivity of DAA methods to β, examining its impact on the\ntrade-off between model quality and KL divergence. Each method was trained using six or more distinct β values to identify\na configuration that achieves stable and effective performance. The specific β values tested for each method are as follows:\nMethod\nβ Values Tested\nDPO\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1}\nIPO\n{0.0007, 0.001, 0.005, 0.01, 0.05, 0.1}\nSimPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0}\nORPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nASFT\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nAPO-Zero\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2}\nCal-DPO\n{0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003}\nNCA\n{0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05}\nTable 6. Range of β values tested for each DAA method on all scenarios.\nFor each β, we tested four learning rates (3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6), training on the UltraFeedback\ndataset. All runs began from an SFT-initialized model trained on UltraChat (lr = 6.0 × 10−6, 1 epoch). The best-performing\nlearning rate for each β was selected to construct Pareto fronts, balancing quality (measured via AlpacaEval 2 LC Win-Rate)\nand KL divergence.\nFor SimPO in the Llama 3.1 8B UF setup, the ratio γ\nβ = 0.5 was kept fixed as recommended by Meng et al. (2024).\nAdditionally, a single learning rate (lr = 6.0 × 10−7) was tested across all β values for this method, as the same datasets\nand model scale were used. For Llama 3.2 TL;DR and UF setups, we tested four learning rates similar to other DAAs.\nBeyond the standard β values described in Table 6, additional values were explored for specific configurations to reach\nthe extreme points of the Pareto front. For example: - {0.00001, 0.00003} for Cal-DPO in Llama 3.2 3B TL;DR and UF\nsetups, - {0.00001, 0.00003, 0.00005} for NCA in Llama 3.2 3B TL;DR, - {0.0003, 0.0005} for APO-Zero in Llama 3.2\n3B TL;DR, - {0.0003, 0.0005, 0.001, 0.003, 0.005} for ASFT in Llama 3.2 3B TL;DR.\nThe hyperparameters resulting in the best performance are presented in Table 7.\nA.3. Generation Details\nWe evaluated model performance on AlpacaEval 2 and ArenaHard for UltraFeedback setups, while for the Reddit TL;DR\nsetup, we used side-by-side comparisons with GPT-4o on a curated golden validation subset of 500 samples. Additionally,\n12\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B TL;DR\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nLearning Rate\nβ\nLearning Rate\nβ\nLearning Rate\nβ\nDPO\n7.0 × 10−7\n0.05\n1.0 × 10−6\n0.01\n1.0 × 10−6\n0.003\nIPO\n1.0 × 10−6\n0.005\n7.0 × 10−7\n0.001\n1.0 × 10−6\n0.001\nSimPO\n3.0 × 10−7\n0.5\n7.0 × 10−7\n1.0\n6.0 × 10−7\n1.0\nORPO\n3.0 × 10−7\n0.5\n5.0 × 10−7\n0.2\n5.0 × 10−7\n0.5\nASFT\n3.0 × 10−7\n0.001\n1.0 × 10−6\n0.2\n7.0 × 10−7\n0.1\nAPO Zero\n3.0 × 10−7\n0.001\n3.0 × 10−7\n0.005\n3.0 × 10−7\n0.003\nNCA\n3.0 × 10−7\n0.0001\n3.0 × 10−7\n0.0005\n3.0 × 10−7\n0.0003\nCal-DPO\n3.0 × 10−7\n0.00003\n5.0 × 10−7\n0.0003\n3.0 × 10−7\n0.0003\nTable 7. Best hyperparameters for each DAA method across setups.\nKL divergence was measured on the validation subset for all setups using the generation hyperparameters listed in Table 8.\nFor ArenaHard, the temperature was set to 0 to adhere to the original benchmark configuration.\nHyperparameter\nValue\nTemperature\n0.9\nTop-k\n40\nTop-p\n1.0\nMax New Tokens\n256 (TL;DR setup), 4096 (UF setup)\nTable 8. Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models.\nB. Equivalence of ASFT Loss and Binary Cross-Entropy Loss\nLemma B.1.\nlog σ(rodds\nθ\n(y, x)) = log πθ(y|x)\nProof.\nlog σ(rodds\nθ\n(y, x)) = log σ(log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(1−πθ(y|x))−log(πθ(y|x)) = log\n1\n1 + 1−πθ(y|x)\nπθ(y|x)\n= −log\n\x10\n1 + 1 −πθ(y|x)\nπθ(y|x)\n\x11\n= −log πθ(y|x) + 1 −πθ(y|x)\nπθ(y|x)\n= log πθ(y|x).\nLemma B.2.\nlog σ(−rodds\nθ\n(y, x)) = log\n\x001 −πθ(y|x)\n\x01\nProof.\nlog σ(−rodds\nθ\n(y, x)) = log σ(−log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(πθ(y|x))−log(1−πθ(y|x)) = log\n1\n1 +\nπθ(y|x)\n1−πθ(y|x)\n=\n−log\n\x10\n1 +\nπθ(y|x)\n1 −πθ(y|x)\n\x11\n= −log 1 −πθ(y|x) + πθ(y|x)\n1 −πθ(y|x)\n= log(1 −πθ(y|x)).\n13\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nTheorem B.3. LASFT is equivalent to the binary cross-entropy loss, encompassing both likelihood and unlikelihood\ncomponents:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nProof. To show that LASFT is equivalent to the BCE loss, we start with the definition:\nLASFT = −log πθ(yw|x) −λ log σ(rodds\nθ\n(yw, x)) −λ log σ(−rodds\nθ\n(yl, x)),\nwhere rodds\nθ\n(y, x) =\nπθ(y|x)\n1−πθ(y,x). Applying Lemma B.1 and Lemma B.2 to the expression, we obtain:\nLASFT = −log πθ(yw|x) −λ log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n= −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)).\nC. Relationship Between ORPO and ASFT Loss Functions\nTheorem C.1. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nProof. We start by defining the ORPO loss:\nLORPO = −log πθ(yw|x) −λ log σ\n\x12\nlog\nπ(yw|x)\n1 −π(yw|x) −log\nπ(yl|x)\n1 −π(yl|x)\n\x13\n.\nExpanding the second term using the identity log σ(x) = x −log(ex + 1), we get:\n−log σ\n\x12\nlog\nπθ(yw|x)\n1 −πθ(yw|x) −log\nπθ(yl|x)\n1 −πθ(yl|x)\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x)(1 −πθ(yl|x))\nπθ(yl|x)(1 −πθ(yw|x)) + 1\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\nπθ(yl|x)(1 −πθ(yw|x))\n\x13\n= −log πθ(yw|x) −log(1 −πθ(yl|x)) + log\n\x00πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\n\x01\n|\n{z\n}\nORPOAlign\n.\nCombining all terms, we obtain:\nLORPO = −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)) + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n= LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\nD. Proof of Theorem 3.4\nTheorem D.1 (Collinearity of β-ASFT and ORPO Gradients). Let\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n,\nwhere\nrodds\nθ\n(y, x) = log\n\x10\nπθ(y|x)\n1−πθ(y|x)\n\x11\n.\n14\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nDefine the ORPO alignment loss as\nLORPOAlign = −log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n.\nThen,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n\r\r∇θ Lβ\nASFTAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r,\ni.e., their gradients become collinear in the same direction as β →0.\nProof. Step 1. Gradient of β-ASFT.\nDenote pw = πθ(yw | x), pl = πθ(yl | x). Then\nrodds\nθ\n(yw, x) = log\n\x10\npw\n1−pw\n\x11\n,\nrodds\nθ\n(yl, x) = log\n\x10\npl\n1−pl\n\x11\n.\nBy definition,\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n.\nFor small β, a first-order Taylor expansion of σ(β z) around 0 yields σ(β z) = 1\n2 + β z\n4 +O(β2). Thus, σ(β rodds\nθ\n(yw, x)) ≈\n1\n2 and σ(−β rodds\nθ\n(yl, x)) ≈1\n2. Taking gradients and applying the chain rule gives each term approximately proportional to\n± β ∇θ[rodds\nθ\n(·)]. Concretely,\n∇θ\n\x02\n−log σ(β rodds\nθ\n(yw, x))\n\x03\n≈−β\n2 ∇θ\n\x02\nrodds\nθ\n(yw, x)\n\x03\n,\n∇θ\n\x02\n−log σ(−β rodds\nθ\n(yl, x))\n\x03\n≈+ β\n2 ∇θ\n\x02\nrodds\nθ\n(yl, x)\n\x03\n.\nHence, summing up,\n∇θ Lβ\nASFTAlign ≈β\n2\nh\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\ni\n.\nObserve that β > 0 implies the overall scalar factor β\n2 is strictly positive in front of the difference of gradients.\nStep 2. Gradient of ORPO alignment loss.\nDefine ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n.\nIts gradient (using the chain rule) is proportional to\n∇θ LORPOAlign ∝−∇θ\n\x02\nrodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x03\n= ∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x).\nUp to a strictly positive logistic factor (since σ(·) ∈(0, 1)), the coefficient in front of ∇θ[rodds\nθ\n(·)] remains negative, but we\ntrack the absolute scalar to see it is positive. Indeed, one can write\n−∇θ\n\x00∆rodds\nθ\n(x)\n\x01\n= κORPO ∇θrodds\nθ\n(yl, x) −κORPO ∇θrodds\nθ\n(yw, x),\nκORPO > 0.\nStep 3. Conclusion (positive collinearity).\nComparing the two gradients:\n∇θ Lβ\nASFTAlign ≈\nβ\n2\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n,\n∇θ LORPOAlign ∝\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n.\nThe ratio is thus strictly positive for small β. Consequently,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nestablishing collinearity in the same direction.\n15\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nE. Proof of Theorem 3.5\nTheorem E.1 (Collinearity of β-ORPO and ORPO Gradients). Let\n∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x),\nand consider\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nIts gradient is collinear with the gradient of the standard ORPO alignment loss\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\nfor any fixed β > 0. Formally,\n∇θ Lβ\nORPOAlign\n\r\r∇θ Lβ\nORPOAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r.\nProof. Step 1. Gradient of β-ORPO.\nLet ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nBy the chain rule,\n∇θ Lβ\nORPOAlign = −\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\nβ ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince σ′(z) = σ(z) [1 −σ(z)], we have\n−\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\n= −β\n\x02\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01\x03\n.\nThus,\n∇θ Lβ\nORPOAlign = −β\nh\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01i\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince β > 0 and 1 −σ(·) > 0, the factor multiplying ∇θ[∆rodds\nθ\n(x)] is strictly negative.\nStep 2. Gradient of standard ORPO (i.e. β = 1).\nFor\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n,\nthe gradient is\n∇θ LORPOAlign = −\n\x02\n1 −σ(∆rodds\nθ\n(x))\n\x03\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nThis also has a strictly negative scalar in front of ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nStep 3. Conclusion (exact positive ratio).\nSince ∇θ Lβ\nORPOAlign and ∇θ LORPOAlign both differ from ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\nby a negative coefficient, it follows that these\ntwo gradients coincide up to a strictly positive factor:\n∇θ Lβ\nORPOAlign = κ(β) ∇θ LORPOAlign,\nκ(β) > 0.\nHence\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nproving the claimed collinearity (in the same direction) for every fixed β > 0.\n16\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nF. Proof of Theorem 3.6\nTheorem F.1 (Unified Collinearity of DPO with IPO, SimPO, NCA, Cal-DPO, and APO-Zero). Let\n∆rref\nθ (x) = rref\nθ\n\x00yw, x\n\x01\n−rref\nθ\n\x00yl, x\n\x01\n,\nand define the DPO loss\nLDPO = −log\n\x10\nσ\n\x00β ∆rref\nθ (x)\n\x01\x11\n,\nβ > 0.\nFor each method X ∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n, as β →0, the gradient of LX is asymptotically\ncollinear (i.e., it differs by a positive factor) with the gradient of LDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nProof of Theorem 3.6. Step 1: DPO as the baseline (tracking its sign).\nBy definition,\nLDPO = −log σ\n\x00β ∆rref\nθ (x)\n\x01\n.\nSince σ(u) = 1/(1 + e−u), for β > 0, one computes\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nObserve that β > 0 and σ(·) ∈(0, 1) imply\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01\n> 0.\nHence the factor multiplying ∇θ ∆rref\nθ (x) is negative. To unify directions by a positive multiple, note\n−∇θ LDPO = β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x),\nwhich has a strictly positive scalar in front. Thus, ∇θ LDPO is collinear with ∇θ ∆rref\nθ , and in particular its negative is a\npositive multiple of ∇θ ∆rref\nθ .\nStep 2: IPO.\nThe IPO loss is\nLIPO =\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x112\n.\nIts gradient is\n∇θ LIPO = 2\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x11\n∇θ ∆rref\nθ (x).\nAs β →0, the term\n1\n2β dominates ∆rref\nθ (x). Hence,\n∆rref\nθ (x) −\n1\n2β ≈−1\n2β ,\nso\n∇θ LIPO ≈−1\nβ ∇θ ∆rref\nθ (x).\nWe compare this with\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nBoth gradients are negative multiples of ∇θ ∆rref\nθ (x). Therefore,\n∇θ LIPO = κIPO(β) ∇θ LDPO,\nwith κIPO(β) > 0 as β →0.\nHence they are collinear in the same direction asymptotically.\n17\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nStep 3: SimPO.\nThe SimPO loss is\nLSimPO = −log σ\n\x00β ∆sθ −γ\n\x01\n,\nwhere ∆sθ = log πθ(yw | x) −log πθ(yl | x). Its gradient takes the form\n∇θ LSimPO = −β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\n∇θ ∆sθ.\nAgain, β > 0 and 1 −σ(·) > 0. Also, σ(β ∆sθ −γ) ∈(0, 1). Thus the prefactor\n−β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\nis strictly negative for each β > 0. Therefore, just like DPO, ∇θ LSimPO is in the negative direction of ∇θ ∆sθ. But ∇θ ∆sθ\nis proportionally the same as ∇θ ∆rref\nθ\nfor small-β expansions (both are differences of log-likelihood or reward-like terms).\nSo\n∇θ LSimPO = κSimPO(β) ∇θ LDPO,\nκSimPO(β) > 0 for small β.\nHence they are collinear with a positive factor in the low-β limit.\nStep 4: NCA.\nDefine\nrref\nw\n= rref\nθ\n\x00yw, x\n\x01\n,\nrref\nl\n= rref\nθ\n\x00yl, x\n\x01\n.\nThen NCA is\nLNCA = −log σ\n\x00β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nl\n\x01\n.\nFor small β, expand\nσ(β z) = 1\n2 + β z\n4\n+ O(β2),\nso log σ(β z) = log 1\n2 + log\n\x10\n1 + β z\n2 + O(β2)\n\x11\n. Each gradient term then yields a linear-in-β combination of ∇θ rref\nw and\n∇θ rref\nl . Collecting terms shows that, as β →0,\n∇θ LNCA ∝β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= β ∇θ ∆rref\nθ (x).\nComparing this with ∇θ LDPO = −β\n\x02\n1 −σ(. . . )\n\x03\n∇θ ∆rref\nθ (x) reveals another negative factor on the DPO side. In ratio\nform,\n∇θ LNCA = κNCA(β) ∇θ LDPO\nwith κNCA(β) > 0 for small β.\nHence collinearity follows.\nStep 5: Cal-DPO.\nThe Cal-DPO loss is\nLCal-DPO = −log σ\n\x00∆rref\nθ (x)\n\x01\n+\n\x00rref\nw −\n1\n2β\n\x012 +\n\x00rref\nl\n+\n1\n2β\n\x012.\nFor β near 0, the large constants ± 1\n2β dominate. The gradient w.r.t. θ in these squared terms is effectively\n∝−1\nβ ∇θ rref\nw\n+\n1\nβ ∇θ rref\nl\n= −1\nβ ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −1\nβ ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO has the same negative sign structure in front of ∇θ ∆rref\nθ , their ratio is again positive. Thus\n∇θ LCal-DPO = κCal-DPO(β) ∇θ LDPO\nwith κCal-DPO(β) > 0 as β →0.\nStep 6: APO-Zero.\nAPO-Zero is given by\nLAPO-Zero = −σ\n\x00β rref\nw\n\x01\n+ σ\n\x00β rref\nl\n\x01\n.\n18\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nIts gradient involves terms ∇θ σ(β rref\nw ) and ∇θ σ(β rref\nl ), each proportional to β ∇θ rref\nw and β ∇θ rref\nl . Subtracting these\nyields\n∇θ LAPO-Zero ∝−β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −β ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO also has a negative constant factor, their ratio has a positive limit. Therefore,\n∇θ LAPO-Zero = κAPO-Zero(β) ∇θ LDPO,\nκAPO-Zero(β) > 0 for small β.\nConclusion.\nIn each method X, one sees that ∇θ LX has the same negative-sign structure around ∇θ ∆rref\nθ (x) as does ∇θ LDPO,\nensuring a positive ratio in the limit. Formally,\n∇θ LX = κX(β) ∇θ LDPO,\nκX(β) > 0,\nas β →0.\nThus,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥,\nwhich completes the proof of their alignment in the same direction.\nG. Pareto fronts for Llama 3.2 setups\nThe results presented in this section correspond to the best hyperparameter configurations identified during the hyperparame-\nter search described in Section 4.2, including the optimal learning rate for each method. This ensures that the Pareto fronts\nreflect the upper performance limits for alignment quality.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL Divergence with SFT Model\n40\n50\n60\n70\n80\n90\nGPT-4 Win Rate (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(a) Llama 3.2 3B TL;DR\n0.0\n0.2\n0.4\n0.6\n0.8\nKL Divergence with SFT Model\n5\n6\n7\n8\n9\n10\n11\n12\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(b) Llama 3.2 3B UF\nFigure 6. Pareto front for alignment quality and KL divergence. Results for Llama 3.2 3B TL;DR and UF setups on GPT-4 Win\nRate vs. ”golden” validation subset and AlpacaEval 2 LC respectively with different β values. Methods are grouped into pairwise and\npointwise categories. For the summarization task (Llama 3.2 3B TL;DR), both pointwise and pairwise methods achieve strong overall\nresults. For the UF setup, methods also perform similarly within overlapping confidence intervals, indicating no clear separation.\nH. Toy Example Details\nTo analyze the differences between pairwise and pointwise ranking methods, especially with respect to the ranking nature of\nalignment losses in LLMs, a simplified toy experiment was conducted under a controlled setup. A dataset of 2000 triplets\n(x, yw, yl) was generated, where x, yw, and yl are real-valued scalars satisfying yw > yl. The data was split into 80% for\ntraining and 20% for testing. When the model processes a scalar input x together with a candidate y, these two numbers\nform a vector in R2, which serves as the input of the Multi-Layer Perceptron (MLP) to predict the reward r.\n19\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA single-hidden-layer MLP with ReLU activation was used in two capacity settings: lower (hidden size = 1) and higher\n(hidden size = 3). The model takes x and a candidate y as input, producing a reward r analogous to training a reward model\nfor RLHF (Stiennon et al., 2020).\nTwo losses were evaluated: the pairwise Bradley-Terry loss (Bradley & Terry, 1952),\nLPairwise = −log\n\x00σ(β(rw −rl))\n\x01\n,\nand the pointwise loss,\nLPointwise = −\n\x02\nlog\n\x00σ(βrw)\n\x01\n+ log\n\x00σ(−βrl)\n\x01\x03\n.\nEach configuration was trained over 100 runs, tuning the learning rate from {0.5, 0.3, 0.1, 0.01, 0.03, 0.05} and β from\n{5.0, 2.0, 1.0, 0.2, 0.1, 0.05, 0.01}. Alignment accuracy was defined as the proportion of cases with rw > rl.\nThe results show that both methods yield comparable performance in the low-capacity regime, while pairwise ranking\nachieves higher accuracy as model capacity increases, mirroring the effects observed in larger-scale experiments from the\nSection 5.3.\nI. GPT-4 Side-By-Side Evaluation Prompt\nFor our Side-By-Side evaluations with GPT-4o, we designed a prompt tailored to the Reddit TL;DR dataset to assess\naccuracy, completeness, relevance, and conciseness. The full prompt used in our experiments is detailed below.\nAct as an impartial judge and evaluate the quality of the summaries provided\nby two AI assistants for the text displayed below. Your evaluation should\nconsider accuracy, completeness, relevance, and conciseness.\nYou will be given a text, Assistant A’s summary, and Assistant B’s summary.\nYour job is to evaluate which assistant’s summary is better based on the\ntext provided.\nBegin your evaluation by comparing both assistants’ summaries with the\noriginal text. Identify and correct any inaccuracies.\nEnsure the summaries are complete, capturing all essential information\nfrom the text without introducing fabricated details.\nAssess the relevance of the information each assistant chose to include\nin their summary, ensuring it reflects the core message of the text.\nEvaluate the conciseness of the summaries, favoring those that efficiently\nconvey the necessary information without unnecessary verbosity.\nAvoid any position biases and ensure the order in which the summaries\nwere presented does not influence your decision.\nDo not allow the length of the summaries to influence your evaluation,\nexcept in the context of conciseness and efficiency.\nDo not favor certain names of the assistants.\nBe as objective as possible.\nYou should only evaluate the summaries provided by both assistants\nand NOT the original text itself.\nIf both summaries are irrelevant, contain hallucinations, or are\ninconsistent with the original text, mark the comparison as inconclusive\nand choose option "C".\nAfter providing your explanation, output your final verdict by strictly\nfollowing this format:\n"""\n20\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nComparison: <One-sentence comparison>\nWinner: <A if assistant A is better, B if assistant B is better, and C for a tie.>\n"""\n21'),
                Paper(arxiv_id='2502.01456', authors=['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], published_at=datetime.datetime(2025, 2, 4, 0, 2, 39, 922000, tzinfo=datetime.timezone.utc), title='Process Reinforcement through Implicit Rewards', summary="Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for the reinforcement learning\n(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of training process reward models (PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable to reward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using only policy rollouts and outcome labels through\nimplict process rewards. PRIME combines well with various advantage functions\nand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting from\nQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkey reasoning benchmarks over the SFT model. Notably, our resulting model,\nEurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning\nbenchmarks with 10% of its training data.", upvotes=48, thumbnail=None, content='INTRODUCTION\nDense process rewards, which provide feedback at each intermediate step rather than only the whole\ntrajectory, have proven effective in inference-time scaling of large language models (LLMs) on\nchallenging reasoning tasks (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Yuan\net al., 2024b). On the training side, they also present superiorities in the reinforcement learning\n(RL) of LLMs, particularly in improving training efficiency (Sutton & Barto, 2018) and credit\nassignment (Leike et al., 2018) compared with sparse outcome rewards. However, successful\napplications of dense rewards in RL for LLMs are limited (Setlur et al., 2024), as current industry-\nleading models primarily depend on verifiable outcome rewards and have not yet demonstrated\nmeaningful progress with dense rewards (DeepSeek-AI et al., 2025; Team et al., 2025).\nWe identify the central challenge as how to acquire and utilize high-quality dense rewards at scale,\nwhich enables online process reward model (PRM) update efficiently. The reason is that, optimizing\ntowards a static reward model eventually leads to overoptimization or reward hacking (Gao et al.,\n∗Core Contributors.\n†Project Lead.\n1Models and data are available at: https://github.com/PRIME-RL/PRIME.\n1\narXiv:2502.01456v1  [cs.LG]  3 Feb 2025\n\nPreprint\nAIME 2024\nAMC\nMinerva Math\nOlympiadBench\nMATH-500\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n26.7\n57.8\n38.6\n42.1\n79.2\n48.9\n3.3\n30.1\n32.7\n29.8\n65.1\n32.2\n13.3\n50.6\n34.6\n40.7\n79.8\n43.8\n16.7\n30.1\n35.3\n31.9\n64.6\n35.7\n9.3\n45.8\n36.8\n43.3\n76.4\n43.3\nEurus-2-7B-PRIME\nEurus-2-7B-SFT\nQwen-2.5-Math-7B-Instruct\nLlama-3.1-70B-Instruct\nGPT-4o-2024-08-06\nFigure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics\nbenchmarks, outperforming advanced math models and larger models. Notably, PRIME brings\nsubstantial performance gain (+16.7%) over Eurus-2-7B-SFT.\n2022) due to distribution shift. Ideally, this can be solved by improving the reward model online (Leike\net al., 2018). However, acquiring dense process labels for training is prohibitively more expensive.\nExisting methods either need to build complicated human annotation pipelines (Lightman et al.,\n2023) or rely on estimation-based methods, which require about 10× more rollouts for each step than\nsampling only the response-level trajectories (Wang et al., 2023; Kazemnejad et al., 2024). Neither of\nthem is scalable in online RL. Moreover, to the best of our knowledge, it remains underexplored how\nto incorporate dense rewards into RL for LLMs.\nIn this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable frame-\nwork for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level\nrewards. At its core, the framework employs recently proposed implicit process reward model-\ning (Yuan et al., 2024b) to train dense reward models with only outcome-level labels. This enables\nPRIME to perform online learning of reward signals using only outcome labels on policy rollouts,\nthereby fundamentally mitigating reward hacking while maintaining the same computational cost as\ntraditional outcome reward models (ORMs). Besides scalability, PRIME also (1) serves as a general\nmethod to fuse token-level dense rewards and sparse outcome rewards by calculating their returns\nseparately before summing together, which is compatible with diverse RL algorithms (Williams, 1992;\nKool et al., 2019; Shao et al., 2024; Ahmadian et al., 2024; Schulman et al., 2017); (2) eliminates the\ndedicated reward modeling stage, which is required by existing works, by simply initializing from the\nSFT model or even the base model (§ 5.6). In summary, starting from one single language model, the\nPRIME framework can efficiently accomplish the generation of dense rewards, the initialization and\nupdating of reward models, as well as the reinforcement learning (RL) training of the policy model.\nTable 1: The comparison of resource requirements between Eurus-\n2-7B-PRIME and Qwen2.5-Math-7B-Instruct.\nModel\nEurus-2-7B-PRIME\nQwen2.5-Math-7B-Instruct\nBase Model\nQwen2.5-Math-7B\nQwen2.5-Math-7B\nSFT Data\n230K (open-source)\n2.5M (open-source & in-house)\nRM Data\n0\n618K (in-house)\nRM\nEurus-2-7B-SFT\nQwen2.5-Math-RM (72B)\nRL Data\n150K queries × 4 samples\n66K queries × 32 samples\nIn\nexperiments,\nwe\ntrain\nQwen2.5-Math-7B-Base (Yang\net al., 2024b) with PRIME after\na lightweight SFT warmup stage.\nCompared to RL using outcome\nrewards only, PRIME achieves\na 2.5× sample efficiency gain\nand a 6.9% performance im-\nprovements on challenging math\nproblems. As shown in Figure 1,\nthrough PRIME, we successfully\nachieve substantial improvement on key mathematical reasoning benchmarks over the SFT model,\nleading to 16.7% improvement on average, and over 20% on AMC&AIME competitions. Our\nfinal model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical\nbenchmarks. Notably, this is achieved with only 10% of the data used by Qwen-Math, as in Table 1.\n2\n\nPreprint\nOur analysis shows that updating the PRM online is key to the success of PRIME (§5.1). We also\nshow that PRIME could generally boost various RL algorithms, including RLOO (Ahmadian et al.,\n2024), REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GRPO (Shao et al., 2024)\n(§5.4). In terms of the design choices of advantage estimate, we observe that Implicit PRMs are better\nto be used as reward models than value models (§5.5).\n2\nREINFORCEMENT LEARNING FOR LLMS AND THE CHALLENGES OF\nINCOPORATING DENSE REWARDS\nReinforcement Learning (RL) aims to learn an optimal policy πθ that maximizes the expected\ncumulative discounted reward, namely return, when interacting with an environment. In the context\nof autoregressive language modeling, state at step t is the concatenation of prompt x and current\nresponse y<t, and the action is the t-th token or step yt.\n2.1\nRL PRELIMINARIES FOR LLMS\nPolicy Gradient. Policy gradient is a fundamental algorithm that directly optimizes this objective.\nCentral to this approach is the advantage function At, which quantifies how much better an action is\ncompared to alternatives in a given state:\n∇θJ(θ) = Ex∼D,y∼πθ\n" T\nX\nt=0\n∇θ log πθ(yt|y<t)At\n#\n(1)\nwhere (x, y) represents a pair of input and output. x is omitted for brevity. In practice, the advantage\nfunction is implemented as cumulative discounted rewards subtracting a baseline:\nAt =\nT\nX\ns=t\nγs−tr(ys) −b\n(2)\nγ ∈[0, 1] is a discount factor that optionally decays future rewards, and r(ys) is the reward provided\nby the environment at time step s with x and y<s being omitted in conditions. Eq. 2 is the general\nformula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and\ndense reward at each step is crucial for RL. Different choices of b include, e.g. directly using values\nWilliams (1992), group average of rewards (Shao et al., 2024), and leave-one-out average of rewards\nAhmadian et al. (2024); Kool et al. (2019).\nValue Models. Though the MC estimate is unbiased, it suffers from high variance because of the\nreliance on all future actions and rewards, which can be random and noisy. Value models, which\npredict expected accumulated rewards starting from a state, are adopted to help reduce the variance\nin advantage estimation, such as Generalized Advantage Estimation (GAE; Schulman et al., 2016):\nAGAE(γ,λ)\nt\n= P∞\ns=0(γλ)sδt+s, where δt = r(yt) + γV (y<t+1) −V (y<t) is the temporal difference\n(TD) error (Sutton, 1988), V is a value model, and λ controls the bias-variance tradeoff in advantage\nestimation. PPO (Schulman et al., 2017) is a representative of such actor-critic algorithms that\nexplicitly train a value model along with the policy.\nReward Sparsity. Although dense rewards can be naturally integrated into the advantage function\nthrough Eq. 2, unfortunately, only outcome reward models (ORMs) are available in most practices\nof LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive\nno rewards (Rafailov et al., 2023; Shao et al., 2024; DeepSeek-AI et al., 2025). In this bandit\nsetting, r(yt) = 0 for t < T while r(yT ) can be non-zero, and Eq. 2 becomes A = r(yT ) −b. This\nformulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback\nonly at the end of the entire generation. This may (1) encourage spurious solutions with incorrect\nprocesses but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter\nthe credit assignment problem (Sutton & Barto, 2018). These drawbacks could be further amplified\non complicated tasks, which require more thinking and execution steps, urging the need of dense\nrewards (Uesato et al., 2022; Lightman et al., 2023). Some may consider employing a value model\nto mitigate the problem, as it predicts values at every step t. However, previous work showed that\nvalue models may not be able to solve the reward sparsity issue effectively due to training challenges,\ndespite the additional computation overhead (Shao et al., 2024; Ahmadian et al., 2024). We will\nalso empirically validate this claim in §5.5.\n3\n\nPreprint\n2.2\nKEY CHALLENGES IN SCALABLE DENSE REWARDS\nThe way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs,\nwhich score model responses over each token or step. However, it is usually infeasible in practice to\nincorporate dense rewards into online RL because of three critical challenges in implementation.\nC1. Process rewards are hard to define. It is difficult to collect step-level labels since reasoning\nsteps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating\nlabels for each token is too costly. Moreover, defining the absolute correctness of intermediate\nprocesses as dense rewards can be ambiguous, as some incorrect steps can also positively contribute\nto the final answer by pruning searching branches (OpenAI, 2024; DeepSeek-AI et al., 2025).\nC2. PRM online updates are not scalable. It is crucial to prevent reward overoptimization or reward\nhacking, which requires the reward model or value model to be updated online along with the policy\nmodel (Schulman et al., 2017; Gao et al., 2022). However, training PRMs often requires extensive\nnuanced step-level annotation, which is infeasible in online RL training. Therefore, this brings about\nconsiderable scalability and generalization concerns in dense rewards for RL.\nC3. Explicit reward modeling brings extra cost. Training reward models requires extensive\nannotation and broad data coverage to ensure a good balance between adaptability to the policy\ndistribution and generalization to distribution shifts. Hence, the explicit training stage introduces a\nvery costly data collection and an additional training overhead, especially for PRMs which typically\nrequire stepwise labels.\nNotably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMs\ninto their large-scale RL training (DeepSeek-AI et al., 2025).\n3\nPRIME\nTo address the above challenges, we propose PRIME, a scalable online RL method with dense\nrewards. The key insight of PRIME is to apply implicit process rewards, which are derivable from the\nImplicit PRM that is trained with only outcome labels (Yuan et al., 2024b). This property enables us to\nupdate the PRMs online to avoid reward hacking. We then design a flexible framework to incorporate\nimplicit process rewards with outcome rewards into any kind of MC advantage estimate. PRIME is\nillustrated in Figure 2 and Algorithm 1. Next, we will detail the implicit process rewards (§3.1) and\nhow we leverage them to calculate advantages (§3.2), and introduce other techniques we used (§3.3).\n3.1\nENABLING SCALABLE REWARD UPDATE WITH IMPLICIT REWARD MODELING\nWe consider dense rewards from the Implicit PRM because of the scalability. In short, Implicit PRM\nenables training an ORM with outcome labels only while repurposing it as a PRM at inference. The\ntraining stage is the same as standard ORM pipelines, with the only difference being representing\nthe reward as rϕ(y) := β log πϕ(y)\nπref(y), where πϕ is the RM and πref is the reference model, both of\nwhich are causal LMs. At inference, the process rewards are obtained by:\nrϕ(yt) := β log πϕ(yt|y<t)\nπref(yt|y<t)\n(3)\nIn PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we\nupdate the Implicit PRM online with on-policy rollouts and outcome supervision and then\ncalculate token-level dense rewards to estimate advantages, which solves C1 and C2 mentioned in\n§2.2 respectively: (1) To prevent overoptimization and reward hacking, it is crucial to update reward\nmodels online. However, updating previous PRMs (Lightman et al., 2023) requires annotating step\nlabels on the latest policy rollouts, which is neither efficient nor scalable during online RL. In contrast,\nthe Implicit PRM only demands outcome labels to train due to its special reward representation,\nand thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which\nhave already been collected to update the policy model. (2) Unlike common PRMs that produce only\nstep-level rewards, the Implicit PRM provides more fine-grained token-level rewards at no additional\ncost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra\noverhead, making it easy to combine with any RL algorithms for advantage estimation.\n4\n\nPreprint\nAlgorithm 1 Process Reinforcement through Implicit Rewards (PRIME)\nInput Language model πθinit; outcome reward verifier ro; dataset D; sample number K; total iteration\nN.\n1: Initialize policy model πθ ←πθinit, πθold ←πθinit, implicit PRM πϕ ←πθinit, reference model\nπref ←πθinit\n2: for iteration = 1, ..., N do\n3:\nSample batch of prompts B ∼D\n4:\nGenerate K responses: {y1, ..., yK} ∼πθ(·|x) for x ∈B\n5:\nCompute outcome rewards: ro\n\x00y1:K\x01\n6:\nApply accuracy filter (§3.3) on all prompts: T ←Filter(x, y1:K, ro\n\x00y1:K\x01\n) for x ∈B\n7:\nForward pass πϕ, πref on each (x, y) ∈T to obatin implicit process reward rϕ(yt) with Eq. 3\n8:\nUpdate Implicit PRM πϕ by CE loss on (x, y, ro (y)) ∈T :\nLCE(ϕ) = −E(x,y,ro(y))∼T [ro (y) · log σ (rϕ (y)) + (1 −ro (y)) · log (1 −σ (rϕ (y)))]\n9:\nCompute advantages A with Eq. 5\n10:\nUpdate policy πθ by PPO loss in Eq. 6\n11:\nUpdate old parameters: θold ←θ\n12: end for\nOutput Optimized policy model πθ\n3.2\nADVANTAGE ESTIMATION AND POLICY UPDATE\nSFT \nModel\nImplicit \nPRM\nPolicy \nModel\nImplicit \nPRM\nPolicy \nModel\nPrompt\nResponse\nOutcome \nVerifier\n𝒓𝒐\n𝒓𝒑\nUpdate\nUpdate\n𝝅𝒓𝒆𝒇\n𝝅𝒓𝒆𝒇\nSFT \nModel\nFigure 2: Illustration of PRIME. PRIME follows\nthat (1) initialize policy model and the Implicit\nPRM both with the reference model; (2) sample\nmultiple responses for each prompt and filter with\noutput accuracy; (3) obtain implicit process re-\nwards by the Implicit PRM and update it using\ncross-entropy (CE) loss; (4) compute advantage\nand policy loss then update the policy model.\nEstimating advantages using Monte Carlo es-\ntimator with a leave-one-out baseline. After\nobtaining token-level dense rewards, we calcu-\nlate advantages based on either MC estimators\nor GAE. To determine the advantage function\nin PRIME, we compare GAE with several MC\nestimators, including REINFORCE (Williams,\n1992), RLOO (Ahmadian et al., 2024), and\nGRPO (Shao et al., 2024). Experimental details\nand results can be found in §5.4.\nWe find that MC estimators, despite being sim-\npler, are strong enough to produce stable results.\nTherefore, we choose MC estimate as our ad-\nvantage function and despite PRIME being com-\npatible with any baseline estimation approaches,\nwe instantiate it with a leave-one-out baseline\nfrom K samples (Ahmadian et al., 2024) in this\npaper, as it performs better in the experiments:\nAi = r(yi\nT ) −\n1\nK −1\nX\nj̸=i\nr(yj\nT )\n(4)\nwhere r(yi\nT ) denotes the reward of i-th response at final step T, K is the number of samples for one\nprompt. The leave-one-out (LOO) baseline helps reduce variances.\nMore specifically, we use an Implicit PRM πϕ and an outcome verifier or reward model ro. We\ncalculate the return of implicit process rewards and outcome rewards separately if both are available,\nsince directly mixing their values may lead to numerical instability (Shao et al., 2024). For implicit\nprocess rewards, we perform a three-step process to calculate return: (1) Use the averaged implicit\nprocess rewards to calculate the leave-one-out baseline; (2) Normalize the process reward at step t by\nsubtracting the baseline; (3) Calculate the discounted return for each response. For outcome rewards,\nwe directly adopt LOO without any modification. Finally, the advantage is set to the combination of\n5\n\nPreprint\nboth returns:\nAi\nt =\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8f0rϕ(yi\ns) −\n1\nK −1\nX\nj̸=i\nrϕ\n\x00yj\x01\n\uf8f9\n\uf8fb\n|\n{z\n}\nRLOO with implicit process rewards\n+ ro\n\x00yi\x01\n−\n1\nK −1\nX\nj̸=i\nro\n\x00yj\x01\n|\n{z\n}\nRLOO with outcome rewards\n(5)\nUpdating policy with PPO clip surrogate loss. We adopt PPO clip surrogate loss for more stable\npolicy updates:\nLCLIP(θ) =Et\n"\nmin\n\x12 πθ(yt|y<t)\nπθold(yt|y<t)At, clip\n\x10 πθ(yt|y<t)\nπθold(yt|y<t), 1 −ϵ, 1 + ϵ\n\x11\nAt\n\x13#\n(6)\nwhere ϵ is a clipping parameter. The loss prevents the updated policy from deviating too far from the\noriginal distribution, which is the prerequisite of importance sampling. The legitimacy of importance\nsampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling\nefficiency.\n3.3\nOTHER TECHNIQUES\nInitializing PRM with SFT/base model. In practice, we find that the starting policy model itself\nserves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in §2.2\nand even outperforms a dedicatedly trained PRM, as shown in § 5.1.\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nw/ filter\nw/o filter\nFigure 3: Impact of online prompt filtering on\ntraining rewards.\nOnline Prompt Filtering. As we sample mul-\ntiple trajectories for each prompt, we introduce\nonline prompt filtering which filters prompts\nwithin a certain accuracy range. This (1) pre-\nserves only the prompts within a certain median-\nlevel difficulty range (Yang et al., 2024b) and (2)\nbalances data distribution for the Implicit PRM\nonline training.\nWe present the ablation study results in Figure 3\nusing RLOO with outcome rewards only, from\nwhich we can see that the online prompt filter\nlargely lowers the variance of RL training.\nHow PRIME addresses challenges in §2.2. In\nsummary, as illustrated in Figure 2 and Algo-\nrithm 1, PRIME adopts implicit process rewards\nfor efficient PRM online update (C2), then inte-\ngrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs\nare directly initialized from SFT or base models, which foregoes explicit reward modeling (C3).\n4\nEXPERIMENTS\n4.1\nIMITATION WARMUP\nWe focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-\nMath-7B-Base (Yang et al., 2024b) for its great mathematical capabilities. We first performed\nsupervised finetuning for RL preparation.\nData Construction. To construct the SFT dataset, we collect reasoning instructions from several open-\nsource datasets. For completion, we employed LLaMA-3.1-70B-Instruct (Meta, 2024) to answer the\ninstructions, with a system prompt requesting the model to perform action-centric chain-of-thought.\nWe finally obtained 230K SFT data, the detailed sources and statistics can be found in § A.\nSFT Results. After finetuning, the performance of our SFT model is reported in Figure 1. Compared\nto baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks.\n6\n\nPreprint\nTable 2: Detailed results of PRIME and RLOO w/ outcome verifier (OV). At the same 240 steps, the\nmodel trained by PRIME is generally better than the model trained by outcome rewards.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nGPT-4o\n-\n9.3\n45.8\n76.4\n36.8\n43.3\n58.9\n48.8\n45.6\nLlama-3.1-70B-Inst.\n-\n20.0\n37.3\n65.0\n37.1\n30.5\n35.0\n34.4\n37.0\nQwen2.5-Math-7B-Inst.\n-\n13.3\n50.6\n79.8\n34.6\n40.7\n11.7\n11.3\n34.6\nEurus-2-7B-SFT\n0\n3.3\n30.1\n66.2\n32.7\n29.8\n21.7\n17.8\n28.8\nRLOO w/ OV Only\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\n80\n20.0\n41.0\n68.2\n38.2\n37.0\n26.7\n26.6\n36.8\n160\n13.3\n42.2\n72.0\n37.1\n38.7\n26.7\n25.6\n36.5\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\n320\n16.7\n51.8\n77.8\n39.7\n41.5\n36.1\n28.5\n41.7\nEurus-2-7B-PRIME\n592\n26.7\n57.8\n79.2\n38.6\n42.1\n33.3\n28.6\n43.9\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\n2.5x Efficient\n6.9% Higher\nPRIME\nRLOO w/ OV Only\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\n256\n288\n320\nSteps\n30\n32\n34\n36\n38\n40\n42\nAvg. Test Acc\nPRIME\nRLOO w/ OV Only\n(b) Test accuracy across different gradient steps.\nFigure 4: The effect of dense reward. We compare PRIME and RLOO with outcome verifier\n(OV). Dense rewards in PRIME lead to 2.5× sample efficiency and 6.9% performance improvement.\nPRIME also substantially outperforms RLOO on downstream tasks.\n4.2\nRL SETTINGS\nRule-based Outcome Verifier. Consistent with recent research that adopts exact match with ground\ntruth as unhackable rewards (Gao et al., 2024; Lambert et al., 2024; DeepSeek-AI et al., 2025), we\ndefine the rule-based ground truth outcome verifiers (OV) for math and coding as follows:\nrmath\no\n(y) =\n\x1a1,\nmatched\n0,\notherwise\nrcode\no\n(y) =\nP #passes\nP #test cases\nHyperparameters. We use veRL (Sheng et al., 2024) to conduct experiments. By default, we\ninitialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For\nhyperparameters, we use a constant 5 × 10−7 learning rate together with AdamW optimizer for\npolicy model, and use a 10−6 learning rate for PRMs. Both policy and PRMs use a batch size of 256\nand micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each\nprompt. We set β = 0.05 for PRM training. We set KL coefficient to 0 in all experiments.\nEvaluation Benchmarks. We evaluate on 7 reasoning benchmarks, focusing on competition-level\nmathematics and programming tasks, including AIME 2024 (Li et al., 2024), AMC (Li et al., 2024),\nMATH-500 (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He\net al., 2024), LeetCode (Guo et al., 2024), and LiveCodeBench (v2) (Jain et al., 2024).\n4.3\nMAIN RESULTS\nAs shown in Figure 1 and Table 2, Eurus-2-7B-PRIME achieves substantial improvements on key\nreasoning benchmarks over the SFT version of the model, leading to 15.1% improvement on average,\nand over 20% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7%\npass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct,\ndemonstrating its excellent reasoning ability.\n7\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n30\n32\n34\n36\n38\n40\nAvg. Test Acc\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(b) Test accuracy across different gradient steps.\nFigure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the\nbest results. Surprisingly, using PRMs trained on extra rollouts hurts the performance in both online\nand offline settings.\n4.4\nDENSE REWARDS V.S. SPARSE REWARDS\nWe first validate the effect of dense rewards compared to RLOO with outcome rewards only. We\ntrain this model for 240 steps. For PRIME, we use the same setting and train the model for 592\nsteps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure 4.\nCompared with sparse reward, PRIME takes 40% of the training steps to achieve the same\ntraining rewards as RLOO and improves the final rewards by 6.9%, with lower variances. On\ndownstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in\nTable 2.\n5\nANALYSIS\n5.1\nDESIGN CHOICES FOR THE IMPLICIT PRM\nThe Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this\nsection, we explore two major factors: (1) the initialization model and (2) the update mechanism.\nSFT model initializes a good PRM. Conventionally, we need to collect data to train RMs\nand PRMs, and then we can use them in RL. However, the Implicit PRM is a language\nmodel, so we can initialize it from any language model with the same tokenizer as the pol-\nicy model.\nTo investigate whether it is still necessary to train a PRM in advance, we con-\nduct experiments with different PRM initialization strategies: with the SFT model itself and\nwith a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT\nwith additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in § B.5).\n0\n50\n100\n150\n200\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nPRM Accuracy\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\nFigure 6: Impact of PRM online update. The\noffline PRM is gradully been overoptimized while\nonline PRMs achieve higher accuracy throughout\ntraining.\nWe report the experiment results in Figure 5.\nSurprisingly, directly using Eurus-2-7B-SFT\nto initialize the PRM greatly outperforms Eu-\nrusPRM which was trained on more samples.\nWe conjecture that initializing policy model and\nPRM from the same model largely alleviates\nthe distribution shift issue, as the PRM is only\ntrained on the online rollouts from the policy\nmodel.\nOnline PRM update is essential. To verify the\neffect of online PRM update, we pair the correct\nand wrong samples and calculate the PRM\nprediction accuracy using rϕ(y).\nWe report\nthe PRM classification accuracy in Figure 6.\nThe figure clearly shows that, online update\nmitigates\noveroptimization\nand\nreward\n8\n\nPreprint\n(a) Policy ref: We use the policy logprob as πref\nfor PRM.\n(b) SFT ref: We retain the initial policy to provide πref for\nPRM and KL.\nFigure 7: Comparison of different reference policy implementations. One uses the running policy’s\nold logprobs as reference (policy ref) while the other uses the initial SFT model as the reference\nmodel (SFT ref).\nhacking. The offline PRM, though starting with\nhigh accuracy, gradually drops during RL training procedure due to distribution shift. In contrast,\nonline PRMs that are trained on policy rollouts show the reverse curve.\nThis is further validated with training rewards and downstream performance. To breakdown, Eurus-2-\n7B-SFT is both used as PRM initialization and the reference model in the main experiment, so the\nPRM is totally trained from scratch, which means the initial PRM outputs zero reward for all tokens.\nTherefore, Figure 4 also demonstrates the effect of online PRM update. For EurusPRM initialization,\nthe online run outperforms the offline run as well in Figure 5.\n5.2\nREFERENCE MODEL CHOICE IS FLEXIBLE\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPolicy ref\nSFT ref\nFigure 8: Different reference model for PRM.\nWe compare two reference model selection strate-\ngies for PRIME. Using the policy model as refer-\nence and using the initial SFT model as reference.\nTheir rewards are similar.\nWe implement two variants of our algorithms to\nexplore the effect of reference model of implicit\nPRM, one using the initial SFT model as the\nreference model (SFT ref) while the other using\nthe running policy’s old logprobs as reference\n(policy ref), as shown in Figure 7a. The policy\nref simply adopts the old logprob of the policy\nmodel as πref, while the SFT ref remains the ini-\ntial SFT model for an additional πref calculation.\nWe compare their performance in this section.\nFrom the training rewards in Figure 8, we find\nthe two strategies are close and have pros and\ncons in different aspects: The Q value calculated\nby implicit PRM is the expectation under the dis-\ntribution of the reference model. So the updat-\ning policy could natrually serve as the reference.\nOn the other hand, KL divergence calculation\nis only allowed when the initial SFT model is\nretained.\n5.3\nSINGLE-FORWARD V.S. DOUBLE-FORWARD\nSince our implicit PRM is concurrently updated in training, for each rollout stage, we can update the\nPRM before the policy model and use the updated PRM to re-calculate the process rewards, which\n9\n\nPreprint\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nPRM Accuracy\nBefore-double forward\nBefore-single forward\nAfter-double forward\n(a) PRM classification accuracy on training samples.\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\nOutcome Training Rewards\nDouble forward\nSingle forward\n(b) Training outcome rewards.\nFigure 9: Single and double forward. While double forward methods obtain higher accuracy after\nonline update, the two variants achieve similar rewards during training.\nwe call the double-forward setting. We investigate the impact of double-forward in both the training\nand test phases. Our default setting applies single-forward, which uses process rewards from old\nPRMs. We plot PRM accuracy on rollouts and training rewards in Figure 9.\nAccordingly, we find that double-forward could increase PRM accuracy, but the training rewards\nremain close between the two methods.\n5.4\nPRIME WITH OTHER RL ALGORITHMS\nAs we stated before, PRIME is equally applicable to other RL algorithms beyond RLOO. In this\nsection, we implement PRIME with REINFORCE (Williams, 1992), GRPO (Shao et al., 2024), and\nPPO (Schulman et al., 2017). Similarly to RLOO, we only modify the advantage estimation functions\nand leave the clip surrogate loss unchanged.\nFirst of all, We compare different REINFORCE-like advantage estimators including REINFORCE,\nGRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms\ncompatible with the compound of outcome verifier reward and process reward, we accordingly make\nadaptions similar to Eq. 5. For GRPO, we have\nAi\nt = ro\n\x00yi\x01\n−mean(ro\n\x00yj\x01\n)\nstd(ro (yj))\n|\n{z\n}\nGRPO with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nrϕ(yi\ns) −mean\n\x12\nrϕ(yj)\n|yj|\n\x13\nstd\n\x10\nrϕ(yj)\n|yj|\n\x11\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n|\n{z\n}\nGRPO with implicit process rewards\n.\n(7)\nFor REINFORCE, we have\nAi\nt =\nro\n\x00yi\x01\n| {z }\nREINFORCE with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t · rϕ(yi\ns)\n|\n{z\n}\nREINFORCE with implicit process rewards\n.\n(8)\nFrom Figure 10 and Table 3, We show that PRIME boosts these algorithms on both efficiency and\nperformance as it does with RLOO. PRIME contributes consistently regardless of the policy update\nmethod, making it a generic algorithm. It indicates that PRIME is a general plug-in for almost any\nRL algorithm for LLM., which largely extends the use cases of PRIME.\nMoreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional\ncomputation cost from the critic model is redundant. This makes it possible to compensate for the\nexpense of the process reward model by using REINFORCE-like algorithms with simpler advantage\nestimators. Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm.\n10\n\nPreprint\nTable 3: Testset results of different RL algorithms.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nRLOO\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\nRLOO w/ PRIME\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\nREINFORCE\n240\n6.7\n47.0\n72.6\n36.0\n37.2\n27.2\n25.0\n36.0\nREINFORCE w/ PRIME\n240\n6.7\n50.0\n76.4\n36.8\n39.1\n27.8\n27.5\n37.8\nGRPO\n240\n10.0\n44.6\n73.2\n37.5\n36.6\n25.0\n25.8\n36.1\nGRPO w/ PRIME\n240\n16.7\n47.0\n75.0\n34.9\n38.2\n28.9\n23.9\n37.8\nPPO\n240\n10.0\n41.0\n73.6\n36.0\n36.3\n28.3\n25.7\n35.8\nPRIME as Value Model\n240\n16.7\n44.6\n72.6\n34.6\n35.7\n27.8\n24.6\n36.6\nPPO w/ PRIME\n240\n13.3\n50.6\n77.4\n37.1\n40.6\n30.0\n26.7\n39.4\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\nREINFORCE w/ PRIME\nGRPO\nGRPO w/ PRIME\nPPO\nPPO w/ PRIME\nFigure 10: PRIME also benefits REINFORCE,\nGRPO, and PPO, achieving similar improve-\nment as RLOO.\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\n+ linear-head value model\n+ Implicit PRM as value\n+ Implicit PRM as reward\nFigure 11: Comparison of value models and reward\nmodels. We show that value models, either the\noriginal PPO one or Implicit PRM, is substaintially\nworse than reward models.\n5.5\nVALUE OR REWARD, HOW TO USE THE IMPLICIT PRM?\nBesides using process rewards to estimate returns, we can also employ the Implicit PRM to predict\nvalues for advantage estimation in Eq. 2. Therefore, we compare four variants of MC estimate\nto determine the best way to incorporate dense supervision. Recall that the Implicit PRM has\nvϕ(y<t+1) = Pt\ni=1 β log πϕ(yi|y<i)\nπref(yi|y<i) with the process reward being rϕ(yt) = vϕ(y<t+1)−vϕ(y<t),\nand we assume a ground-truth outcome verifier ro, γ = 1, then we represent the variants as follows:\n(1) REINFORCE: At = ro(y).\n(2) On top of (1), using a linear-head value model V to calculate the baseline: At = ro(y)−V (y<t).\nThis is the original PPO in Figure 10 as we set γ = 1 and λ = 1.\n(3) On top of (1), using values from the Implicit PRM to serve as the baseline: At = ro(y) −\nvϕ(y<t). This is equivalent to PPO with its value model being replaced by values from the Implicit\nPRM when γ = 1 and λ = 1.\n(4) On top of (1), using process rewards from the Implicit PRM to calculate the return: At =\nro(y) + PT\ns=t rϕ(ys). This is the REINFORCE w/ PRIME in Figure 10.\nFigure 11 reports the results. Comparing PPO and REINFORCE, we find that an additional value\nmodel does not benefit policy performance. Notably, using rewards from the Implicit PRM to\ncalculate returns, which is the default setting in PRIME, greatly outperforms all three baselines,\nregardless of where the values come from. This indicates that PRMs work better than value models\nin RL for LLMs.\n5.6\n“ZERO” EXPERIMENTS\nDeepSeek-AI et al. (2025) proposed DeepSeek-R1-Zero, which is directly trained from a base model\nwith reinforcement learning. To further investigate the “Zero” setting, we also perform RL from\n11\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500\n0.525\nOutcome Training Rewards\nPRIME-Zero\nPRIME\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n32\n34\n36\n38\n40\n42\n44\n46\n48\nAvg. Test Acc\nPRIME-Zero\nPRIME\nQwen2.5-Math-7B-Instruct\n(b) Math test accuracy across different gradient steps.\nFigure 12: “Zero” RL from Qwen2.5-Math-7B. RL from the base model converges way faster than\nthe SFT model, surpassing the instruct version within 32 steps.\n0\n20\n40\n60\n80\nSteps\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nOutcome Training Rewards\nPRIME-Zero\n(a) Outcome training rewards (10-step moving).\n0\n16\n32\n48\n64\n80\n96\nSteps\n42\n44\n46\n48\n50\n52\nAvg. Test Acc\nQwen2.5-32B-Instruct\nPRIME-Zero\n(b) Math test accuracy across different gradient steps.\nFigure 13: “Zero” RL from Qwen2.5-32B-Base. RL from a 32B base model shows more promising\ngain, surpassing the instruct version within 16 steps.\nQwen2.5-Math-7B-Base and Qwen2.5-32B-Base (Yang et al., 2024a), skipping the SFT phase. We\npresent the experimental results in Figure 12 and Figure 13. The observations are as follows:\n(1) RL from base model is suprisingly efficient and effective. Comparing PRIME from Qwen2.5-\nMath-7B and Eurus-2-7B-SFT, the “Zero” setting converges much faster. This indicates that directly\nperforming RL from a base model might be a strong alternative to the conventional SFT-RL pipeline.\n(2) Larger models benefit more. Comparing 7B and 32B models, we see that the 32B model\ngains more on both training rewards and test performance. This is aligned with the conclusion in\nDeepSeek-AI et al. (2025).\n(3) Saturation could be a potential issue. Although PRIME-Zero obtains impressive performance\ngain, we find it quickly saturated at a very early stage (about 50 steps), which hinders further\nimprovement like in DeepSeek-AI et al. (2025). This is possibly attributed to the decrease of response\ndiversity, and we leave this as future work.\n6\nRELATED WORK\nRL for LLM Reasoning. In the context of LLMs, reinforcement learning has been widely used for\naligning human preferences (Christiano et al., 2017; Ouyang et al., 2022; Cui et al., 2024), but the\nopen-source community mostly adopt the data-driven imitation learning methods (Yuan et al., 2024a;\nYue et al., 2024; Wei et al., 2024; Liu et al., 2024) to enhance the reasoning capabities of LLMs. Over\nthe past few months, the paradigm gradually shifted. OpenAI o1 (Jaech et al., 2024) first showed\nthe tremendous potential of large-sacle RL for reasoning LLMs, and recent works have verified the\nscaling effect of the simple RL recipe with merely outcome rewards (DeepSeek-AI et al., 2025; Team\n12\n\nPreprint\net al., 2025). Meanwhile, the role of dense rewards in RL remains underexplored, which is the main\nfocus of PRIME.\nImplicit Rewards. Implicit rewards are broadly adopted in LLM alignment (Rafailov et al., 2023;\nChen et al., 2024b; Azar et al., 2024; Ethayarajh et al., 2024; Rosset et al., 2024; Chen et al., 2024a).\nRafailov et al. (2024) first showed that optimizing DPO objective learns a Q function implicitly. Zhou\net al. (2024) utilized implicit rewards in PPO, and showed that dense implicit rewards are better than\nsparse ones. Yuan et al. (2024b) further extended the conclusion to any loss funtion optimizing\nEq. 3.\n7\nCONCLUSION\nAs the fuel of LLMs, data, will be depleted in the near future, we are entering a new era of\nsearch and exploration, which is exemplified by reinforcement learning (Sutton, 2019). This work\ndevelops PRIME, which produces and leverages dense rewards in online RL for LLM reasoning.\nThroughout the experiments, we validate that PRIME (1) greatly benefits sample efficiency and policy\nperformance, (2) is easy to use with minimum cost, and (3) is a general method that works with broad\nRL algorithms together.\nREFERENCES\nArash Ahmadian, Chris Crem'),
                Paper(arxiv_id='2502.01341', authors=['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], published_at=datetime.datetime(2025, 2, 4, 10, 51, 54, 103000, tzinfo=datetime.timezone.utc), title='AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding', summary='Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.', upvotes=29, thumbnail=None, content='1. Introduction\nVision-Language Models (VLMs) have gained significant\ntraction in recent years as a powerful framework for multi-\nmodal document understanding tasks that involve interpret-\n1ServiceNow 2York University 3Mila 4 ´Ecole de Technolo-\ngie Sup´erieure\n5Universit´e de Montr´eal\n6McGill University\n7University of Waterloo\n8CIFAR AI Chair\n9Polytechnique\nMontr´eal 10University of British Columbia. Correspondence to:\nAhmed Masry <ahmed.masry@servicenow.com>, Sai Rajeswar\n<sai.mudumba@servicenow.com>.\nLlama-3.2-3B-Perciever R.\nLlama-3.2-3B-MLP\nLlama-3.2-3B-Ovis\nLlama-3.2-3B-Align (ours)\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\nDocVQA\nInfoVQA\nDeepForm\nKLC\nWTQ\nTabFact\nChartQA\nTextVQA\nTableVQA\nFigure 1: Performance of Different VLM Connectors.\nThe proposed ALIGN connector outperforms other methods\nacross benchmarks using the same training configuration.\nRadial distance is proportion of maximal score, truncated at\n0.7 (black dot).\ning both the visual and textual contents of scanned docu-\nments (Kim et al., 2022; Lee et al., 2023; Liu et al., 2023a;\n2024; Hu et al., 2024; Wang et al., 2023a; Rodriguez et al.,\n2024b). Such tasks are common in real-world commercial\napplications, including invoice parsing (Park et al., 2019),\nform reading (Jaume et al., 2019), and document question\nanswering (Mathew et al., 2021b). VLM architectures typ-\nically consist of three components: (i) a vision encoder to\nprocess raw images, (ii) a Large Language Model (LLM)\npre-trained on text, and (iii) a connector module that maps\nthe visual features from the vision encoder into the LLM’s\nsemantic space.\nA central challenge in this pipeline is to effectively map the\ncontinuous feature embeddings of the vision encoder into\nthe latent space of the LLM while preserving the semantic\nproperties of visual concepts. Existing approaches can be\nbroadly categorized into deep fusion and shallow fusion\n1\narXiv:2502.01341v1  [cs.CL]  3 Feb 2025\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nmethods. Deep fusion methods, such as NVLM (Dai et al.,\n2024), Flamingo (Alayrac et al., 2022), CogVLM (Wang\net al., 2023b), and LLama 3.2-Vision (Grattafiori et al.,\n2024), integrate visual and textual features by introducing\nadditional cross-attention and feed-forward layers at each\nlayer of the LLM. While effective at enhancing cross-modal\ninteraction, these methods substantially increase the param-\neter count of the VLM compared to the base LLM, resulting\nin high computational overhead and reduced efficiency.\nIn contrast, shallow fusion methods project visual features\nfrom the vision encoder into the LLM input embedding\nspace using either multilayer perceptrons (MLPs) (Liu et al.,\n2023b; 2024) or attention-based mechanisms such as the\nPerceiver Resampler (Li et al., 2023; Laurenc¸on et al., 2024;\nAlayrac et al., 2022), before concatenating them with the\ntextual prompt’s input embeddings. This approach is more\nparameter-efficient and computationally lighter than deep\nfusion methods, but it lacks a mechanism to ensure the pro-\njected embeddings remain within the region spanned by\nthe LLM’s text embeddings – i.e. regions the LLM was\npretrained to understand. As a result, unconstrained vi-\nsual features can produce out-of-distribution (OOD) and\nnoisy inputs, leading to misalignment between modalities\nand often degrading overall performance. Recent methods\nlike Ovis (Lu et al., 2024) attempt to alleviate these issues\nby introducing separate visual embeddings indexed from\nthe vision encoder outputs and combined together to con-\nstruct the visual inputs to the LLM. However, this approach\nsignificantly increases parameter count due to the massive\nembedding matrix and requires extensive training to learn a\nnew embedding space without guaranteeing alignment with\nthe LLM’s input latent space.\nTo address these limitations, this paper introduces ALIGN-\nVLM, a novel framework that sidesteps direct projection\nof visual features into the LLM embedding space. Instead,\nour proposed connector, ALIGN, maps visual features into\nprobability distributions over the LLM’s existing pretrained\nvocabulary embeddings, which are then combined into a\nweighted representation of the text embeddings. By con-\nstraining each visual feature as a convex combination of the\nLLM text embeddings, our approach leverages the linguistic\npriors already encoded in the LLM’s text space. This en-\nsures that the resulting visual features lie within the convex\nhull of the LLM’s embedding space, reducing the risk of\nnoisy or out-of-distribution inputs and improving alignment\nbetween modalities. Our experimental results show that\nthis approach improves performance on various document\nunderstanding tasks, outperforming prior connector meth-\nods by effectively fusing visual and linguistic content. We\nsummarize our main contributions as follows:\n• We propose a novel connector, ALIGN, to bridge the\nrepresentation gap between vision and text modalities.\n• We introduce a family of Vision-Language Models,\nALIGNVLM, that achieves state-of-the-art perfor-\nmance on multimodal document understanding tasks\nby leveraging ALIGN.\n• We conduct extensive experiments demonstrating the\nrobustness and effectiveness of ALIGN across different\nmodel sizes ranging from 1B to 8B parameters.\nOur code and models will be public upon acceptance.'),
                Paper(arxiv_id='2502.01534', authors=['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], published_at=datetime.datetime(2025, 2, 4, 1, 4, 33, 630000, tzinfo=datetime.timezone.utc), title='Preference Leakage: A Contamination Problem in LLM-as-a-judge', summary='Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between data\ngenerator LLM and judge LLM: being the same model, having an inheritance\nrelationship, and belonging to the same model family. Through extensive\nexperiments, we empirically confirm the bias of judges towards their related\nstudent models caused by preference leakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that preference leakage is a pervasive\nissue that is harder to detect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings imply that preference leakage\nis a widespread and challenging problem in the area of LLM-as-a-judge. We\nrelease all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage.', upvotes=28, thumbnail=None, content='Preference Leakage: A Contamination Problem in LLM-as-a-judge\nDawei Li * 1 Renliang Sun * 2 Yue Huang 3 Ming Zhong 4 Bohan Jiang 1\nJiawei Han 4 Xiangliang Zhang 3 Wei Wang 2 Huan Liu 1\nAbstract\nLarge Language Models (LLMs) as judges and\nLLM-based data synthesis have emerged as\ntwo fundamental LLM-driven data annotation\nmethods in model development. While their com-\nbination significantly enhances the efficiency of\nmodel training and evaluation, little attention has\nbeen given to the potential contamination brought\nby this new model development paradigm. In this\nwork, we expose preference leakage, a contami-\nnation problem in LLM-as-a-judge caused by the\nrelatedness between the synthetic data generators\nand LLM-based evaluators. To study this issue,\nwe first define three common relatednesses\nbetween data generator LLM and judge LLM:\nbeing the same model, having an inheritance\nrelationship, and belonging to the same model\nfamily.\nThrough extensive experiments, we\nempirically confirm the bias of judges towards\ntheir related student models caused by preference\nleakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that prefer-\nence leakage is a pervasive issue that is harder to\ndetect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings\nimply that preference leakage is a widespread\nand challenging problem in the area of LLM-\nas-a-judge.\nWe release all codes and data at:\nhttps://github.com/David-Li0406/\nPreference-Leakage1.\n1. Introduction\nRecent\nadvancements\nin\nLarge\nLanguage\nModels\n(LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong\net al., 2024; Zhang et al., 2024a) have empowered various\n*Equal contribution 1Arizona State University 2University of\nCalifornia, Los Angeles 3University of Notre Dame 4University\nof Illinois Urbana Champaign. Correspondence to: Dawei Li\n<daweili5@asu.edu>.\n1More resources on LLM-as-a-judge are on the website:\nhttps://llm-as-a-judge.github.io/\ndownstream tasks and applications. However, this also\nposes substantial challenges to the automatic evaluation\nof these models. Representatively, LLM-based AI agents’\nfocus transfer from traditional natural language processing\ntasks (Yang et al., 2023; Zhang et al., 2023) to real-world\n(Liu et al., 2023b; Huang et al., 2023), open-ended response\ngeneration (Wu et al., 2024), which greatly limits the\napplicability of traditional n-gram matching methods (e.g.,\nBLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu\net al., 2016; Reiter, 2018) or model-based evaluators (Zhang\net al., 2020; Zhong et al., 2022) for evaluation.\nTo address these challenges, the paradigm of LLM-as-a-\njudge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a;\nZhong et al., 2024; Li et al., 2025) has been proposed, de-\nsigned to leverage LLM as evaluators to assess response\nquality. By combining powerful LLMs with well-designed\nprompting strategies, LLM-as-a-judge enables human-like\nevaluation of long-form and open-ended generation in a\nmore cost-efficient and scalable manner. However, recent\nstudies point out some weaknesses of such assessment. For\ninstance, Ye et al. (2024) explores various biases and vulner-\nabilities of LLM-as-a-judge, highlighting the importance of\ndeveloping a reliable and fair LLM-based evaluation system.\nIn this work, we aim to introduce another concern in LLM-\nas-a-Judge–Preference Leakage. This issue arises when the\nLLMs used for data generation and evaluation are closely re-\nlated, as illustrated in Figure 1. Synthetic data generated by\nLLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c)\nhas become a cornerstone of model training (Lee et al.,\n2025). When combined with LLM-as-a-Judge, they offer\nsignificant efficiency gains in model development. However,\nlimited attention has been given to the potential contami-\nnation that occurs when the generator and evaluator LLMs\nshare a close relationship. During our preliminary study,\nwe find this issue is particularly pervasive in popular LLM-\nas-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al.,\n2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant\nstudies (more details can be found in Appendix A), due to\nthe common reliance on the most advanced LLMs, such\nas GPT-4 (Achiam et al., 2023), for both data synthesis\nand evaluation to ensure the highest quality outputs. In our\nwork, we reveal this relatedness—akin to the overlap be-\ntween training data and evaluation sets in traditional data\n1\narXiv:2502.01534v1  [cs.LG]  3 Feb 2025\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ncontamination—would introduce a systematic bias of judge\nLLMs towards their related student models (i.e., the model\ndistilled by the data generator which is related to the judge).\nCompared to other biases in LLM-as-a-Judge, such as length\nbias or egocentric bias (Ye et al., 2024; Panickssery et al.,\n2024), preference leakage is subtler and more challenging\nto detect, especially given that most LLMs do not disclose\ntheir training data.\nTo investigate and reveal the preference leakage problem,\nwe first define three relatednesses between data generator\nLLM and judge LLM: being the same model, having an\ninheritance relationship, and belonging to the same model\nfamily. Each of these scenarios is commonly encountered\nin real-world applications. Then, we pose and answer three\ncore research questions about preference leakage:\n• RQ1: Does preference leakage introduce systematic\nbiases in LLM-based evaluation? To answer it, we\nconduct experiments with various LLM baselines in two\nwidely recognized LLM-as-a-judge benchmarks, also in-\ntroduce the preference leakage score to quantify the bias\ncaused by preference leakage. The analysis results sug-\ngest an obvious bias of judging LLMs toward their related\nstudent models.\n• RQ2: What is the severity of preference leakage under\nvarious scenarios? We conduct experiments under vari-\nous relatedness settings, tuning techniques, and data mix-\ning strategies to address it, finding that preference leakage\nconsistently affects judge LLMs. Moreover, the severity\nof preference leakage correlates with the degree of relat-\nedness between the data generator and LLM judges, as\nwell as the proportion of synthetic data.\n• RQ3: What are the underlying mechanisms causing\npreference leakage? For this question, we analyze LLMs’\nrecognition capabilities on their related student models’\ngeneration as well as the distribution of bias across differ-\nent question types and judgment dimensions. The analysis\nreveals that preference leakage is a subtle, hard-to-detect\nissue, particularly affecting subjective questions and judg-\nment dimensions.\nTo summarize, our contributions in this work are as follows:\n• We introduce preference leakage, a contamination issue\narising from the relatedness between the data generator\nand judge LLMs.\n• We conduct extensive experiments across various LLMs\nand benchmarks to study how and to what extent the\npotential bias brought by preference leakage influences\njudgment.\n• Our further analysis reveals that preference leakage is\nprevalent in diverse scenarios and difficult for judge LLMs\nto detect, providing valuable insights for future research\non this challenging issue.\n2. Related Work\n2.1. LLM-as-a-Judge\nLLM-as-a-Judge, introduced by Zheng et al. (2023), lever-\nages LLMs to automatically evaluate responses and assign\nrewards. This approach has gained widespread adoption\nin areas such as model alignment (Zhang et al., 2024d)\nand benchmarking (Liu et al., 2023a; Zhang et al., 2024b;\nGao et al., 2023; Zhong et al., 2024), driving significant\nprogress in the field. Building on this concept, Zhuge et al.\n(2024) proposed Agent-as-a-Judge, where agentic systems\nare employed to evaluate other agentic systems. Addition-\nally, Prometheus, a series of open-source LLMs tailored for\nLLM-as-a-Judge (Kim et al., 2023; 2024), addresses the\nprohibitive costs associated with proprietary models, further\ndemocratizing the technology.\nDespite its promising potential, recent studies have high-\nlighted the vulnerabilities and limitations of LLM-as-a-\nJudge. Notable concerns include biases during evaluation.\nFor example, Zheng et al. (2023) identify position bias,\nwhere LLMs may favor responses based on their order in\nthe input, thereby compromising fairness. Other studies (Ye\net al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al.,\n2023; Huang et al., 2024) further emphasize the risks of\nevaluation biases. Thakur et al. (2024) assessed the judg-\nment capabilities of LLM judges, finding that only the most\nadvanced models align reasonably well with human evalu-\nators. Moreover, a recent study (Shi et al., 2024) revealed\nthe susceptibility of LLM-as-a-Judge to adversarial attacks,\nleading to incorrect judgments. In this paper, we explore an-\nother critical vulnerability of LLM-as-a-Judge—preference\nleakage—which poses additional risks to the reliability of\nthis evaluation paradigm.\n2.2. Data Leakage\nThe possible overlap between training data and evaluation\nbenchmarks has become a central issue, since LLMs are usu-\nally trained on extensive web corpora (Dodge et al., 2021).\nThis phenomenon, known as data leakage, can artificially\nimprove the performance of LLMs and undermine the re-\nliability of the assessment (Deng et al., 2024a; Jiang et al.,\n2024b).\nSeveral researchers have proposed methods to detect and\nmitigate data contamination. Deng et al. (2024b) proposed\na retrieval-based approach to assess the degree of overlap\nbetween pre-training text and benchmark data. Golchin &\nSurdeanu (2023) have developed “guided instruction” to\nflag contaminated instances. Dong et al. (2024b) proposed\nthe CDD method to identify peaks in the output distribution\nto detect data contamination. Several studies analyze data\nleakage for specific LLMs (Balloccu et al., 2024) and report\ncontamination such as cross-language contamination (Yao\n2\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nEvaluation\nTestset\nTraining\nCorpus\nData leakage\nTrain\nTraining\nCorpus\nEvaluation \nTestset\nEvaluate\nData Leakage!\nSynthetic \nData\nData \nGenerator\nTrained\nModel\nTrained\nModel\nTrained\nModel\nJudge\nJudge\nModel\nPreference Leakage!\nRelatedness \nOverlap\nLLM for Data \nSynthesis\nLLM-as-\na-Judge\nPreference leakage\nTrain\n(1). Same model\n(2). Inheritance\nSynthetic \ndata\n(3). Within the \nsame model family\nSynthesize\nFigure 1. Overview of preference leakage. We make a comparison between data leakage and preference leakage and present three types of\nrelatedness: being the same model, having an inheritance relationship and belonging to the same model family.\net al., 2024) and task contamination (Li & Flanigan, 2024)\nthat can evade traditional detection methods. To address data\ncontamination issues, Ni et al. (2024) have used web user\nquery detection and benchmark mixture. White et al. (2024)\nuse the most recent information to update the problem.\n3. Preference Leakage\nIn this section, we first provide the formal definition of data\ncontamination as the preliminary (Section 3.1). Based on\nthe concept, we demonstrate how LLM-based data synthesis\nand evaluation can lead to the evolving preference leakage\nproblem (Section 3.2).\n3.1. Preliminary: Data Leakage\nData leakage, also known as data contamination, refers to\nthe inadvertent inclusion of information from the evalua-\ntion benchmarks into the training corpus thus creating an\noverlap between training and testing sets (Kaufman et al.,\n2012). This overlap would significantly influence the eval-\nuation fairness by inflating the models’ performance since\nthe model has prior exposure to and memorized information\nthat it’s expected to generalize during testing (Elangovan\net al., 2021).\nFormally, let T represent the training corpus and E be the\nevaluation set during test time. Data contamination occurs\nif:\nT ∩E ̸= ∅,\n(1)\nwhere ∩denotes the intersection between the two sets. Such\noverlap violates the fundamental assumption that training\nand testing datasets should be disjoint to ensure an unbiased\nassessment of the model’s generalization ability.\n3.2. From Data Leakage to Preference Leakage\nWith the advent of LLMs, synthetic data generated by these\nmodels (Tan et al., 2024) has been widely adopted in var-\nious stages of model training, including pre-training, rein-\nforcement learning with AI feedback (RLAIF) and super-\nvised fine-tuning. Concurrently, the concept of LLM-as-\na-judge has emerged, where LLMs are employed to auto-\nmate the evaluation process. While these LLM-as-an-oracle\napproaches reduce human effort in data annotation, signif-\nicantly enhancing the efficiency and scalability of model\ntraining and evaluation, they also blur the lines between\nmodels and data, introducing evolving challenges (Shu-\nmailov et al., 2024; Dai et al., 2024).\nIn this work, we examine the evolving contamination prob-\nlem brought by LLM-as-a-oracle and formally propose the\nconcept of preference leakage. This refers to a situation\nin which the LLMs used for synthetic data generation and\nevaluation are related. Formally, we define this as:\nLLMG ∩LLMJ ̸= ∅,\n(2)\nwhere LLMG and LLMJ denote the LLMs used for train-\ning data generation and evaluation. ∩represents the related-\nness between the two (sets of) LLMs. This relatedness may\ninvolve:\n• Being the same model: the data generator and evaluator\nare the same model:\nLLMG = LLMJ.\n(3)\n• Inheritance relationship: one model is trained on syn-\nthetic data generated by the other:\nLLMG = Inherit(LLMJ),\n(4)\nLLMJ = Inherit(LLMG).\n(5)\n3\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n• Within the same model family: the data generator and\nevaluator belong to the same model family (e.g., GPT\nfamily (Achiam et al., 2023) and Gemini family (Team\net al., 2024)):\nLLMG, LLMJ ∈FX.\n(6)\nDue to this relatedness, the preference of the judge models\n(e.g., format, style and wording) can be leaked to the student\nmodels through the synthetic data, resulting in non-trivial\nbias from the judge LLMs during the test time.\n4. Main Experiment\n4.1. Experiment Setup\nModels. We choose three powerful LLMs as data generator/\njudge models. They are GPT-4o-2024-11-20 (Achiam et al.,\n2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA-\n3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student\nmodel, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and\nQwen-2.5-14B (Yang et al., 2024). To avoid potential prefer-\nence leakage due to distilling data from other LLMs during\nthe instruction-tuning process, we choose to use the -PRE-\nTRAINED version rather than the -INSTRUCT version of\nthese student models.\nEvaluation Datasets. We choose two representative pair-\nwise evaluation datasets, Arena-Hard (Li et al., 2024e)\nand AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the\ntrained student models. Arena-Hard includes 500 challeng-\ning questions in English. Additionally, the evaluation agree-\nment between Arena-Hard and Chatbot Arena (Zheng et al.,\n2023)’s hard prompts achieved a 96.7% Spearman corre-\nlation, demonstrating the consistency of Arena-Hard with\nhuman preferences (Li et al., 2024e). AlpacaEval 2.0 is an\nimproved evaluation method based on AlpacaEval (Li et al.,\n2023) and contains 805 questions. Compared to version 1.0,\nAlpacaEval 2.0 significantly reduces the effect of text length\non the evaluation results.\nImplementation Details. In our main experiment, we ex-\namine the preference leakage introduced by using the same\ndata generator and evaluator in supervised fine-tuning (SFT).\nWe will discuss other relatedness and learning methods in\nSection 5. To obtain synthetic datasets, We first randomly\nsample 30,000 prompts from the Ultrafeedback dataset (Cui\net al., 2024). The Ultrafeedback dataset includes instruc-\ntions from several publicly available high-quality datasets\nsuch as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al.,\n2023), and Evol-Instruct (Xu et al., 2023). For each data gen-\nerator model, we provide these prompts for them to produce\nsynthetic responses, resulting in three synthetic instruction\ndatasets. We then use each dataset to supervised fine-tune\nthe student model, obtaining three different versions for each\nbaseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-Gemini-\n1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each\ntwo student models and obtain three model pairs. For each\nmodel pair, we perform the pairwise comparison using the\nthree judge models respectively.\nMetrics & Annotation Based on our hypothesis, preference\nleakage would lead to bias of judge LLMs towards their\nrelated student models. Following this principle, we design\nthe preference leakage score PLS(i, j) to measure the bias\nin model pair (i, j) caused by preference leakage:\nPLS(i, j) =\n\x10\nWR(i,i)−AVG(i,j)\nAVG(i,j)\n\x11\n+\n\x10\nWR(j,j)−AVG(j,i)\nAVG(j,i)\n\x11\n2\n,\n(7)\nAVG(i, j) = WR(i, i) + WR(i, j)\n2\n.\n(8)\nHere WR(i, j) represents the win-rate score from judge\nmodel i to student model j. Intuitively, a large preference\nleakage score indicates that the two judge models demon-\nstrate strong bias toward their related student models, sug-\ngesting a significant preference leakage phenomenon.\nWhile our proposed preference leakage score quantifies the\ndegree of preference leakage in each model pair, we also\nperform manual annotation to assess the preference leakage\nin each individual model. We randomly select 100 questions\nfrom AlpacaEval 2.0 and have three well-trained annota-\ntors perform pairwise comparisons independently for each\nresponse pair. After the annotation, the majority voting is\napplied to each response pair to get the final annotation\nresults.\nMore details about model training, metric explanation, and\nannotation process can be found in Appendix B.\nModel\nData Generator/ Judge Pair\nArena-Hard\nAlpacaEval 2.0\nAvg.\nGPT-4o & Gemini-1.5\n28.7%\n18.4%\n23.6%\nGPT-4o & LLaMA-3.3\n-6.7%\n1.4%\n-2.7%\nMistral-7B\nLLaMA-3.3 & Gemini-1.5\n13.1%\n19.8%\n16.4%\nGPT-4o & Gemini-1.5\n37.1%\n18.6%\n27.9%\nGPT-4o & LLaMA-3.3\n1.0%\n2.3%\n1.7%\nQwen-2.5-14B\nLLaMA-3.3 & Gemini-1.5\n25.4%\n18.4%\n21.9%\nTable 1. Preference leakage score result on Arena-Hard and Al-\npacaEval 2.0. The blue background indicates a negative prefer-\nence leakage score value and the purple background indicates a\npositive value. The deeper the color, the larger the absolute value.\n4.2. Main Results\nIn our main experiment, we aim to provide insights into\nRQ1.\nPreference leakage exists in most model pairs. The origi-\nnal judgment results from Arena-Hard and AlpacaEval 2.0,\nalong with the calculated preference leakage scores, are\nshown in Figure 2, Figure 3, and Table 1. As the results\ndemonstrate, in most model pairs (except Mistral-GPT-4o vs\n4\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n18.2%\n39.8%\n42.0%\n27.4%\n43.8%\n28.8%\n38.4%\n34.6%\n27.0%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n46.2%\n42.7%\n11.1%\n50.4%\n35.0%\n14.6%\n55.8%\n27.0%\n17.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n9.2%\n31.4%\n59.4%\n14.6%\n30.0%\n55.4%\n22.2%\n30.8%\n47.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n22.0%\n33.5%\n44.5%\n28.8%\n50.2%\n21.6%\n49.8%\n29.0%\n21.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n52.1%\n40.7%\n7.2%\n39.0%\n51.8%\n9.2%\n57.4%\n29.6%\n13.0%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n10.0%\n29.4%\n60.6%\n16.4%\n48.4%\n35.2%\n24.6%\n30.0%\n44.4%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nTie\nModel B Wins\nFigure 2. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n36.8%\n63.2%\n49.5%\n50.5%\n55.1%\n44.9%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n65.8%\n34.2%\n60.3%\n39.7%\n61.6%\n38.4%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n22.6%\n77.4%\n39.5%\n60.5%\n43.1%\n56.9%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n39.3%\n60.7%\n52.4%\n47.6%\n57.8%\n42.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n63.3%\n36.7%\n59.3%\n40.7%\n61.5%\n38.5%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n26.2%\n73.8%\n42.9%\n57.1%\n50.1%\n49.9%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nModel B Wins\nFigure 3. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in\nAlpacaEval 2.0.\nMistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-\n3.3), the judge LLMs exhibit a strong preference toward\ntheir related student models, leading to large positive val-\nues in the preference leakage scores. This finding suggests\nthat preference leakage, along with the resulting bias, is\nwidespread in SFT when the data generator and evaluator\nare the same.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n73.6%\n8.8%\n17.6%\n79.5%\n1.7%18.8%\nLLaMA-2 vs Others\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n76.2%\n17.9% 6.0%\n79.8%\n20.2%0.0%\nJudge Model\nLLaMA-2 vs Claude-v1\nModel A Wins\nTie\nModel B Wins\nFigure 4. Comparison between GPT-4 and human’s judgment for\nLLaMA-2 from MTBench.\nEvaluators’ bias towards certain LLMs can be inherited\nby its student models. From Figure 2 and Figure 3, we find\nan obvious preference of GPT-4o towards Mistral/ Qwen-\nLLaMA-3.3 and this leads to the low preference leakage\nscore in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and\nQwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate\nthe source of this preference, we examine whether the GPT-\n4 evaluator has a bias toward LLaMA series models. Using\nthe MTBench (Zheng et al., 2023) dataset, which includes\npairwise comparison judgments from both humans and GPT-\n4, we compare GPT-4’s and human evaluators’ judgments\non LLaMA-2 vs other models (including Vicuna, Alpaca,\nGPT-3.5, and GPT-4, which are preferred by GPT-4 due\nto preference leakage or egocentric bias) and LLaMA-2 vs\nClaude. The results, shown in Figure 4, reveal a clear pref-\nerence for LLaMA-2 by GPT-4. Consequently, we conclude\nthat evaluators’ bias can be inherited. In this case, GPT-4’s\nbias toward LLaMA has been passed on to LLaMA’s stu-\ndent models. This inheritance, combined with the opaque\ntraining data of LLMs, makes preference leakage a more\ncomplex and challenging problem.\nModel pairs with similar performance tend to have more\n5\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n53.0%\n47.0%\n40.2%\n59.8%\n49.4%\n50.6%\n58.4%\n41.6%\nJudge Model\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n62.0%\n38.0%\n76.2%\n23.8%\n72.1%\n27.9%\n67.8%\n32.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n36.0%\n64.0%\n17.1%\n82.9%\n39.0%\n61.0%\n46.0%\n54.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\nModel A Wins\nModel B Wins\nFigure 5. Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0.\nobvious preference leakage. As shown in Table 1, we ob-\nserve that the preference leakage scores for Mistral-GPT-4o\nvs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini-\n1.5 (23.6% and 27.9% respectively) are consistently higher\nthan that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9%\nrespectively). We think that this is largely due to the more\ncomparable performance between the student models of\nGPT-4o and Gemini-1.5. Intuitively, when the quality of the\ntwo responses is similar, the evaluator may rely more heav-\nily on its inherent preferences to make a judgment, thereby\nexacerbating the preference leakage issue.\nLarger student models cause more bias from judge\nLLMs. Another observation from Table 1 is that the over-\nall preference leakage score for Qwen-2.5-14B is higher\nthan that for Mistral-7B. Drawing on insights from previous\nstudies on data leakage, which suggest that larger and more\npowerful LLMs are more capable of memorizing extensive\ninformation and are thus more susceptible to data contamina-\ntion (Bordt et al., 2024; Duan et al., 2024), we attribute this\ndifference in preference leakage to the size and capabilities\nof the student LLMs. We assume that larger student models,\ndue to their better performance and generalization abilities,\nare more capable of learning and memorizing the hidden\npreference pattern from the synthetic data, thus leading to a\nmore serious preference leakage.\nDifferent data generator/ judge LLMs result in varying\ndegrees of bias under preference leakage. While we have\nconcluded that student model pairs with similar performance\nor more powerful student models tend to exhibit greater\npreference leakage, we also examine whether different data\ngenerator and judge LLMs contribute to varying degrees\nof preference leakage. Analyzing the manual annotation\nresults presented in Table 5, we observe that Gemini-1.5\nshows a strong bias toward its students, followed by GPT-4o,\nwith LLaMA-3.3 displaying the least bias. This variation in\npreference leakage may stem from differences in the level\nof leaked preference in the synthetic responses generated\nby the data generator LLMs. For instance, an LLM with a\ndistinctive style or format in its responses offers more op-\nportunities for student models to learn these characteristics,\npotentially leading to more pronounced preference leakage\nduring evaluation. Future work could further quantify the\nextent of leaked preference for each data generator model.\n5. Further Analysis\nIn this section, we conduct relatedness analysis, learning\nmethod analysis and data mixing analysis (Section 5.1 - 5.3)\nto answer RQ2. Due to the cost consideration, we conduct\nthese analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5.\nMoreover, we perform recognition analysis and category\nanalysis to answer RQ3.\nArena-Hard AlpacaEval 2.0\nAvg.\nSame Model\n28.7%\n18.4%\n23.6%\nInheritance\nw/ same ins.\n17.8%\n20.7%\n19.3%\nInheritance\nw/ different ins.\n18.3%\n26.3%\n22.3%\nSame Family\nw/ same series\n10.1%\n7.6%\n8.9%\nSame Family\nw/ different series\n3.3%\n2.2%\n2.8%\nTable 2. Preference leakage score in different relatedness between\nthe data generator and the judging LLM.\n5.1. Relatedness Analysis\nWe demonstrate the impact of different relatedness condi-\ntions between the data generator and the judge LLM on the\npreference leakage problem, as shown in Table 2.\nPreference leakage under inheritance settings causes ob-\nvious bias of judges towards their related students. For\nthe inheritance relationship, we consider the situation where\nthe data generator is inherited from the judge model. We\nconducted the following two experiments: (1). we give the\nsame instructions again as in the SFT stage (Inheritance w/\nsame ins.), or (2). we sample the same number of different\ninstructions from the Ultrafeedback (Inherence w/ different\nins.). Then, we let the fine-tuned Mistral model generate\nthe answers and use these generated data to fine-tune a new\nMistral student model. From the results, with the same in-\nstructions, the average preference leakage score is 19.3%. In\ncomparison, the score with different instructions is 22.3%.\n6\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFirstly, in an inheritance setting, data generators can inherit\njudges’ preferences, which are then passed on to new stu-\ndent models, thereby compromising the fairness of their\nevaluation. Second, even when different instructions are\nused, judges’ preferences leaked to data generators can still\nbe transferred to the new student model through synthetic\ndata, leading to a high preference leakage score.\nModels within the same series tend to cause more sig-\nnificant bias. For two models within the same family, we\nconsider two settings: (1) Same series, where training data\nis generated by GPT-4o and Gemini-1.5-flash, and judged\nby GPT-4-turbo and Gemini-1.5-pro; (2) Different series,\nwhere training data is still generated by GPT-4o and Gemini-\n1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro.\nIn the same series setting, the average preference leakage\nscore is 8.9%, indicating that despite using different mod-\nels for data generation and judgment, their relatedness in\nterms of model family leads to some preference leakage.\nIn contrast, the different series setting yields a significantly\nlower leakage score of 2.8%, likely due to differences in\narchitecture, training data, and other factors, reducing the\ninfluence of model-related biases in evaluation.\nArena-Hard\nAlpacaEval 2.0\nAvg.\nSFT\n28.7%\n18.4%\n23.6%\nDPO\n7.7%\n2.7%\n5.2%\nICL\n-4.2%\n-1.1%\n-2.7%\nTable 3. Preference leakage score in different learning methods.\n5.2. Learning Method Analysis\nWe also compare three learning methods, supervised\nfine-tuning (SFT), direct preference optimization (DPO)\n(Rafailov et al., 2024), and in-context learning (ICL) (Dong\net al., 2024a), to explore the different influences to them un-\nder preference leakage. We first build a data pool based on\nhuman-written instruction-tuning data from OASST (K¨opf\net al., 2024), LIMA (Zhou et al., 2024), and MOSS (Sun\net al., 2024b) to supervised fine-tune the pre-trained model.\nFor DPO, we sample 2 responses for each instruction from\nsampled UltraFeedback instruction and prompt each data\ngenerator to produce the pairwise feedback. Then we use\nthe DPO loss to further train the fine-tuned policy on each\nsynthetic pairwise dataset. Appendix C shows the prompt\nwe use to craft synthetic pairwise feedback. For ICL, we\nsample 4 instruction-response pairs from each LLMs’ syn-\nthetic dataset as the demonstration during inference.\nTuning approaches would leak judges’ preference to the\nstudent models. Various learning methods show significant\ndifferences in preference leakage scores across learning\nmethods. SFT exhibits the highest average leakage score at\n23.6%. In contrast, DPO achieves a much lower score of\n5.2%, likely because its focus on preferences helps minimize\nthe unintended transfer of judge model biases. Meanwhile,\nICL, which relies on contextual examples without updating\nmodel parameters, is least affected by the data generator’s\npreferences, resulting in the lowest leakage scores.\n20\n40\n60\n80\n100\nContamination Ratio (%)\n0\n5\n10\n15\n20\n25\n30\nPreference Leakage Score (%)\nAlpacaEval2.0 - Manual\nArenaHard - Manual\nAlpacaEval2.0 - Synthetic\nArenaHard - Synthetic\nFigure 6. Experiment results on data mixing. ‘Manual’ represents\nthe original synthetic data mixed with manually-written data. ‘Syn-\nthetic’ represents the original data mixed with other synthetic data.\n5.3. Data Mixing Analysis\nIn real-world applications, synthetic data from a single LLM\nis often mixed with manually-written data or other multi-\nsource synthetic data to train student models. To mimic\nthese scenarios and explore how much synthetic data could\nlead to preference leakage, we conduct a data mixing anal-\nysis. Specifically, we randomly sample 10%, 30%, 50%,\nand 70% from the original synthetic dataset and mix it with\nmanually-written data and multi-source synthetic data, re-\nspectively, in order to maintain a consistent total volume of\ntraining data (30,000). For the manually-written data, we\nsample from the data pool collected in Section 5.2. For the\nmulti-source synthetic data, we use the original synthetic\ndata from Ultrafeedback, which includes responses gener-\nated by various LLMs (e.g., WizardLM, Flcon, etc.). After\nobtaining the mixing training data, we train the student mod-\nels using SFT and calculate their preference leakage scores\nbased on the judgment results. Figure 6 presents the results\nwith two mixing strategies across two benchmarks.\nThe degree of preference leakage is directly proportional\nto the amount of synthetic data. We observe a strong\ncorrelation between the proportion of synthetic data in the\nmixture and the preference leakage score, with no clear\nthreshold separating cases with preference leakage from\nthose without. This suggests that preference leakage can\noccur even with a small amount of leaked synthetic data,\nposing significant challenges for its detection.\n5.4. Can Judges Recognize Student Models?\nPrevious studies demonstrate the LLM judges can recog-\nnize and thus prefer their own generation (Panickssery et al.,\n2024). In this work, we pose a similar question: Does prefer-\nence leakage also source from the LLM judges’ recognition\n7\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nMathematics\nBusiness\nDaily Life\nScience\nWriting\nOthers\nProgramming\n0\n10\n20\n30\nPreference Leakage Score (%)\n7.7\n16.5\n17.2\n17.3\n21.0\n23.8\n31.4\n(a) Question Type\nCompleteness\nClarity\nRichness\nSatisfaction\nFactuality\nLogical\nOthers\nCreativity\nFairness\n20\n24\n28\n32\nPreference Leakage Score (%)\n27.9\n28.6\n28.8\n29.0\n29.2\n30.2\n30.4\n30.7\n32.4\n(b) Judgment dimension\nFigure 7. Category analysis results on question type and judgment dimension.\nTask\nModel\nAccuracy\nStudent Recognition\nGPT-4o\n60.0%\nGemini-1.5\n25.4%\nLLaMA-3.3\n54.2%\nResponse Classification\nBERT\n82.4%\nTable 4. Student recognition (binary classification) and response\nclassification results (three-class classification).\nof their related student models’ generation? To study this,\nwe follow Panickssery et al. (2024) to prompt the three\njudge LLMs and test whether they could recognize their\nrelated student models’ generation. Additionally, we split\nthree student models’ generation into training and testing\nsets, and train a BERT classifier to perform a three-class\nclassification inspired by the previous study on detecting\nhuman-AI text (Zhang et al., 2024c). Detailed instruction\nand training settings can be found in Appendix D.\nJudge LLMs do not show good performance in recogniz-\ning the generation of their student models. As the result\npresented in Table 4, we find that the recognition perfor-\nmance of each judge LLM in the content of related students\nis poor, with accuracy around the performance of random\nguess. Moreover, we observe no correlation between recog-\nnition performance and the preference leakage degree for\njudge LLMs. For instance, while Gemini-1.5 leads to the\nmost preference leakage (as shown in Section 4.2), it per-\nforms the worst in recognition tasks. These suggest that\npreference leakage is subtler and harder-to-detect for judge\nLLMs, in contrast to the more obvious egocentric bias.\nCertain features embedded in student models through\nsynthetic data. Although judge LLMs do not perform\nwell in related student recognition, we notice the fine-tuned\nBERT classification demonstrates a high accuracy score in\nclassifier response generated by each student model. This\nsuggests that certain characteristics—such as style and for-\nmat—are embedded in the student models through the syn-\nthetic responses. This finding further supports the existence\nof preference leakage and lays the groundwork for future\nresearch aimed at detecting and preventing it.\n5.5. Impact on Question Type & Judgment Dimension\nIn this section, we explore the impact of preference leakage\nacross various question types and judgment dimensions. For\nthe question type analysis, we first propose several general\nquestion types based on the question clusters introduced by\nArena-Hard. Then, we prompt GPT-4o to map each question\nin Arena-Hard and AlpacaEval to one of the question types\nand calculate the preference leakage score for each question\ncategory. For the judgment dimension analysis, we follow\nthe judgment dimensions introduced by Liu et al. (2023a)\nand also utilize GPT-4o to map the rationale generated by\njudge LLMs to one or multiple judgment dimensions. More\ndetailed prompt can be found in Appendix E. The analysis\nresults are presented in Figure 7.\nSubjective question and judgment dimension tend to\nlead to more bias. For question type analysis, we find ob-\njective questions with a definitive answer, like mathematical\nones, demonstrate the least preference leakage. By contrast,\nsubjective questions that have more than one standard an-\nswer, such as programming and writing, usually lead to a\nmore obvious preference leakage. This observation is also\napplied to judgment dimension analysis, as objective di-\nmensions (like completeness) have an overall lower leakage\ndegree compared with subjective ones (like fairness). This\nsuggests that preference leakage tends to be more significant\nin objective questions and dimensions, where the contami-\nnated model is more likely to receive biased preference.\n6. Conclusion\nIn this work, we formally highlight the preference leakage\nproblem in LLM-as-a-judge systems. The results of our\nmain experiment, measured using the proposed preference\nleakage score, reveal a clear bias in each judge toward its\nrespective student model. We also observe that this bias\nis more pronounced in comparable model pairs and larger\nstudent models. Furthermore, we conduct additional anal-\nysis on various factors, including the relationship between\nthe data generator and judge LLMs, model tuning tech-\n8\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nniques, and data mixing strategies. Our findings suggest\nthat preference leakage can cause significant bias across\ndiverse scenarios. Finally, through recognition and category\nanalyses, we investigate the underlying mechanisms of pref-\nerence leakage, demonstrating that it is a challenging and\nhard-to-detect issue, especially in subjective questions and\njudgment dimensions. In the future, we aim to explore meth-\nods for detecting, preventing, and mitigating this evolving\nchallenge in LLM-as-a-judge systems.\nImpact Statements\nBy revealing preference leakage, this work could help build\nmore trustworthy and ethically grounded AI systems. The\nrelatedness between data generators and evaluators can sys-\ntematically bias evaluations, potentially compromising the\nfairness and reliability of the automatic evaluation paradigm.\nThese biased evaluations may indirectly affect downstream\ntasks such as AI alignment and decision-making systems,\nleading to unintended ethical risks. To mitigate preference\nleakage, we hope that researchers will propose more reli-\nable evaluation methods, diversify training data sources, and\ndevelop contamination-resistant benchmarks in the future.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. ArXiv preprint,\nabs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nBalloccu, S., Schmidtov´a, P., Lango, M., and Duˇsek, O.\nLeak, cheat, repeat: Data contamination and evaluation\nmalpractices in closed-source llms. In Proceedings of the\n18th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pp. 67–93, 2024.\nBordt, S., Nori, H., and Caruana, R. Elephants never forget:\nTesting language models for memorization of tabular data.\nIn NeurIPS 2023 Second Table Representation Learning\nWorkshop, 2024.\nChen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B.\nHumans or llms as the judge? a study on judgement\nbiases. arXiv preprint arXiv:2402.10669, 2024.\nCui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y.,\nXie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting\nlanguage models with scaled ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nDai, S., Xu, C., Xu, S., Pang, L., Dong, Z., and Xu, J. Bias\nand unfairness in information retrieval systems: New\nchallenges in the llm era. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 6437–6447, 2024.\nDeng, C., Zhao, Y., Heng, Y., Li, Y., Cao, J., Tang, X.,\nand Cohan, A. Unveiling the spectrum of data contami-\nnation in language models: A survey from detection to\nremediation. arXiv preprint arXiv:2406.14644, 2024a.\nDeng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A.\nInvestigating data contamination in modern benchmarks\nfor large language models. In Proceedings of the 2024\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pp. 8698–8711,\n2024b.\nDodge, J., Sap, M., Marasovi´c, A., Agnew, W., Ilharco, G.,\nGroeneveld, D., Mitchell, M., and Gardner, M. Docu-\nmenting large webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1286–1305, 2021.\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia,\nH., Xu, J., Wu, Z., Chang, B., et al. A survey on in-\ncontext learning. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing,\npp. 1107–1128, 2024a.\nDong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and\nLi, G. Generalization or memorization: Data contamina-\ntion and trustworthy evaluation for large language models.\narXiv preprint arXiv:2402.15938, 2024b.\nDuan, S., Khona, M., Iyer, A., Schaeffer, R., and Fiete, I. R.\nUncovering latent memories: Assessing data leakage and\nmemorization patterns in large language models. arXiv\npreprint arXiv:2406.14549, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nElangovan, A., He, J., and Verspoor, K. Memorization vs.\ngeneralization: Quantifying data leakage in nlp perfor-\nmance evaluation. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pp. 1325–1335, 2021.\nGan, R., Wu, Z., Sun, R., Lu, J., Wu, X., Zhang, D.,\nPan, K., Yang, P., Yang, Q., Zhang, J., et al. Ziya2:\nData-centric learning is all llms need. arXiv preprint\narXiv:2311.03301, 2023.\n9\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nGao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan,\nX. Human-like summarization evaluation with chatgpt.\narXiv preprint arXiv:2304.02554, 2023.\nGolchin, S. and Surdeanu, M. Time travel in llms: Trac-\ning data contamination in large language models')]}
2025-02-06 00:46:32,186 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:46:59,892 - INFO - Total execution time: 27.04 seconds (0.45 minutes)
2025-02-06 00:46:59,903 - INFO - Papers: {'2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=126, thumbnail=None, content='OmniHuman-1: Rethinking the Scaling-Up of One-Stage\nConditioned Human Animation Models\nGaojie Lin∗\nJianwen Jiang∗†\nJiaqi Yang∗\nZerong Zheng∗\nChao Liang\nByteDance\nhttps://omnihuman-lab.github.io/\nFigure 1. The video frames generated by OmniHuman based on input audio and image. The generated results feature head and gesture\nmovements, as well as facial expressions, that match the audio. OmniHuman generates highly realistic videos with any aspect ratio and body\nproportion, and significantly improves gesture generation and object interaction over existing methods, due to the data scaling up enabled by\nomni-conditions training.\nAbstract\nEnd-to-end human animation, such as audio-driven talking\nhuman generation, has undergone notable advancements in\nthe recent few years. However, existing methods still strug-\ngle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper,\nwe propose OmniHuman, a Diffusion Transformer-based\n∗Equal contributions\n†Project lead\nframework that scales up data by mixing motion-related con-\nditions into the training phase. To this end, we introduce two\ntraining principles for these mixed conditions, along with\nthe corresponding model architecture and inference strategy.\nThese designs enable OmniHuman to fully leverage data-\ndriven motion generation, ultimately achieving highly realis-\ntic human video generation. More importantly, OmniHuman\nsupports various portrait contents (face close-up, portrait,\nhalf-body, full-body), supports both talking and singing, han-\ndles human-object interactions and challenging body poses,\n1\narXiv:2502.01061v1  [cs.CV]  3 Feb 2025\n\nand accommodates different image styles. Compared to exist-\ning end-to-end audio-driven methods, OmniHuman not only\nproduces more realistic videos, but also offers greater flexi-\nbility in inputs. It also supports multiple driving modalities\n(audio-driven, video-driven and combined driving signals).\nVideo samples are provided on the project page.\n1. Introduction\nSince the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15'),
                Paper(arxiv_id='2502.01237', authors=['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], published_at=datetime.datetime(2025, 2, 4, 3, 10, 49, 348000, tzinfo=datetime.timezone.utc), title='The Differences Between Direct Alignment Algorithms are a Blur', summary='Direct Alignment Algorithms (DAAs) simplify language model alignment by\nreplacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement\nLearning from Human Feedback (RLHF) with direct policy optimization. DAAs can\nbe classified by their ranking losses (pairwise vs. pointwise), by the rewards\nused in those losses (e.g., likelihood ratios of policy and reference policy,\nor odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that one-stage methods underperform\ntwo-stage methods. To address this, we incorporate an explicit SFT phase and\nintroduce the beta parameter, controlling the strength of preference\noptimization, into single-stage ORPO and ASFT. These modifications improve\ntheir performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT),\nmatching two-stage methods like DPO. Further analysis reveals that the key\nfactor is whether the approach uses pairwise or pointwise objectives, rather\nthan the specific implicit reward or loss function. These results highlight the\nimportance of careful evaluation to avoid premature claims of performance gains\nor overall superiority in alignment algorithms.', upvotes=100, thumbnail=None, content='The Differences Between Direct Alignment Algorithms are a Blur\nAlexey Gorbatovski 1 Boris Shaposhnikov 1 Viacheslav Sinii 1 Alexey Malakhov 1 Daniil Gavrilov 1\nAbstract\nDirect Alignment Algorithms (DAAs) simplify\nlanguage model alignment by replacing reinforce-\nment learning (RL) and reward modeling (RM) in\nReinforcement Learning from Human Feedback\n(RLHF) with direct policy optimization. DAAs\ncan be classified by their ranking losses (pair-\nwise vs. pointwise), by the rewards used in those\nlosses (e.g., likelihood ratios of policy and ref-\nerence policy, or odds ratios), or by whether a\nSupervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that\none-stage methods underperform two-stage meth-\nods. To address this, we incorporate an explicit\nSFT phase and introduce the β parameter, control-\nling the strength of preference optimization, into\nsingle-stage ORPO and ASFT. These modifica-\ntions improve their performance in Alpaca Eval\n2 by +3.46 (ORPO) and +8.27 (ASFT), match-\ning two-stage methods like DPO. Further anal-\nysis reveals that the key factor is whether the\napproach uses pairwise or pointwise objectives,\nrather than the specific implicit reward or loss\nfunction. These results highlight the importance\nof careful evaluation to avoid premature claims of\nperformance gains or overall superiority in align-\nment algorithms.\n1. Introduction\nLarge Language Models (LLMs) demonstrate strong text\ngeneration capabilities, yet aligning them with human val-\nues remains challenging due to underspecified objectives,\nlimited training signals, and the complexity of human in-\ntent (Ouyang et al., 2022; Stiennon et al., 2020). Tradi-\ntional alignment pipelines typically involve Supervised Fine-\nTuning (SFT), reward modeling, and reinforcement learning\nto shape model outputs.\nRecently, Direct Alignment Algorithms (DAAs) have\n1T-Tech.\nCorrespondence\nto:\nBoris\nShaposhnikov\n<b.shaposhnikov@tbank.ru>.\nemerged as an alternative, integrating human preferences\ninto policy optimization without explicit reward modeling\nor reinforcement learning (Rafailov et al., 2023; Hong et al.,\n2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024;\nXiao et al., 2024; D’Oosterlinck et al., 2024; Wang et al.,\n2024). These methods differ in theoretical design (pairwise\nvs. pointwise), implementation details (e.g., reference pol-\nicy vs. odds ratio), and whether an SFT phase is required\n(one-stage vs. two-stage). This diversity raises key ques-\ntions about their relationships, comparative advantages, and\nthe role of SFT.\nIn this paper, we show that one-stage methods (e.g., ORPO,\nASFT) can incorporate an explicit SFT phase, improving\nperformance. We introduce a scaling parameter β that uni-\nfies their formulation with other DAAs, revealing shared\noptimization dynamics between methods using either an\nodds ratio or a reference-based reward. Through theoretical\nand empirical analysis, we systematically compare DAAs,\nemphasizing pairwise vs. pointwise preference optimiza-\ntion. We also show that, while SFT is beneficial, using the\nfull dataset is not always necessary, which reduces com-\nputational costs. To structure our analysis, we address the\nfollowing research questions:\nRQ1: Does an explicit SFT stage improve the alignment\nquality of ORPO and ASFT?\nRQ2: Does the tempering factor enhance the alignment\nquality of ASFT and ORPO?\nRQ3: What factors of DAAs affect alignment quality?\nRQ4: How does the final alignment quality depend on the\namount of data used in the SFT stage?\nBy answering these questions, we clarify key trade-offs in\nalignment strategies and provide guidance for optimizing\nLLM training pipelines.\n2. Preliminaries\n2.1. Modeling Sequences\nGiven a sequence y of length |y|, the log-probability can be\nwritten as log p(y) = P|y|\ni=1 log p(yi | y<i), which may also\nbe conditioned on another sequence x. In practice, optimiz-\ning normalized log-probability\n1\n|y| log p(y) = log\n\x00p(y)\n1\n|y| \x01\n1\narXiv:2502.01237v1  [cs.LG]  3 Feb 2025\n\nThe Differences Between Direct Alignment Algorithms are a Blur\noften improves numerical stability and leads to better train-\ning. However, once normalized, the resulting quantity is\nno longer a strict probability measure. Throughout this pa-\nper, whenever we write p(y), we refer to this normalized\nversion p(y)\n1\n|y| . Whenever a method does not apply this\nnormalization, we indicate it explicitly.\nWelleck et al. (2019) introduced a log-unlikelihood term\nthat reduces the probability of certain undesirable tokens:\nlog\n\x001 −p(c | y<i)\n\x01\nfor c ∈C. It can be extended to an\nentire sequence as log\n\x001 −p(y)\n\x01\n.\n2.2. Reinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF)\n(Ouyang et al., 2022; Stiennon et al., 2020) is a prominent\napproach to aligning language models. It generally has three\nstages:\n• Supervised Fine-Tuning (SFT). During the SFT stage,\nthe model πθ is trained to follow instructions by max-\nimizing the probability of correct output y given in-\nput x. For a single training pair (x, y), we define the\nper-sample SFT loss as LSFT(πθ, x, y) = −log πθ(y |\nx). During fine-tuning, we minimize the expectation\nof this per-sample loss over the training dataset D:\nE(x,y) ∼D\nh\nLSFT(πθ, x, y)\ni\n.\n• Reward Modeling (RM). A reward model rψ(x, y) pro-\nduces a satisfaction score. It is trained on preference\npairs using the Bradley-Terry model (Bradley & Terry,\n1952): LRM(rψ) = −E(x,yw,yl)∼D\n\x02\nlog σ\n\x00rψ(x, yw) −\nrψ(x, yl)\n\x01\x03\n, where yw is the preferred response and yl is\nthe less preferred one.\n• Reward\nMaximization.\nThe\nobjective\nis\nto\ngenerate\nresponses\nthat\nmaximize\nthe\nlearned\nreward,\nwith\na\nKL\npenalty\nto\nprevent\nreward\nhacking:\nmaxπθ Ex∼D, y∼πθ(y|x)\n\x02\nrϕ(x, y)\n\x03\n−\nβ DKL\n\x02\nπθ(x, y) ∥πref(x, y)\n\x03\n. Reinforcement learning\n(RL) algorithms are commonly used to optimize this\nobjective (Schulman et al., 2017; Ouyang et al., 2022).\n2.3. Direct Alignment Algorithms\nDirect alignment algorithms replace the reward modeling\nand RL stages (but keep the SFT phase) with a single align-\nment step. Various preference-optimization loss functions\nhave been proposed, employing these core components:\n• rref\nθ (y, x) = log\n\x00 πθ(y|x)\nπref(y|x)\n\x01\nfrom DPO (Rafailov et al.,\n2023), which acts as an implicit reward β rref\nθ . No length\nnormalization is used.\n• rodds\nθ\n(y, x) = log\n\x00πθ(y|x)\n1−πθ(y|x)\n\x01\nproposed in ORPO (Hong\net al., 2024), representing the odds of generating y versus\nnot generating it.\nSeveral Direct Alignment Algorithms use these notations.\nInformation on sequence probability normalization for these\nmethods is presented in Appendix A.1.\n• Direct Preference Optimization (DPO) (Rafailov\net al., 2023):\nLDPO\n=\n−log σ\n\x00β rref\nθ (yw, x) −\nβ rref\nθ (yl, x)\n\x01\n.This method does not normalize probabili-\nties by length.2\n• Identity Preference Optimization (IPO) (Azar et al.,\n2023): LIPO =\n\x00rref\nθ (yw, x) −rref\nθ (yl, x) −\n1\n2β\n\x012.\n• Simple Preference Optimization (SimPO) (Meng\net al., 2024): LSimPO = −log σ\n\x00β log πθ(yw, x) −\nβ log πθ(yl, x) −γ\n\x01\n.\n• Noise\nContrastive\nAlignment\n(NCA)\n(Chen\net al., 2024):\nLNCA\n=\n−log σ\n\x00β rref\nθ (yw, x)\n\x01\n−\n0.5 log σ\n\x00−β rref\nθ (yw, x)\n\x01\n−0.5 log σ\n\x00−β rref\nθ (yl, x)\n\x01\n.\n• Calibrated\nDirect\nPreference\nOptimization\n(Cal-DPO) (Xiao et al., 2024):\nLCal−DPO\n=\n−log σ\n\x00rref\nθ (yw, x) −rref\nθ (yl, x)\n\x01\n+\n\x00rref\nθ (yw, x) −\n1\n2β\n\x012 +\n\x00rref\nθ (yl, x) +\n1\n2β\n\x012.\n• Anchored Preference Optimization Zero (APO-\nZero) (D’Oosterlinck et al., 2024):\nLAPO−Zero\n=\n−σ\n\x00β rref\nθ (yw, x)\n\x01\n+ σ\n\x00β rref\nθ (yl, x)\n\x01\n.\n2.4. Single-Stage Alignment Methods\nSingle-stage alignment (as a subset of DAA methods)\nmerges SFT and direct alignment in one step by adding their\nlosses: LSingle(πθ) = −E(x,yw,yl)∼D\n\x02\nLSFT(πθ, x, yw) +\nλ LAlign(πθ, x, yw, yl)\n\x03\n, where λ is a hyperparameter, and\nno reference policy πref is required.\nIn this paper, we focus on:\n• Odds Ratio Preference Optimization (ORPO)\n(Hong et al., 2024): LORPO = −log πθ(yw|x) −\nλ log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLORPOAlign\n.\n• Aligned Supervised Fine-Tuning (ASFT) (Wang\net al., 2024):\nLASFT\n=\n−log πθ(yw|x) −\nλ\n\x10\nlog σ\n\x00rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLASFTAlign\n\x11\n.\n2Unless otherwise noted, the expectation over (x, yw, yl) ∼D\nis taken.\n2\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n3. Method\nMany DAAs have been proposed, raising questions about\ntheir differences and significance. They can be categorized\nin various ways. For example, one classification separates\nsingle-stage methods, which perform alignment directly\nafter obtaining a base model (ASFT and ORPO), from two-\nstage methods (which perform SFT before alignment), as\nin DPO, IPO, SimPO, etc. Under this scheme, ASFT and\nORPO are single-stage methods.\nAnother classification considers whether rref or rodds is\nused as an implicit reward. ASFT and ORPO also differ\nfrom other losses by using an odds ratio, whereas other\nmethods in Section 2 use normalized policy probabilities.1\nDAAs can also be distinguished by whether their loss func-\ntion is optimized for pairwise or pointwise preferences.\nDPO, for instance, increases the policy’s probability of\nchoosing preferred sequences relative to rejected ones. In\ncontrast, ASFT simply increases or decreases probabilities\nfor chosen or rejected sequences without comparing them\ndirectly.\n3.1. Generalizing ASFT and ORPO\nDespite these classifications, it can still be difficult to pin-\npoint the essential differences among DAAs, especially\nwhen design choices limit generalization. ASFT and ORPO,\nfor example, lack a parameter β, probably because they\nwere conceived as single-stage methods, making the dis-\ntance from a reference policy unnecessary. It might seem\nodd to introduce such a parameter in single-stage methods,\nbut we will show that for both ASFT and ORPO, the single-\nstage design and the absence of β are not strictly required.\n3.1.1. ORPO AND ASFT CAN OPERATE WITHOUT THE\nSFT LOSS TERM AND AS TWO-STAGE METHODS.\nWe begin by inspecting the ASFT objective and demonstrate\nthat it combines both likelihood and unlikelihood terms:\nTheorem 3.1. LASFT is equivalent to the Binary Cross-\nEntropy (BCE) loss, encapsulating both likelihood and un-\nlikelihood components:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nThe proof of Theorem 3.1 is provided in Appendix B. Conse-\nquently,\nLASFTAlign = −\n\x10\nlog πθ(yw|x) + log\n\x001 −πθ(yl|x)\n\x01\x11\n.\nNext, we derive a direct relationship between LORPO and\n1SimPO does not explicitly use a reference policy, but can be\ntreated similarly if a uniform reference policy is assumed.\nLASFT, showing that the latter provides an upper bound on\nthe former:\nTheorem 3.2. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x))\n+ πθ(yl|x)(1 −πθ(yw|x))\n\x01\n,\nwhere the additional term is symmetric in yw and yl.\nThe proof of Theorem 3.2 is provided in Appendix C. As for\nLASFTAlign, the alignment term is then\nLORPOAlign = −log πθ(yw|x) −log(1 −πθ(yl|x))\n+ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nCorollary 3.3. LORPO ≤LASFT and LORPOAlign ≤\nLASFTAlign.\nThis follows from the fact that the additional term in LORPO\nis non-positive when πθ(yw|x) and πθ(yl|x) lie in [0, 1], and\nπθ(yw|x) + πθ(yl|x) ≤1.\nThese findings yield two main observations:\n• LASFT provides an upper bound on LORPO. Minimiz-\ning the former also minimizes the latter.\n• LASFT can be viewed as a minimal form of a DAA\nloss, reflecting the structure of BCE.\nAn essential insight from these formulations is that the SFT\nterm in the ASFT and ORPO losses is already included in\nthe full loss. We hypothesize that this feature may allow us\nto omit the SFT term in the complete loss, first performing\nan SFT phase and then using only the alignment terms for\nmodel alignment. From this perspective, one can experi-\nment with these methods in both single-stage and two-stage\nconfigurations to see which approach is more effective.\n3.1.2. TEMPERING ASFT AND ORPO\nWe now consider the original single-stage methods from Sec-\ntion 2.4 and examine how the alignment terms LORPOAlign\nand LASFTAlign compare. These terms optimize preferences\nand, depending on the coefficient λ, can dominate or have a\nsmaller impact on the final loss.\nLASFTAlign and LORPOAlign strongly resemble the DAA\nlosses discussed in Section 2.3. The single-stage analogue\nof rref\nθ\nis rodds\nθ\n. Inspired by this analogy, we introduce a\ncoefficient β to scale rodds\nθ\n:\nLβ\nASFTAlign\n= −log σ(βrodds\nθ\n(yw, x)) −log σ(−βrodds\nθ\n(yl, x)),\nLβ\nORPOAlign\n= −log σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x)).\n3\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nBoth Lβ\nASFT and Lβ\nORPO generalize their vanilla counter-\nparts (recovering them when β = 1). As in DPO, β can\nbe viewed as a temperature or scaling parameter that regu-\nlates the intensity of the preference for “good” odds. This\nbecomes clearer when looking at the gradients:\n∇θLβ\nASFTAlign = −β\nh\nσ(βrodds\nθ\n(yl, x))∇θrodds\nθ\n(yl, x)\n+\n\x001 −σ(βrodds\nθ\n(yw, x))\n\x01\n∇θrodds\nθ\n(yw, x)\ni\n,\n∇θLβ\nORPOAlign = −β\nh\x00∇θrodds\nθ\n(yw, x) −∇θrodds\nθ\n(yl, x)\n\x01\n×\n\x10\n1 −σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x))\n\x11i\n,\nwhere ∇θrodds\nθ\n(y, x) =\n∇θ log πθ(y|x)\n1−πθ(y|x) .\nWhen β →0,\nσ(β · · · ) ≈1\n2, both methods aggressively improve the odds\nratio (increasing for yw and decreasing for yl). As β in-\ncreases, the updates become bounded by the factor σ(β · · · )\n(similar to a reward threshold in DPO). Hence, once the\nmodel improves, further updates are limited, either individ-\nually for Lβ\nASFTAlign or by pairwise ranking in Lβ\nORPOAlign.\nThis alignment with other DAAs allows for a direct com-\nparison of all methods in different setups, clarifying which\naspects are most critical for successful performance.\n3.2. On the Difference Between Direct Alignment\nAlgorithms\nDifferent methods can be grouped by the type of ”reward”\nfunction used in their loss. In general terms, Lβ\nASFTAlign\nand Lβ\nORPOAlign employ an odds ratio, while DPO, IPO,\nSimPO, NCA, Cal-DPO, and APO-Zero use a ratio between\nthe probability of the policy and that of a reference policy.\nThe following theorems make this classification clearer:\nTheorem 3.4. The gradient of Lβ\nASFTAlign becomes\ncollinear with the gradient of LORPOAlign as β →0. For-\nmally,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nindicating that both gradients point in the same direction.\nThe proof of Theorem 3.4 is provided in Appendix D.1.\nA related property applies to Lβ\nORPOAlign:\nTheorem 3.5. The gradient of Lβ\nORPOAlign is collinear with\nthe gradient of LORPOAlign for any β > 0. Formally,\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nβ > 0.\nThe proof of Theorem 3.5 is provided in Appendix E.1.\nFinally:\nTheorem\n3.6.\nFor\neach\nmethod\nX\n∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n,\nas\nβ →0, the gradient of LX is collinear with the gradient of\nLDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nThe proof of Theorem 3.6 is provided in Appendix F.1.\nThese theorems suggest that for sufficiently small β, these\nloss functions are split into two categories with indistin-\nguishable gradient directions. Although the magnitudes may\ndiffer and they may not be collinear for β ̸→0, one could in-\nfer that their performance should be similar when β is small.\nFrom this perspective, two main distinctions arise among\nthese methods: the use of an odds ratio (rodds\nθ\n) and the use\nof the ratio to a reference policy (rref\nθ ). Both choices might\ninfluence the final performance of these methods. Further-\nmore, it remains an open question whether odds-ratio-based\napproaches outperform reference-policy-based ones (e.g.,\nDPO), and how these distinctions compare to the contrast\nbetween pointwise and pairwise preference formulations.\nFrom traditional learning-to-rank (Liu et al., 2009) research,\npairwise methods often produce more direct and less noisy\nranking signals than pointwise techniques, which could lead\nto superior performance in practice (Burges et al., 2005; Li,\n2011; Melnikov et al., 2016). In the following sections, we\npresent experimental results that provide further insight into\nwhich aspects most strongly influence DAA training.\n4. Experimental Setup\nWe systematically compare and evaluate DAA methods us-\ning a standard training and instruction-following evaluation\nframework (Tunstall et al., 2023; Meng et al., 2024; Gorba-\ntovski et al., 2024). Our main experiments use the Llama\n3.1 8B model (AI@Meta, 2024), trained on the UltraChat\n(Ding et al., 2023) and UltraFeedback (UF) (Cui et al., 2023)\ndatasets, and evaluated on the AlpacaEval 2 (Dubois et al.,\n2024; Li et al., 2023) and ArenaHard (Li et al., 2024) bench-\nmarks. For the Reddit TL;DR (Stiennon et al., 2020) task,\nwe employ the Llama 3.2 3B model, comparing it side by\nside with the “golden” validation split (Rafailov et al., 2023;\n2024) using the prompt in Appendix I.\n4.1. Base vs SFT-Initialized Models.\nTo investigate the impact of SFT and the applicability of\none-stage loss LAlign component, we use the UF dataset for\nSFT (avoiding additional knowledge from UltraChat), and\nfor pairwise preference optimization. We carefully tuned the\nhyperparameters to optimize each method’s performance.\nFor the Base-initialized setup, we perform a grid search over\n4\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nORPO\nASFT\n0\n20\n40\n60\n80\n100\nLlama 3.2 3B TL;DR\n(GPT-4 WinRate, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nLlama 3.2 3B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0\n10\n20\n30\nLlama 3.1 8B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nFigure 1. Impact of the β Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning β (Section 3.1.2) affects\nboth ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2\nLC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each\nmethod’s best configuration at β = 1 as the baseline. See Section 5.2 for more details.\nlearning rates {6 × 10−6, 8 × 10−6, 1 × 10−5}, inspired\nby values suggested in ORPO and ASFT, and explore λ ∈\n{0.1, 0.2, 0.5, 1.0} for 1 and 2 training epochs keeping a\nsimilar budget to compare with the SFT-initialized setup.\nIn the SFT-initialized setup, we experiment with both\nLORPOAlign and LASFTAlign alone, as well as in combina-\ntion with LSFT, following the original methods. We tune\nthe learning rates {5 × 10−7, 7 × 10−7, 1 × 10−6} for one\nepoch, starting from an SFT model trained for 1 epoch at\n6 × 10−6.\n4.2. β Sensitivity.\nBuilding on the theoretical insights from Section 3.2, where\nDAA losses share indistinguishable gradient directions as\nβ →0, we evaluate each method across various β values to\nexamine quality-KL trade-offs. In classical DPO, β regu-\nlates the KL penalty from the reference policy, but setting\nβ too small can induce training instability. Therefore, we\nconduct a thorough sweep of at least six β values per DAA,\nexploring the performance limit of each method. To broaden\nour analysis, we consider three scenarios:\n1. Llama 3.2 3B TL;DR. A relatively simpler Reddit\nTL;DR summarization task, evaluated via GPT side-\nby-side comparison on 500 samples from the “golden”\nvalidation split (Rafailov et al., 2023; 2024).\n2. Llama 3.2 3B UF. The UltraChat and UF datasets\nserve as more challenging alignment settings due to\ntheir coverage of diverse and complex tasks, includ-\ning common sense reasoning, mathematical problem-\nsolving, code generation, logical reasoning, creative\nwriting, and general knowledge.\n3. Llama 3.1 8B UF. A larger, more capable model on the\nsame UltraChat and UF datasets, allowing us to assess\nhow increased model capacity influences β-sensitivity\nin these diverse tasks.\nFor the UF-based experiments, we measure model qual-\nity primarily using the AlpacaEval 2 Length-Controlled\n(LC) Win-Rate and ArenaHard (AH) WR, and then track\nKL divergence from a reference model to construct Pareto\nfronts. For the TL;DR scenario, we rely on GPT-based\npreference judgments using ‘gpt-4o-2024-08-06‘ model.\nConcretely, in each scenario we train models for differ-\nent values β, combining them with four possible learning\nrates {1 × 10−6, 7 × 10−7, 5 × 10−7, 3 × 10−7}. Further\nimplementation details, including training procedures and\ngeneration hyperparameters, are provided in Appendix A.\n4.3. SFT Quality.\nAlthough in principle single-stage methods do not require\na separate SFT phase, in practice an SFT-trained reference\nmodel often improves the final performance of two-stage\npipelines (see Section 5.1). Prior work, such as (Zhou et al.,\n2024), has shown that a small but high-quality dataset can be\nsufficient for instruction tuning. However, beyond response\nquality, it remains unclear how the amount of SFT data in-\nfluences alignment effectiveness. This raises a fundamental\nquestion: how much supervised data is actually needed to\nproduce a reference model that yields high-quality results\nafter the subsequent alignment step?\nTo investigate this, we prepared seven SFT checkpoints by\ntraining Llama 3.1 8B Base on 1%, 3%, 5%, 10%, 25%,\n50%, and 100% of the UltraChat dataset (2,079, 6,236,\n10,393, 20,786, 51,966, 103,932, and 207,865 records, re-\nspectively) using our SFT-initialized procedure. We then\napplied each alignment method – using optimal hyperparam-\neters from our β-sensitivity experiments (Appendix Table 7)\n– to these seven SFT checkpoints and the original base model.\nFinally, we evaluated all resulting aligned models on Al-\npacaEval 2 LC, analyzing their performance relative to the\nfraction of SFT data used.\n5\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nASFT\nCal-DPO\nNCA\nAPO Zero\nORPO\nSimPO\nIPO\nDPO\nSFT\n436\n5\n59\n457\n2\n41\n459\n5\n36\n463\n3\n34\n451\n3\n46\n458\n1\n41\n457\n2\n41\n456\n5\n39\n178\n24\n298\nWin\nTie\nLose\nWin / Tie / Lose Rate %\n35.6 / 4.8 / 59.6\n91.2 / 1.0 / 7.8\n91.4 / 0.4 / 8.2\n91.6 / 0.2 / 8.2\n90.2 / 0.6 / 9.2\n92.6 / 0.6 / 6.8\n91.8 / 1.0 / 7.2\n91.4 / 0.4 / 8.2\n87.2 / 1.0 / 11.8\nFigure 2. GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best\nhyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT\nachieves 87.2%, maintaining robust summarization performance. See Section 5.2 for more details.\n5. Results\n5.1. RQ1: Does an explicit SFT stage improve the\nalignment quality of ORPO and ASFT?\nAs shown in Table 1, the performance of ORPO and ASFT\nmethods improves significantly when the alignment loss\nLAlign is applied after a preceding SFT stage. In particular,\nORPO achieves results comparable to classical DPO in both\nLC Win Rate and AH WR metrics. In contrast, ASFT shows\nnotable gains in AH WR after the SFT stage, although it\nstill underperforms compared to ORPO or DPO.\nInit\nMethod\nLC% (std)\nWR% (std)\nAH% (CI)\nBase\nSFT\n6.7 (0.43)\n4.5 (0.63)\n3.5 (-0.7, 0.8)\nSFT\nORPO\n24.1 (0.84)\n17.8 (1.17)\n15.3 (-1.6, 1.8)\nSFT\nASFT\n16.4 (0.72)\n11.9 (0.99)\n10.6 (-1.2, 1.3)\nBase\nORPO\n14.8 (0.71)\n10.3 (0.95)\n8.4 (-1.3, 1.3)\nBase\nASFT\n14.5 (0.73)\n10.2 (0.94)\n7.5 (-1.1, 1.2)\nSFT\nORPO†\n13.4 (0.69)\n9.3 (0.91)\n7.7 (-0.9, 1.1)\nSFT\nASFT†\n11.4 (0.63)\n7.5 (0.83)\n7.5 (-1.1, 1.1)\nSFT\nDPO\n23.4 (0.85)\n20.0 (1.18)\n17.5 (-1.8, 1.8)\nTable 1. Base and SFT-initialized alignment methods on the\nLlama 3.1 8B model with the UF dataset. SFT-initialized meth-\nods demonstrate better performance compared to their traditional\nformulations without LSFT. Results marked with † correspond to\ntraining with LSFT, using the best hyperparameters: lr = 1×10−6\nfor ORPO and lr = 7 × 10−7 for ASFT. For other setups, the\nbest hyperparameters are: lr = 5 × 10−7 for standard SFT\nORPO/ASFT, and lr = 1 × 10−5/6 × 10−6 for Base ORPO/ASFT.\nFor single-stage methods, the use of λ = 1 provides the best\nresults within the explored grid of λ ∈{0.1, 0.2, 0.5, 1.0},\nespecially after two epochs of training. However, combining\nLSFT and LAlign in a single-stage setup leads to suboptimal\nresults compared to explicitly separating these phases, even\nwhen starting from an SFT-trained model. Incorporating an\nexplicit SFT stage improves overall performance for ORPO\nand ASFT methods. Therefore, all further experiments focus\non applying the LAlign components of ORPO and ASFT on\ntop of an SFT-trained model.\n5.2. RQ2: Does the tempering factor enhance the\nalignment quality of ASFT and ORPO?\nFigure 1 illustrates that introducing the β parameter (as\ndescribed in Section 3.1.2) improves the performance of\nboth ASFT and ORPO LAlign in our tested scenarios. For a\nfair comparison, we used the best-performing learning rate\nfor each baseline — LASFTAlign and LORPOAlign — while\nfixing β = 1. In the Llama 3.2 3B TL;DR experiment,\nthese adjustments led to an improvement of +7.0 for ORPO\nand +43.4 for ASFT in GPT-4 WR. In the Llama 3.1 8B\nUF setup, tuning β provided additional gains of +3.46 for\nORPO and +8.27 for ASFT on the AlpacaEval 2 LC WR.\n5.3. RQ3: What factors of DAAs affect alignment\nquality?\nBased on Section 3, we perform a comprehensive evaluation\nof alignment losses, including DPO, IPO, SimPO, NCA,\nCal-DPO, and APO-Zero, as well as enhanced Lβ\nASFTAlign\nand Lβ\nORPOAlign with the introduced parameter β. Unlike\nclassical methods where β typically regulates KL diver-\ngence against a reference policy πref, β in Lβ\nASFTAlign and\nLβ\nORPOAlign directly modulates the strength of preference\noptimization. To explore the upper limits of each method’s\nperformance, we performed an extensive hyperparameter\nsearch, analyzing both alignment quality and KL divergence.\nFull implementation details, including training setups and\nevaluation criteria, are provided in Appendix A.\nLlama 3.2 3B TL;DR: Figure 2 presents a comparison of\nall methods on the Reddit TL;DR validation subset, using\ntheir best hyperparameters. Most methods achieve a GPT-4\nWin Rate exceeding 90%, indicating robust summarization\nperformance on this relatively straightforward task. ASFT\nis slightly lower at 87.2% Win Rate, but still demonstrates\nstrong overall results.\nLlama 3.2 3B UF and Llama 3.1 8B UF: Table 2 summa-\n6\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nAlpacaEval 2\nArenaHard\nAlpacaEval 2\nArenaHard\nLC% (std)\nWR% (std)\nWR% (CI)\nLC% (std)\nWR% (std)\nWR% (CI)\nSFT\n5.02 (0.34)\n3.21 (0.55)\n1.4 (-0.4, 0.4)\n10.27 (0.54)\n5.44 (0.70)\n2.6 (-0.5, 0.6)\nDPO\n11.43 (0.58)\n11.79 (0.99)\n6.8 (-1.0, 0.9)\n26.82 (0.77)\n23.69 (1.25)\n19.0 (-1.9, 1.8)\nIPO\n11.24 (0.60)\n11.67 (1.01)\n6.8 (-1.0, 1.1)\n28.18 (0.83)\n24.43 (1.26)\n19.1 (-1.6, 1.5)\nSimPO\n10.56 (0.44)\n11.94 (0.95)\n6.4 (-1.0, 1.1)\n27.65 (0.77)\n25.62 (1.29)\n21.5 (-1.9, 1.9)\nORPO\n10.67 (0.50)\n12.23 (0.97)\n6.6 (-1.0, 1.1)\n28.25 (0.71)\n28.59 (1.33)\n20.9 (-2.0, 2.0)\nAPO Zero\n10.36 (0.53)\n11.22 (0.98)\n6.0 (-1.0, 0.9)\n23.15 (0.76)\n19.03 (1.18)\n17.3 (-1.8, 1.8)\nNCA\n10.33 (0.53)\n11.02 (0.97)\n5.1 (-0.7, 0.8)\n23.21 (0.80)\n18.67 (1.17)\n15.1 (-1.5, 1.6)\nCal-DPO\n10.62 (0.57)\n10.15 (0.94)\n4.8 (-0.9, 0.9)\n23.19 (0.82)\n18.85 (1.18)\n15.2 (-1.5, 1.6)\nASFT\n10.63 (0.55)\n9.21 (0.88)\n5.1 (-0.9, 0.9)\n20.82 (0.79)\n16.34 (1.13)\n13.5 (-1.6, 1.5)\nTable 2. AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF. The SFT model was trained on the UltraChat\ndataset. The best hyperparameters for each method were selected according to Section 4.2. Bold values indicate the best performance for\neach benchmark, while underlined values represent the second-best performance. See Section 5.3 for more details.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nKL Divergence with SFT Model\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\nFigure 3. Pareto front for alignment quality and KL divergence.\nResults for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are\ngrouped into pairwise and pointwise categories, with pairwise\nachieving higher LC values while remaining within overlapping\nconfidence intervals. See Section 5.3 for more details.\nrizes the results for both Llama 3.2 3B UF and Llama 3.1\n8B UF setups. For the smaller 3B model, the methods per-\nform similarly on LC WR, with slight differences emerging\non AH. Although these differences align with the pairwise\nvs. pointwise distinction (e.g., DPO, IPO, ORPO, SimPO\nvs. APO-Zero, NCA, Cal-DPO, ASFT), no single approach\nconsistently dominates across metrics. The overlap in con-\nfidence intervals further indicates that the results for these\nmethods are statistically similar in this setup, with no clear\nseparation.\nIn contrast, the 8B model reveals a clearer performance\ndifferentiation. Pairwise methods consistently outperformed\npointwise ones on AlpacaEval 2 and ArenaHard metrics,\nwith ORPO achieving the highest overall alignment quality.\nAs illustrated in Figure 3, pairwise approaches dominated\nthe KL Pareto front for the larger model, demonstrating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy (%)\nPointwise Pairwise\n0.42\n0.44\n0.71\n0.89\n=0.290\n=0.456\nMLP Dim = 1\nMLP Dim = 3\nFigure 4. Pairwise vs. Pointwise Ranking Methods on Toy Ex-\nample. Model capacity impacts ranking accuracy, with pairwise\nmethods outperforming pointwise ones as capacity increases. This\nbehavior is consistent with results observed in Llama experiments\non the UF dataset. See Section 5.3 for more details.\ntheir ability to more effectively balance alignment quality\nand divergence. Pareto fronts for the remaining setups are\nincluded in Appendix G for completeness.\nThese observations suggest that model capacity plays a sig-\nnificant role in amplifying the advantages of pairwise rank-\ning, where LLMs act as rankers (similar to Liu et al. (2024)).\nFor smaller models, such as the 3B setup, limited capacity\nmay hinder the ability to fully exploit pairwise gradient sig-\nnals. This hypothesis is supported by additional evidence\nfrom the toy example experiment (Figure 4), where pairwise\nmethods demonstrated performance similar to pointwise\nmethods with weaker MLPs but achieved better ranking\naccuracy as the model capacity increased. Full details of the\ntoy example setup are provided in Appendix H.\n5.4. RQ4: How does the final alignment quality depend\non the amount of data used in the SFT stage?\nIn Section 5.1, we show that DAAs designed to bypass\nthe SFT phase still underperform compared to models\nthat undergo SFT and are then aligned using a similar\n7\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\n25\nAlpaca Eval 2 LC WR (%)\nDPO\nIPO\nSimPO\nORPO\nSFT\nLine Type\nFraction\nFull\n(a) Pairwise\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\nAlpaca Eval 2 LC WR (%)\nAPO Zero\nNCA\nCal-DPO\nASFT\nSFT\nLine Type\nFraction\nFull\n(b) Pointwise\nFigure 5. Impact of SFT Dataset Size on Alignment Quality. Performance of the pairwise (a) and pointwise (b) alignment methods on\nAlpacaEval 2 (LC WR metric) when the SFT policy is trained on different fractions of the UltraChat dataset. Even a small fraction of SFT\ndata (e.g., 5–10%) yields substantial gains over starting from the raw base model. See Section 5.4 for more details.\npreference-optimization loss function without the SFT term.\nAs discussed in Section 4.3, this raises the question of how\nmuch supervised data is needed to compensate for the ad-\nditional computation and achieve comparable alignment\nperformance.\nTo investigate this, we trained seven SFT models on progres-\nsively larger UltraChat subsets (1% to 100%) and applied\neach alignment algorithm to these models and the non-fine-\ntuned base model, yielding eight initializations per method.\nFigures 5(a) and 5(b) summarize the results for pairwise\nand pointwise alignment methods, respectively. As the plots\nshow, no method starting from the raw base model can\nmatch the final quality of a method trained with the entire\nSFT dataset. However, even a modest size expansion of the\nSFT dataset yields substantial improvements in alignment\nquality: for example, moving from 3% to 5% of the data\nmore than doubles the AlpacaEval 2 LC score for the final\nmodel. Crucially, using only 10% of UltraChat for SFT\nyields nearly the same quality as using the entire dataset.\nAdding an SFT phase requires more overall training, but\nit pays off significantly in the final result. Moreover, one\ndoes not need the entire supervised corpus to realize most\nof these gains; even 5–10% of the data is often enough for\nDAAs to reach most of their potential.\n6. Conclusion\nThis paper presents a comprehensive theoretical and empiri-\ncal analysis of DAAs. Theoretically, we demonstrated that\nwithin each category - odds-based (rodds) and reference-\npolicy-based (rref) – gradient directions of popular methods\nalign as β →0, revealing shared optimization dynamics\nwithin these groups. We also showed that single-stage losses\n(e.g., ASFT, ORPO) can be extended to two-stage pipelines\nwith an explicit SFT step and optional β-scaling, enabling\ngreater flexibility. Experimentally, we addressed four core\nresearch questions (RQ1–4), exploring single- vs. two-stage\ntraining, implicit rewards, objective types, and the impact\nof the SFT phase. Our key findings are:\n• Include an SFT phase.\nAn SFT stage consistently\nimproves alignment performance (RQ1), with ORPO\nachieving +9.3 LC / +6.9 AH and ASFT +1.9 LC / +3.1\nAH in the setup from Section 4.1. Even 5–10% of the\nsupervised dataset often suffices to achieve near-optimal\nresults (RQ4).\n• Pairwise methods outperform pointwise objectives.\nAlignment quality depends more on the choice between\npairwise and pointwise objectives than on the formula-\ntion of implicit reward (e.g., rodds or rref). Pairwise\nmethods generally perform better (e.g., ORPO outper-\nforming ASFT by +7.43 LC / +7.4 AH in the Llama\n3.1 8B UF setup), particularly in larger models (RQ3).\nAmong these, ORPO and SimPO also stand out as prac-\ntical options for memory-constrained scenarios, as they\ndo not rely on a reference policy.\n• Choose hyperparameters carefully. Alignment per-\nformance is highly sensitive to learning rates and the\ncoefficient β. We provide optimal configurations for dif-\nferent methods based on comprehensive grid searches\nin our experimental setups, highlighting the added gains\nfrom tuning β in odds-based methods, where it controls\nthe strength of preference optimization (RQ2).\nLimitations and Future Work. Although our study sys-\ntematically compares DAAs, it has several limitations. We\ntested a limited set of datasets (UltraChat, UltraFeedback,\nReddit TL;DR) and benchmarks (AlpacaEval 2, ArenaHard),\nwhich may affect generalizability to other domains. The re-\nliance on GPT-based evaluators can introduce biases. More-\nover, we evaluated on 3B–8B models, so the observed ad-\nvantages of pairwise over pointwise objectives could shift\nat larger scales.\n8\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nReferences\nAI@Meta.\nLlama 3 model card.\n2024.\nURL\nhttps://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nAzar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,\nD., Valko, M., and Munos, R. A general theoretical\nparadigm to understand learning from human preferences,\n2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,\nT. J., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,\nEl-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez,\nD., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,\nN., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-\nCandlish, S., Olah, C., Mann, B., and Kaplan, J. Train-\ning a helpful and harmless assistant with reinforcement\nlearning from human feedback. ArXiv, abs/2204.05862,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:248118878.\nBradley, R. A. and Terry, M. E. Rank Analysis of Inclom-\nplete Block Design: The Method of Paired Comparisons.\nBiometrika, 39(3-4):324–345, 12 1952.\nISSN 0006-\n3444. doi: 10.1093/biomet/39.3-4.324. URL https:\n//doi.org/10.1093/biomet/39.3-4.324.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,\nHamilton, N., and Hullender, G. Learning to rank using\ngradient descent. In Proceedings of the 22nd international\nconference on Machine learning, pp. 89–96, 2005.\nChen, H., He, G., Yuan, L., Cui, G., Su, H., and Zhu, J.\nNoise contrastive alignment of language models with\nexplicit rewards, 2024. URL https://arxiv.org/\nabs/2402.05369.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback, 2023.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDing, N., Chen, Y., Xu, B., Qin, Y., Hu, S., Liu, Z., Sun, M.,\nand Zhou, B. Enhancing chat language models by scaling\nhigh-quality instructional conversations. In Bouamor,\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\n2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 3029–3051, Singapore, December\n2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.183. URL https://\naclanthology.org/2023.emnlp-main.183.\nD’Oosterlinck, K., Xu, W., Develder, C., Demeester, T.,\nSingh, A., Potts, C., Kiela, D., and Mehri, S. Anchored\npreference optimization and contrastive revisions: Ad-\ndressing underspecification in alignment, 2024. URL\nhttps://arxiv.org/abs/2408.06266.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nGorbatovski, A., Shaposhnikov, B., Malakhov, A., Sur-\nnachev, N., Aksenov, Y., Maksimov, I., Balagansky, N.,\nand Gavrilov, D. Learn your reference model for real\ngood alignment. arXiv preprint arXiv:2404.09656, 2024.\nHong, J., Lee, N., and Thorne, J. Orpo: Monolithic prefer-\nence optimization without reference model, 2024. URL\nhttps://arxiv.org/abs/2403.07691.\nKingma,\nD. P. and Ba,\nJ.\nAdam:\nA method\nfor stochastic optimization.\nCoRR, abs/1412.6980,\n2014.\nURL https://api.semanticscholar.\norg/CorpusID:6628106.\nLi, H. A short introduction to learning to rank. IEICE\nTRANSACTIONS on Information and Systems, 94(10):\n1854–1862, 2011.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline, 2024.\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-\nval: An automatic evaluator of instruction-following\nmodels.\nhttps://github.com/tatsu-lab/\nalpaca_eval, 5 2023.\nLiu, T., Qin, Z., Wu, J., Shen, J., Khalman, M., Joshi,\nR., Zhao, Y., Saleh, M., Baumgartner, S., Liu, J., et al.\nLipo: Listwise preference optimization through learning-\nto-rank. arXiv preprint arXiv:2402.01878, 2024.\nLiu, T.-Y. et al. Learning to rank for information retrieval.\nFoundations and Trends® in Information Retrieval, 3(3):\n225–331, 2009.\nMelnikov, V., H¨ullermeier, E., Kaimann, D., Frick, B., and\nGupta, P. Pairwise versus pointwise ranking: A case\nstudy. Schedae Informaticae, pp. 73–83, 2016.\nMeng, Y., Xia, M., and Chen, D. Simpo: Simple preference\noptimization with a reference-free reward. arXiv preprint\narXiv:2405.14734, 2024.\n9\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P. F., Leike, J.,\nand Lowe, R. Training language models to follow instruc-\ntions with human feedback. In Koyejo, S., Mohamed, S.,\nAgarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\nAdvances in Neural Information Processing Systems, vol-\nume 35, pp. 27730–27744. Curran Associates, Inc., 2022.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023. URL https://arxiv.org/abs/\n2305.18290.\nRafailov, R., Chittepu, Y., Park, R., Sikchi, H., Hejna, J.,\nKnox, B., Finn, C., and Niekum, S. Scaling laws for\nreward model overoptimization in direct alignment algo-\nrithms. arXiv preprint arXiv:2406.02900, 2024.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\nspeed: System optimizations enable training deep learn-\ning models with over 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pp. 3505–3506,\n2020.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O.\nProximal policy optimization al-\ngorithms.\nCoRR, abs/1707.06347, 2017.\nURL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1707.html#SchulmanWDRK17.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\nR., Voss, C., Radford, A., Amodei, D., and Christiano,\nP. Learning to summarize from human feedback. In\nNeurIPS, 2020.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Ra-\nsul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier,\nC., Habib, N., et al. Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944, 2023.\nWang, R., Sun, J., Hua, S., and Fang, Q. Asft: Aligned\nsupervised fine-tuning through absolute likelihood, 2024.\nURL https://arxiv.org/abs/2409.10571.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nand Weston, J. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319, 2019.\nXiao, T., Yuan, Y., Zhu, H., Li, M., and Honavar, V. G. Cal-\ndpo: Calibrated direct preference optimization for lan-\nguage model alignment, 2024. URL https://arxiv.\norg/abs/2412.14516.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\n10\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA. Implementation Details\nA.1. Probability Normalization\nAs discussed in Section 2.1, not all DDAs incorporate length-based probability normalization by default. In this paper,\nhowever, we consistently apply such normalization wherever probabilities are involved. This choice avoids introducing extra\nnotation and reduces the cognitive load on the reader. Table 3 summarizes the methods that originally include length-based\nnormalization.\nMethod\nUse normalization\nDPO (Rafailov et al., 2023)\n✗\nIPO (Azar et al., 2023)\n✗\nSimPO (Meng et al., 2024)\n✓\nNCA (Chen et al., 2024)\n✗\nCal-DPO (Xiao et al., 2024)\n✗\nAPO-Zero (D’Oosterlinck et al., 2024)\n✗\nORPO (Hong et al., 2024)\n✓\nASFT (Wang et al., 2024)\n✓\nTable 3. Methods that include (✓) or omit (✗) length-based probability normalization in their original formulation.\nA.2. Training Details\nOur experiments were conducted using the Llama 3.2 3B and Llama 3.1 8B Base models (AI@Meta, 2024). The training\nsetup, datasets, and hyperparameters were designed to ensure reproducibility and consistency. Unless otherwise noted, the\nhyperparameters in Table 4 were used across all experiments.\nHyperparameter\nValue\nMax Tokens Length\n1024 (TL;DR setup), 4096 (UF setup)\nEpochs\n1 (or 2 when specified)\nLearning Rate (SFT)\n6.0 × 10−6\nLearning Rate (Base Init.)\n{6.0 × 10−6, 8.0 × 10−6, 1.0 × 10−5}\nLearning Rate (Alignment)\n{3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6}\nOptimizer\nAdam (Kingma & Ba, 2014)\nAdam β1\n0.9\nAdam β2\n0.95\nBatch Size\n128\nLearning Schedule\nLinear Decay\nWarm-up Ratio\n0.03\nMax Gradient Norm\n2\nMemory Optimization\nDeepSpeed (Rasley et al., 2020)\nAttention Mechanism\nFlash Attention 2 (Dao, 2023)\nTable 4. Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models.\nTraining was performed on 8 NVIDIA A100 GPUs with 80GB memory each. Depending on the number of epochs, training\nfor each configuration took between 3 to 6 hours.\nA.2.1. DATASETS.\nWe used two primary datasets:\n• Reddit TL;DR (Bai et al., 2022): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B\nmodel.\n11\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n• UltraChat (Ding et al., 2023): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B and\nLlama 3.1 8B models.\n• UltraFeedback (Cui et al., 2023): used for both SFT (in the Base vs. SFT-initialized comparison, where we selected\nchosen subset from preference pairs) and for pairwise preference optimization in all DAA methods.\nThe dataset sizes are summarized in Table 5. For Base vs. SFT-initialized setups, only UltraFeedback was used. For\nβ-sensitivity experiments, the models were first trained on UltraChat for SFT and subsequently fine-tuned on UltraFeedback.\nThe Reddit TL;DR dataset was processed to remove duplicates, retaining only uniquely preferred summaries for SFT.\nDataset\nTraining Examples\nValidation Examples\nUltraChat\n207,865\n23,110\nUltraFeedback\n61,135\n2,000\nReddit TL;DR (SFT)\n41,947\n11,941\nReddit TL;DR (Preference)\n73,396\n21,198\nTable 5. Summary of dataset sizes used for training and validation.\nA.2.2. β-SENSITIVITY EXPERIMENTS.\nWe conducted a comprehensive analysis to evaluate the sensitivity of DAA methods to β, examining its impact on the\ntrade-off between model quality and KL divergence. Each method was trained using six or more distinct β values to identify\na configuration that achieves stable and effective performance. The specific β values tested for each method are as follows:\nMethod\nβ Values Tested\nDPO\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1}\nIPO\n{0.0007, 0.001, 0.005, 0.01, 0.05, 0.1}\nSimPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0}\nORPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nASFT\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nAPO-Zero\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2}\nCal-DPO\n{0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003}\nNCA\n{0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05}\nTable 6. Range of β values tested for each DAA method on all scenarios.\nFor each β, we tested four learning rates (3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6), training on the UltraFeedback\ndataset. All runs began from an SFT-initialized model trained on UltraChat (lr = 6.0 × 10−6, 1 epoch). The best-performing\nlearning rate for each β was selected to construct Pareto fronts, balancing quality (measured via AlpacaEval 2 LC Win-Rate)\nand KL divergence.\nFor SimPO in the Llama 3.1 8B UF setup, the ratio γ\nβ = 0.5 was kept fixed as recommended by Meng et al. (2024).\nAdditionally, a single learning rate (lr = 6.0 × 10−7) was tested across all β values for this method, as the same datasets\nand model scale were used. For Llama 3.2 TL;DR and UF setups, we tested four learning rates similar to other DAAs.\nBeyond the standard β values described in Table 6, additional values were explored for specific configurations to reach\nthe extreme points of the Pareto front. For example: - {0.00001, 0.00003} for Cal-DPO in Llama 3.2 3B TL;DR and UF\nsetups, - {0.00001, 0.00003, 0.00005} for NCA in Llama 3.2 3B TL;DR, - {0.0003, 0.0005} for APO-Zero in Llama 3.2\n3B TL;DR, - {0.0003, 0.0005, 0.001, 0.003, 0.005} for ASFT in Llama 3.2 3B TL;DR.\nThe hyperparameters resulting in the best performance are presented in Table 7.\nA.3. Generation Details\nWe evaluated model performance on AlpacaEval 2 and ArenaHard for UltraFeedback setups, while for the Reddit TL;DR\nsetup, we used side-by-side comparisons with GPT-4o on a curated golden validation subset of 500 samples. Additionally,\n12\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B TL;DR\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nLearning Rate\nβ\nLearning Rate\nβ\nLearning Rate\nβ\nDPO\n7.0 × 10−7\n0.05\n1.0 × 10−6\n0.01\n1.0 × 10−6\n0.003\nIPO\n1.0 × 10−6\n0.005\n7.0 × 10−7\n0.001\n1.0 × 10−6\n0.001\nSimPO\n3.0 × 10−7\n0.5\n7.0 × 10−7\n1.0\n6.0 × 10−7\n1.0\nORPO\n3.0 × 10−7\n0.5\n5.0 × 10−7\n0.2\n5.0 × 10−7\n0.5\nASFT\n3.0 × 10−7\n0.001\n1.0 × 10−6\n0.2\n7.0 × 10−7\n0.1\nAPO Zero\n3.0 × 10−7\n0.001\n3.0 × 10−7\n0.005\n3.0 × 10−7\n0.003\nNCA\n3.0 × 10−7\n0.0001\n3.0 × 10−7\n0.0005\n3.0 × 10−7\n0.0003\nCal-DPO\n3.0 × 10−7\n0.00003\n5.0 × 10−7\n0.0003\n3.0 × 10−7\n0.0003\nTable 7. Best hyperparameters for each DAA method across setups.\nKL divergence was measured on the validation subset for all setups using the generation hyperparameters listed in Table 8.\nFor ArenaHard, the temperature was set to 0 to adhere to the original benchmark configuration.\nHyperparameter\nValue\nTemperature\n0.9\nTop-k\n40\nTop-p\n1.0\nMax New Tokens\n256 (TL;DR setup), 4096 (UF setup)\nTable 8. Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models.\nB. Equivalence of ASFT Loss and Binary Cross-Entropy Loss\nLemma B.1.\nlog σ(rodds\nθ\n(y, x)) = log πθ(y|x)\nProof.\nlog σ(rodds\nθ\n(y, x)) = log σ(log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(1−πθ(y|x))−log(πθ(y|x)) = log\n1\n1 + 1−πθ(y|x)\nπθ(y|x)\n= −log\n\x10\n1 + 1 −πθ(y|x)\nπθ(y|x)\n\x11\n= −log πθ(y|x) + 1 −πθ(y|x)\nπθ(y|x)\n= log πθ(y|x).\nLemma B.2.\nlog σ(−rodds\nθ\n(y, x)) = log\n\x001 −πθ(y|x)\n\x01\nProof.\nlog σ(−rodds\nθ\n(y, x)) = log σ(−log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(πθ(y|x))−log(1−πθ(y|x)) = log\n1\n1 +\nπθ(y|x)\n1−πθ(y|x)\n=\n−log\n\x10\n1 +\nπθ(y|x)\n1 −πθ(y|x)\n\x11\n= −log 1 −πθ(y|x) + πθ(y|x)\n1 −πθ(y|x)\n= log(1 −πθ(y|x)).\n13\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nTheorem B.3. LASFT is equivalent to the binary cross-entropy loss, encompassing both likelihood and unlikelihood\ncomponents:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nProof. To show that LASFT is equivalent to the BCE loss, we start with the definition:\nLASFT = −log πθ(yw|x) −λ log σ(rodds\nθ\n(yw, x)) −λ log σ(−rodds\nθ\n(yl, x)),\nwhere rodds\nθ\n(y, x) =\nπθ(y|x)\n1−πθ(y,x). Applying Lemma B.1 and Lemma B.2 to the expression, we obtain:\nLASFT = −log πθ(yw|x) −λ log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n= −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)).\nC. Relationship Between ORPO and ASFT Loss Functions\nTheorem C.1. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nProof. We start by defining the ORPO loss:\nLORPO = −log πθ(yw|x) −λ log σ\n\x12\nlog\nπ(yw|x)\n1 −π(yw|x) −log\nπ(yl|x)\n1 −π(yl|x)\n\x13\n.\nExpanding the second term using the identity log σ(x) = x −log(ex + 1), we get:\n−log σ\n\x12\nlog\nπθ(yw|x)\n1 −πθ(yw|x) −log\nπθ(yl|x)\n1 −πθ(yl|x)\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x)(1 −πθ(yl|x))\nπθ(yl|x)(1 −πθ(yw|x)) + 1\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\nπθ(yl|x)(1 −πθ(yw|x))\n\x13\n= −log πθ(yw|x) −log(1 −πθ(yl|x)) + log\n\x00πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\n\x01\n|\n{z\n}\nORPOAlign\n.\nCombining all terms, we obtain:\nLORPO = −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)) + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n= LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\nD. Proof of Theorem 3.4\nTheorem D.1 (Collinearity of β-ASFT and ORPO Gradients). Let\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n,\nwhere\nrodds\nθ\n(y, x) = log\n\x10\nπθ(y|x)\n1−πθ(y|x)\n\x11\n.\n14\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nDefine the ORPO alignment loss as\nLORPOAlign = −log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n.\nThen,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n\r\r∇θ Lβ\nASFTAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r,\ni.e., their gradients become collinear in the same direction as β →0.\nProof. Step 1. Gradient of β-ASFT.\nDenote pw = πθ(yw | x), pl = πθ(yl | x). Then\nrodds\nθ\n(yw, x) = log\n\x10\npw\n1−pw\n\x11\n,\nrodds\nθ\n(yl, x) = log\n\x10\npl\n1−pl\n\x11\n.\nBy definition,\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n.\nFor small β, a first-order Taylor expansion of σ(β z) around 0 yields σ(β z) = 1\n2 + β z\n4 +O(β2). Thus, σ(β rodds\nθ\n(yw, x)) ≈\n1\n2 and σ(−β rodds\nθ\n(yl, x)) ≈1\n2. Taking gradients and applying the chain rule gives each term approximately proportional to\n± β ∇θ[rodds\nθ\n(·)]. Concretely,\n∇θ\n\x02\n−log σ(β rodds\nθ\n(yw, x))\n\x03\n≈−β\n2 ∇θ\n\x02\nrodds\nθ\n(yw, x)\n\x03\n,\n∇θ\n\x02\n−log σ(−β rodds\nθ\n(yl, x))\n\x03\n≈+ β\n2 ∇θ\n\x02\nrodds\nθ\n(yl, x)\n\x03\n.\nHence, summing up,\n∇θ Lβ\nASFTAlign ≈β\n2\nh\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\ni\n.\nObserve that β > 0 implies the overall scalar factor β\n2 is strictly positive in front of the difference of gradients.\nStep 2. Gradient of ORPO alignment loss.\nDefine ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n.\nIts gradient (using the chain rule) is proportional to\n∇θ LORPOAlign ∝−∇θ\n\x02\nrodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x03\n= ∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x).\nUp to a strictly positive logistic factor (since σ(·) ∈(0, 1)), the coefficient in front of ∇θ[rodds\nθ\n(·)] remains negative, but we\ntrack the absolute scalar to see it is positive. Indeed, one can write\n−∇θ\n\x00∆rodds\nθ\n(x)\n\x01\n= κORPO ∇θrodds\nθ\n(yl, x) −κORPO ∇θrodds\nθ\n(yw, x),\nκORPO > 0.\nStep 3. Conclusion (positive collinearity).\nComparing the two gradients:\n∇θ Lβ\nASFTAlign ≈\nβ\n2\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n,\n∇θ LORPOAlign ∝\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n.\nThe ratio is thus strictly positive for small β. Consequently,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nestablishing collinearity in the same direction.\n15\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nE. Proof of Theorem 3.5\nTheorem E.1 (Collinearity of β-ORPO and ORPO Gradients). Let\n∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x),\nand consider\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nIts gradient is collinear with the gradient of the standard ORPO alignment loss\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\nfor any fixed β > 0. Formally,\n∇θ Lβ\nORPOAlign\n\r\r∇θ Lβ\nORPOAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r.\nProof. Step 1. Gradient of β-ORPO.\nLet ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nBy the chain rule,\n∇θ Lβ\nORPOAlign = −\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\nβ ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince σ′(z) = σ(z) [1 −σ(z)], we have\n−\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\n= −β\n\x02\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01\x03\n.\nThus,\n∇θ Lβ\nORPOAlign = −β\nh\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01i\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince β > 0 and 1 −σ(·) > 0, the factor multiplying ∇θ[∆rodds\nθ\n(x)] is strictly negative.\nStep 2. Gradient of standard ORPO (i.e. β = 1).\nFor\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n,\nthe gradient is\n∇θ LORPOAlign = −\n\x02\n1 −σ(∆rodds\nθ\n(x))\n\x03\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nThis also has a strictly negative scalar in front of ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nStep 3. Conclusion (exact positive ratio).\nSince ∇θ Lβ\nORPOAlign and ∇θ LORPOAlign both differ from ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\nby a negative coefficient, it follows that these\ntwo gradients coincide up to a strictly positive factor:\n∇θ Lβ\nORPOAlign = κ(β) ∇θ LORPOAlign,\nκ(β) > 0.\nHence\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nproving the claimed collinearity (in the same direction) for every fixed β > 0.\n16\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nF. Proof of Theorem 3.6\nTheorem F.1 (Unified Collinearity of DPO with IPO, SimPO, NCA, Cal-DPO, and APO-Zero). Let\n∆rref\nθ (x) = rref\nθ\n\x00yw, x\n\x01\n−rref\nθ\n\x00yl, x\n\x01\n,\nand define the DPO loss\nLDPO = −log\n\x10\nσ\n\x00β ∆rref\nθ (x)\n\x01\x11\n,\nβ > 0.\nFor each method X ∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n, as β →0, the gradient of LX is asymptotically\ncollinear (i.e., it differs by a positive factor) with the gradient of LDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nProof of Theorem 3.6. Step 1: DPO as the baseline (tracking its sign).\nBy definition,\nLDPO = −log σ\n\x00β ∆rref\nθ (x)\n\x01\n.\nSince σ(u) = 1/(1 + e−u), for β > 0, one computes\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nObserve that β > 0 and σ(·) ∈(0, 1) imply\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01\n> 0.\nHence the factor multiplying ∇θ ∆rref\nθ (x) is negative. To unify directions by a positive multiple, note\n−∇θ LDPO = β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x),\nwhich has a strictly positive scalar in front. Thus, ∇θ LDPO is collinear with ∇θ ∆rref\nθ , and in particular its negative is a\npositive multiple of ∇θ ∆rref\nθ .\nStep 2: IPO.\nThe IPO loss is\nLIPO =\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x112\n.\nIts gradient is\n∇θ LIPO = 2\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x11\n∇θ ∆rref\nθ (x).\nAs β →0, the term\n1\n2β dominates ∆rref\nθ (x). Hence,\n∆rref\nθ (x) −\n1\n2β ≈−1\n2β ,\nso\n∇θ LIPO ≈−1\nβ ∇θ ∆rref\nθ (x).\nWe compare this with\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nBoth gradients are negative multiples of ∇θ ∆rref\nθ (x). Therefore,\n∇θ LIPO = κIPO(β) ∇θ LDPO,\nwith κIPO(β) > 0 as β →0.\nHence they are collinear in the same direction asymptotically.\n17\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nStep 3: SimPO.\nThe SimPO loss is\nLSimPO = −log σ\n\x00β ∆sθ −γ\n\x01\n,\nwhere ∆sθ = log πθ(yw | x) −log πθ(yl | x). Its gradient takes the form\n∇θ LSimPO = −β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\n∇θ ∆sθ.\nAgain, β > 0 and 1 −σ(·) > 0. Also, σ(β ∆sθ −γ) ∈(0, 1). Thus the prefactor\n−β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\nis strictly negative for each β > 0. Therefore, just like DPO, ∇θ LSimPO is in the negative direction of ∇θ ∆sθ. But ∇θ ∆sθ\nis proportionally the same as ∇θ ∆rref\nθ\nfor small-β expansions (both are differences of log-likelihood or reward-like terms).\nSo\n∇θ LSimPO = κSimPO(β) ∇θ LDPO,\nκSimPO(β) > 0 for small β.\nHence they are collinear with a positive factor in the low-β limit.\nStep 4: NCA.\nDefine\nrref\nw\n= rref\nθ\n\x00yw, x\n\x01\n,\nrref\nl\n= rref\nθ\n\x00yl, x\n\x01\n.\nThen NCA is\nLNCA = −log σ\n\x00β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nl\n\x01\n.\nFor small β, expand\nσ(β z) = 1\n2 + β z\n4\n+ O(β2),\nso log σ(β z) = log 1\n2 + log\n\x10\n1 + β z\n2 + O(β2)\n\x11\n. Each gradient term then yields a linear-in-β combination of ∇θ rref\nw and\n∇θ rref\nl . Collecting terms shows that, as β →0,\n∇θ LNCA ∝β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= β ∇θ ∆rref\nθ (x).\nComparing this with ∇θ LDPO = −β\n\x02\n1 −σ(. . . )\n\x03\n∇θ ∆rref\nθ (x) reveals another negative factor on the DPO side. In ratio\nform,\n∇θ LNCA = κNCA(β) ∇θ LDPO\nwith κNCA(β) > 0 for small β.\nHence collinearity follows.\nStep 5: Cal-DPO.\nThe Cal-DPO loss is\nLCal-DPO = −log σ\n\x00∆rref\nθ (x)\n\x01\n+\n\x00rref\nw −\n1\n2β\n\x012 +\n\x00rref\nl\n+\n1\n2β\n\x012.\nFor β near 0, the large constants ± 1\n2β dominate. The gradient w.r.t. θ in these squared terms is effectively\n∝−1\nβ ∇θ rref\nw\n+\n1\nβ ∇θ rref\nl\n= −1\nβ ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −1\nβ ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO has the same negative sign structure in front of ∇θ ∆rref\nθ , their ratio is again positive. Thus\n∇θ LCal-DPO = κCal-DPO(β) ∇θ LDPO\nwith κCal-DPO(β) > 0 as β →0.\nStep 6: APO-Zero.\nAPO-Zero is given by\nLAPO-Zero = −σ\n\x00β rref\nw\n\x01\n+ σ\n\x00β rref\nl\n\x01\n.\n18\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nIts gradient involves terms ∇θ σ(β rref\nw ) and ∇θ σ(β rref\nl ), each proportional to β ∇θ rref\nw and β ∇θ rref\nl . Subtracting these\nyields\n∇θ LAPO-Zero ∝−β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −β ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO also has a negative constant factor, their ratio has a positive limit. Therefore,\n∇θ LAPO-Zero = κAPO-Zero(β) ∇θ LDPO,\nκAPO-Zero(β) > 0 for small β.\nConclusion.\nIn each method X, one sees that ∇θ LX has the same negative-sign structure around ∇θ ∆rref\nθ (x) as does ∇θ LDPO,\nensuring a positive ratio in the limit. Formally,\n∇θ LX = κX(β) ∇θ LDPO,\nκX(β) > 0,\nas β →0.\nThus,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥,\nwhich completes the proof of their alignment in the same direction.\nG. Pareto fronts for Llama 3.2 setups\nThe results presented in this section correspond to the best hyperparameter configurations identified during the hyperparame-\nter search described in Section 4.2, including the optimal learning rate for each method. This ensures that the Pareto fronts\nreflect the upper performance limits for alignment quality.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL Divergence with SFT Model\n40\n50\n60\n70\n80\n90\nGPT-4 Win Rate (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(a) Llama 3.2 3B TL;DR\n0.0\n0.2\n0.4\n0.6\n0.8\nKL Divergence with SFT Model\n5\n6\n7\n8\n9\n10\n11\n12\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(b) Llama 3.2 3B UF\nFigure 6. Pareto front for alignment quality and KL divergence. Results for Llama 3.2 3B TL;DR and UF setups on GPT-4 Win\nRate vs. ”golden” validation subset and AlpacaEval 2 LC respectively with different β values. Methods are grouped into pairwise and\npointwise categories. For the summarization task (Llama 3.2 3B TL;DR), both pointwise and pairwise methods achieve strong overall\nresults. For the UF setup, methods also perform similarly within overlapping confidence intervals, indicating no clear separation.\nH. Toy Example Details\nTo analyze the differences between pairwise and pointwise ranking methods, especially with respect to the ranking nature of\nalignment losses in LLMs, a simplified toy experiment was conducted under a controlled setup. A dataset of 2000 triplets\n(x, yw, yl) was generated, where x, yw, and yl are real-valued scalars satisfying yw > yl. The data was split into 80% for\ntraining and 20% for testing. When the model processes a scalar input x together with a candidate y, these two numbers\nform a vector in R2, which serves as the input of the Multi-Layer Perceptron (MLP) to predict the reward r.\n19\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA single-hidden-layer MLP with ReLU activation was used in two capacity settings: lower (hidden size = 1) and higher\n(hidden size = 3). The model takes x and a candidate y as input, producing a reward r analogous to training a reward model\nfor RLHF (Stiennon et al., 2020).\nTwo losses were evaluated: the pairwise Bradley-Terry loss (Bradley & Terry, 1952),\nLPairwise = −log\n\x00σ(β(rw −rl))\n\x01\n,\nand the pointwise loss,\nLPointwise = −\n\x02\nlog\n\x00σ(βrw)\n\x01\n+ log\n\x00σ(−βrl)\n\x01\x03\n.\nEach configuration was trained over 100 runs, tuning the learning rate from {0.5, 0.3, 0.1, 0.01, 0.03, 0.05} and β from\n{5.0, 2.0, 1.0, 0.2, 0.1, 0.05, 0.01}. Alignment accuracy was defined as the proportion of cases with rw > rl.\nThe results show that both methods yield comparable performance in the low-capacity regime, while pairwise ranking\nachieves higher accuracy as model capacity increases, mirroring the effects observed in larger-scale experiments from the\nSection 5.3.\nI. GPT-4 Side-By-Side Evaluation Prompt\nFor our Side-By-Side evaluations with GPT-4o, we designed a prompt tailored to the Reddit TL;DR dataset to assess\naccuracy, completeness, relevance, and conciseness. The full prompt used in our experiments is detailed below.\nAct as an impartial judge and evaluate the quality of the summaries provided\nby two AI assistants for the text displayed below. Your evaluation should\nconsider accuracy, completeness, relevance, and conciseness.\nYou will be given a text, Assistant A’s summary, and Assistant B’s summary.\nYour job is to evaluate which assistant’s summary is better based on the\ntext provided.\nBegin your evaluation by comparing both assistants’ summaries with the\noriginal text. Identify and correct any inaccuracies.\nEnsure the summaries are complete, capturing all essential information\nfrom the text without introducing fabricated details.\nAssess the relevance of the information each assistant chose to include\nin their summary, ensuring it reflects the core message of the text.\nEvaluate the conciseness of the summaries, favoring those that efficiently\nconvey the necessary information without unnecessary verbosity.\nAvoid any position biases and ensure the order in which the summaries\nwere presented does not influence your decision.\nDo not allow the length of the summaries to influence your evaluation,\nexcept in the context of conciseness and efficiency.\nDo not favor certain names of the assistants.\nBe as objective as possible.\nYou should only evaluate the summaries provided by both assistants\nand NOT the original text itself.\nIf both summaries are irrelevant, contain hallucinations, or are\ninconsistent with the original text, mark the comparison as inconclusive\nand choose option "C".\nAfter providing your explanation, output your final verdict by strictly\nfollowing this format:\n"""\n20\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nComparison: <One-sentence comparison>\nWinner: <A if assistant A is better, B if assistant B is better, and C for a tie.>\n"""\n21'),
                Paper(arxiv_id='2502.01456', authors=['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], published_at=datetime.datetime(2025, 2, 4, 0, 2, 39, 922000, tzinfo=datetime.timezone.utc), title='Process Reinforcement through Implicit Rewards', summary="Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for the reinforcement learning\n(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of training process reward models (PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable to reward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using only policy rollouts and outcome labels through\nimplict process rewards. PRIME combines well with various advantage functions\nand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting from\nQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkey reasoning benchmarks over the SFT model. Notably, our resulting model,\nEurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning\nbenchmarks with 10% of its training data.", upvotes=48, thumbnail=None, content='Preprint\nPROCESS REINFORCEMENT THROUGH\nIMPLICIT REWARDS\nGanqu Cui2,1†∗, Lifan Yuan3†∗, Zefan Wang1∗, Hanbin Wang4∗, Wendi Li1∗,\nBingxiang He1∗, Yuchen Fan2,5∗, Tianyu Yu1∗, Qixin Xu1∗, Weize Chen1, Jiarui Yuan1,\nHuayu Chen1, Kaiyan Zhang1, Xingtai Lv1, Shuo Wang1, Yuan Yao1, Xu Han1,\nHao Peng3, Yu Cheng2,6, Zhiyuan Liu1, Maosong Sun1, Bowen Zhou2,1, Ning Ding1†\n1Tsinghua University\n2Shanghai AI Lab\n3University of Illinois Urbana-Champaign\n4Peking University\n5Shanghai Jiaotong University\n6CUHK\ncuiganqu@pjlab.org.cn\nlifan4@illinois.edu\nhttps://github.com/PRIME-RL/PRIME\nABSTRACT\nDense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While dense\nrewards also offer an appealing choice for the reinforcement learning (RL) of\nLLMs since their fine-grained rewards have the potential to address some inher-\nent issues of outcome rewards, such as training efficiency and credit assignment,\nthis potential remains largely unrealized. This can be primarily attributed to the\nchallenges of training process reward models (PRMs) online, where collecting\nhigh-quality process labels is prohibitively expensive, making them particularly\nvulnerable to reward hacking. To address these challenges, we propose PRIME\n(Process Reinforcement through IMplicit rEwards), which enables online PRM\nupdates using only policy rollouts and outcome labels through implict process\nrewards. PRIME combines well with various advantage functions and forgoes the\ndedicated reward model training phase that existing approaches require, substan-\ntially reducing the development overhead. We demonstrate PRIME’s effectiveness\non competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME\nachieves a 15.1% average improvement across several key reasoning benchmarks\nover the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses\nQwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its train-\ning data.1\n1\nINTRODUCTION\nDense process rewards, which provide feedback at each intermediate step rather than only the whole\ntrajectory, have proven effective in inference-time scaling of large language models (LLMs) on\nchallenging reasoning tasks (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Yuan\net al., 2024b). On the training side, they also present superiorities in the reinforcement learning\n(RL) of LLMs, particularly in improving training efficiency (Sutton & Barto, 2018) and credit\nassignment (Leike et al., 2018) compared with sparse outcome rewards. However, successful\napplications of dense rewards in RL for LLMs are limited (Setlur et al., 2024), as current industry-\nleading models primarily depend on verifiable outcome rewards and have not yet demonstrated\nmeaningful progress with dense rewards (DeepSeek-AI et al., 2025; Team et al., 2025).\nWe identify the central challenge as how to acquire and utilize high-quality dense rewards at scale,\nwhich enables online process reward model (PRM) update efficiently. The reason is that, optimizing\ntowards a static reward model eventually leads to overoptimization or reward hacking (Gao et al.,\n∗Core Contributors.\n†Project Lead.\n1Models and data are available at: https://github.com/PRIME-RL/PRIME.\n1\narXiv:2502.01456v1  [cs.LG]  3 Feb 2025\n\nPreprint\nAIME 2024\nAMC\nMinerva Math\nOlympiadBench\nMATH-500\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n26.7\n57.8\n38.6\n42.1\n79.2\n48.9\n3.3\n30.1\n32.7\n29.8\n65.1\n32.2\n13.3\n50.6\n34.6\n40.7\n79.8\n43.8\n16.7\n30.1\n35.3\n31.9\n64.6\n35.7\n9.3\n45.8\n36.8\n43.3\n76.4\n43.3\nEurus-2-7B-PRIME\nEurus-2-7B-SFT\nQwen-2.5-Math-7B-Instruct\nLlama-3.1-70B-Instruct\nGPT-4o-2024-08-06\nFigure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics\nbenchmarks, outperforming advanced math models and larger models. Notably, PRIME brings\nsubstantial performance gain (+16.7%) over Eurus-2-7B-SFT.\n2022) due to distribution shift. Ideally, this can be solved by improving the reward model online (Leike\net al., 2018). However, acquiring dense process labels for training is prohibitively more expensive.\nExisting methods either need to build complicated human annotation pipelines (Lightman et al.,\n2023) or rely on estimation-based methods, which require about 10× more rollouts for each step than\nsampling only the response-level trajectories (Wang et al., 2023; Kazemnejad et al., 2024). Neither of\nthem is scalable in online RL. Moreover, to the best of our knowledge, it remains underexplored how\nto incorporate dense rewards into RL for LLMs.\nIn this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable frame-\nwork for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level\nrewards. At its core, the framework employs recently proposed implicit process reward model-\ning (Yuan et al., 2024b) to train dense reward models with only outcome-level labels. This enables\nPRIME to perform online learning of reward signals using only outcome labels on policy rollouts,\nthereby fundamentally mitigating reward hacking while maintaining the same computational cost as\ntraditional outcome reward models (ORMs). Besides scalability, PRIME also (1) serves as a general\nmethod to fuse token-level dense rewards and sparse outcome rewards by calculating their returns\nseparately before summing together, which is compatible with diverse RL algorithms (Williams, 1992;\nKool et al., 2019; Shao et al., 2024; Ahmadian et al., 2024; Schulman et al., 2017); (2) eliminates the\ndedicated reward modeling stage, which is required by existing works, by simply initializing from the\nSFT model or even the base model (§ 5.6). In summary, starting from one single language model, the\nPRIME framework can efficiently accomplish the generation of dense rewards, the initialization and\nupdating of reward models, as well as the reinforcement learning (RL) training of the policy model.\nTable 1: The comparison of resource requirements between Eurus-\n2-7B-PRIME and Qwen2.5-Math-7B-Instruct.\nModel\nEurus-2-7B-PRIME\nQwen2.5-Math-7B-Instruct\nBase Model\nQwen2.5-Math-7B\nQwen2.5-Math-7B\nSFT Data\n230K (open-source)\n2.5M (open-source & in-house)\nRM Data\n0\n618K (in-house)\nRM\nEurus-2-7B-SFT\nQwen2.5-Math-RM (72B)\nRL Data\n150K queries × 4 samples\n66K queries × 32 samples\nIn\nexperiments,\nwe\ntrain\nQwen2.5-Math-7B-Base (Yang\net al., 2024b) with PRIME after\na lightweight SFT warmup stage.\nCompared to RL using outcome\nrewards only, PRIME achieves\na 2.5× sample efficiency gain\nand a 6.9% performance im-\nprovements on challenging math\nproblems. As shown in Figure 1,\nthrough PRIME, we successfully\nachieve substantial improvement on key mathematical reasoning benchmarks over the SFT model,\nleading to 16.7% improvement on average, and over 20% on AMC&AIME competitions. Our\nfinal model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical\nbenchmarks. Notably, this is achieved with only 10% of the data used by Qwen-Math, as in Table 1.\n2\n\nPreprint\nOur analysis shows that updating the PRM online is key to the success of PRIME (§5.1). We also\nshow that PRIME could generally boost various RL algorithms, including RLOO (Ahmadian et al.,\n2024), REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GRPO (Shao et al., 2024)\n(§5.4). In terms of the design choices of advantage estimate, we observe that Implicit PRMs are better\nto be used as reward models than value models (§5.5).\n2\nREINFORCEMENT LEARNING FOR LLMS AND THE CHALLENGES OF\nINCOPORATING DENSE REWARDS\nReinforcement Learning (RL) aims to learn an optimal policy πθ that maximizes the expected\ncumulative discounted reward, namely return, when interacting with an environment. In the context\nof autoregressive language modeling, state at step t is the concatenation of prompt x and current\nresponse y<t, and the action is the t-th token or step yt.\n2.1\nRL PRELIMINARIES FOR LLMS\nPolicy Gradient. Policy gradient is a fundamental algorithm that directly optimizes this objective.\nCentral to this approach is the advantage function At, which quantifies how much better an action is\ncompared to alternatives in a given state:\n∇θJ(θ) = Ex∼D,y∼πθ\n" T\nX\nt=0\n∇θ log πθ(yt|y<t)At\n#\n(1)\nwhere (x, y) represents a pair of input and output. x is omitted for brevity. In practice, the advantage\nfunction is implemented as cumulative discounted rewards subtracting a baseline:\nAt =\nT\nX\ns=t\nγs−tr(ys) −b\n(2)\nγ ∈[0, 1] is a discount factor that optionally decays future rewards, and r(ys) is the reward provided\nby the environment at time step s with x and y<s being omitted in conditions. Eq. 2 is the general\nformula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and\ndense reward at each step is crucial for RL. Different choices of b include, e.g. directly using values\nWilliams (1992), group average of rewards (Shao et al., 2024), and leave-one-out average of rewards\nAhmadian et al. (2024); Kool et al. (2019).\nValue Models. Though the MC estimate is unbiased, it suffers from high variance because of the\nreliance on all future actions and rewards, which can be random and noisy. Value models, which\npredict expected accumulated rewards starting from a state, are adopted to help reduce the variance\nin advantage estimation, such as Generalized Advantage Estimation (GAE; Schulman et al., 2016):\nAGAE(γ,λ)\nt\n= P∞\ns=0(γλ)sδt+s, where δt = r(yt) + γV (y<t+1) −V (y<t) is the temporal difference\n(TD) error (Sutton, 1988), V is a value model, and λ controls the bias-variance tradeoff in advantage\nestimation. PPO (Schulman et al., 2017) is a representative of such actor-critic algorithms that\nexplicitly train a value model along with the policy.\nReward Sparsity. Although dense rewards can be naturally integrated into the advantage function\nthrough Eq. 2, unfortunately, only outcome reward models (ORMs) are available in most practices\nof LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive\nno rewards (Rafailov et al., 2023; Shao et al., 2024; DeepSeek-AI et al., 2025). In this bandit\nsetting, r(yt) = 0 for t < T while r(yT ) can be non-zero, and Eq. 2 becomes A = r(yT ) −b. This\nformulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback\nonly at the end of the entire generation. This may (1) encourage spurious solutions with incorrect\nprocesses but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter\nthe credit assignment problem (Sutton & Barto, 2018). These drawbacks could be further amplified\non complicated tasks, which require more thinking and execution steps, urging the need of dense\nrewards (Uesato et al., 2022; Lightman et al., 2023). Some may consider employing a value model\nto mitigate the problem, as it predicts values at every step t. However, previous work showed that\nvalue models may not be able to solve the reward sparsity issue effectively due to training challenges,\ndespite the additional computation overhead (Shao et al., 2024; Ahmadian et al., 2024). We will\nalso empirically validate this claim in §5.5.\n3\n\nPreprint\n2.2\nKEY CHALLENGES IN SCALABLE DENSE REWARDS\nThe way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs,\nwhich score model responses over each token or step. However, it is usually infeasible in practice to\nincorporate dense rewards into online RL because of three critical challenges in implementation.\nC1. Process rewards are hard to define. It is difficult to collect step-level labels since reasoning\nsteps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating\nlabels for each token is too costly. Moreover, defining the absolute correctness of intermediate\nprocesses as dense rewards can be ambiguous, as some incorrect steps can also positively contribute\nto the final answer by pruning searching branches (OpenAI, 2024; DeepSeek-AI et al., 2025).\nC2. PRM online updates are not scalable. It is crucial to prevent reward overoptimization or reward\nhacking, which requires the reward model or value model to be updated online along with the policy\nmodel (Schulman et al., 2017; Gao et al., 2022). However, training PRMs often requires extensive\nnuanced step-level annotation, which is infeasible in online RL training. Therefore, this brings about\nconsiderable scalability and generalization concerns in dense rewards for RL.\nC3. Explicit reward modeling brings extra cost. Training reward models requires extensive\nannotation and broad data coverage to ensure a good balance between adaptability to the policy\ndistribution and generalization to distribution shifts. Hence, the explicit training stage introduces a\nvery costly data collection and an additional training overhead, especially for PRMs which typically\nrequire stepwise labels.\nNotably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMs\ninto their large-scale RL training (DeepSeek-AI et al., 2025).\n3\nPRIME\nTo address the above challenges, we propose PRIME, a scalable online RL method with dense\nrewards. The key insight of PRIME is to apply implicit process rewards, which are derivable from the\nImplicit PRM that is trained with only outcome labels (Yuan et al., 2024b). This property enables us to\nupdate the PRMs online to avoid reward hacking. We then design a flexible framework to incorporate\nimplicit process rewards with outcome rewards into any kind of MC advantage estimate. PRIME is\nillustrated in Figure 2 and Algorithm 1. Next, we will detail the implicit process rewards (§3.1) and\nhow we leverage them to calculate advantages (§3.2), and introduce other techniques we used (§3.3).\n3.1\nENABLING SCALABLE REWARD UPDATE WITH IMPLICIT REWARD MODELING\nWe consider dense rewards from the Implicit PRM because of the scalability. In short, Implicit PRM\nenables training an ORM with outcome labels only while repurposing it as a PRM at inference. The\ntraining stage is the same as standard ORM pipelines, with the only difference being representing\nthe reward as rϕ(y) := β log πϕ(y)\nπref(y), where πϕ is the RM and πref is the reference model, both of\nwhich are causal LMs. At inference, the process rewards are obtained by:\nrϕ(yt) := β log πϕ(yt|y<t)\nπref(yt|y<t)\n(3)\nIn PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we\nupdate the Implicit PRM online with on-policy rollouts and outcome supervision and then\ncalculate token-level dense rewards to estimate advantages, which solves C1 and C2 mentioned in\n§2.2 respectively: (1) To prevent overoptimization and reward hacking, it is crucial to update reward\nmodels online. However, updating previous PRMs (Lightman et al., 2023) requires annotating step\nlabels on the latest policy rollouts, which is neither efficient nor scalable during online RL. In contrast,\nthe Implicit PRM only demands outcome labels to train due to its special reward representation,\nand thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which\nhave already been collected to update the policy model. (2) Unlike common PRMs that produce only\nstep-level rewards, the Implicit PRM provides more fine-grained token-level rewards at no additional\ncost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra\noverhead, making it easy to combine with any RL algorithms for advantage estimation.\n4\n\nPreprint\nAlgorithm 1 Process Reinforcement through Implicit Rewards (PRIME)\nInput Language model πθinit; outcome reward verifier ro; dataset D; sample number K; total iteration\nN.\n1: Initialize policy model πθ ←πθinit, πθold ←πθinit, implicit PRM πϕ ←πθinit, reference model\nπref ←πθinit\n2: for iteration = 1, ..., N do\n3:\nSample batch of prompts B ∼D\n4:\nGenerate K responses: {y1, ..., yK} ∼πθ(·|x) for x ∈B\n5:\nCompute outcome rewards: ro\n\x00y1:K\x01\n6:\nApply accuracy filter (§3.3) on all prompts: T ←Filter(x, y1:K, ro\n\x00y1:K\x01\n) for x ∈B\n7:\nForward pass πϕ, πref on each (x, y) ∈T to obatin implicit process reward rϕ(yt) with Eq. 3\n8:\nUpdate Implicit PRM πϕ by CE loss on (x, y, ro (y)) ∈T :\nLCE(ϕ) = −E(x,y,ro(y))∼T [ro (y) · log σ (rϕ (y)) + (1 −ro (y)) · log (1 −σ (rϕ (y)))]\n9:\nCompute advantages A with Eq. 5\n10:\nUpdate policy πθ by PPO loss in Eq. 6\n11:\nUpdate old parameters: θold ←θ\n12: end for\nOutput Optimized policy model πθ\n3.2\nADVANTAGE ESTIMATION AND POLICY UPDATE\nSFT \nModel\nImplicit \nPRM\nPolicy \nModel\nImplicit \nPRM\nPolicy \nModel\nPrompt\nResponse\nOutcome \nVerifier\n𝒓𝒐\n𝒓𝒑\nUpdate\nUpdate\n𝝅𝒓𝒆𝒇\n𝝅𝒓𝒆𝒇\nSFT \nModel\nFigure 2: Illustration of PRIME. PRIME follows\nthat (1) initialize policy model and the Implicit\nPRM both with the reference model; (2) sample\nmultiple responses for each prompt and filter with\noutput accuracy; (3) obtain implicit process re-\nwards by the Implicit PRM and update it using\ncross-entropy (CE) loss; (4) compute advantage\nand policy loss then update the policy model.\nEstimating advantages using Monte Carlo es-\ntimator with a leave-one-out baseline. After\nobtaining token-level dense rewards, we calcu-\nlate advantages based on either MC estimators\nor GAE. To determine the advantage function\nin PRIME, we compare GAE with several MC\nestimators, including REINFORCE (Williams,\n1992), RLOO (Ahmadian et al., 2024), and\nGRPO (Shao et al., 2024). Experimental details\nand results can be found in §5.4.\nWe find that MC estimators, despite being sim-\npler, are strong enough to produce stable results.\nTherefore, we choose MC estimate as our ad-\nvantage function and despite PRIME being com-\npatible with any baseline estimation approaches,\nwe instantiate it with a leave-one-out baseline\nfrom K samples (Ahmadian et al., 2024) in this\npaper, as it performs better in the experiments:\nAi = r(yi\nT ) −\n1\nK −1\nX\nj̸=i\nr(yj\nT )\n(4)\nwhere r(yi\nT ) denotes the reward of i-th response at final step T, K is the number of samples for one\nprompt. The leave-one-out (LOO) baseline helps reduce variances.\nMore specifically, we use an Implicit PRM πϕ and an outcome verifier or reward model ro. We\ncalculate the return of implicit process rewards and outcome rewards separately if both are available,\nsince directly mixing their values may lead to numerical instability (Shao et al., 2024). For implicit\nprocess rewards, we perform a three-step process to calculate return: (1) Use the averaged implicit\nprocess rewards to calculate the leave-one-out baseline; (2) Normalize the process reward at step t by\nsubtracting the baseline; (3) Calculate the discounted return for each response. For outcome rewards,\nwe directly adopt LOO without any modification. Finally, the advantage is set to the combination of\n5\n\nPreprint\nboth returns:\nAi\nt =\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8f0rϕ(yi\ns) −\n1\nK −1\nX\nj̸=i\nrϕ\n\x00yj\x01\n\uf8f9\n\uf8fb\n|\n{z\n}\nRLOO with implicit process rewards\n+ ro\n\x00yi\x01\n−\n1\nK −1\nX\nj̸=i\nro\n\x00yj\x01\n|\n{z\n}\nRLOO with outcome rewards\n(5)\nUpdating policy with PPO clip surrogate loss. We adopt PPO clip surrogate loss for more stable\npolicy updates:\nLCLIP(θ) =Et\n"\nmin\n\x12 πθ(yt|y<t)\nπθold(yt|y<t)At, clip\n\x10 πθ(yt|y<t)\nπθold(yt|y<t), 1 −ϵ, 1 + ϵ\n\x11\nAt\n\x13#\n(6)\nwhere ϵ is a clipping parameter. The loss prevents the updated policy from deviating too far from the\noriginal distribution, which is the prerequisite of importance sampling. The legitimacy of importance\nsampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling\nefficiency.\n3.3\nOTHER TECHNIQUES\nInitializing PRM with SFT/base model. In practice, we find that the starting policy model itself\nserves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in §2.2\nand even outperforms a dedicatedly trained PRM, as shown in § 5.1.\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nw/ filter\nw/o filter\nFigure 3: Impact of online prompt filtering on\ntraining rewards.\nOnline Prompt Filtering. As we sample mul-\ntiple trajectories for each prompt, we introduce\nonline prompt filtering which filters prompts\nwithin a certain accuracy range. This (1) pre-\nserves only the prompts within a certain median-\nlevel difficulty range (Yang et al., 2024b) and (2)\nbalances data distribution for the Implicit PRM\nonline training.\nWe present the ablation study results in Figure 3\nusing RLOO with outcome rewards only, from\nwhich we can see that the online prompt filter\nlargely lowers the variance of RL training.\nHow PRIME addresses challenges in §2.2. In\nsummary, as illustrated in Figure 2 and Algo-\nrithm 1, PRIME adopts implicit process rewards\nfor efficient PRM online update (C2), then inte-\ngrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs\nare directly initialized from SFT or base models, which foregoes explicit reward modeling (C3).\n4\nEXPERIMENTS\n4.1\nIMITATION WARMUP\nWe focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-\nMath-7B-Base (Yang et al., 2024b) for its great mathematical capabilities. We first performed\nsupervised finetuning for RL preparation.\nData Construction. To construct the SFT dataset, we collect reasoning instructions from several open-\nsource datasets. For completion, we employed LLaMA-3.1-70B-Instruct (Meta, 2024) to answer the\ninstructions, with a system prompt requesting the model to perform action-centric chain-of-thought.\nWe finally obtained 230K SFT data, the detailed sources and statistics can be found in § A.\nSFT Results. After finetuning, the performance of our SFT model is reported in Figure 1. Compared\nto baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks.\n6\n\nPreprint\nTable 2: Detailed results of PRIME and RLOO w/ outcome verifier (OV). At the same 240 steps, the\nmodel trained by PRIME is generally better than the model trained by outcome rewards.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nGPT-4o\n-\n9.3\n45.8\n76.4\n36.8\n43.3\n58.9\n48.8\n45.6\nLlama-3.1-70B-Inst.\n-\n20.0\n37.3\n65.0\n37.1\n30.5\n35.0\n34.4\n37.0\nQwen2.5-Math-7B-Inst.\n-\n13.3\n50.6\n79.8\n34.6\n40.7\n11.7\n11.3\n34.6\nEurus-2-7B-SFT\n0\n3.3\n30.1\n66.2\n32.7\n29.8\n21.7\n17.8\n28.8\nRLOO w/ OV Only\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\n80\n20.0\n41.0\n68.2\n38.2\n37.0\n26.7\n26.6\n36.8\n160\n13.3\n42.2\n72.0\n37.1\n38.7\n26.7\n25.6\n36.5\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\n320\n16.7\n51.8\n77.8\n39.7\n41.5\n36.1\n28.5\n41.7\nEurus-2-7B-PRIME\n592\n26.7\n57.8\n79.2\n38.6\n42.1\n33.3\n28.6\n43.9\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\n2.5x Efficient\n6.9% Higher\nPRIME\nRLOO w/ OV Only\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\n256\n288\n320\nSteps\n30\n32\n34\n36\n38\n40\n42\nAvg. Test Acc\nPRIME\nRLOO w/ OV Only\n(b) Test accuracy across different gradient steps.\nFigure 4: The effect of dense reward. We compare PRIME and RLOO with outcome verifier\n(OV). Dense rewards in PRIME lead to 2.5× sample efficiency and 6.9% performance improvement.\nPRIME also substantially outperforms RLOO on downstream tasks.\n4.2\nRL SETTINGS\nRule-based Outcome Verifier. Consistent with recent research that adopts exact match with ground\ntruth as unhackable rewards (Gao et al., 2024; Lambert et al., 2024; DeepSeek-AI et al., 2025), we\ndefine the rule-based ground truth outcome verifiers (OV) for math and coding as follows:\nrmath\no\n(y) =\n\x1a1,\nmatched\n0,\notherwise\nrcode\no\n(y) =\nP #passes\nP #test cases\nHyperparameters. We use veRL (Sheng et al., 2024) to conduct experiments. By default, we\ninitialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For\nhyperparameters, we use a constant 5 × 10−7 learning rate together with AdamW optimizer for\npolicy model, and use a 10−6 learning rate for PRMs. Both policy and PRMs use a batch size of 256\nand micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each\nprompt. We set β = 0.05 for PRM training. We set KL coefficient to 0 in all experiments.\nEvaluation Benchmarks. We evaluate on 7 reasoning benchmarks, focusing on competition-level\nmathematics and programming tasks, including AIME 2024 (Li et al., 2024), AMC (Li et al., 2024),\nMATH-500 (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He\net al., 2024), LeetCode (Guo et al., 2024), and LiveCodeBench (v2) (Jain et al., 2024).\n4.3\nMAIN RESULTS\nAs shown in Figure 1 and Table 2, Eurus-2-7B-PRIME achieves substantial improvements on key\nreasoning benchmarks over the SFT version of the model, leading to 15.1% improvement on average,\nand over 20% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7%\npass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct,\ndemonstrating its excellent reasoning ability.\n7\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n30\n32\n34\n36\n38\n40\nAvg. Test Acc\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(b) Test accuracy across different gradient steps.\nFigure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the\nbest results. Surprisingly, using PRMs trained on extra rollouts hurts the performance in both online\nand offline settings.\n4.4\nDENSE REWARDS V.S. SPARSE REWARDS\nWe first validate the effect of dense rewards compared to RLOO with outcome rewards only. We\ntrain this model for 240 steps. For PRIME, we use the same setting and train the model for 592\nsteps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure 4.\nCompared with sparse reward, PRIME takes 40% of the training steps to achieve the same\ntraining rewards as RLOO and improves the final rewards by 6.9%, with lower variances. On\ndownstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in\nTable 2.\n5\nANALYSIS\n5.1\nDESIGN CHOICES FOR THE IMPLICIT PRM\nThe Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this\nsection, we explore two major factors: (1) the initialization model and (2) the update mechanism.\nSFT model initializes a good PRM. Conventionally, we need to collect data to train RMs\nand PRMs, and then we can use them in RL. However, the Implicit PRM is a language\nmodel, so we can initialize it from any language model with the same tokenizer as the pol-\nicy model.\nTo investigate whether it is still necessary to train a PRM in advance, we con-\nduct experiments with different PRM initialization strategies: with the SFT model itself and\nwith a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT\nwith additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in § B.5).\n0\n50\n100\n150\n200\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nPRM Accuracy\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\nFigure 6: Impact of PRM online update. The\noffline PRM is gradully been overoptimized while\nonline PRMs achieve higher accuracy throughout\ntraining.\nWe report the experiment results in Figure 5.\nSurprisingly, directly using Eurus-2-7B-SFT\nto initialize the PRM greatly outperforms Eu-\nrusPRM which was trained on more samples.\nWe conjecture that initializing policy model and\nPRM from the same model largely alleviates\nthe distribution shift issue, as the PRM is only\ntrained on the online rollouts from the policy\nmodel.\nOnline PRM update is essential. To verify the\neffect of online PRM update, we pair the correct\nand wrong samples and calculate the PRM\nprediction accuracy using rϕ(y).\nWe report\nthe PRM classification accuracy in Figure 6.\nThe figure clearly shows that, online update\nmitigates\noveroptimization\nand\nreward\n8\n\nPreprint\n(a) Policy ref: We use the policy logprob as πref\nfor PRM.\n(b) SFT ref: We retain the initial policy to provide πref for\nPRM and KL.\nFigure 7: Comparison of different reference policy implementations. One uses the running policy’s\nold logprobs as reference (policy ref) while the other uses the initial SFT model as the reference\nmodel (SFT ref).\nhacking. The offline PRM, though starting with\nhigh accuracy, gradually drops during RL training procedure due to distribution shift. In contrast,\nonline PRMs that are trained on policy rollouts show the reverse curve.\nThis is further validated with training rewards and downstream performance. To breakdown, Eurus-2-\n7B-SFT is both used as PRM initialization and the reference model in the main experiment, so the\nPRM is totally trained from scratch, which means the initial PRM outputs zero reward for all tokens.\nTherefore, Figure 4 also demonstrates the effect of online PRM update. For EurusPRM initialization,\nthe online run outperforms the offline run as well in Figure 5.\n5.2\nREFERENCE MODEL CHOICE IS FLEXIBLE\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPolicy ref\nSFT ref\nFigure 8: Different reference model for PRM.\nWe compare two reference model selection strate-\ngies for PRIME. Using the policy model as refer-\nence and using the initial SFT model as reference.\nTheir rewards are similar.\nWe implement two variants of our algorithms to\nexplore the effect of reference model of implicit\nPRM, one using the initial SFT model as the\nreference model (SFT ref) while the other using\nthe running policy’s old logprobs as reference\n(policy ref), as shown in Figure 7a. The policy\nref simply adopts the old logprob of the policy\nmodel as πref, while the SFT ref remains the ini-\ntial SFT model for an additional πref calculation.\nWe compare their performance in this section.\nFrom the training rewards in Figure 8, we find\nthe two strategies are close and have pros and\ncons in different aspects: The Q value calculated\nby implicit PRM is the expectation under the dis-\ntribution of the reference model. So the updat-\ning policy could natrually serve as the reference.\nOn the other hand, KL divergence calculation\nis only allowed when the initial SFT model is\nretained.\n5.3\nSINGLE-FORWARD V.S. DOUBLE-FORWARD\nSince our implicit PRM is concurrently updated in training, for each rollout stage, we can update the\nPRM before the policy model and use the updated PRM to re-calculate the process rewards, which\n9\n\nPreprint\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nPRM Accuracy\nBefore-double forward\nBefore-single forward\nAfter-double forward\n(a) PRM classification accuracy on training samples.\n0\n50\n100\n150\n200\n250\n300\n350\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\nOutcome Training Rewards\nDouble forward\nSingle forward\n(b) Training outcome rewards.\nFigure 9: Single and double forward. While double forward methods obtain higher accuracy after\nonline update, the two variants achieve similar rewards during training.\nwe call the double-forward setting. We investigate the impact of double-forward in both the training\nand test phases. Our default setting applies single-forward, which uses process rewards from old\nPRMs. We plot PRM accuracy on rollouts and training rewards in Figure 9.\nAccordingly, we find that double-forward could increase PRM accuracy, but the training rewards\nremain close between the two methods.\n5.4\nPRIME WITH OTHER RL ALGORITHMS\nAs we stated before, PRIME is equally applicable to other RL algorithms beyond RLOO. In this\nsection, we implement PRIME with REINFORCE (Williams, 1992), GRPO (Shao et al., 2024), and\nPPO (Schulman et al., 2017). Similarly to RLOO, we only modify the advantage estimation functions\nand leave the clip surrogate loss unchanged.\nFirst of all, We compare different REINFORCE-like advantage estimators including REINFORCE,\nGRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms\ncompatible with the compound of outcome verifier reward and process reward, we accordingly make\nadaptions similar to Eq. 5. For GRPO, we have\nAi\nt = ro\n\x00yi\x01\n−mean(ro\n\x00yj\x01\n)\nstd(ro (yj))\n|\n{z\n}\nGRPO with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nrϕ(yi\ns) −mean\n\x12\nrϕ(yj)\n|yj|\n\x13\nstd\n\x10\nrϕ(yj)\n|yj|\n\x11\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n|\n{z\n}\nGRPO with implicit process rewards\n.\n(7)\nFor REINFORCE, we have\nAi\nt =\nro\n\x00yi\x01\n| {z }\nREINFORCE with outcome rewards\n+\n|yi|\nX\ns=t\nγs−t · rϕ(yi\ns)\n|\n{z\n}\nREINFORCE with implicit process rewards\n.\n(8)\nFrom Figure 10 and Table 3, We show that PRIME boosts these algorithms on both efficiency and\nperformance as it does with RLOO. PRIME contributes consistently regardless of the policy update\nmethod, making it a generic algorithm. It indicates that PRIME is a general plug-in for almost any\nRL algorithm for LLM., which largely extends the use cases of PRIME.\nMoreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional\ncomputation cost from the critic model is redundant. This makes it possible to compensate for the\nexpense of the process reward model by using REINFORCE-like algorithms with simpler advantage\nestimators. Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm.\n10\n\nPreprint\nTable 3: Testset results of different RL algorithms.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nRLOO\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\nRLOO w/ PRIME\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\nREINFORCE\n240\n6.7\n47.0\n72.6\n36.0\n37.2\n27.2\n25.0\n36.0\nREINFORCE w/ PRIME\n240\n6.7\n50.0\n76.4\n36.8\n39.1\n27.8\n27.5\n37.8\nGRPO\n240\n10.0\n44.6\n73.2\n37.5\n36.6\n25.0\n25.8\n36.1\nGRPO w/ PRIME\n240\n16.7\n47.0\n75.0\n34.9\n38.2\n28.9\n23.9\n37.8\nPPO\n240\n10.0\n41.0\n73.6\n36.0\n36.3\n28.3\n25.7\n35.8\nPRIME as Value Model\n240\n16.7\n44.6\n72.6\n34.6\n35.7\n27.8\n24.6\n36.6\nPPO w/ PRIME\n240\n13.3\n50.6\n77.4\n37.1\n40.6\n30.0\n26.7\n39.4\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\nREINFORCE w/ PRIME\nGRPO\nGRPO w/ PRIME\nPPO\nPPO w/ PRIME\nFigure 10: PRIME also benefits REINFORCE,\nGRPO, and PPO, achieving similar improve-\nment as RLOO.\n0\n50\n100\n150\n200\n250\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nREINFORCE\n+ linear-head value model\n+ Implicit PRM as value\n+ Implicit PRM as reward\nFigure 11: Comparison of value models and reward\nmodels. We show that value models, either the\noriginal PPO one or Implicit PRM, is substaintially\nworse than reward models.\n5.5\nVALUE OR REWARD, HOW TO USE THE IMPLICIT PRM?\nBesides using process rewards to estimate returns, we can also employ the Implicit PRM to predict\nvalues for advantage estimation in Eq. 2. Therefore, we compare four variants of MC estimate\nto determine the best way to incorporate dense supervision. Recall that the Implicit PRM has\nvϕ(y<t+1) = Pt\ni=1 β log πϕ(yi|y<i)\nπref(yi|y<i) with the process reward being rϕ(yt) = vϕ(y<t+1)−vϕ(y<t),\nand we assume a ground-truth outcome verifier ro, γ = 1, then we represent the variants as follows:\n(1) REINFORCE: At = ro(y).\n(2) On top of (1), using a linear-head value model V to calculate the baseline: At = ro(y)−V (y<t).\nThis is the original PPO in Figure 10 as we set γ = 1 and λ = 1.\n(3) On top of (1), using values from the Implicit PRM to serve as the baseline: At = ro(y) −\nvϕ(y<t). This is equivalent to PPO with its value model being replaced by values from the Implicit\nPRM when γ = 1 and λ = 1.\n(4) On top of (1), using process rewards from the Implicit PRM to calculate the return: At =\nro(y) + PT\ns=t rϕ(ys). This is the REINFORCE w/ PRIME in Figure 10.\nFigure 11 reports the results. Comparing PPO and REINFORCE, we find that an additional value\nmodel does not benefit policy performance. Notably, using rewards from the Implicit PRM to\ncalculate returns, which is the default setting in PRIME, greatly outperforms all three baselines,\nregardless of where the values come from. This indicates that PRMs work better than value models\nin RL for LLMs.\n5.6\n“ZERO” EXPERIMENTS\nDeepSeek-AI et al. (2025) proposed DeepSeek-R1-Zero, which is directly trained from a base model\nwith reinforcement learning. To further investigate the “Zero” setting, we also perform RL from\n11\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500\n0.525\nOutcome Training Rewards\nPRIME-Zero\nPRIME\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n32\n34\n36\n38\n40\n42\n44\n46\n48\nAvg. Test Acc\nPRIME-Zero\nPRIME\nQwen2.5-Math-7B-Instruct\n(b) Math test accuracy across different gradient steps.\nFigure 12: “Zero” RL from Qwen2.5-Math-7B. RL from the base model converges way faster than\nthe SFT model, surpassing the instruct version within 32 steps.\n0\n20\n40\n60\n80\nSteps\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nOutcome Training Rewards\nPRIME-Zero\n(a) Outcome training rewards (10-step moving).\n0\n16\n32\n48\n64\n80\n96\nSteps\n42\n44\n46\n48\n50\n52\nAvg. Test Acc\nQwen2.5-32B-Instruct\nPRIME-Zero\n(b) Math test accuracy across different gradient steps.\nFigure 13: “Zero” RL from Qwen2.5-32B-Base. RL from a 32B base model shows more promising\ngain, surpassing the instruct version within 16 steps.\nQwen2.5-Math-7B-Base and Qwen2.5-32B-Base (Yang et al., 2024a), skipping the SFT phase. We\npresent the experimental results in Figure 12 and Figure 13. The observations are as follows:\n(1) RL from base model is suprisingly efficient and effective. Comparing PRIME from Qwen2.5-\nMath-7B and Eurus-2-7B-SFT, the “Zero” setting converges much faster. This indicates that directly\nperforming RL from a base model might be a strong alternative to the conventional SFT-RL pipeline.\n(2) Larger models benefit more. Comparing 7B and 32B models, we see that the 32B model\ngains more on both training rewards and test performance. This is aligned with the conclusion in\nDeepSeek-AI et al. (2025).\n(3) Saturation could be a potential issue. Although PRIME-Zero obtains impressive performance\ngain, we find it quickly saturated at a very early stage (about 50 steps), which hinders further\nimprovement like in DeepSeek-AI et al. (2025). This is possibly attributed to the decrease of response\ndiversity, and we leave this as future work.\n6\nRELATED WORK\nRL for LLM Reasoning. In the context of LLMs, reinforcement learning has been widely used for\naligning human preferences (Christiano et al., 2017; Ouyang et al., 2022; Cui et al., 2024), but the\nopen-source community mostly adopt the data-driven imitation learning methods (Yuan et al., 2024a;\nYue et al., 2024; Wei et al., 2024; Liu et al., 2024) to enhance the reasoning capabities of LLMs. Over\nthe past few months, the paradigm gradually shifted. OpenAI o1 (Jaech et al., 2024) first showed\nthe tremendous potential of large-sacle RL for reasoning LLMs, and recent works have verified the\nscaling effect of the simple RL recipe with merely outcome rewards (DeepSeek-AI et al., 2025; Team\n12\n\nPreprint\net al., 2025). Meanwhile, the role of dense rewards in RL remains underexplored, which is the main\nfocus of PRIME.\nImplicit Rewards. Implicit rewards are broadly adopted in LLM alignment (Rafailov et al., 2023;\nChen et al., 2024b; Azar et al., 2024; Ethayarajh et al., 2024; Rosset et al., 2024; Chen et al., 2024a).\nRafailov et al. (2024) first showed that optimizing DPO objective learns a Q function implicitly. Zhou\net al. (2024) utilized implicit rewards in PPO, and showed that dense implicit rewards are better than\nsparse ones. Yuan et al. (2024b) further extended the conclusion to any loss funtion optimizing\nEq. 3.\n7\nCONCLUSION\nAs the fuel of LLMs, data, will be depleted in the near future, we are entering a new era of\nsearch and exploration, which is exemplified by reinforcement learning (Sutton, 2019). This work\ndevelops PRIME, which produces and leverages dense rewards in online RL for LLM reasoning.\nThroughout the experiments, we validate that PRIME (1) greatly benefits sample efficiency and policy\nperformance, (2) is easy to use with minimum cost, and (3) is a general method that works with broad\nRL algorithms together.\nREFERENCES\nArash Ahmadian, Chris Cremer, Matthias Gall´e, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,\nAhmet ¨Ust¨un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning\nfrom human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal\nValko, and R´emi Munos. A general theoretical paradigm to understand learning from human\npreferences. International Conference on Artificial Intelligence and Statistics, abs/2310.12036,\n2024.\nChangyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakan-\ntham, and Min Lin. Bootstrapping language models with dpo implicit rewards. arXiv preprint\narXiv:2406.09760, 2024a.\nHuayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment\nof language models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024b.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie,\nRuobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language\nmodels with scaled ai feedback. In ICML, 2024.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,\nQihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,\nZhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao\nWu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong,\nKai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao,\nLitong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang,\nMeng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang,\nQinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang,\nShuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng\nYe, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng\n13\n\nPreprint\nLiang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan\nWang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang,\nXinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen,\nXiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li,\nY. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang,\nYi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan,\nYiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia\nHe, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang,\nZhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen\nZhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\nURL https://arxiv.org/abs/2501.12948.\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\nalignment as prospect theoretic optimization. ICML, 2024.\nJiaxuan Gao, Shusheng Xu, Wenjie Ye, Weiling Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju\nWang, and Yi Wu. On designing effective rl reward at training time for llm reasoning. ArXiv,\nabs/2410.15115, 2024.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\nInternational Conference on Machine Learning, 2022.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,\nYu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming–the\nrise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han,\nYujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: A\nchallenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific\nproblems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n3828–3850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.\n211/.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence\nwith apps. arXiv preprint arXiv:2105.09938, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021b.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nAmirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,\nAaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through\nrefined credit assignment. arXiv preprint arXiv:2410.01679, 2024.\nWouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for\nfree! In DeepRLStructPred@ICLR, 2019. URL https://api.semanticscholar.org/\nCorpusID:198489118.\n14\n\nPreprint\nNathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\nBrahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya\nMalik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris\nWilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hanna Hajishirzi.\nT¨ulu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124, 2024.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent\nalignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems,\n35:3843–3857, 2022.\nJia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif\nRasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in\nai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository,\n13:9, 2024.\nRongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and\nGe Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852,\n2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\nde Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal,\nAlexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,\nNando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. ArXiv,\nabs/2305.20050, 2023.\nZihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing\nfrontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084,\n2024.\nMeta. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nOpenAI. Openai o1 system card. ArXiv, abs/2412.16720, 2024.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems, 35:27730–\n27744, 2022.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\nin Neural Information Processing Systems, 36, 2023.\nRafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q∗: Your language model is\nsecretly a q-function. arXiv preprint arXiv:2404.12358, 2024.\nCorby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and\nTengyang Xie. Direct nash optimization: Teaching language models to self-improve with general\npreferences. ArXiv, abs/2404.03715, 2024.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel.\nHigh-\ndimensional continuous control using generalized advantage estimation. In 4th International\nConference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings, 2016.\n15\n\nPreprint\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal,\nAlekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated\nprocess verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/\n2402.03300.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,\nHaibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint\narXiv: 2409.19256, 2024.\nSkunkworksAI. reasoning-0.01, 2024.\nRichard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019.\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:\n9–44, 1988.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with\nllms. arXiv preprint arXiv:2501.12599, 2025.\nQwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data.\narXiv preprint arXiv:2410.01560, 2024.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang\nSui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ArXiv,\nabs/2312.08935, 2023.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering\ncode generation with oss-instruct. In Forty-first International Conference on Machine Learning,\n2024.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229–256, 1992.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,\nLe Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu\nCui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115,\n2024a.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,\nJianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,\nXingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert\nmodel via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122.\n16\n\nPreprint\nLifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,\nRuobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.\nAdvancing llm reasoning generalists with preference trees. ArXiv, 2024a.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels, 2024b. URL https://arxiv.\norg/abs/2412.01981.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nXiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the\nweb. ArXiv, abs/2405.03548, 2024.\nKaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li,\nGanqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou.\nUltramedical: Building specialized generalists in biomedicine, 2024.\nTianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and\nXiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv\npreprint arXiv:2402.14658, 2024.\nZhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. Weak-to-strong\nsearch: Align large language models via searching over small language models. arXiv preprint\narXiv:2405.19262, 2024.\n17\n\nPreprint\nTable 4: Actions in action-centric chain-of-thought reasoning framework.\nAction Name\nDescription\nASSESS\nAnalyze current situation, identify key elements and goals\nADVANCE\nMove forward with reasoning - calculate, conclude, or form hypothesis\nVERIFY\nCheck accuracy of current approach, look for errors\nSIMPLIFY\nBreak complex problems into simpler parts\nSYNTHESIZE\nCombine multiple pieces of information into complete solution\nPIVOT\nChange strategy when current approach isn’t working\nOUTPUT\nSummarize thought process and present final answer\nTable 5: Data statistics of SFT data.\nTask\nDataset\nSize\nAvg. Response Length\nSource\nMath\nMathInstruct-MATH (Yue et al., 2023)\n12715\n964.01\nhttps://huggingface.co/datasets/TIGER-Lab/MathInstruct\nOpenMathIns-2-Aug Math (Toshniwal et al., 2024)\n15086\n1202.25\nhttps://huggingface.co/datasets/nvidia/OpenMathInstruct-2\nNumina (Li et al., 2024)\n55845\n1331.61\nhttps://huggingface.co/datasets/AI-MO/NuminaMath-CoT\nReasoning-001 (SkunkworksAI, 2024)\n29831\n1316.49\nhttps://huggingface.co/datasets/SkunkworksAI/reasoning-0.01\nCoding\nCode-Feedback (Zheng et al., 2024)\n27663\n1805.16\nhttps://huggingface.co/datasets/m-a-p/Code-Feedback\nMagicoder (Wei et al., 2024)\n24480\n1828.72\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K\nMagicoder-OSS (Wei et al., 2024)\n28980\n1850.05\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\nBiomedicine\nUltraMedical mc (Zhang et al., 2024)\n35163\n891.06\nhttps://huggingface.co/datasets/TsinghuaC3I/UltraMedical\nTotal / Avg.\n-\n229763\n1390.75\n-\nA\nSFT DATA & TRAINING DETAILS\nWe first perform supervised finetuning on the base model to get a starter model for RL.\nAction-centric chain-of-thought reasoning. We apply imitation learning (supervised finetuning) as\na warmup stage to teach models to learn certain reasoning patterns. To this end, we first design an\naction-centric chain-of-thought reasoning framework. Table 4 shows the actions in the action-centric\nchain-of-thought reasoning framework. When the model generates answers, it conducts multi-step\nreasoning and chooses one of the 7 actions at each step. The response begins with the ASSESS action\nand ends with the OUTPUT action.\nConstruction of the SFT dataset. To construct the SFT dataset, we collect reasoning instructions\nfrom several open-source datasets. It is noteworthy that we did not include many datasets with\nground-truth answers in SFT, even though they are of higher quality. However, we reserve them for\nlater RL training. The reason is that we aim to use different datasets for SFT and RL to diversify the\nexploration in RL, and we consider ground-truth more essential in RL than in SFT. For completion,\nwe employ LLaMA-3.1-70B-Instruct to answer the instructions, with a system prompt requesting the\nmodel to perform an action-centric chain-of-thought. Table 5 summarizes the key statistics of the\ndatasets used for SFT. The datasets span mathematics, coding, and biomedicine. We finally obtain\n230K SFT data and the average response length is 1390 tokens.\nSFT Training. During the SFT phase, we conduct full parameter fine-tuning with a learning rate\nof 1e-05, utilizing the AdamW optimizer alongside a cosine annealing learning rate schedule and a\nwarmup ratio of 0.1. The batch size was set to 96, with a fixed random seed of 42. The model was\ntrained on 230K datasets for 3 epochs.\nB\nRL DATA PREPROCESSING\nB.1\nRL DATA COLLECTION AND PREPROCESSING\nWe curate a high-quality RL training dataset of mathematics and coding problems with outcome\nverifiers (LaTeX answers for math and test cases for coding). For math, we source from NuminaMath-\nCoT (Li et al., 2024), which contains about 860K math problems. The problems span from Chinese\nhigh school mathematics to International Mathematical Olympiad competition questions. For coding,\nwe source from APPS (Hendrycks et al., 2021a), CodeContests (Li et al., 2022), TACO (Li et al.,\n2023), and Codeforces2. To further increase data quality, we conduct detailed cleaning and filtering.\nFinally, we retain 457k math problems and 27k coding problems.\n2https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions\n18\n\nPreprint\nB.2\nDATA FILTERING AND QUESTION-TYPE CLASSIFICATION\nThe preprocessing pipeline employs a systematic rule-based approach to filter and classify mathemati-\ncal problems to create a high-quality dataset with solvable problems, appropriate difficulty levels, and\ncorrect solutions. We exclude problems containing figures or diagrams since they require visual pro-\ncessing capabilities. We also remove proof questions due to difficulties in answer verification. Based\non specific patterns, the remaining problems are classified into question-answering, multiple-choice,\nor fill-in-the-blank questions. Since fill-in-the-blank questions comprise less than 400 examples\ncompared to the much larger set of multiple-choice questions, we focus solely on multiple-choice\nquestions for further processing.\nB.3\nCONVERTING TO DIRECT QUESTION-ANSWER FORMAT\nWe transform multiple-choice questions into a direct question-answer format through three sequential\nstages: rule-based filtering, LLM-based filtering, and LLM-based formatting.\nWe first identify and remove questions that inherently require multiple-choice options - specifically,\nthose where comparing specific statements or properties is essential to the problem-solving process.\nThese questions cannot be meaningfully converted to a direct question-answer format. The initial\nfiltering employs simple rule-based pattern matching, searching for keywords like ”following” and\n”statement” that typically indicate option-dependent problems.\nFollowing the rule-based filtering, we employ Llama-3.1-8B-Instruct to perform a more nuanced\nclassification of the remaining questions. Our pilot study revealed that while the LLM occasionally\nmisclassifies questions, it tends to err on the conservative side - marking potentially convertible\nquestions as requiring options rather than the reverse. Given our large dataset, we accepted this\nconservative approach to maintain quality.\nFor questions classified as convertible, we implement a two-phase reformatting process: 1) Question\nReformatting: Removing choice indicators and restructuring the question to elicit direct answers. 2)\nSolution Reformatting: Converting multiple-choice solutions into step-by-step derivations, ensuring\nall final answers are presented in standard LaTeX boxed format. This systematic approach maintains\nmathematical rigor while creating a standardized format suitable for downstream applications.\nB.4\nPROBLEM AND SOLUTION VALIDATION\nThe final stage involves merging all question-answer pairs and performing LLM-based comprehensive\nvalidation. We identify two key aspects in validation: solvability and correctness.\nWe leverage state-of-the-art mathematical reasoning models, including QwQ-32B-Preview (Team,\n2024) and Qwen2.5-Math-72B-Instruct (Yang et al., 2024b), employing a self-consistency approach\nto determine problem solvability, and if solvable, verify the correctness of solutions provided in the\noriginal dataset.\nTo enhance validation accuracy, we first analyzed sample problems to identify characteristics of\nsolvable and unsolvable cases and created synthetic unsolvable problems featuring missing conditions\nor logical contradictions. Based on these samples, we developed specialized prompts to improve\nthe models’ ability to distinguish solvability. Each problem undergoes five independent validation\nattempts, where the LLM: 1) Provides step-by-step solutions using LaTeX formatting. 2) Identifies\nunsolvability due to missing conditions or logical contradictions. 3) Generates complete reason-\ning traces for solvable problems. 4) Presents final answers in standardized LaTeX boxed format\n(\\boxed{...}). 5) Document any impediments to solution completion.\nWe evaluate two key consistency measures across multiple validation attempts: 1) Status Consistency:\nagreement on problem solvability. 2) Answer Consistency: consistency of solutions across different\nattempts and agreement between generated solutions and ground truth. The final dataset retains only\nproblems that demonstrate consistent solvability across validation attempts, agreement in solutions\nacross multiple attempts, and alignment with ground truth answers. This rigorous validation process\nensures the resulting dataset comprises well-defined, solvable problems with verified, accurate\nsolutions.\n19\n\nPreprint\nTable 6: Data statistics of EurusPRM training dataset.\nDataset\nGenerator Model\nNum. Inst\nResp/Inst\nStep-level/Response-level\nUltraInteract\nLlama-3.1-8B-Inst\n20177\n8\nResponse-level\nLlama-3.1-8B-Base\n13570\n8\nResponse-level\nQwen2.5-72B-Inst\n4758\n8\nResponse-level\nQwen2.5-Math-7B-Base\n25713\n8\nResponse-level\nNumina-SynMath\nLlama-3.1-8B-Inst\n4783\n8\nResponse-level\nQwen2.5-Math-7B-Base\n5806\n8\nResponse-level\nNumina-Olympiads\nLlama-3.1-8B-Inst\n2909\n8\nResponse-level\nQwen2.5-Math-7B-Base\n4739\n8\nResponse-level\nB.5\nPRM DATA\nThe dataset statistics of training EurusPRM are shown in Table 6.\n20'),
                Paper(arxiv_id='2502.01341', authors=['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], published_at=datetime.datetime(2025, 2, 4, 10, 51, 54, 103000, tzinfo=datetime.timezone.utc), title='AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding', summary='Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.', upvotes=29, thumbnail=None, content='ALIGNVLM: Bridging Vision and Language Latent Spaces\nfor Multimodal Understanding\nAhmed Masry 1 2 Juan A. Rodriguez 1 3 4 Tianyu Zhang 1 3 5 Suyuchen Wang 1 3 5 Chao Wang 1 Aarash Feizi 1 3 6\nAkshay Kalkunte Suresh 1 Abhay Puri 1 Xiangru Jian 1 7 Pierre-Andr´e No¨el 1 Sathwik Tejaswi Madhusudhan 1\nMarco Pedersoli 1 4 Bang Liu 1 5 8 Nicolas Chapados 1 Yoshua Bengio 3 5 8 Enamul Hoque 2 Christopher Pal 1 3 8 9\nIssam H. Laradji 1 10 David Vazquez 1 Perouz Taslakian 1 Spandana Gella 1 Sai Rajeswar 1 3\nAbstract\nAligning visual features with language embed-\ndings is a key challenge in vision-language mod-\nels (VLMs). The performance of such models\nhinges on having a good connector that maps vi-\nsual features generated by a vision encoder to a\nshared embedding space with the LLM while pre-\nserving semantic similarity. Existing connectors,\nsuch as multilayer perceptrons (MLPs), often pro-\nduce out-of-distribution or noisy inputs, leading\nto misalignment between the modalities. In this\nwork, we propose a novel vision-text alignment\nmethod, ALIGNVLM, that maps visual features\nto a weighted average of LLM text embeddings.\nOur approach leverages the linguistic priors en-\ncoded by the LLM to ensure that visual features\nare mapped to regions of the space that the LLM\ncan effectively interpret. ALIGNVLM is particu-\nlarly effective for document understanding tasks,\nwhere scanned document images must be accu-\nrately mapped to their textual content. Our exten-\nsive experiments show that ALIGNVLM achieves\nstate-of-the-art performance compared to prior\nalignment methods. We provide further analysis\ndemonstrating improved vision-text feature align-\nment and robustness to noise.\n1. Introduction\nVision-Language Models (VLMs) have gained significant\ntraction in recent years as a powerful framework for multi-\nmodal document understanding tasks that involve interpret-\n1ServiceNow 2York University 3Mila 4 ´Ecole de Technolo-\ngie Sup´erieure\n5Universit´e de Montr´eal\n6McGill University\n7University of Waterloo\n8CIFAR AI Chair\n9Polytechnique\nMontr´eal 10University of British Columbia. Correspondence to:\nAhmed Masry <ahmed.masry@servicenow.com>, Sai Rajeswar\n<sai.mudumba@servicenow.com>.\nLlama-3.2-3B-Perciever R.\nLlama-3.2-3B-MLP\nLlama-3.2-3B-Ovis\nLlama-3.2-3B-Align (ours)\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\nDocVQA\nInfoVQA\nDeepForm\nKLC\nWTQ\nTabFact\nChartQA\nTextVQA\nTableVQA\nFigure 1: Performance of Different VLM Connectors.\nThe proposed ALIGN connector outperforms other methods\nacross benchmarks using the same training configuration.\nRadial distance is proportion of maximal score, truncated at\n0.7 (black dot).\ning both the visual and textual contents of scanned docu-\nments (Kim et al., 2022; Lee et al., 2023; Liu et al., 2023a;\n2024; Hu et al., 2024; Wang et al., 2023a; Rodriguez et al.,\n2024b). Such tasks are common in real-world commercial\napplications, including invoice parsing (Park et al., 2019),\nform reading (Jaume et al., 2019), and document question\nanswering (Mathew et al., 2021b). VLM architectures typ-\nically consist of three components: (i) a vision encoder to\nprocess raw images, (ii) a Large Language Model (LLM)\npre-trained on text, and (iii) a connector module that maps\nthe visual features from the vision encoder into the LLM’s\nsemantic space.\nA central challenge in this pipeline is to effectively map the\ncontinuous feature embeddings of the vision encoder into\nthe latent space of the LLM while preserving the semantic\nproperties of visual concepts. Existing approaches can be\nbroadly categorized into deep fusion and shallow fusion\n1\narXiv:2502.01341v1  [cs.CL]  3 Feb 2025\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nmethods. Deep fusion methods, such as NVLM (Dai et al.,\n2024), Flamingo (Alayrac et al., 2022), CogVLM (Wang\net al., 2023b), and LLama 3.2-Vision (Grattafiori et al.,\n2024), integrate visual and textual features by introducing\nadditional cross-attention and feed-forward layers at each\nlayer of the LLM. While effective at enhancing cross-modal\ninteraction, these methods substantially increase the param-\neter count of the VLM compared to the base LLM, resulting\nin high computational overhead and reduced efficiency.\nIn contrast, shallow fusion methods project visual features\nfrom the vision encoder into the LLM input embedding\nspace using either multilayer perceptrons (MLPs) (Liu et al.,\n2023b; 2024) or attention-based mechanisms such as the\nPerceiver Resampler (Li et al., 2023; Laurenc¸on et al., 2024;\nAlayrac et al., 2022), before concatenating them with the\ntextual prompt’s input embeddings. This approach is more\nparameter-efficient and computationally lighter than deep\nfusion methods, but it lacks a mechanism to ensure the pro-\njected embeddings remain within the region spanned by\nthe LLM’s text embeddings – i.e. regions the LLM was\npretrained to understand. As a result, unconstrained vi-\nsual features can produce out-of-distribution (OOD) and\nnoisy inputs, leading to misalignment between modalities\nand often degrading overall performance. Recent methods\nlike Ovis (Lu et al., 2024) attempt to alleviate these issues\nby introducing separate visual embeddings indexed from\nthe vision encoder outputs and combined together to con-\nstruct the visual inputs to the LLM. However, this approach\nsignificantly increases parameter count due to the massive\nembedding matrix and requires extensive training to learn a\nnew embedding space without guaranteeing alignment with\nthe LLM’s input latent space.\nTo address these limitations, this paper introduces ALIGN-\nVLM, a novel framework that sidesteps direct projection\nof visual features into the LLM embedding space. Instead,\nour proposed connector, ALIGN, maps visual features into\nprobability distributions over the LLM’s existing pretrained\nvocabulary embeddings, which are then combined into a\nweighted representation of the text embeddings. By con-\nstraining each visual feature as a convex combination of the\nLLM text embeddings, our approach leverages the linguistic\npriors already encoded in the LLM’s text space. This en-\nsures that the resulting visual features lie within the convex\nhull of the LLM’s embedding space, reducing the risk of\nnoisy or out-of-distribution inputs and improving alignment\nbetween modalities. Our experimental results show that\nthis approach improves performance on various document\nunderstanding tasks, outperforming prior connector meth-\nods by effectively fusing visual and linguistic content. We\nsummarize our main contributions as follows:\n• We propose a novel connector, ALIGN, to bridge the\nrepresentation gap between vision and text modalities.\n• We introduce a family of Vision-Language Models,\nALIGNVLM, that achieves state-of-the-art perfor-\nmance on multimodal document understanding tasks\nby leveraging ALIGN.\n• We conduct extensive experiments demonstrating the\nrobustness and effectiveness of ALIGN across different\nmodel sizes ranging from 1B to 8B parameters.\nOur code and models will be public upon acceptance.\n2. Related Work\n2.1. Vision-Language Models\nOver the past few years, Vision-Language Models (VLMs)\nhave achieved remarkable progress, largely due to advances\nin Large Language Models (LLMs). Initially demonstrating\nbreakthroughs in text understanding and generation (Brown\net al., 2020; Raffel et al., 2023; Achiam et al., 2023;\nGrattafiori et al., 2024; Qwen et al., 2025; Team, 2024),\nLLMs are now increasingly used to effectively interpret vi-\nsual inputs (Liu et al., 2023b; Li et al., 2024; Wang et al.,\n2024; Chen et al., 2024b; Dai et al., 2024; Drouin et al.,\n2024; Rodriguez et al., 2022). This progress has enabled\nreal-world applications across diverse domains, particularly\nin multimodal document understanding for tasks like form\nreading (Svetlichnaya, 2020), document question answer-\ning (Mathew et al., 2021b), and chart question answer-\ning (Masry et al., 2022). VLMs commonly adopt a three-\ncomponent architecture: a pretrained vision encoder (Zhai\net al., 2023; Radford et al., 2021), a LLM, and a connector\nmodule. A key challenge for VLMs is effectively aligning\nvisual features with the LLM’s semantic space to enable\naccurate and meaningful multimodal interpretation.\n2.2. Vision-Language Alignment for Multimodal Models\nExisting vision-language alignment approaches can be clas-\nsified into deep fusion and shallow fusion. Deep fusion\nmethods integrate visual and textual features by modifying\nthe LLM’s architecture, adding cross-attention and feed-\nforward layers. For example, Flamingo (Alayrac et al.,\n2022) employs the Perceiver Resampler, which uses fixed\nlatent embeddings to attend to vision features and fuses\nthem into the LLM via gated cross-attention layers. Simi-\nlarly, NVLM (Dai et al., 2024) adopts cross-gated attention\nwhile replacing the Perceiver Resampler with a simpler\nMLP. CogVLM (Wang et al., 2023b) extends this approach\nby incorporating new feed-forward (FFN) and QKV lay-\ners for the vision modality within every layer of the LLM.\nWhile these methods improve cross-modal alignment, they\nsignificantly increase parameter counts and computational\noverhead, making them less efficient.\nOn the other hand, shallow fusion methods are more compu-\ntationally efficient, mapping visual features into the LLM’s\n2\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nembedding space without altering its architecture. These\nmethods can be categorized into three main types: (1) MLP-\nbased mapping, such as LLaVA (Liu et al., 2023b) and\nPaliGemma (Beyer et al., 2024), which use multilayer per-\nceptrons (MLP) to project visual features but often pro-\nduce misaligned or noisy features due to a lack of con-\nstraints (Rodriguez et al., 2024b); (2) cross-attention mech-\nanisms, BLIP-2 (Li et al., 2023) uses Q-Former, which\nutilizes a fixed set of latent embeddings to cross-attend to\nvisual features, but that may still produce noisy or OOD\nvisual features; and (3) visual embeddings, such as those\nintroduced by Ovis (Lu et al., 2024), which use embeddings\nindexed by the vision encoder’s outputs to produce the vi-\nsual inputs. While this regularizes feature mapping, it adds\nsubstantial parameter overhead and creates a new vision em-\nbedding space, risking misalignment with the LLM’s text\nembedding space. Encoder-free VLMs, like Fuyu-8B 1 and\nEVE (Diao et al., 2024), eliminate dedicated vision encoders\nbut show degraded performance (Beyer et al., 2024).\nIn contrast, ALIGNVLM maps visual features from the vi-\nsion encoder into probability distributions over the LLM’s\ntext embeddings, using them to compute a convex combi-\nnation. By leveraging the linguistic priors encoded in the\nLLM’s vocabulary, ALIGNVLM ensures that visual features\nremain within the convex hull of the text embeddings, mit-\nigating noisy or out-of-distribution inputs and enhancing\nalignment, particularly for tasks that require joint modalities\nrepresentation like multimodal document understanding.\n3. Methodology\n3.1. Model Architecture\nThe overall model architecture, shown in Figure 2, consists\nof three main components:\n(1) Vision Encoder.\nTo handle high-resolution images of\ndifferent aspect ratios, we divide each input image into mul-\ntiple tiles according to one of the predefined aspect ratios\n(e.g., 1:1, 1:2, . . . , 9:1) chosen via a coverage ratio (Lu\net al., 2024; Chen et al., 2024a). Due to limited compu-\ntational resources, we set the maximum number of tiles\nto 9. Each tile is further partitioned into 14 × 14 patches,\nprojected into vectors, and processed by a SigLip-400M vi-\nsion encoder (Zhai et al., 2023) to extract contextual visual\nfeatures.\nEach tile t ∈{1, · · · , T} is divided into Nt patches\nPt = {pt,1, · · · , pt,Nt},\nwhere pt,i is the i-th patch of tile t. The vision encoder\n1https://www.adept.ai/blog/fuyu-8b\nmaps these patches to a set of visual feature vectors\nFt = VisionEncoder(Pt)\nFt = {ft,1, · · · , ft,Nt},\nft,i ∈Rd.\nFinally, we concatenate the feature sets across all tiles into\na single output\nF = concat\n\x10\nF1, F2, · · · , FT\n\x11\n.\n(2) ALIGN Module.\nThis module aligns the visual fea-\ntures with the LLM. A linear layer W1 ∈RD×d first\nprojects the visual features F ∈RT ·Nt×d to the LLM’s\ntoken embedding space: one RD vector per token. A sec-\nond linear layer W2 ∈RV ×D (initialized from the LLM’s\nlanguage-model head) followed by a softmax, produces a\nprobability simplex Pvocab over the LLM’s vocabulary (V\ntokens)\nPvocab =\n(1)\nsoftmax(LayerNorm(W2 LayerNorm(W1F)))\nWe then use the LLM text embeddings Etext ∈RV ×D to\ncompute a weighted sum\nF′\nalign = P⊤\nvocabEtext.\n(2)\nFinally, we concatenate F′\nalign with the tokenized text em-\nbeddings to form the LLM input\nHinput = concat\n\x00F′\nalign, Etext(x)\n\x01\n,\nwhere Etext(x) is obtained by tokenizing the input text x =\n(x1, · · · , xM) and selecting the corresponding embeddings\nfrom Etext such that\nEtext(x) =\n\x02\nEtext(x1), · · · , Etext(xM)\n\x03\n.\n(3)\n(3) Large Language Model.\nWe feed the concatenated\nvision and text vectors, Hinput, into the LLM, which then\ngenerates output text auto-regressively. To demonstrate\nthe effectiveness of our alignment technique, we experi-\nment with the Llama 3.1 model family (Grattafiori et al.,\n2024). These models offer state-of-the-art performance and\npermissive licenses, making them suitable for commercial\napplications. In particular, we utilize Llama 3.2-1B, Llama\n3.2-3B, and Llama 3.1-8B.\n3.2. Motivation and relation with existing methods\nBy construction, each RD representation in F′\nalign is con-\nstrained to the convex hull of the points Etext, thus concen-\ntrating the visual features in the part of latent space that\nthe LLM can effectively interpret. Moreover, we argue that\n3\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nVision \nEncoder\nLinear\nLayer Norm\nLM Head (LLM)\nLayer Norm\nSoftmax\nWeighted \nAverage Sum\nVision Inputs\nLLM Embedding Matrix\nText Inputs\nFull Embedding\nMatrix\nSelected Text \nEmbeddings\nText \nTokenizer\nAlign Module\nLLM\nQuestion: What percentage of \nAmericans are online?\nResponse: 90%\nFigure 2: ALIGNVLM Model Architecture. The vision encoder extracts image features, which are processed to produce\nprobabilities over the LLM embeddings. A weighted average combines these probabilities with embeddings to generate\nvision input vectors. Text inputs are tokenized, and the corresponding embeddings are selected from the embedding matrix,\nwhich is then used as input to the LLM. We display the vision layers in blue , and the text layers in purple .\nour initialization of W2 to the language model head is an\ninductive bias toward recycling some of the semantics of\nthese text tokens into visual tokens. This contrasts with\npast methods that have been proposed to adapt the vision\nencoder outputs F ∈RT ·Nt×d to an F′ ∈RT ·Nt×D to be\nfed to the LLM. Here, we consider two examples in more\ndetail, highlighting these contrasts.\n(1) MLP Connector (Liu et al., 2023b) applies a linear pro-\njection with parameters WMLP ∈RD×d and bMLP ∈RD,\nfollowed by an activation function σ (e.g., ReLU)\nF′\nMLP = σ(WMLPF + bMLP).\nThese parameters are all learned from scratch, with no par-\nticular bias aligning them to text embeddings.\n(2) Visual Embedding Table (Lu et al., 2024) introduces an\nentire new set of visual embeddings EVET ∈RK×D which,\ntogether with the weights WVET ∈RK×d, specifies\nF′\nVET = softmax(WVETF)⊤EVET.\nWhen D < d, our W2W1 amounts to a low-rank version\nof WVET. There is thus much more to learn to obtain F′\nVET,\nand there is again no explicit pressure to align it with the\ntext embeddings.\n3.3. Training Datasets & Stages\nWe train our model in three stages:\nStage 1.\nThis stage focuses on training the ALIGN Mod-\nule to map visual features to the LLM’s text embeddings\neffectively. We use the CC-12M dataset (Changpinyo et al.,\n2021), a large-scale web dataset commonly used for VLM\npretraining (Liu et al., 2023b), which contains 12M image-\ntext pairs. However, due to broken or unavailable links,\nwe retrieved 8.1M pairs. This dataset facilitates the align-\nment of visual features with the text embedding space of\nthe LLM. During this stage, we train the full model, as this\napproach improves performance and stabilizes the training\nof the ALIGN Module.\nStage 2.\nThe goal is to enhance the model’s document\nunderstanding capabilities, such as OCR, document struc-\nture comprehension, in-depth reasoning, and instruction-\nfollowing. We leverage the BigDocs-7.5M dataset (Ro-\ndriguez et al., 2024a), a curated collection of license-\npermissive datasets designed for multimodal document un-\nderstanding. This dataset aligns with the Accountability,\nResponsibility, and Transparency (ART) principles (Bom-\nmasani et al., 2023; Vogus & Llans´oe, 2021), ensuring com-\npliance for commercial applications. As in Stage 1, we train\nthe full model during this stage.\nStage 3.\nTo enhance the model’s instruction-tuning ca-\npabilities, particularly for downstream tasks like question\nanswering, we further train it on the DocDownstream (Ro-\ndriguez et al., 2024a; Hu et al., 2024) instruction tuning\ndataset. In this stage, the vision encoder is frozen, focusing\ntraining exclusively on the LLM and ALIGN module.\n4. Experimental Setup\nSetup.\nWe conduct all experiments using 8 nodes of H100\nGPUs, totaling 64 GPUs. For model training, we leverage\n4\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nTable 1: Main Results on General Document Benchmarks. We compare ALIGNVLM (ours) with state-of-the-art\n(SOTA) open and closed-source instructed models, and with base models that we trained using the process described in\nSection 3.3. ALIGNVLM models outperform all Base VLM models trained in the same data regime. Our models also\nperform competitively across document benchmarks even compared with SOTA models, in which the data regime is more\ntargeted and optimized. Color coding for comparison: closed-source models , open-source models below 7B parameters ,\nopen-source models between 7-12B parameters .\nModel\nDocVQA\nVAL\nInfoVQA\nVAL\nDeepForm\nTEST\nKLC\nTEST\nWTQ\nTEST\nTabFact\nTEST\nChartQA\nTEST\nTextVQA\nVAL\nTableVQA\nTEST\nAvg. Score\nClosed-Source VLMs\n(Opaque Training Data)\nClaude-3.5 Sonnet\n88.48\n59.05\n31.41\n24.82\n47.13\n53.48\n51.84\n71.42\n81.27\n56.54\nGeminiPro-1.5\n91.23\n73.94\n32.16\n24.07\n50.29\n71.22\n34.68\n68.16\n80.43\n58.46\nGPT-4o 20240806\n92.80\n66.37\n38.39\n29.92\n46.63\n81.10\n85.70\n70.46\n72.87\n64.91\nOpen-Source Instruct VLMs\n(Semi-Opaque Training Data)\nJanus-1.3B (Wu et al., 2024a)\n30.15\n17.09\n0.62\n15.06\n9.30\n51.34\n57.20\n51.97\n18.67\n27.93\nQwen2-VL-2B (Wang et al., 2024)\n89.16\n64.11\n32.38\n25.18\n38.20\n57.21\n73.40\n79.90\n43.07\n55.84\nInternVL-2.5-2B (Chen et al., 2024b)\n87.70\n61.85\n13.14\n16.58\n36.33\n57.26\n74.96\n76.85\n42.20\n51.87\nDeepSeek-VL2-Tiny-3.4B (Wu et al., 2024b)\n88.57\n63.88\n25.11\n19.04\n35.07\n52.15\n80.92\n80.48\n56.30\n55.72\nPhi3.5-Vision-4B (Abdin et al., 2024)\n86.00\n56.20\n10.47\n7.49\n17.18\n30.43\n82.16\n73.12\n70.70\n48.19\nQwen2-VL-7B (Wang et al., 2024)\n93.83\n76.12\n34.55\n23.37\n52.52\n74.68\n83.16\n84.48\n53.97\n64.08\nLLaVA-NeXT-7B (Xu et al., 2024)\n63.51\n30.90\n1.30\n5.35\n20.06\n52.83\n52.12\n65.10\n32.87\n36.00\nDocOwl1.5-8B (Hu et al., 2024)\n80.73\n49.94\n68.84\n37.99\n38.87\n79.67\n68.56\n68.91\n52.60\n60.68\nInternVL-2.5-8B (Chen et al., 2024b)\n91.98\n75.36\n34.55\n22.31\n50.33\n74.75\n82.84\n79.00\n52.10\n62.58\nOvis-1.6-Gemma2-9B (Lu et al., 2024)\n88.84\n73.97\n45.16\n23.91\n50.72\n76.66\n81.40\n77.73\n48.33\n62.96\nLlama3.2-11B (Grattafiori et al., 2024)\n82.71\n36.62\n1.78\n3.47\n23.03\n58.33\n23.80\n54.28\n22.40\n34.04\nPixtral-12B (Agrawal et al., 2024)\n87.67\n49.45\n27.37\n24.07\n45.18\n73.53\n71.80\n76.09\n67.13\n58.03\nDocument Understanding Instructed Models\n(Instruction Tuned on BigDocs-7.5M + DocDownStream (Rodriguez et al., 2024a; Hu et al., 2024))\nQwen2-VL-2B (base+) (Qwen et al., 2025)\n57.23\n31.88\n49.31\n34.39\n31.61\n64.75\n68.60\n61.01\n47.53\n49.59\nALIGNVLM-Llama-3.2-1B (ours)\n72.42\n38.16\n60.47\n33.71\n28.66\n71.31\n65.44\n48.81\n50.29\n52.14\nALIGNVLM-Llama-3.2-3B (ours)\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.10\n58.81\nDocOwl1.5-8B (base+) (Hu et al., 2024)\n78.70\n47.62\n64.39\n36.93\n35.69\n72.65\n65.80\n67.30\n49.03\n57.56\nLlama3.2-11B (base+) (Grattafiori et al., 2024)\n78.99\n44.27\n67.05\n37.22\n40.18\n78.04\n71.40\n68.46\n56.73\n60.26\nALIGNVLM-Llama-3.1-8B (ours)\n81.18\n53.75\n63.25\n35.50\n45.31\n83.04\n75.00\n64.60\n64.33\n62.88\nthe MS-Swift framework (Zhao et al., 2024) for its flexibil-\nity. Additionally, we utilize the DeepSpeed framework (Am-\ninabadi et al., 2022), specifically the ZeRO-3 configuration,\nto optimize efficient parallel training across multiple nodes.\nDetailed hyperparameters are outlined in Appendix A.1.\nBaselines.\nOur work focuses on architectural innovations,\nso we ensure that all baselines are trained on the same\ndatasets. To enable fair comparisons, we evaluate our mod-\nels against a set of Base VLMs fine-tuned on the same\ninstruction-tuning tasks (Stages 2 and 3) as our models,\nusing the BigDocs-7.5M and BigDocs-DocDownstream\ndatasets. This approach ensures consistent training data,\navoiding biases introduced by the Instruct versions of\nVLMs, which are often trained on undisclosed instruction-\ntuning datasets. Due to the scarcity of recently released\npublicly available Base VLMs, we primarily compare our\nmodel against the following Base VLMs of varying sizes:\nQwen2-VL-2B (Wang et al., 2024), DocOwl1.5-8B (Hu\net al., 2024), and LLama 3.2-11B (Grattafiori et al., 2024).\nFor additional context, we also include results from\nthe Instruct versions of recent VLMs of different sizes:\nPhi3.5-Vision-4B (Abdin et al., 2024), Qwen2-VL-2B and\n7B (Wang et al., 2024), LLaVA-NeXT-7B (Liu et al.,\n2024), InternVL2.5-2B and 8B (Chen et al., 2024b), Janus-\n1.3B (Wu et al., 2024a), DeepSeek-VL2-Tiny (Wu et al.,\n2024b), Ovis1.6-Gemma-9B (Lu et al., 2024), Llama3.2-\n11B (Grattafiori et al., 2024), DocOwl1.5-8B (Hu et al.,\n2024), and Pixtral-12B (Agrawal et al., 2024).\nEvaluation Benchmarks.\nWe evaluate our models on a\ndiverse range of document understanding benchmarks that\nassess the model’s capabilities in OCR, chart reasoning,\ntable processing, or form comprehension. In particular, we\nemploy the VLMEvalKit (Duan et al., 2024) framework\nand report the results on the following popular benchmarks:\n5\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nTable 2: Impact of Connector Designs on VLM Performance: We present the results of experiments evaluating different\nconnector designs for conditioning LLMs on visual features. Our proposed ALIGN connector is compared against a basic\nMulti-Layer Perceptron (MLP), the Perceiver Resampler, and Ovis. The results demonstrate that ALIGN consistently\noutperforms these alternatives across all benchmarks.\nModel\nDocVQA\nVAL\nInfoVQA\nVAL\nDeepForm\nTEST\nKLC\nTEST\nWTQ\nTEST\nTabFact\nTEST\nChartQA\nTEST\nTextVQA\nVAL\nTableVQA\nTEST\nAvg. Score\nLlama-3.2-3B-MLP\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n53.06\nLlama-3.2-3B-Perciever R.\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n50.68\nLlama-3.2-3B-Ovis\n74.68\n42.11\n58.02\n33.50\n33.13\n76.67\n67.92\n52.60\n53.93\n54.72\nLlama-3.2-3B-ALIGN (ours)\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.10\n58.81\nDocVQA (Mathew et al., 2021b), InfoVQA (Mathew et al.,\n2021a), DeepForm (Svetlichnaya, 2020), KLC (Stanisławek\net al., 2021), WTQ (Pasupat & Liang, 2015), TabFact (Chen\net al., 2020), ChartQA (Masry et al., 2022), TextVQA (Singh\net al., 2019), and TableVQA (Kim et al., 2024).\n5. Results\n5.1. Main Results\nTable 1 presents the performance of ALIGNVLM com-\npared to state-of-the-art (SOTA) open- and closed-source\ninstructed models, as well as baseline Base VLMs fine-tuned\nin the same instruction-tuning setup. The results demon-\nstrate that ALIGNVLM consistently outperforms all Base\nVLMs within the same size category and achieves com-\npetitive performance against SOTA Instruct VLMs despite\nbeing trained on a more limited data regime. Below, we\nprovide a detailed analysis.\nALIGNVLM vs. Base VLMs.\nOur ALIGNVLM mod-\nels, based on Llama 3.2-1B and Llama 3.2-3B, significantly\noutperform the corresponding Base VLM, Qwen2-VL-2B,\nby up to 9.22%. Notably, ALIGNVLM-Llama-3.2-3B sur-\npasses DocOwl1.5-8B, which has 4B more parameters,\ndemonstrating the effectiveness of ALIGN in enhancing mul-\ntimodal capabilities compared to traditional shallow fusion\nmethods (e.g., MLPs). Furthermore, our 8B model achieves\na 2.62% improvement over Llama3.2-11B despite sharing\nthe same Base LLM, Llama3.1-8B. Since all models in this\ncomparison were trained on the same instruction-tuning\nsetup, this experiment provides a controlled evaluation, iso-\nlating the impact of architectural differences rather than\ndataset biases. Consequently, these results suggest that\nALIGNVLM outperforms VLMs with shallow fusion tech-\nniques and surpasses parameter-heavy deep fusion VLMs,\nsuch as Llama3.2-11B, while maintaining a more efficient\narchitecture.\nALIGNVLM vs. Instruct VLMs.\nEven as open-source\nInstruct models are trained on significantly larger, of-\nten undisclosed instruction-tuning datasets, ALIGNVLM\nachieves superior performance. For instance, ALIGNVLM-\nLlama-3.2-3B (58.81%) outperforms all instructed VLMs in\nits size category, surpassing its closest competitor, Qwen2-\nVL-2B (55.84%), by 2.97%. Additionally, our 8B model\noutperforms significantly larger models such as Llama 3.2-\n11B and PixTral-12B by substantial margins. It also sur-\npasses InternVL-2.5-8B and performs competitively with\nQwen2-VL-7B, though a direct comparison may not be\nentirely fair since Qwen2-VL-7B was trained on an undis-\nclosed instruction-tuning dataset. Finally, ALIGNVLM also\nexhibits comparable performance to closed-source models\nlike GeminiPro-1.5 and GPT4o.\nOverall, these results validate the effectiveness of ALIGN\nand establish ALIGNVLM as a state-of-the-art model for\nmultimodal document understanding.\n5.2. Impact of Connector Designs on VLM Performance\nTo assess the effectiveness of our ALIGN module, we com-\npare it against three different and widely used shallow fusion\nVLM connectors: MLP, Perceiver Resampler, and Ovis. The\nresults in Table 2 show that ALIGN consistently outperforms\nall alternatives, demonstrating its superiority both in align-\ning visual and textual modalities and in multimodal docu-\nment understanding. MLP and Perceiver Resampler achieve\nthe lowest performance, 53.06% and 50.68%, respectively,\ndue to their direct feature projection, which lacks an explicit\nmechanism to align visual features with the LLM’s text\nspace, leading to misalignment. Ovis introduces a separate\nvisual embedding table, but this additional complexity does\nnot significantly improve alignment, yielding only 54.72%\naccuracy. In contrast, ALIGN ensures that visual features\nremain within the convex hull of the LLM’s text latent space,\nleveraging the linguistic priors of the LLM to enhance align-\nment and mitigate noisy embeddings. This design leads to\nthe highest performance (58.81%), establishing ALIGN as\nthe most effective connector for integrating vision and lan-\nguage in multimodal document understanding. We provide\nsome example outputs of the Llama-3.2-3B models with\ndifferent connector designs in Appendix A.3.\n6\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nFigure 3: Probability distribution over the LLM text\ntokens, showing dense probabilities and higher values for\ntokens associated with white space in document images.\n5.3. Probability Distribution over Text Tokens Analysis\nTo better understand the behavior of ALIGN, we examine\nthe probability distribution, Pvocab in Eq (1), over the LLM’s\ntext vocabulary generated from visual features. Specifically,\nwe process 100 document images through the vision en-\ncoder and ALIGN, then average the resulting probability\ndistributions across all image patches. The final distribution\nis shown in Figure 3. As illustrated, the distribution is dense\n(rather than sparse), with the highest probability assigned to\na single token being 0.0118. This can be explained by the\nvision feature space being continuous and of much higher\ncardinality than the discrete text space. Indeed, while the\nLLM has 128K distinct vocabulary tokens, an image patch\n(e.g., 14×14 pixels) contains continuous, high-dimensional\ninformation that cannot be effectively mapped to a single or\na few discrete tokens.\nFurthermore, we observe that tokens on the left side of the\ndistribution in Figure 3 have higher probabilities than the\nrest. Upon investigation, we found that these tokens corre-\nspond to patches that are predominantly white – a common\nfeature in document images. Further analysis of the associ-\nated text tokens reveals that they predominantly consist of\npunctuation marks, as illustrated further in Appendix A.2.\nThis suggests that the model repurposes punctuation marks\nto represent whitespaces. This may be attributed to the fact\nthat both punctuation and whitespaces act as structural cues\nand separators. Other possibilities include whitespaces be-\ning rarely directly-required to perform a task, and LLMs\nmay pay less specific attention to common tokens such as\npunctuation.\n5.4. Pixel-Level Tasks Analysis\nTo rigorously evaluate the ability of vision-language mod-\nels to integrate fine-grained visual and textual pixel-level\ncues, we test our model on the VCR benchmark (Zhang\n0\n20\n40\n60\nExact Match (%)\nVCR EN Hard\nVCR EN Easy\n48.07\n65.84\n37.89\n51.43\nLlama-3.2-3B-Align (Ours)\nLlama-3.2-3B-MLP\nFigure 4: Comparison of Llama-3.2-3b-ALIGN and Llama-\n3.2-3B-MLP on the Easy and Hard VCR tasks.\net al., 2024), which requires the model to recover partially\noccluded texts with pixel-level hints from the revealed parts\nof the text. This task challenges VLM’s alignment of text\nand image in extreme situations. Current state-of-the-art\nmodels like GPT-4V (OpenAI et al., 2023), Claude 3.5 Son-\nnet (Anthropic, 2024), and Llama-3.2 (Dubey et al., 2024)\nsignificantly underperform humans on hard VCR task due to\ntheir inability to process subtle pixel-level cues in occluded\ntext regions. These models frequently discard critical vi-\nsual tokens during image tokenization on semantic priors,\noverlooking the interplay between partial character strokes\nand contextual visual scenes. To evaluate performance on\nVCR, we modify our Stage 3 SFT dataset composition by\nreplacing the exclusive use of DocDownstream with a 5:1\nblended ratio of DocDownstream and VCR training data.\nThis adjustment enables direct evaluation of our architecture\nALIGN’s ability to leverage pixel-level character cues.\nFrom the experimental outcomes, it is evident that ALIGN-\nVLM consistently outperforms the MLP Connector Model\nacross both easy and hard settings of the pixel-level VCR\ntask (see Figure 4), with improvements ranging from 10.18%\non the hard setting to 14.41% on the easy setting.\nWe provide a case study on VCR in Figure 5, featuring four\nrepresentative examples. In Figure 5a, it is evident that the\nMLP connector model fails to capture semantic consistency\nas effectively as ALIGNVLM. The phrase “The commune\nfirst census in written history in” (where the words in italics\nare generated by the model while the rest are in the image)\nis not as semantically coherent as the phrase generated by\nALIGN “The commune first appears in written history in”.\nBeyond the issue of semantic fluency, in Figure 5b we also\nobserve that ALIGNVLM successfully identifies the uncov-\nered portion of the letter “g” in “accounting” and uses it as\na pixel-level hint to infer the correct word. In contrast, the\nMLP model fails to effectively attend to this crucial detail.\n7\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nGT:\n(appears in written\nhistory in)\nMLP:\n(census in written his-\ntory in) ✗\nALIGN\n(appears in written\nhistory in) ✓\n(a) Positive Example 1\nGT:\n(the system used for\nassigning)\nMLP:\n(the system used for\naccounting) ✗\nALIGN\n(the system used for\nassigning) ✓\n(b) Positive Example 2\nGT:\n(mines situated near\nLlanengan on)\nMLP:\n(mines situated near\nLlanengan on) ✓\nALIGN (mines situated near\nLlanongan on) ✗\n(c) Negative Example 1\nGT:\n(Gorden County\nis home to)\nMLP:\n(Gorden County\nis home to) ✓\nALIGN\n(Garden County\nis home to) ✗\n(d) Negative Example 2\nFigure 5: Case Study for Pixel-Level Tasks. We provide examples of our proposed ALIGN connector compared with a the\nMulti-Layer Perceptron (MLP) connector. The ALIGN connector tends to better map visual elements to common words. GT\nis the ground truth.\nFigures 5c and 5d show examples where ALIGNVLM fails\non the VCR task. These carefully picked instances show\nthat our method mistakes names of landmarks with common\nwords when the two are very similar. As seen in the exam-\nples, ALIGNVLM mistakes “Llanengan” for “Llanongan”\nand “Gorden” for “Garden”. In both instances, the pairs\ndiffer by one character, indicating perhaps that ALIGNVLM\ntends to align vision representations to more common to-\nkens in the vocabulary. One approach that would potentially\nmitigate such errors would be to train ALIGNVLM with\nmore contextually-relevant data.\n5.5. Robustness to Noise Analysis\nTo evaluate the robustness of our ALIGN connector to noisy\nvisual features, we conduct an experiment where random\nGaussian noise is added to the visual features produced by\nthe vision encoder before passing them into the connector.\nSpecifically, given the visual features F ∈RN×d output\nby the vision encoder (where N is the number of feature\nvectors and d is their dimensionality), we perturbed them as\neF = F + N,\nN ∼N(0, σ = 3).\nTable 3: Robustness to Noise. Comparison of Avg. Scores\nwith and without Gaussian noise (σ = 3), including perfor-\nmance drop (∆).\nModel\nWithout Noise\nWith Noise\nDrop (∆)\nLlama-3.2-3B-MLP\n53.06\n27.52\n↓25.54\nLlama-3.2-3B-ALIGN (ours)\n58.81\n57.14\n↓1.67\nAs shown in Table 3, our ALIGN connector demonstrates\nhigh robustness to noise, with only a 1.67% average drop in\nperformance. In contrast, the widely adopted MLP connec-\ntor suffers a significant performance degradation of 25.54%,\nhighlighting its vulnerability to noisy inputs. These em-\npirical results support our hypothesis that leveraging the\nknowledge encoded in the LLM’s text embeddings and con-\nstraining the visual features within the convex hull of the\ntext latent space act as a regularization mechanism, reducing\nthe model’s sensitivity to noisy visual features.\n6. Conclusion\nWe introduce ALIGN, a novel connector designed to align\nvision and language latent spaces in vision-language mod-\nels (VLMs), specifically enhancing multimodal document\nunderstanding. By improving cross-modal alignment and\nminimizing noisy embeddings, our models, ALIGNVLM,\nwhich leverage ALIGN, achieve state-of-the-art performance\nacross diverse document understanding tasks. This includes\noutperforming base VLMs trained on the same datasets and\nopen-source instruct models trained on undisclosed data.\nExtensive experiments and ablations validate the robustness\nand effectiveness of ALIGN compared to existing connec-\ntor designs, establishing it as a significant contribution to\nvision-language modeling. Future work will explore train-\ning on more diverse instruction-tuning datasets to generalize\nbeyond document understanding to broader domains.\n8\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nImpact Statement\nThis work contributes to the multimodal AI research com-\nmunity by introducing a novel approach for fusing vision\nand language modalities within large language models. By\nleveraging our framework in the context of generative mod-\nels, we enable more effective integration of visual and tex-\ntual information, enhancing the generative models’ ability\nto generate free-form text for multimodal tasks. However,\nlike all generative models, our approach is subject to poten-\ntial biases and hallucinations—challenges inherent to large\nlanguage models that must be carefully considered in de-\nployment. Since these issues are not unique to our approach,\nwe do not highlight any specific concerns here.\nReferences\nAbdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan,\nA. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl,\nH., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai,\nM., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W.,\nChen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X.,\nDixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao,\nM., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S.,\nHaider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter,\nD., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis,\nN., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J.,\nKurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang,\nC., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M.,\nLiu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh,\nA., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra,\nA., Modi, H., Nguyen, A., Norick, B., Patra, B., Perez-\nBecker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M.,\nRen, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O.,\nSaarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah,\nS., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X.,\nTanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang,\nG., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R.,\nWen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao,\nB., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F.,\nYang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C.,\nZhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y.,\nZhang, Y., and Zhou, X. Phi-3 technical report: A highly\ncapable language model locally on your phone, 2024.\nURL https://arxiv.org/abs/2404.14219.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAgrawal, P., Antoniak, S., Hanna, E. B., Bout, B., Chaplot,\nD., Chudnovsky, J., Costa, D., Monicault, B. D., Garg, S.,\nGervet, T., Ghosh, S., H´eliou, A., Jacob, P., Jiang, A. Q.,\nKhandelwal, K., Lacroix, T., Lample, G., Casas, D. L.,\nLavril, T., Scao, T. L., Lo, A., Marshall, W., Martin, L.,\nMensch, A., Muddireddy, P., Nemychnikova, V., Pellat,\nM., Platen, P. V., Raghuraman, N., Rozi`ere, B., Sablay-\nrolles, A., Saulnier, L., Sauvestre, R., Shang, W., Solet-\nskyi, R., Stewart, L., Stock, P., Studnia, J., Subramanian,\nS., Vaze, S., Wang, T., and Yang, S. Pixtral 12b, 2024.\nURL https://arxiv.org/abs/2410.07073.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr,\nI., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,\nReynolds, M., Ring, R., Rutherford, E., Cabi, S., Han,\nT., Gong, Z., Samangooei, S., Monteiro, M., Menick,\nJ., Borgeaud, S., Brock, A., Nematzadeh, A., Shar-\nifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O.,\nZisserman, A., and Simonyan, K.\nFlamingo: a vi-\nsual language model for few-shot learning, 2022. URL\nhttps://arxiv.org/abs/2204.14198.\nAminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A.,\nLi, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O.,\nand He, Y. Deepspeed inference: Enabling efficient infer-\nence of transformer models at unprecedented scale, 2022.\nURL https://arxiv.org/abs/2207.00032.\nAnthropic. The claude 3 model family: Opus, sonnet, haiku.\n2024.\nBeyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang,\nX., Salz, D., Neumann, M., Alabdulmohsin, I., Tschan-\nnen, M., Bugliarello, E., Unterthiner, T., Keysers, D.,\nKoppula, S., Liu, F., Grycner, A., Gritsenko, A., Houlsby,\nN., Kumar, M., Rong, K., Eisenschlos, J., Kabra, R.,\nBauer, M., Boˇsnjak, M., Chen, X., Minderer, M., Voigt-\nlaender, P., Bica, I., Balazevic, I., Puigcerver, J., Papalam-\npidi, P., Henaff, O., Xiong, X., Soricut, R., Harmsen, J.,\nand Zhai, X. Paligemma: A versatile 3b vlm for trans-\nfer, 2024. URL https://arxiv.org/abs/2407.\n07726.\nBommasani, R., Klyman, K., Longpre, S., Kapoor, S.,\nMaslej, N., Xiong, B., Zhang, D., and Liang, P. The foun-\ndation model transparency index, 2023. URL https:\n//arxiv.org/abs/2310.12941.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R.\nConceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts, 2021. URL\nhttps://arxiv.org/abs/2102.08981.\nChen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li,\nS., Zhou, X., and Wang, W. Y. Tabfact: A large-scale\n9\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\ndataset for table-based fact verification. In International\nConference Learning Representations, 2020.\nChen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E.,\nTong, W., Hu, K., Luo, J., Ma, Z., Ma, J., Wang, J.,\nDong, X., Yan, H., Guo, H., He, C., Shi, B., Jin, Z.,\nXu, C., Wang, B., Wei, X., Li, W., Zhang, W., Zhang,\nB., Cai, P., Wen, L., Yan, X., Dou, M., Lu, L., Zhu, X.,\nLu, T., Lin, D., Qiao, Y., Dai, J., and Wang, W. How\nfar are we to gpt-4v? closing the gap to commercial\nmultimodal models with open-source suites, 2024a. URL\nhttps://arxiv.org/abs/2404.16821.\nChen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S.,\nZhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl:\nScaling up vision foundation models and aligning for\ngeneric visual-linguistic tasks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 24185–24198, 2024b.\nDai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J.,\nRintamaki, T., Shoeybi, M., Catanzaro, B., and Ping,\nW. Nvlm: Open frontier-class multimodal llms. arXiv\npreprint arXiv: 2409.11402, 2024.\nDiao, H., Cui, Y., Li, X., Wang, Y., Lu, H., and Wang, X.\nUnveiling encoder-free vision-language models. arXiv\npreprint arXiv:2406.11832, 2024.\nDrouin, A., Gasse, M., Caccia, M., Laradji, I. H., Verme,\nM. D., Marty, T., Boisvert, L., Thakkar, M., Cappart, Q.,\nVazquez, D., Chapados, N., and Lacoste, A. Workarena:\nHow capable are web agents at solving common knowl-\nedge work tasks?, 2024. URL https://arxiv.org/\nabs/2403.07718.\nDuan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu,\nY., Dong, X., Zang, Y., Zhang, P., Wang, J., et al.\nVlmevalkit: An open-source toolkit for evaluating large\nmulti-modality models.\nIn Proceedings of the 32nd\nACM International Conference on Multimedia, pp. 11198–\n11201, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Yang, A., Fan, A.,\nGoyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravanku-\nmar, A., and et al. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783, 2024.\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian,\nA., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A.,\nVaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn,\nA., Yang, A., Mitra, A., Sravankumar, A., Korenev,\nA., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A.,\nGregerson, A., Spataru, A., Roziere, B., Biron, B., Tang,\nB., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra,\nC., McConnell, C., Keller, C., Touret, C., Wu, C., Wong,\nC., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D.,\nPintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes,\nD., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan,\nE., Smith, E. M., Radenovic, F., Guzm´an, F., Zhang, F.,\nSynnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail,\nG., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Ko-\nrevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A.,\nKloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J.,\nLee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J.,\nShah, J., van der Linde, J., Billock, J., Hong, J., Lee, J.,\nFu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton,\nJ., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia,\nJ., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li,\nK., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik,\nK., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary,\nL., van der Maaten, L., Chen, L., Tan, L., Jenkins, L.,\nMartin, L., Madaan, L., Malo, L., Blecher, L., Landzaat,\nL., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh,\nM., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham,\nM., Rita, M., Pavlova, M., Kambadur, M., Lewis, M.,\nSi, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N.,\nBashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N.,\nDuchenne, O., C¸ elebi, O., Alrassy, P., Zhang, P., Li, P.,\nVasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan,\nP., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan,\nR., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic,\nR., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R.,\nSauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva,\nR., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S.,\nSingh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang,\nS., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang,\nS., Vandenhende, S., Batra, S., Whitman, S., Sootla, S.,\nCollot, S., Gururangan, S., Borodinsky, S., Herman, T.,\nFowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speck-\nbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V.,\nGupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do,\nV., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong,\nW., Fu, W., Meers, W., Martinet, X., Wang, X., Wang,\nX., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Gold-\nschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang,\nY., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z.,\nPapakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey,\nA., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand,\nA., Menon, A., Sharma, A., Boesenberg, A., Baevski, A.,\nFeinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A.,\nLupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poul-\nton, A., Ryan, A., Ramchandani, A., Dong, A., Franco,\nA., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A.,\nBharambe, A., Eisenman, A., Yazdan, A., James, B.,\nMaurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola,\nB. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock,\nB., Wasti, B., Spence, B., Stojkovic, B., Gamido, B.,\nMontalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C.,\n10\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nWang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C.,\nTindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty,\nD., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine,\nD., David, D., Parikh, D., Liskovich, D., Foss, D., Wang,\nD., Le, D., Holland, D., Dowling, E., Jamil, E., Mont-\ngomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T.,\nBrinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun,\nF., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Cag-\ngioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz,\nG., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov,\nG., Guangyi, Zhang, Lakshminarayanan, G., Inan, H.,\nShojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H.,\nRudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan,\nH., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I.,\nVeliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli,\nJ., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J.,\nChan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J.,\nJin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J.,\nMcPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U,\nK. H., Saxena, K., Khandelwal, K., Zand, K., Matosich,\nK., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh,\nK., Huang, K., Chawla, K., Huang, K., Chen, L., Garg,\nL., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L.,\nMoshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M.,\nBhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso,\nM., Groshev, M., Naumov, M., Lathi, M., Keneally, M.,\nLiu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel,\nM., Vyatskov, M., Samvelyan, M., Clark, M., Macey,\nM., Wang, M., Hermoso, M. J., Metanat, M., Rastegari,\nM., Bansal, M., Santhanam, N., Parks, N., White, N.,\nBawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta,\nN., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O.,\nHart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P.,\nSaab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P.,\nDollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P.,\nLiang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R.,\nNayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan,\nR., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta,\nS., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S.,\nDhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma,\nS., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay,\nS., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S.,\nZhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe,\nS., Chintala, S., Max, S., Chen, S., Kehoe, S., Satter-\nfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho,\nS., Virk, S., Subramanian, S., Choudhury, S., Goldman,\nS., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson,\nT., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked,\nT., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V.,\nKumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mi-\nhailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W.,\nBouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X.,\nWu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y.,\nQi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu,\nWang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait,\nZ., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao,\nZ., and Ma, Z. The llama 3 herd of models, 2024. URL\nhttps://arxiv.org/abs/2407.21783.\nHu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li,\nC., Zhang, J., Jin, Q., Huang, F., and Zhou, J. mplug-\ndocowl 1.5: Unified structure learning for ocr-free doc-\nument understanding, 2024. URL https://arxiv.\norg/abs/2403.12895.\nJaume, G., Ekenel, H. K., and Thiran, J.-P.\nFunsd: A\ndataset for form understanding in noisy scanned doc-\numents, 2019.\nURL https://arxiv.org/abs/\n1905.13538.\nKim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim,\nJ., Hwang, W., Yun, S., Han, D., and Park, S.\nOcr-\nfree document understanding transformer, 2022. URL\nhttps://arxiv.org/abs/2111.15664.\nKim, Y., Yim, M., and Song, K. Y. Tablevqa-bench: A\nvisual question answering benchmark on multiple table\ndomains. arXiv preprint arXiv:2404.19205, 2024.\nLaurenc¸on, H., Tronchon, L., Cord, M., and Sanh, V. What\nmatters when building vision-language models?, 2024.\nURL https://arxiv.org/abs/2405.02246.\nLee, K., Joshi, M., Turc, I., Hu, H., Liu, F., Eisenschlos, J.,\nKhandelwal, U., Shaw, P., Chang, M.-W., and Toutanova,\nK. Pix2struct: Screenshot parsing as pretraining for visual\nlanguage understanding, 2023. URL https://arxiv.\norg/abs/2210.03347.\nLi, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H.,\nZhang, K., Zhang, P., Li, Y., Liu, Z., and Li, C. Llava-\nonevision: Easy visual task transfer, 2024. URL https:\n//arxiv.org/abs/2408.03326.\nLi, J., Li, D., Savarese, S., and Hoi, S.\nBlip-2: Boot-\nstrapping language-image pre-training with frozen im-\nage encoders and large language models, 2023. URL\nhttps://arxiv.org/abs/2301.12597.\nLiu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines\nwith visual instruction tuning, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning, 2023b.\nLiu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S.,\nand Lee, Y. J.\nLlava-next:\nImproved reasoning,\nocr, and world knowledge, January 2024.\nURL\nhttps://llava-vl.github.io/blog/\n2024-01-30-llava-next/.\n11\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nLu, S., Li, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K.,\nand Ye, H.-J. Ovis: Structural embedding alignment for\nmultimodal large language model, 2024. URL https:\n//arxiv.org/abs/2405.20797.\nMasry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque,\nE. Chartqa: A benchmark for question answering about\ncharts with visual and logical reasoning. arXiv preprint\narXiv:2203.10244, 2022.\nMathew, M., Bagal, V., Tito, R. P., Karatzas, D., Valveny,\nE., and Jawahar, C. V. Infographicvqa, 2021a. URL\nhttps://arxiv.org/abs/2104.12756.\nMathew, M., Karatzas, D., and Jawahar, C. V. Docvqa:\nA dataset for vqa on document images, 2021b. URL\nhttps://arxiv.org/abs/2007.00398.\nOpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,\nAkkaya, I., Aleman, F. L., Almeida, D., Altenschmidt,\nJ., Altman, S., Anadkat, S., Avila, R., Babuschkin, I.,\nBalaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian,\nM., Belgum, J., Bello, I., et al. Gpt-4 technical report.\narXiv preprint arXiv: 2303.08774, 2023.\nPark, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., and\nLee, H. Cord: A consolidated receipt dataset for post-\nocr parsing. Document Intelligence Workshop at Neural\nInformation Processing Systems, 2019.\nPasupat, P. and Liang, P. Compositional semantic pars-\ning on semi-structured tables. In Annual Meeting of the\nAssociation for Computational Linguistics, 2015.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng,\nB., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H.,\nYang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J.,\nLin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L.,\nLi, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R.,\nLi, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su,\nY., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and\nQiu, Z. Qwen2.5 technical report, 2025. URL https:\n//arxiv.org/abs/2412.15115.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision, 2021.\nURL https://arxiv.org/abs/2103.00020.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Explor-\ning the limits of transfer learning with a unified text-to-\ntext transformer, 2023. URL https://arxiv.org/\nabs/1910.10683.\nRodriguez, J., Jian, X., Panigrahi, S. S., Zhang, T., Feizi,\nA., Puri, A., Kalkunte, A., Savard, F., Masry, A., Nayak,\nS., Awal, R., Massoud, M., Abaskohi, A., Li, Z., Wang,\nS., No¨el, P.-A., Richter, M. L., Vadacchino, S., Agar-\nwal, S., Biswas, S., Shanian, S., Zhang, Y., Bolger, N.,\nMacDonald, K., Fauvel, S., Tejaswi, S., Sunkara, S., Mon-\nteiro, J., Dvijotham, K. D., Scholak, T., Chapados, N.,\nKharagani, S., Hughes, S., ¨Ozsu, M., Reddy, S., Ped-\nersoli, M., Bengio, Y., Pal, C., Laradji, I., Gella, S.,\nTaslakian, P., Vazquez, D., and Rajeswar, S. Bigdocs:\nAn open and permissively-licensed dataset for training\nmultimodal models on document and code tasks, 2024a.\nURL https://arxiv.org/abs/2412.04626.\nRodriguez, J. A., Vazquez, D., Laradji, I., Pedersoli, M.,\nand Rodriguez, P. Ocr-vqgan: Taming text-within-image\ngeneration, 2022. URL https://arxiv.org/abs/\n2210.11248.\nRodriguez, J. A., Puri, A., Agarwal, S., Laradji, I. H.,\nRodriguez, P., Rajeswar, S., Vazquez, D., Pal, C., and\nPedersoli, M.\nStarvector: Generating scalable vec-\ntor graphics code from images and text, 2024b. URL\nhttps://arxiv.org/abs/2312.11556.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In IEEE Conference Computer\nVision Pattern Recognition, 2019.\nStanisławek, T., Grali´nski, F., Wr´oblewska, A., Lipi´nski,\nD., Kaliska, A., Rosalska, P., Topolski, B., and Biecek,\nP. Kleister: key information extraction datasets involv-\ning long documents with complex layouts. In Interna-\ntional Conference on Document Analysis and Recogni-\ntion, 2021.\nSvetlichnaya, S. Deepform: Understand structured docu-\nments at scale, 2020.\nTeam, G. Gemini: A family of highly capable multimodal\nmodels, 2024.\nURL https://arxiv.org/abs/\n2312.11805.\nVogus, C. and Llans´oe, E. Making transparency meaningful:\nA framework for policymakers. Center for Democracy\nand Technology, 2021.\nWang, D., Raman, N., Sibue, M., Ma, Z., Babkin, P.,\nKaur, S., Pei, Y., Nourbakhsh, A., and Liu, X.\nDo-\ncllm: A layout-aware generative language model for mul-\ntimodal document understanding, 2023a. URL https:\n//arxiv.org/abs/2401.00908.\nWang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen,\nK., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du,\nM., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and\n12\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nLin, J. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution, 2024. URL\nhttps://arxiv.org/abs/2409.12191.\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji,\nJ., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual\nexpert for pretrained language models. arXiv preprint\narXiv:2311.03079, 2023b.\nWu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W.,\nXie, Z., Yu, X., Ruan, C., and Luo, P. Janus: Decoupling\nvisual encoding for unified multimodal understanding\nand generation, 2024a. URL https://arxiv.org/\nabs/2410.13848.\nWu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao,\nH., Ma, Y., Wu, C., Wang, B., Xie, Z., Wu, Y., Hu, K.,\nWang, J., Sun, Y., Li, Y., Piao, Y., Guan, K., Liu, A.,\nXie, X., You, Y., Dong, K., Yu, X., Zhang, H., Zhao,\nL., Wang, Y., and Ruan, C. Deepseek-vl2: Mixture-of-\nexperts vision-language models for advanced multimodal\nunderstanding, 2024b. URL https://arxiv.org/\nabs/2412.10302.\nXu, R., Yao, Y., Guo, Z., Cui, J., Ni, Z., Ge, C., Chua, T.-S.,\nLiu, Z., Sun, M., and Huang, G. Llava-uhd: an lmm\nperceiving any aspect ratio and high-resolution images.\nEuropean Conference on Computer Vision, 2024. doi:\n10.48550/arXiv.2403.11703.\nZhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sig-\nmoid loss for language image pre-training, 2023. URL\nhttps://arxiv.org/abs/2303.15343.\nZhang, T., Wang, S., Li, L., Zhang, G., Taslakian, P., Ra-\njeswar, S., Fu, J., Liu, B., and Bengio, Y. Vcr: Visual\ncaption restoration. arXiv preprint arXiv: 2406.06462,\n2024.\nZhao, Y., Huang, J., Hu, J., Wang, X., Mao, Y., Zhang,\nD., Jiang, Z., Wu, Z., Ai, B., Wang, A., Zhou, W., and\nChen, Y. Swift:a scalable lightweight infrastructure for\nfine-tuning, 2024. URL https://arxiv.org/abs/\n2408.05517.\n13\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nA. Appendix\nA.1. Experimental Setup\nWe provide detailed hyperparameters of our experiments in Table 4.\nTable 4: Detailed hyperparameters for each training stage across different LLM backbones.\nLLM Backbone\nLlama 3.2-1B\nLlama 3.2-3B\nLlama 3.1-8B\nStage-1\nStage-2\nStage-3\nStage-1\nStage-2\nStage-3\nStage-1\nStage-2\nStage-3\nTrainable Parameters\nFull Model\nFull Model\nLLM & ALIGN\nFull Model\nFull Model\nLLM & ALIGN\nFull Model\nFull Model\nLLM & ALIGN\nBatch Size\n512\n512\n512\n512\n256\n256\n512\n256\n256\nText Max Length\n1024\n2048\n2048\n1024\n2048\n2048\n1024\n2048\n2048\nEpochs\n1\n1\n5\n1\n1\n5\n1\n1\n5\nLearning Rate\n1 × 10−5\n5 × 10−5\n5 × 10−5\n1 × 10−5\n5 × 10−5\n5 × 10−5\n1 × 10−5\n1 × 10−5\n1 × 10−5\nA.2. Vision-to-Text\nIn this experiment, we analyze how ALIGN maps visual features to the LLM’s text tokens. To do so, we manually curate\na small dataset of image crops, each containing either a single word or a small set of visual text elements. Unlike the\nprocessing of high-resolution images described earlier (Section 3.1), these image crops are not divided into tiles. Instead,\nthe backbone image encoder processes each crop as a single tile, producing 14 × 14 features from the input image. The\nresulting features pass through the Softmax operation (Equation 1), yielding a probability distribution over the LLM’s text\ntokens for each feature (region). We examine the decoded text tokens from specific image regions to better understand how\nvisual features are mapped to textual representations.\nAs shown in Figure 6, white regions in the images tend to assign higher probabilities to punctuation tokens, such as commas\nor periods. Since punctuation structures written text, while white space separates document components like paragraphs,\ntables, and sections, ALIGN appears to leverage these implicit patterns to align visual structures with semantically meaningful\nrepresentations in the LLM’s embedding space.\nFigure 6: Mapping Visual-to-Text tokens. The left column shows the visual input to the model. In contrast, the right\ncolumn visualizes the decoded tokens on a 14×14 grid, displaying the top k=2 tokens corresponding to the most likely LLM\ntokens predicted for the respective visual feature in each cell.\n14\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nA.3. Case Studies\nIn this section, we provide case studies for the experiments in Section 5.1. Specifically, we provide examples of our\nLlama-3.2-3B-ALIGN, and its counterpart model with alternative connectors Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis\non three different datasets: KLC (Stanisławek et al., 2021), DocVQA (Mathew et al., 2021b), and TextVQA (Singh et al.,\n2019). The examples are shown in Figure 7, 8, and 9.\n15\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nQuestion: What is the value for the charity\nname?\nGT:\n(Ardingly College Ltd.)\nMLP:\n(Ardington College Ltd.) ✗\nOvis:\n(Ardington College Ltd.) ✗\nALIGN:\n(Ardingly College Ltd.) ✓\n(a) Positive Example #1\nQuestion: What is the value for the ad-\ndress postcode?\nGT:\n(SW2 2QP)\nMLP:\n(SW22 0PQ) ✗\nOvis:\n(SW2 2OP) ✗\nALIGN:\n(SW2 2QP) ✓\n(b) Positive Example #2\nQuestion: What is the value for the char-\nity name?\nGT:\n(Human Appeal)\nMLP:\n(Humanitarian Agenda) ✗\nOvis:\n(Human Appeal) ✓\nALIGN:\n(Human Rightsappeal) ✗\n(c) Negative Example #1\nQuestion: What is the value for the post\ntown address?\nGT:\n(Bishop’s Stortford)\nMLP:\n(Stortford) ✗\nOvis:\n(Bishop’s Stortford) ✓\nALIGN:\n(Stortford) ✗\n(d) Negative Example #2\nFigure 7: Case Study for Connector Comparison on the KLC dataset (Stanisławek et al., 2021). We show four\nqualitative examples (including two correct and two incorrect examples) comparing Llama-3.2-3B-ALIGN to the same\narchitecture with different connectors, Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis. “GT” denotes the ground truth.\n16\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nQuestion: What does the afternoon session\nbegin on June 29?\nGT:\n(1:00)\nMLP:\n(2:45) ✗\nOvis:\n(3:30) ✗\nALIGN:\n(1:00) ✓\n(a) Positive Example #1\nQuestion: What levels does the second table indi-\ncate?\nGT:\n(hematocrit data - Massachusetts)\nMLP:\n(SATISFACTORY) ✗\nOvis:\n(Females) ✗\nALIGN:\n(hematocrit data - Massachusetts) ✓\n(b) Positive Example #2\nQuestion: What type of policy is described\nin this document?\nGT:\n(Policy on Document Control)\nMLP:\n(Policy on Document Control) ✓\nOvis:\n(General Provisions) ✗\nALIGN:\n(Document Control) ✗\n(c) Negative Example #1\nQuestion: What was the diet fed to the #1\ngroup?\nGT:\n(basal diet)\nMLP:\n(basel diet) ✓\nOvis:\n(Whole blood) ✗\nALIGN:\n(control diet) ✗\n(d) Negative Example #2\nFigure 8: Case Study for Connector Comparison on the DocVQA dataset (Mathew et al., 2021b). We show four\nqualitative examples (including two correct and two incorrect examples) comparing Llama-3.2-3B-ALIGN to the same\narchitecture with different connectors, Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis. “GT” denotes the ground truth.\n17\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nQuestion: What greeting is written on the letter?\nGT:\n(good bye)\nMLP:\n(good) ✗\nOvis:\n(good buy) ✗\nALIGN:\n(good bye) ✓\n(a) Positive Example #1\nQuestion: What indoor temperature is shown?\nGT:\n(68.4)\nMLP:\n(68 F) ✗\nOvis:\n(40.0) ✗\nALIGN:\n(68.4) ✓\n(b) Positive Example #2\nQuestion: What type of club is advertised?\nGT:\n(health club)\nMLP:\n(topnote health club) ✗\nOvis:\n(health club) ✓\nALIGN:\n(professional passionate personal) ✗\n(c) Negative Example #1\nQuestion: What credit card is this?\nGT:\n(hadiah plus)\nMLP:\n(hadiah plus) ✓\nOvis:\n(american big loyalty program) ✗\nALIGN:\n(hadia plus) ✗\n(d) Negative Example #2\nFigure 9: Case Study for Connector Comparison on the TextVQA dataset (Singh et al., 2019). We show four qualitative\nexamples (including two correct and two incorrect examples) comparing Llama-3.2-3B-ALIGN to the same architecture\nwith different connectors, Llama-3.2-3B-MLP and Llama-3.2-3B-Ovis. “GT” denotes the ground truth.\n18'),
                Paper(arxiv_id='2502.01534', authors=['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], published_at=datetime.datetime(2025, 2, 4, 1, 4, 33, 630000, tzinfo=datetime.timezone.utc), title='Preference Leakage: A Contamination Problem in LLM-as-a-judge', summary='Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between data\ngenerator LLM and judge LLM: being the same model, having an inheritance\nrelationship, and belonging to the same model family. Through extensive\nexperiments, we empirically confirm the bias of judges towards their related\nstudent models caused by preference leakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that preference leakage is a pervasive\nissue that is harder to detect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings imply that preference leakage\nis a widespread and challenging problem in the area of LLM-as-a-judge. We\nrelease all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage.', upvotes=28, thumbnail=None, content='Preference Leakage: A Contamination Problem in LLM-as-a-judge\nDawei Li * 1 Renliang Sun * 2 Yue Huang 3 Ming Zhong 4 Bohan Jiang 1\nJiawei Han 4 Xiangliang Zhang 3 Wei Wang 2 Huan Liu 1\nAbstract\nLarge Language Models (LLMs) as judges and\nLLM-based data synthesis have emerged as\ntwo fundamental LLM-driven data annotation\nmethods in model development. While their com-\nbination significantly enhances the efficiency of\nmodel training and evaluation, little attention has\nbeen given to the potential contamination brought\nby this new model development paradigm. In this\nwork, we expose preference leakage, a contami-\nnation problem in LLM-as-a-judge caused by the\nrelatedness between the synthetic data generators\nand LLM-based evaluators. To study this issue,\nwe first define three common relatednesses\nbetween data generator LLM and judge LLM:\nbeing the same model, having an inheritance\nrelationship, and belonging to the same model\nfamily.\nThrough extensive experiments, we\nempirically confirm the bias of judges towards\ntheir related student models caused by preference\nleakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that prefer-\nence leakage is a pervasive issue that is harder to\ndetect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings\nimply that preference leakage is a widespread\nand challenging problem in the area of LLM-\nas-a-judge.\nWe release all codes and data at:\nhttps://github.com/David-Li0406/\nPreference-Leakage1.\n1. Introduction\nRecent\nadvancements\nin\nLarge\nLanguage\nModels\n(LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong\net al., 2024; Zhang et al., 2024a) have empowered various\n*Equal contribution 1Arizona State University 2University of\nCalifornia, Los Angeles 3University of Notre Dame 4University\nof Illinois Urbana Champaign. Correspondence to: Dawei Li\n<daweili5@asu.edu>.\n1More resources on LLM-as-a-judge are on the website:\nhttps://llm-as-a-judge.github.io/\ndownstream tasks and applications. However, this also\nposes substantial challenges to the automatic evaluation\nof these models. Representatively, LLM-based AI agents’\nfocus transfer from traditional natural language processing\ntasks (Yang et al., 2023; Zhang et al., 2023) to real-world\n(Liu et al., 2023b; Huang et al., 2023), open-ended response\ngeneration (Wu et al., 2024), which greatly limits the\napplicability of traditional n-gram matching methods (e.g.,\nBLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu\net al., 2016; Reiter, 2018) or model-based evaluators (Zhang\net al., 2020; Zhong et al., 2022) for evaluation.\nTo address these challenges, the paradigm of LLM-as-a-\njudge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a;\nZhong et al., 2024; Li et al., 2025) has been proposed, de-\nsigned to leverage LLM as evaluators to assess response\nquality. By combining powerful LLMs with well-designed\nprompting strategies, LLM-as-a-judge enables human-like\nevaluation of long-form and open-ended generation in a\nmore cost-efficient and scalable manner. However, recent\nstudies point out some weaknesses of such assessment. For\ninstance, Ye et al. (2024) explores various biases and vulner-\nabilities of LLM-as-a-judge, highlighting the importance of\ndeveloping a reliable and fair LLM-based evaluation system.\nIn this work, we aim to introduce another concern in LLM-\nas-a-Judge–Preference Leakage. This issue arises when the\nLLMs used for data generation and evaluation are closely re-\nlated, as illustrated in Figure 1. Synthetic data generated by\nLLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c)\nhas become a cornerstone of model training (Lee et al.,\n2025). When combined with LLM-as-a-Judge, they offer\nsignificant efficiency gains in model development. However,\nlimited attention has been given to the potential contami-\nnation that occurs when the generator and evaluator LLMs\nshare a close relationship. During our preliminary study,\nwe find this issue is particularly pervasive in popular LLM-\nas-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al.,\n2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant\nstudies (more details can be found in Appendix A), due to\nthe common reliance on the most advanced LLMs, such\nas GPT-4 (Achiam et al., 2023), for both data synthesis\nand evaluation to ensure the highest quality outputs. In our\nwork, we reveal this relatedness—akin to the overlap be-\ntween training data and evaluation sets in traditional data\n1\narXiv:2502.01534v1  [cs.LG]  3 Feb 2025\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ncontamination—would introduce a systematic bias of judge\nLLMs towards their related student models (i.e., the model\ndistilled by the data generator which is related to the judge).\nCompared to other biases in LLM-as-a-Judge, such as length\nbias or egocentric bias (Ye et al., 2024; Panickssery et al.,\n2024), preference leakage is subtler and more challenging\nto detect, especially given that most LLMs do not disclose\ntheir training data.\nTo investigate and reveal the preference leakage problem,\nwe first define three relatednesses between data generator\nLLM and judge LLM: being the same model, having an\ninheritance relationship, and belonging to the same model\nfamily. Each of these scenarios is commonly encountered\nin real-world applications. Then, we pose and answer three\ncore research questions about preference leakage:\n• RQ1: Does preference leakage introduce systematic\nbiases in LLM-based evaluation? To answer it, we\nconduct experiments with various LLM baselines in two\nwidely recognized LLM-as-a-judge benchmarks, also in-\ntroduce the preference leakage score to quantify the bias\ncaused by preference leakage. The analysis results sug-\ngest an obvious bias of judging LLMs toward their related\nstudent models.\n• RQ2: What is the severity of preference leakage under\nvarious scenarios? We conduct experiments under vari-\nous relatedness settings, tuning techniques, and data mix-\ning strategies to address it, finding that preference leakage\nconsistently affects judge LLMs. Moreover, the severity\nof preference leakage correlates with the degree of relat-\nedness between the data generator and LLM judges, as\nwell as the proportion of synthetic data.\n• RQ3: What are the underlying mechanisms causing\npreference leakage? For this question, we analyze LLMs’\nrecognition capabilities on their related student models’\ngeneration as well as the distribution of bias across differ-\nent question types and judgment dimensions. The analysis\nreveals that preference leakage is a subtle, hard-to-detect\nissue, particularly affecting subjective questions and judg-\nment dimensions.\nTo summarize, our contributions in this work are as follows:\n• We introduce preference leakage, a contamination issue\narising from the relatedness between the data generator\nand judge LLMs.\n• We conduct extensive experiments across various LLMs\nand benchmarks to study how and to what extent the\npotential bias brought by preference leakage influences\njudgment.\n• Our further analysis reveals that preference leakage is\nprevalent in diverse scenarios and difficult for judge LLMs\nto detect, providing valuable insights for future research\non this challenging issue.\n2. Related Work\n2.1. LLM-as-a-Judge\nLLM-as-a-Judge, introduced by Zheng et al. (2023), lever-\nages LLMs to automatically evaluate responses and assign\nrewards. This approach has gained widespread adoption\nin areas such as model alignment (Zhang et al., 2024d)\nand benchmarking (Liu et al., 2023a; Zhang et al., 2024b;\nGao et al., 2023; Zhong et al., 2024), driving significant\nprogress in the field. Building on this concept, Zhuge et al.\n(2024) proposed Agent-as-a-Judge, where agentic systems\nare employed to evaluate other agentic systems. Addition-\nally, Prometheus, a series of open-source LLMs tailored for\nLLM-as-a-Judge (Kim et al., 2023; 2024), addresses the\nprohibitive costs associated with proprietary models, further\ndemocratizing the technology.\nDespite its promising potential, recent studies have high-\nlighted the vulnerabilities and limitations of LLM-as-a-\nJudge. Notable concerns include biases during evaluation.\nFor example, Zheng et al. (2023) identify position bias,\nwhere LLMs may favor responses based on their order in\nthe input, thereby compromising fairness. Other studies (Ye\net al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al.,\n2023; Huang et al., 2024) further emphasize the risks of\nevaluation biases. Thakur et al. (2024) assessed the judg-\nment capabilities of LLM judges, finding that only the most\nadvanced models align reasonably well with human evalu-\nators. Moreover, a recent study (Shi et al., 2024) revealed\nthe susceptibility of LLM-as-a-Judge to adversarial attacks,\nleading to incorrect judgments. In this paper, we explore an-\nother critical vulnerability of LLM-as-a-Judge—preference\nleakage—which poses additional risks to the reliability of\nthis evaluation paradigm.\n2.2. Data Leakage\nThe possible overlap between training data and evaluation\nbenchmarks has become a central issue, since LLMs are usu-\nally trained on extensive web corpora (Dodge et al., 2021).\nThis phenomenon, known as data leakage, can artificially\nimprove the performance of LLMs and undermine the re-\nliability of the assessment (Deng et al., 2024a; Jiang et al.,\n2024b).\nSeveral researchers have proposed methods to detect and\nmitigate data contamination. Deng et al. (2024b) proposed\na retrieval-based approach to assess the degree of overlap\nbetween pre-training text and benchmark data. Golchin &\nSurdeanu (2023) have developed “guided instruction” to\nflag contaminated instances. Dong et al. (2024b) proposed\nthe CDD method to identify peaks in the output distribution\nto detect data contamination. Several studies analyze data\nleakage for specific LLMs (Balloccu et al., 2024) and report\ncontamination such as cross-language contamination (Yao\n2\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nEvaluation\nTestset\nTraining\nCorpus\nData leakage\nTrain\nTraining\nCorpus\nEvaluation \nTestset\nEvaluate\nData Leakage!\nSynthetic \nData\nData \nGenerator\nTrained\nModel\nTrained\nModel\nTrained\nModel\nJudge\nJudge\nModel\nPreference Leakage!\nRelatedness \nOverlap\nLLM for Data \nSynthesis\nLLM-as-\na-Judge\nPreference leakage\nTrain\n(1). Same model\n(2). Inheritance\nSynthetic \ndata\n(3). Within the \nsame model family\nSynthesize\nFigure 1. Overview of preference leakage. We make a comparison between data leakage and preference leakage and present three types of\nrelatedness: being the same model, having an inheritance relationship and belonging to the same model family.\net al., 2024) and task contamination (Li & Flanigan, 2024)\nthat can evade traditional detection methods. To address data\ncontamination issues, Ni et al. (2024) have used web user\nquery detection and benchmark mixture. White et al. (2024)\nuse the most recent information to update the problem.\n3. Preference Leakage\nIn this section, we first provide the formal definition of data\ncontamination as the preliminary (Section 3.1). Based on\nthe concept, we demonstrate how LLM-based data synthesis\nand evaluation can lead to the evolving preference leakage\nproblem (Section 3.2).\n3.1. Preliminary: Data Leakage\nData leakage, also known as data contamination, refers to\nthe inadvertent inclusion of information from the evalua-\ntion benchmarks into the training corpus thus creating an\noverlap between training and testing sets (Kaufman et al.,\n2012). This overlap would significantly influence the eval-\nuation fairness by inflating the models’ performance since\nthe model has prior exposure to and memorized information\nthat it’s expected to generalize during testing (Elangovan\net al., 2021).\nFormally, let T represent the training corpus and E be the\nevaluation set during test time. Data contamination occurs\nif:\nT ∩E ̸= ∅,\n(1)\nwhere ∩denotes the intersection between the two sets. Such\noverlap violates the fundamental assumption that training\nand testing datasets should be disjoint to ensure an unbiased\nassessment of the model’s generalization ability.\n3.2. From Data Leakage to Preference Leakage\nWith the advent of LLMs, synthetic data generated by these\nmodels (Tan et al., 2024) has been widely adopted in var-\nious stages of model training, including pre-training, rein-\nforcement learning with AI feedback (RLAIF) and super-\nvised fine-tuning. Concurrently, the concept of LLM-as-\na-judge has emerged, where LLMs are employed to auto-\nmate the evaluation process. While these LLM-as-an-oracle\napproaches reduce human effort in data annotation, signif-\nicantly enhancing the efficiency and scalability of model\ntraining and evaluation, they also blur the lines between\nmodels and data, introducing evolving challenges (Shu-\nmailov et al., 2024; Dai et al., 2024).\nIn this work, we examine the evolving contamination prob-\nlem brought by LLM-as-a-oracle and formally propose the\nconcept of preference leakage. This refers to a situation\nin which the LLMs used for synthetic data generation and\nevaluation are related. Formally, we define this as:\nLLMG ∩LLMJ ̸= ∅,\n(2)\nwhere LLMG and LLMJ denote the LLMs used for train-\ning data generation and evaluation. ∩represents the related-\nness between the two (sets of) LLMs. This relatedness may\ninvolve:\n• Being the same model: the data generator and evaluator\nare the same model:\nLLMG = LLMJ.\n(3)\n• Inheritance relationship: one model is trained on syn-\nthetic data generated by the other:\nLLMG = Inherit(LLMJ),\n(4)\nLLMJ = Inherit(LLMG).\n(5)\n3\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n• Within the same model family: the data generator and\nevaluator belong to the same model family (e.g., GPT\nfamily (Achiam et al., 2023) and Gemini family (Team\net al., 2024)):\nLLMG, LLMJ ∈FX.\n(6)\nDue to this relatedness, the preference of the judge models\n(e.g., format, style and wording) can be leaked to the student\nmodels through the synthetic data, resulting in non-trivial\nbias from the judge LLMs during the test time.\n4. Main Experiment\n4.1. Experiment Setup\nModels. We choose three powerful LLMs as data generator/\njudge models. They are GPT-4o-2024-11-20 (Achiam et al.,\n2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA-\n3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student\nmodel, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and\nQwen-2.5-14B (Yang et al., 2024). To avoid potential prefer-\nence leakage due to distilling data from other LLMs during\nthe instruction-tuning process, we choose to use the -PRE-\nTRAINED version rather than the -INSTRUCT version of\nthese student models.\nEvaluation Datasets. We choose two representative pair-\nwise evaluation datasets, Arena-Hard (Li et al., 2024e)\nand AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the\ntrained student models. Arena-Hard includes 500 challeng-\ning questions in English. Additionally, the evaluation agree-\nment between Arena-Hard and Chatbot Arena (Zheng et al.,\n2023)’s hard prompts achieved a 96.7% Spearman corre-\nlation, demonstrating the consistency of Arena-Hard with\nhuman preferences (Li et al., 2024e). AlpacaEval 2.0 is an\nimproved evaluation method based on AlpacaEval (Li et al.,\n2023) and contains 805 questions. Compared to version 1.0,\nAlpacaEval 2.0 significantly reduces the effect of text length\non the evaluation results.\nImplementation Details. In our main experiment, we ex-\namine the preference leakage introduced by using the same\ndata generator and evaluator in supervised fine-tuning (SFT).\nWe will discuss other relatedness and learning methods in\nSection 5. To obtain synthetic datasets, We first randomly\nsample 30,000 prompts from the Ultrafeedback dataset (Cui\net al., 2024). The Ultrafeedback dataset includes instruc-\ntions from several publicly available high-quality datasets\nsuch as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al.,\n2023), and Evol-Instruct (Xu et al., 2023). For each data gen-\nerator model, we provide these prompts for them to produce\nsynthetic responses, resulting in three synthetic instruction\ndatasets. We then use each dataset to supervised fine-tune\nthe student model, obtaining three different versions for each\nbaseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-Gemini-\n1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each\ntwo student models and obtain three model pairs. For each\nmodel pair, we perform the pairwise comparison using the\nthree judge models respectively.\nMetrics & Annotation Based on our hypothesis, preference\nleakage would lead to bias of judge LLMs towards their\nrelated student models. Following this principle, we design\nthe preference leakage score PLS(i, j) to measure the bias\nin model pair (i, j) caused by preference leakage:\nPLS(i, j) =\n\x10\nWR(i,i)−AVG(i,j)\nAVG(i,j)\n\x11\n+\n\x10\nWR(j,j)−AVG(j,i)\nAVG(j,i)\n\x11\n2\n,\n(7)\nAVG(i, j) = WR(i, i) + WR(i, j)\n2\n.\n(8)\nHere WR(i, j) represents the win-rate score from judge\nmodel i to student model j. Intuitively, a large preference\nleakage score indicates that the two judge models demon-\nstrate strong bias toward their related student models, sug-\ngesting a significant preference leakage phenomenon.\nWhile our proposed preference leakage score quantifies the\ndegree of preference leakage in each model pair, we also\nperform manual annotation to assess the preference leakage\nin each individual model. We randomly select 100 questions\nfrom AlpacaEval 2.0 and have three well-trained annota-\ntors perform pairwise comparisons independently for each\nresponse pair. After the annotation, the majority voting is\napplied to each response pair to get the final annotation\nresults.\nMore details about model training, metric explanation, and\nannotation process can be found in Appendix B.\nModel\nData Generator/ Judge Pair\nArena-Hard\nAlpacaEval 2.0\nAvg.\nGPT-4o & Gemini-1.5\n28.7%\n18.4%\n23.6%\nGPT-4o & LLaMA-3.3\n-6.7%\n1.4%\n-2.7%\nMistral-7B\nLLaMA-3.3 & Gemini-1.5\n13.1%\n19.8%\n16.4%\nGPT-4o & Gemini-1.5\n37.1%\n18.6%\n27.9%\nGPT-4o & LLaMA-3.3\n1.0%\n2.3%\n1.7%\nQwen-2.5-14B\nLLaMA-3.3 & Gemini-1.5\n25.4%\n18.4%\n21.9%\nTable 1. Preference leakage score result on Arena-Hard and Al-\npacaEval 2.0. The blue background indicates a negative prefer-\nence leakage score value and the purple background indicates a\npositive value. The deeper the color, the larger the absolute value.\n4.2. Main Results\nIn our main experiment, we aim to provide insights into\nRQ1.\nPreference leakage exists in most model pairs. The origi-\nnal judgment results from Arena-Hard and AlpacaEval 2.0,\nalong with the calculated preference leakage scores, are\nshown in Figure 2, Figure 3, and Table 1. As the results\ndemonstrate, in most model pairs (except Mistral-GPT-4o vs\n4\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n18.2%\n39.8%\n42.0%\n27.4%\n43.8%\n28.8%\n38.4%\n34.6%\n27.0%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n46.2%\n42.7%\n11.1%\n50.4%\n35.0%\n14.6%\n55.8%\n27.0%\n17.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n9.2%\n31.4%\n59.4%\n14.6%\n30.0%\n55.4%\n22.2%\n30.8%\n47.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n22.0%\n33.5%\n44.5%\n28.8%\n50.2%\n21.6%\n49.8%\n29.0%\n21.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n52.1%\n40.7%\n7.2%\n39.0%\n51.8%\n9.2%\n57.4%\n29.6%\n13.0%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n10.0%\n29.4%\n60.6%\n16.4%\n48.4%\n35.2%\n24.6%\n30.0%\n44.4%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nTie\nModel B Wins\nFigure 2. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n36.8%\n63.2%\n49.5%\n50.5%\n55.1%\n44.9%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n65.8%\n34.2%\n60.3%\n39.7%\n61.6%\n38.4%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n22.6%\n77.4%\n39.5%\n60.5%\n43.1%\n56.9%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n39.3%\n60.7%\n52.4%\n47.6%\n57.8%\n42.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n63.3%\n36.7%\n59.3%\n40.7%\n61.5%\n38.5%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n26.2%\n73.8%\n42.9%\n57.1%\n50.1%\n49.9%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nModel B Wins\nFigure 3. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in\nAlpacaEval 2.0.\nMistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-\n3.3), the judge LLMs exhibit a strong preference toward\ntheir related student models, leading to large positive val-\nues in the preference leakage scores. This finding suggests\nthat preference leakage, along with the resulting bias, is\nwidespread in SFT when the data generator and evaluator\nare the same.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n73.6%\n8.8%\n17.6%\n79.5%\n1.7%18.8%\nLLaMA-2 vs Others\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n76.2%\n17.9% 6.0%\n79.8%\n20.2%0.0%\nJudge Model\nLLaMA-2 vs Claude-v1\nModel A Wins\nTie\nModel B Wins\nFigure 4. Comparison between GPT-4 and human’s judgment for\nLLaMA-2 from MTBench.\nEvaluators’ bias towards certain LLMs can be inherited\nby its student models. From Figure 2 and Figure 3, we find\nan obvious preference of GPT-4o towards Mistral/ Qwen-\nLLaMA-3.3 and this leads to the low preference leakage\nscore in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and\nQwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate\nthe source of this preference, we examine whether the GPT-\n4 evaluator has a bias toward LLaMA series models. Using\nthe MTBench (Zheng et al., 2023) dataset, which includes\npairwise comparison judgments from both humans and GPT-\n4, we compare GPT-4’s and human evaluators’ judgments\non LLaMA-2 vs other models (including Vicuna, Alpaca,\nGPT-3.5, and GPT-4, which are preferred by GPT-4 due\nto preference leakage or egocentric bias) and LLaMA-2 vs\nClaude. The results, shown in Figure 4, reveal a clear pref-\nerence for LLaMA-2 by GPT-4. Consequently, we conclude\nthat evaluators’ bias can be inherited. In this case, GPT-4’s\nbias toward LLaMA has been passed on to LLaMA’s stu-\ndent models. This inheritance, combined with the opaque\ntraining data of LLMs, makes preference leakage a more\ncomplex and challenging problem.\nModel pairs with similar performance tend to have more\n5\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n53.0%\n47.0%\n40.2%\n59.8%\n49.4%\n50.6%\n58.4%\n41.6%\nJudge Model\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n62.0%\n38.0%\n76.2%\n23.8%\n72.1%\n27.9%\n67.8%\n32.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n36.0%\n64.0%\n17.1%\n82.9%\n39.0%\n61.0%\n46.0%\n54.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\nModel A Wins\nModel B Wins\nFigure 5. Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0.\nobvious preference leakage. As shown in Table 1, we ob-\nserve that the preference leakage scores for Mistral-GPT-4o\nvs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini-\n1.5 (23.6% and 27.9% respectively) are consistently higher\nthan that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9%\nrespectively). We think that this is largely due to the more\ncomparable performance between the student models of\nGPT-4o and Gemini-1.5. Intuitively, when the quality of the\ntwo responses is similar, the evaluator may rely more heav-\nily on its inherent preferences to make a judgment, thereby\nexacerbating the preference leakage issue.\nLarger student models cause more bias from judge\nLLMs. Another observation from Table 1 is that the over-\nall preference leakage score for Qwen-2.5-14B is higher\nthan that for Mistral-7B. Drawing on insights from previous\nstudies on data leakage, which suggest that larger and more\npowerful LLMs are more capable of memorizing extensive\ninformation and are thus more susceptible to data contamina-\ntion (Bordt et al., 2024; Duan et al., 2024), we attribute this\ndifference in preference leakage to the size and capabilities\nof the student LLMs. We assume that larger student models,\ndue to their better performance and generalization abilities,\nare more capable of learning and memorizing the hidden\npreference pattern from the synthetic data, thus leading to a\nmore serious preference leakage.\nDifferent data generator/ judge LLMs result in varying\ndegrees of bias under preference leakage. While we have\nconcluded that student model pairs with similar performance\nor more powerful student models tend to exhibit greater\npreference leakage, we also examine whether different data\ngenerator and judge LLMs contribute to varying degrees\nof preference leakage. Analyzing the manual annotation\nresults presented in Table 5, we observe that Gemini-1.5\nshows a strong bias toward its students, followed by GPT-4o,\nwith LLaMA-3.3 displaying the least bias. This variation in\npreference leakage may stem from differences in the level\nof leaked preference in the synthetic responses generated\nby the data generator LLMs. For instance, an LLM with a\ndistinctive style or format in its responses offers more op-\nportunities for student models to learn these characteristics,\npotentially leading to more pronounced preference leakage\nduring evaluation. Future work could further quantify the\nextent of leaked preference for each data generator model.\n5. Further Analysis\nIn this section, we conduct relatedness analysis, learning\nmethod analysis and data mixing analysis (Section 5.1 - 5.3)\nto answer RQ2. Due to the cost consideration, we conduct\nthese analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5.\nMoreover, we perform recognition analysis and category\nanalysis to answer RQ3.\nArena-Hard AlpacaEval 2.0\nAvg.\nSame Model\n28.7%\n18.4%\n23.6%\nInheritance\nw/ same ins.\n17.8%\n20.7%\n19.3%\nInheritance\nw/ different ins.\n18.3%\n26.3%\n22.3%\nSame Family\nw/ same series\n10.1%\n7.6%\n8.9%\nSame Family\nw/ different series\n3.3%\n2.2%\n2.8%\nTable 2. Preference leakage score in different relatedness between\nthe data generator and the judging LLM.\n5.1. Relatedness Analysis\nWe demonstrate the impact of different relatedness condi-\ntions between the data generator and the judge LLM on the\npreference leakage problem, as shown in Table 2.\nPreference leakage under inheritance settings causes ob-\nvious bias of judges towards their related students. For\nthe inheritance relationship, we consider the situation where\nthe data generator is inherited from the judge model. We\nconducted the following two experiments: (1). we give the\nsame instructions again as in the SFT stage (Inheritance w/\nsame ins.), or (2). we sample the same number of different\ninstructions from the Ultrafeedback (Inherence w/ different\nins.). Then, we let the fine-tuned Mistral model generate\nthe answers and use these generated data to fine-tune a new\nMistral student model. From the results, with the same in-\nstructions, the average preference leakage score is 19.3%. In\ncomparison, the score with different instructions is 22.3%.\n6\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFirstly, in an inheritance setting, data generators can inherit\njudges’ preferences, which are then passed on to new stu-\ndent models, thereby compromising the fairness of their\nevaluation. Second, even when different instructions are\nused, judges’ preferences leaked to data generators can still\nbe transferred to the new student model through synthetic\ndata, leading to a high preference leakage score.\nModels within the same series tend to cause more sig-\nnificant bias. For two models within the same family, we\nconsider two settings: (1) Same series, where training data\nis generated by GPT-4o and Gemini-1.5-flash, and judged\nby GPT-4-turbo and Gemini-1.5-pro; (2) Different series,\nwhere training data is still generated by GPT-4o and Gemini-\n1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro.\nIn the same series setting, the average preference leakage\nscore is 8.9%, indicating that despite using different mod-\nels for data generation and judgment, their relatedness in\nterms of model family leads to some preference leakage.\nIn contrast, the different series setting yields a significantly\nlower leakage score of 2.8%, likely due to differences in\narchitecture, training data, and other factors, reducing the\ninfluence of model-related biases in evaluation.\nArena-Hard\nAlpacaEval 2.0\nAvg.\nSFT\n28.7%\n18.4%\n23.6%\nDPO\n7.7%\n2.7%\n5.2%\nICL\n-4.2%\n-1.1%\n-2.7%\nTable 3. Preference leakage score in different learning methods.\n5.2. Learning Method Analysis\nWe also compare three learning methods, supervised\nfine-tuning (SFT), direct preference optimization (DPO)\n(Rafailov et al., 2024), and in-context learning (ICL) (Dong\net al., 2024a), to explore the different influences to them un-\nder preference leakage. We first build a data pool based on\nhuman-written instruction-tuning data from OASST (K¨opf\net al., 2024), LIMA (Zhou et al., 2024), and MOSS (Sun\net al., 2024b) to supervised fine-tune the pre-trained model.\nFor DPO, we sample 2 responses for each instruction from\nsampled UltraFeedback instruction and prompt each data\ngenerator to produce the pairwise feedback. Then we use\nthe DPO loss to further train the fine-tuned policy on each\nsynthetic pairwise dataset. Appendix C shows the prompt\nwe use to craft synthetic pairwise feedback. For ICL, we\nsample 4 instruction-response pairs from each LLMs’ syn-\nthetic dataset as the demonstration during inference.\nTuning approaches would leak judges’ preference to the\nstudent models. Various learning methods show significant\ndifferences in preference leakage scores across learning\nmethods. SFT exhibits the highest average leakage score at\n23.6%. In contrast, DPO achieves a much lower score of\n5.2%, likely because its focus on preferences helps minimize\nthe unintended transfer of judge model biases. Meanwhile,\nICL, which relies on contextual examples without updating\nmodel parameters, is least affected by the data generator’s\npreferences, resulting in the lowest leakage scores.\n20\n40\n60\n80\n100\nContamination Ratio (%)\n0\n5\n10\n15\n20\n25\n30\nPreference Leakage Score (%)\nAlpacaEval2.0 - Manual\nArenaHard - Manual\nAlpacaEval2.0 - Synthetic\nArenaHard - Synthetic\nFigure 6. Experiment results on data mixing. ‘Manual’ represents\nthe original synthetic data mixed with manually-written data. ‘Syn-\nthetic’ represents the original data mixed with other synthetic data.\n5.3. Data Mixing Analysis\nIn real-world applications, synthetic data from a single LLM\nis often mixed with manually-written data or other multi-\nsource synthetic data to train student models. To mimic\nthese scenarios and explore how much synthetic data could\nlead to preference leakage, we conduct a data mixing anal-\nysis. Specifically, we randomly sample 10%, 30%, 50%,\nand 70% from the original synthetic dataset and mix it with\nmanually-written data and multi-source synthetic data, re-\nspectively, in order to maintain a consistent total volume of\ntraining data (30,000). For the manually-written data, we\nsample from the data pool collected in Section 5.2. For the\nmulti-source synthetic data, we use the original synthetic\ndata from Ultrafeedback, which includes responses gener-\nated by various LLMs (e.g., WizardLM, Flcon, etc.). After\nobtaining the mixing training data, we train the student mod-\nels using SFT and calculate their preference leakage scores\nbased on the judgment results. Figure 6 presents the results\nwith two mixing strategies across two benchmarks.\nThe degree of preference leakage is directly proportional\nto the amount of synthetic data. We observe a strong\ncorrelation between the proportion of synthetic data in the\nmixture and the preference leakage score, with no clear\nthreshold separating cases with preference leakage from\nthose without. This suggests that preference leakage can\noccur even with a small amount of leaked synthetic data,\nposing significant challenges for its detection.\n5.4. Can Judges Recognize Student Models?\nPrevious studies demonstrate the LLM judges can recog-\nnize and thus prefer their own generation (Panickssery et al.,\n2024). In this work, we pose a similar question: Does prefer-\nence leakage also source from the LLM judges’ recognition\n7\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nMathematics\nBusiness\nDaily Life\nScience\nWriting\nOthers\nProgramming\n0\n10\n20\n30\nPreference Leakage Score (%)\n7.7\n16.5\n17.2\n17.3\n21.0\n23.8\n31.4\n(a) Question Type\nCompleteness\nClarity\nRichness\nSatisfaction\nFactuality\nLogical\nOthers\nCreativity\nFairness\n20\n24\n28\n32\nPreference Leakage Score (%)\n27.9\n28.6\n28.8\n29.0\n29.2\n30.2\n30.4\n30.7\n32.4\n(b) Judgment dimension\nFigure 7. Category analysis results on question type and judgment dimension.\nTask\nModel\nAccuracy\nStudent Recognition\nGPT-4o\n60.0%\nGemini-1.5\n25.4%\nLLaMA-3.3\n54.2%\nResponse Classification\nBERT\n82.4%\nTable 4. Student recognition (binary classification) and response\nclassification results (three-class classification).\nof their related student models’ generation? To study this,\nwe follow Panickssery et al. (2024) to prompt the three\njudge LLMs and test whether they could recognize their\nrelated student models’ generation. Additionally, we split\nthree student models’ generation into training and testing\nsets, and train a BERT classifier to perform a three-class\nclassification inspired by the previous study on detecting\nhuman-AI text (Zhang et al., 2024c). Detailed instruction\nand training settings can be found in Appendix D.\nJudge LLMs do not show good performance in recogniz-\ning the generation of their student models. As the result\npresented in Table 4, we find that the recognition perfor-\nmance of each judge LLM in the content of related students\nis poor, with accuracy around the performance of random\nguess. Moreover, we observe no correlation between recog-\nnition performance and the preference leakage degree for\njudge LLMs. For instance, while Gemini-1.5 leads to the\nmost preference leakage (as shown in Section 4.2), it per-\nforms the worst in recognition tasks. These suggest that\npreference leakage is subtler and harder-to-detect for judge\nLLMs, in contrast to the more obvious egocentric bias.\nCertain features embedded in student models through\nsynthetic data. Although judge LLMs do not perform\nwell in related student recognition, we notice the fine-tuned\nBERT classification demonstrates a high accuracy score in\nclassifier response generated by each student model. This\nsuggests that certain characteristics—such as style and for-\nmat—are embedded in the student models through the syn-\nthetic responses. This finding further supports the existence\nof preference leakage and lays the groundwork for future\nresearch aimed at detecting and preventing it.\n5.5. Impact on Question Type & Judgment Dimension\nIn this section, we explore the impact of preference leakage\nacross various question types and judgment dimensions. For\nthe question type analysis, we first propose several general\nquestion types based on the question clusters introduced by\nArena-Hard. Then, we prompt GPT-4o to map each question\nin Arena-Hard and AlpacaEval to one of the question types\nand calculate the preference leakage score for each question\ncategory. For the judgment dimension analysis, we follow\nthe judgment dimensions introduced by Liu et al. (2023a)\nand also utilize GPT-4o to map the rationale generated by\njudge LLMs to one or multiple judgment dimensions. More\ndetailed prompt can be found in Appendix E. The analysis\nresults are presented in Figure 7.\nSubjective question and judgment dimension tend to\nlead to more bias. For question type analysis, we find ob-\njective questions with a definitive answer, like mathematical\nones, demonstrate the least preference leakage. By contrast,\nsubjective questions that have more than one standard an-\nswer, such as programming and writing, usually lead to a\nmore obvious preference leakage. This observation is also\napplied to judgment dimension analysis, as objective di-\nmensions (like completeness) have an overall lower leakage\ndegree compared with subjective ones (like fairness). This\nsuggests that preference leakage tends to be more significant\nin objective questions and dimensions, where the contami-\nnated model is more likely to receive biased preference.\n6. Conclusion\nIn this work, we formally highlight the preference leakage\nproblem in LLM-as-a-judge systems. The results of our\nmain experiment, measured using the proposed preference\nleakage score, reveal a clear bias in each judge toward its\nrespective student model. We also observe that this bias\nis more pronounced in comparable model pairs and larger\nstudent models. Furthermore, we conduct additional anal-\nysis on various factors, including the relationship between\nthe data generator and judge LLMs, model tuning tech-\n8\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nniques, and data mixing strategies. Our findings suggest\nthat preference leakage can cause significant bias across\ndiverse scenarios. Finally, through recognition and category\nanalyses, we investigate the underlying mechanisms of pref-\nerence leakage, demonstrating that it is a challenging and\nhard-to-detect issue, especially in subjective questions and\njudgment dimensions. In the future, we aim to explore meth-\nods for detecting, preventing, and mitigating this evolving\nchallenge in LLM-as-a-judge systems.\nImpact Statements\nBy revealing preference leakage, this work could help build\nmore trustworthy and ethically grounded AI systems. The\nrelatedness between data generators and evaluators can sys-\ntematically bias evaluations, potentially compromising the\nfairness and reliability of the automatic evaluation paradigm.\nThese biased evaluations may indirectly affect downstream\ntasks such as AI alignment and decision-making systems,\nleading to unintended ethical risks. To mitigate preference\nleakage, we hope that researchers will propose more reli-\nable evaluation methods, diversify training data sources, and\ndevelop contamination-resistant benchmarks in the future.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. ArXiv preprint,\nabs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nBalloccu, S., Schmidtov´a, P., Lango, M., and Duˇsek, O.\nLeak, cheat, repeat: Data contamination and evaluation\nmalpractices in closed-source llms. In Proceedings of the\n18th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pp. 67–93, 2024.\nBordt, S., Nori, H., and Caruana, R. Elephants never forget:\nTesting language models for memorization of tabular data.\nIn NeurIPS 2023 Second Table Representation Learning\nWorkshop, 2024.\nChen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B.\nHumans or llms as the judge? a study on judgement\nbiases. arXiv preprint arXiv:2402.10669, 2024.\nCui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y.,\nXie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting\nlanguage models with scaled ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nDai, S., Xu, C., Xu, S., Pang, L., Dong, Z., and Xu, J. Bias\nand unfairness in information retrieval systems: New\nchallenges in the llm era. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 6437–6447, 2024.\nDeng, C., Zhao, Y., Heng, Y., Li, Y., Cao, J., Tang, X.,\nand Cohan, A. Unveiling the spectrum of data contami-\nnation in language models: A survey from detection to\nremediation. arXiv preprint arXiv:2406.14644, 2024a.\nDeng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A.\nInvestigating data contamination in modern benchmarks\nfor large language models. In Proceedings of the 2024\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pp. 8698–8711,\n2024b.\nDodge, J., Sap, M., Marasovi´c, A., Agnew, W., Ilharco, G.,\nGroeneveld, D., Mitchell, M., and Gardner, M. Docu-\nmenting large webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1286–1305, 2021.\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia,\nH., Xu, J., Wu, Z., Chang, B., et al. A survey on in-\ncontext learning. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing,\npp. 1107–1128, 2024a.\nDong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and\nLi, G. Generalization or memorization: Data contamina-\ntion and trustworthy evaluation for large language models.\narXiv preprint arXiv:2402.15938, 2024b.\nDuan, S., Khona, M., Iyer, A., Schaeffer, R., and Fiete, I. R.\nUncovering latent memories: Assessing data leakage and\nmemorization patterns in large language models. arXiv\npreprint arXiv:2406.14549, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nElangovan, A., He, J., and Verspoor, K. Memorization vs.\ngeneralization: Quantifying data leakage in nlp perfor-\nmance evaluation. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pp. 1325–1335, 2021.\nGan, R., Wu, Z., Sun, R., Lu, J., Wu, X., Zhang, D.,\nPan, K., Yang, P., Yang, Q., Zhang, J., et al. Ziya2:\nData-centric learning is all llms need. arXiv preprint\narXiv:2311.03301, 2023.\n9\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nGao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan,\nX. Human-like summarization evaluation with chatgpt.\narXiv preprint arXiv:2304.02554, 2023.\nGolchin, S. and Surdeanu, M. Time travel in llms: Trac-\ning data contamination in large language models. arXiv\npreprint arXiv:2308.08493, 2023.\nHu, S., Luo, Y., Wang, H., Cheng, X., Liu, Z., and Sun, M.\nWon’t get fooled again: Answering questions with false\npremises. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 5626–5643, 2023.\nHuang, Y., Shi, J., Li, Y., Fan, C., Wu, S., Zhang, Q., Liu, Y.,\nZhou, P., Wan, Y., Gong, N. Z., et al. Metatool benchmark\nfor large language models: Deciding whether to use tools\nand which to use. arXiv preprint arXiv:2310.03128, 2023.\nHuang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y.,\nGao, C., Huang, Y., Lyu, W., Zhang, Y., et al. Posi-\ntion: Trustllm: Trustworthiness in large language models.\nIn International Conference on Machine Learning, pp.\n20166–20270. PMLR, 2024.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Car-\nney, A., et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiang, B., Li, D., Tan, Z., Zhou, X., Rao, A., Lerman, K.,\nBernard, H. R., and Liu, H. Assessing the impact of\nconspiracy theories using large language models. arXiv\npreprint arXiv:2412.07019, 2024a.\nJiang, M., Liu, K. Z., Zhong, M., Schaeffer, R., Ouyang,\nS., Han, J., and Koyejo, S. Investigating data contami-\nnation for pre-training language models. arXiv preprint\narXiv:2401.06059, 2024b.\nKaufman, S., Rosset, S., Perlich, C., and Stitelman, O. Leak-\nage in data mining: Formulation, detection, and avoid-\nance. ACM Transactions on Knowledge Discovery from\nData (TKDD), 6(4):1–21, 2012.\nKim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun,\nS., Shin, S., Kim, S., Thorne, J., et al. Prometheus: Induc-\ning fine-grained evaluation capability in language models.\nIn The Twelfth International Conference on Learning\nRepresentations, 2023.\nKim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J.,\nWelleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M.\nPrometheus 2: An open source language model special-\nized in evaluating other language models. arXiv preprint\narXiv:2405.01535, 2024.\nKoo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M.,\nand Kang, D.\nBenchmarking cognitive biases in\nlarge language models as evaluators.\narXiv preprint\narXiv:2309.17012, 2023.\nK¨opf, A., Kilcher, Y., von R¨utte, D., Anagnostidis, S.,\nTam, Z. R., Stevens, K., Barhoum, A., Nguyen, D., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations-\ndemocratizing large language model alignment. Advances\nin Neural Information Processing Systems, 36, 2024.\nLee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J.,\nLu, K. R., Bishop, C., Hall, E., Carbune, V., Rastogi,\nA., et al. Rlaif vs. rlhf: Scaling reinforcement learning\nfrom human feedback with ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nLee, S., Zhou, J., Ao, C., Li, K., Du, X., He, S., Liu, J., Yang,\nM., Wen, Z., and Ni, S. Distillation quantification for\nlarge language models. arXiv preprint arXiv:2501.12619,\n2025.\nLi, C. and Flanigan, J. Task contamination: Language mod-\nels may not be few-shot anymore. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 38,\npp. 18471–18480, 2024.\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z.,\nBhattacharjee, A., Jiang, Y., Chen, C., Wu, T., et al. From\ngeneration to judgment: Opportunities and challenges of\nllm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024a.\nLi, D., Tan, Z., Chen, T., and Liu, H. Contextualization dis-\ntillation from large language model for knowledge graph\ncompletion. arXiv preprint arXiv:2402.01729, 2024b.\nLi, D., Yang, S., Tan, Z., Baik, J. Y., Yun, S., Lee, J.,\nChacko, A., Hou, B., Duong-Tran, D., Ding, Y., et al.\nDalk: Dynamic co-augmentation of llms and kg to an-\nswer alzheimer’s disease questions with scientific litera-\nture. arXiv preprint arXiv:2405.04819, 2024c.\nLi, D., Tan, Z., and Liu, H. Exploring large language models\nfor feature selection: A data-centric perspective. ACM\nSIGKDD Explorations Newsletter, 26(2):44–53, 2025.\nLi, M., Chen, L., Chen, J., He, S., Gu, J., and Zhou, T. Selec-\ntive reflection-tuning: Student-selected data recycling for\nllm instruction-tuning. arXiv preprint arXiv:2402.10110,\n2024d.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline. arXiv preprint arXiv:2406.11939, 2024e.\n10\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval:\nAn automatic evaluator of instruction-following models,\n2023.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74–81, 2004.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. In Proceedings of\nthe 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 3214–\n3252, 2022.\nLiu, C.-W., Lowe, R., Serban, I., Noseworthy, M., Charlin,\nL., and Pineau, J. How NOT to evaluate your dialogue sys-\ntem: An empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In Su, J., Duh, K.,\nand Carreras, X. (eds.), Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Process-\ning, pp. 2122–2132, Austin, Texas, 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1230.\nURL https://aclanthology.org/D16-1230.\nLiu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes\ngood data for alignment? a comprehensive study of auto-\nmatic data selection in instruction tuning. In The Twelfth\nInternational Conference on Learning Representations,\n2024.\nLiu, X., Lei, X., Wang, S., Huang, Y., Feng, Z., Wen, B.,\nCheng, J., Ke, P., Xu, Y., Tam, W. L., et al. Alignbench:\nBenchmarking chinese alignment of large language mod-\nels. arXiv preprint arXiv:2311.18743, 2023a.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nDing, H., Men, K., Yang, K., et al. Agentbench: Evalu-\nating llms as agents. arXiv preprint arXiv:2308.03688,\n2023b.\nNi, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neu-\nbig, G., and You, Y. Mixeval: Deriving wisdom of the\ncrowd from llm benchmark mixtures. arXiv preprint\narXiv:2406.06565, 2024.\nPanickssery, A., Bowman, S. R., and Feng, S. Llm evalu-\nators recognize and favor their own generations. arXiv\npreprint arXiv:2404.13076, 2024.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, pp. 311–318,\n2002.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nReiter, E. A structured review of the validity of BLEU.\nComputational Linguistics, 44(3):393–401, 2018. doi: 10.\n1162/coli a 00322. URL https://aclanthology.\norg/J18-3002.\nShi, J., Yuan, Z., Liu, Y., Huang, Y., Zhou, P., Sun,\nL., and Gong, N. Z.\nOptimization-based prompt in-\njection attack to llm-as-a-judge.\nIn Proceedings of\nthe 2024 on ACM SIGSAC Conference on Computer\nand Communications Security, CCS ’24, pp. 660–674,\nNew York, NY, USA, 2024. Association for Comput-\ning Machinery. ISBN 9798400706363. doi: 10.1145/\n3658644.3690291.\nURL https://doi.org/10.\n1145/3658644.3690291.\nShumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Ander-\nson, R., and Gal, Y. Ai models collapse when trained on\nrecursively generated data. Nature, 631(8022):755–759,\n2024.\nSun, R., Liu, M., Yang, S., Wang, R., He, J., and Zhang, J.\nFostering natural conversation in large language models\nwith nico: a natural interactive conversation dataset. arXiv\npreprint arXiv:2408.09330, 2024a.\nSun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Liu, X.,\nYan, H., Shao, Y., Tang, Q., Zhang, S., Zhao, X., Chen,\nK., Zheng, Y., Zhou, Z., Li, R., Zhan, J., Zhou, Y.,\nLi, L., Yang, X., Wu, L., Yin, Z., Huang, X., Jiang,\nY.-G., and Qiu, X.\nMoss: An open conversational\nlarge language model. Machine Intelligence Research,\n2024b. ISSN 2731-5398. URL https://github.\ncom/OpenMOSS/MOSS.\nTan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee,\nA., Karami, M., Li, J., Cheng, L., and Liu, H. Large\nlanguage models for data annotation and synthesis: A sur-\nvey. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 930–957,\n2024.\nTeam, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L.,\nGulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S.,\net al. Gemini 1.5: Unlocking multimodal understand-\ning across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nThakur, A. S., Choudhary, K., Ramayapally, V. S.,\nVaidyanathan, S., and Hupkes, D. Judging the judges:\nEvaluating alignment and vulnerabilities in llms-as-\njudges. arXiv preprint arXiv:2406.12624, 2024.\n11\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nTong, Y., Li, D., Wang, S., Wang, Y., Teng, F., and Shang,\nJ. Can llms learn from previous mistakes? investigat-\ning llms’ errors to boost for reasoning. arXiv preprint\narXiv:2403.20046, 2024.\nWang, S., Tong, Y., Zhang, H., Li, D., Zhang, X., and Chen,\nT. Bpo: Towards balanced preference optimization be-\ntween knowledge breadth and depth in alignment. arXiv\npreprint arXiv:2411.10914, 2024.\nWhite, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain,\nS., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S.,\net al. Livebench: A challenging, contamination-free llm\nbenchmark. arXiv preprint arXiv:2406.19314, 2024.\nWu, S., Huang, Y., Gao, C., Chen, D., Zhang, Q., Wan, Y.,\nZhou, T., Zhang, X., Gao, J., Xiao, C., et al. Unigen: A\nunified framework for textual dataset generation using\nlarge language models. arXiv preprint arXiv:2406.18966,\n2024.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115, 2024.\nYang, S., Sun, R., and Wan, X. A new dataset and empirical\nstudy for sentence simplification in chinese. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n8306–8321, 2023.\nYao, F., Zhuang, Y., Sun, Z., Xu, S., Kumar, A., and Shang,\nJ. Data contamination can cross language barriers. arXiv\npreprint arXiv:2406.13236, 2024.\nYe, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N.,\nGao, T., Geyer, W., Huang, C., Chen, P.-Y., et al. Justice\nor prejudice? quantifying biases in llm-as-a-judge. arXiv\npreprint arXiv:2410.02736, 2024.\nZhang, H., Li, D., Li, Y., Shang, C., Shi, C., and Jiang, Y.\nAssisting language learners: Automated trans-lingual def-\ninition generation via contrastive prompt learning. arXiv\npreprint arXiv:2306.06058, 2023.\nZhang, H., Shang, C., Wang, S., Zhang, D., Yao, F., Sun,\nR., Yu, Y., Yang, Y., and Wei, F. Shifcon: Enhancing\nnon-dominant language capabilities with a shift-based\ncontrastive framework. arXiv preprint arXiv:2410.19453,\n2024a.\nZhang, H., Wu, Y., Li, D., Yang, Z., Zhao, R., Jiang, Y., and\nTan, F. Balancing speciality and versatility: a coarse to\nfine framework for supervised fine-tuning large language\nmodel. arXiv preprint arXiv:2404.10306, 2024b.\nZhang, Q., Gao, C., Chen, D., Huang, Y., Huang, Y.,\nSun, Z., Zhang, S., Li, W., Fu, Z., Wan, Y., and Sun,\nL. LLM-as-a-coauthor: Can mixed human-written and\nmachine-generated text be detected? In Duh, K., Gomez,\nH., and Bethard, S. (eds.), Findings of the Association\nfor Computational Linguistics: NAACL 2024, pp. 409–\n436, Mexico City, Mexico, June 2024c. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-naacl.29. URL https://aclanthology.\norg/2024.findings-naacl.29/.\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi,\nY. Bertscore: Evaluating text generation with bert. In\nInternational Conference on Learning Representations,\n2020.\nZhang, X., Peng, B., Tian, Y., Zhou, J., Jin, L., Song,\nL., Mi, H., and Meng, H.\nSelf-alignment for fac-\ntuality:\nMitigating hallucinations in LLMs via self-\nevaluation. In Ku, L.-W., Martins, A., and Srikumar,\nV. (eds.), Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pp. 1946–1965, Bangkok, Thailand,\nAugust 2024d. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.acl-long.107. URL https:\n//aclanthology.org/2024.acl-long.107/.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nZheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z.,\nand Ma, Y. Llamafactory: Unified efficient fine-tuning of\n100+ language models. arXiv preprint arXiv:2403.13372,\n2024.\nZhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P.,\nZhu, C., Ji, H., and Han, J. Towards a unified multi-\ndimensional evaluator for text generation.\nIn Gold-\nberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npp. 2023–2038. Association for Computational Linguis-\ntics, 2022.\ndoi: 10.18653/V1/2022.EMNLP-MAIN.\n131.\nURL https://doi.org/10.18653/v1/\n2022.emnlp-main.131.\nZhong, M., Zhang, A., Wang, X., Hou, R., Xiong, W., Zhu,\nC., Chen, Z., Tan, L., Bi, C., Lewis, M., et al. Law of the\nweakest link: Cross capabilities of large language models.\narXiv preprint arXiv:2409.19951, 2024.\n12\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\nZhuge, M., Zhao, C., Ashley, D., Wang, W., Khizbullin, D.,\nXiong, Y., Liu, Z., Chang, E., Krishnamoorthi, R., Tian,\nY., et al. Agent-as-a-judge: Evaluate agents with agents.\narXiv preprint arXiv:2410.10934, 2024.\n13\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nA. Preliminary Study of Preference Leakage in Real World\nIn our preliminary study, we investigate whether preference leakage is a real-world issue in mainstream leaderboards and\nbenchmarks. To this end, we examine two widely used LLM-as-a-judge leaderboards (AlpacaEval 2.0 and Arena-Hard) and\na well-known benchmark (MTBench). All three rely on GPT-4 as the judge model and report pairwise judgment results for\nvarious LLMs. Our analysis reveals that several candidate models distilled from GPT-4 or other GPT-series models (e.g.,\nVicuna and Alpaca) appear across all these leaderboards and benchmarks, suggesting that preference leakage is a pervasive\nissue in these datasets. Besides, we also examine if preference leakage exists in LLM-relevant research studies and also find\na bunch of work utilizing the same or related model(s) to do distillation/ data synthesis and evaluation (Yang et al., 2023;\nLiu et al., 2024; Lee et al., 2024; Li et al., 2024d; Wang et al., 2024; Sun et al., 2024a). All of these suggest preference\nleakage to be a widespread problem in both LLM-as-a-judge datasets and LLM-relevant research.\nB. Experiment Details\nB.1. Training Details\nWe use LLaMA-Factory (Zheng et al., 2024), an efficient LLM tuning library for our experiment. The maximum sequence\nlength is set to 1024 tokens, and a cutoff length of 1024 tokens is enforced to prevent excessive tokenization. The data\npreprocessing will be done in parallel with 16 workers to speed up the preparation process. The training use a per-device\nbatch size of 2, with gradient accumulation over 2 steps to simulate a larger batch size for SFT and a per-device batch size of\n1, with gradient accumulation over 4 steps to simulate a larger batch size for DPO. The learning rate is set to 1.0e-5 and each\nmodel will be trained for 3 epochs. A cosine learning rate scheduler is used with a warmup ratio of 0.1 to gradually increase\nthe learning rate during the initial steps. All of the experiments use BF16 precision to speed up training while maintaining\nnumerical stability. All the experiments are conducted in an 8 Nvidia A100 GPU cluster with CUDA version 11.8.\nJudge Model\nMistral-GPT-4o vs Mistral-Gemini-1.5\nMistral-GPT-4o Wins\nMistral-Gemini-1.5 Wins\nGPT-4o\n55.1%\n44.9%\nGemini-1.5\n36.8%\n63.2%\nPreference Leakage Score\n18.4%\nTable 5. A case on AlpacaEval 2.0 with the model pair Mistral-GPT-4o vs Mistral-Gemini-1.5 to demonstrate how the preference leakage\nscore is calculated.\nB.2. Detailed Explanation for Preference Leakage Score\nWe present a case in Table B.1 to show how we calculate the preference leakage score for the Mistral-GPT-4o vs Mistral-\nGemini-1.5 pair on AlpacaEval 2.0. Based on the definition of preference leakage score, we first calculate:\nAVG(Mistral-GPT-4o, Mistral-Gemini-1.5) = 55.1 + 36.8\n2\n= 45.95%\n(9)\nAVG(Mistral-Gemini-1.5, Mistral-GPT-4o) = 63.2 + 44.9\n2\n= 54.05%\n(10)\nAfter that, we calculate the preference leakage score:\nPLS(Mistral-GPT-4o, Mistral-Gemini-1.5) =\n\x00 55.1−45.95\n45.95\n\x01\n+\n\x00 63.2−54.05\n54.05\n\x01\n2\n= 18.4%\n(11)\n.\nB.3. Manual Annotation Details\nWe randomly sample 100 questions from AlpacaEval 2.0 and ask three well-trained annotators to conduct pairwise\ncomparisons of the responses from each model pair for these questions. For annotation efficiency, we also develop an\nannotation tool that involves the function of uploading multiple model responses, jumping to specific problems, and\n14\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ndownloading annotation results (Figure 8). After annotation, we adopt the majority voting to get the final label for each\nresponse pair. We also calculate the average agreement of three annotators and find it to be 78.6, indicating a relatively\nconsistent annotation result.\nC. Learning Method Analysis Details\nThe table below presents the prompt we use to generate synthetic pairwise feedback from each model.\nPairwise Feedback Prompt\nPlease act as an impartial judge and evaluate the quality of the responses provided\nby two AI assistants to the user question displayed below.\nYour evaluation should\nconsider correctness and helpfulness.\nYou will be given assistant A’s answer,\nand assistant B’s answer.\nYour job is to evaluate which assistant’s answer is\nbetter.\nYou should independently solve the user question step-by-step first.\nThen compare both assistants’ answers with your answer.\nIdentify and correct\nany mistakes.\nAvoid any position biases and ensure that the order in which the\nresponses were presented does not influence your decision.\nDo not allow the length\nof the responses to influence your evaluation.\nDo not favor certain names of the\nassistants.\nBe as objective as possible.\nAfter providing your explanation, output\nyour final verdict by strictly following this format:\n"[[A]]" if assistant A is\nbetter, "[[B]]" if assistant B is better.\n## Instruction:\n[The Start of Assistant A’s Answer]\n[RESPONSE A]\n[The End of Assistant A’s Answer]\n[The Start of Assistant B’s Answer]\n[RESPONSE B]\n[The End of Assistant B’s Answer]\nPlease output the generated content in a json format, for example:\n{ "reason":\n//\nstring, reasons behind the chosen preferred answer "prefered answer":\n// string,\nthe prefered answer you selected, [[A]] or [[B]] }\nFormatted the abovementioned schema and produce the reason and preferred answer:\nD. Recogniton Analysis Details\nThe table below presents the prompt we use for the recognition analysis.\nRecognition Prompt\nGiven an instruction and a response, your task is to judge whether this response\nis generated by a model that is trained on a synthetic dataset you produced (your\nstudent model).\n## Instruction:\n[INSTRUCTION]\n## Response:\n[Response]\nPlease output the generated content in a json format, for example:\n"reason":\n//\nstring, reasons behind the judgment "judgment":\n// string, whether the answer is\ngenerated by your student model, choose from yes or no\nFormatted the abovementioned schema and produce the reason and judgment:\nFor response classification, we split all the response from three student models into training (80%) and testing (20%) subsets.\n15\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFigure 8. The annotation tool we develop for annotation efficiency.\nThen, we finetune a BERT-base-uncased model in the training set. The model is trained for 3 epochs with a learning rate of\n2e-5, a batch size of 16 for both training and evaluation, and a weight decay of 0.01, with evaluations conducted at the end\nof each epoch.\nE. Category Analysis Details\nThe tables below present the prompt we use for question type and judgment dimension cateogory analysis.\nQuestion Type Categorization Prompt\nGiven a question, please categorize it to one of the following categories:\n1.\nComputer Science & Programming\n2.\nMathematics & Statistics\n3.\nScience & Engineering\n4.\nBusiness & Finance\n5.\nWriting & Communication\n6.\nSocial & Daily Life\n7.\nOthers\n## Question:\n[QUESTION]\nPlease output the generated content in a json format, for example:\n{ "question\ncategory":\n// string, specific category name, such as "Computer Science &\nProgramming" }\nFormatted the abovementioned schema and categorize the given question:\n16\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nJudgment Dimension Categorization Prompt\nGiven a pairwise comparison judgment made by an AI, please categorize each\nconsidered aspect in the rationale to one of the following categories:\n{\n"Factuality":\n"Whether the information provided in the response is accurate, based\non reliable facts and data.",\n"User Satisfaction":\n"Whether the response meets the user’s question and needs, and\nprovides a comprehensive and appropriate answer to the question.",\n"Logical Coherence":\n"Whether the response maintains overall consistency and\nlogical coherence between different sections, avoiding self-contradiction.",\n"Richness":\n"Whether the response includes rich info, depth, context, diversity,\ndetailed explanations and examples to meet user needs and provide a comprehensive\nunderstanding.",\n"Creativity":\n"Whether the response is innovative or unique, providing novel\ninsights or solutions.",\n"Fairness and Responsibility":\n"Whether the advice or information provided in the\nresponse is feasible, carries acertain degree of responsibility, and considers\npotential risks and consequences.",\n"Completeness":\n"Whether the response provides sufficient information and details\nto meet the user’s needs, and whether it avoids omitting important aspects.",\n"Clarity":\n"Whether the response is clear and understandable, and whether it uses\nconcise language and structure so that the user can easily understand it.",\n"Others":\n"Other aspects which is not listed above."\n}\n## Judgment:\n[JUDGMENT]\nPlease output the generated content in a json format, for example:\n{ "Factuality":\n// list, all aspects that belong to this category, such as ["correctness",\n"mistakes"] ...\n}\nFormatted the abovementioned schema and categorize aspects in the judgment:\n17')]}
2025-02-06 00:52:01,862 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 00:52:30,231 - INFO - Total execution time: 27.60 seconds (0.46 minutes)
2025-02-06 00:52:30,239 - INFO - Papers: {'2025-02-04': [Paper(arxiv_id='2502.01061', authors=['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], published_at=datetime.datetime(2025, 2, 4, 0, 37, 57, 949000, tzinfo=datetime.timezone.utc), title='OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models', summary='End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)', upvotes=127, thumbnail=None, content='Since the emergence of the Diffusion Transformer-based\n(DiT) video diffusion models, the field of general video\ngeneration, including Text-to-Video and Image-to-Video [3–\n6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82] has made significant\nprogress in producing highly realistic video content. A key\nfactor driving this advancement is the large-scale training\ndata, typically formatted as video-text pairs. Expanding\nthe training dataset enables DiT networks to learn motion\npriors for various objects and scenes, resulting in strong\ngeneralization capabilities during inference.\nBuilding upon these pretrained video diffusion networks,\nend-to-end human animation models, either for pose-driven\nhuman animation or audio-driven talking human generation,\nhave developed rapidly since last year [8, 18, 26, 34, 52, 54,\n62, 70, 71]. Despite achieving realistic results, these models\nare trained on highly filtered datasets to simplify the learning\nprocess, restricting their applicability to limited scenarios.\nFor instance, most existing end-to-end audio-conditioned\nmodels are limited to facial or portrait animation, while\nmost pose-conditioned models can only handle full-body\nimages captured from a front-facing perspective with a static\nbackground. To date, no prior work has attempted to scale\nup training data for more generalizable human animation.\nScaling up human animation data may seem straightfor-\nward, but unfortunately it is not. Directly adding more data\nis not always beneficial for network training. Take audio-\nconditioned models as an example: audio is primarily as-\nsociated with facial expressions and has little correlation\nwith body poses, background motion, camera movement,\nor lighting changes. As a result, raw training data must\nbe filtered and cropped to minimize the influence of these\nunrelated factors. Additionally, audio-conditioned models\noften undergo further data cleaning based on lip-sync accu-\nracy, which is also important to stabilize training. Similarly,\npose-conditioned models require extensive filtering, crop-\nping, and cleaning. Unfortunately, these processes discard\na substantial amount of data, making dataset scaling a fu-\ntile effort, despite the fact that much of the discarded data\ncontains valuable motion patterns essential for training data\nexpansion.\nIn this paper, we address the challenges of scaling up\nhuman animation data and models. Our key insight is that\nincorporating multiple conditioning signals, such as text, au-\ndio, and pose, during training can significantly reduce data\nwastage. This approach offers two main advantages. On\none hand, data that would otherwise be discarded for single-\ncondition models (e.g., audio- or pose-conditioned) can be\nleveraged in tasks with weaker or more general conditions,\nsuch as text conditioning. Training on such data allows the\nmodel to learn more diverse motion patterns, mitigating the\nlimitations imposed by data filtering. On the other hand, dif-\nferent conditioning signals can complement each other. For\nexample, while audio alone cannot precisely control body\nposes, stronger conditions such as pose inputs can provide\nadditional guidance. By integrating stronger conditioning\nsignals alongside audio data during training, we aim to re-\nduce overfitting and improve the generalization of generated\nresults.\nBased on the above considerations, we designed the omni-\nconditions training strategy, which follows two proposed\ntraining principles: (1) stronger conditioned tasks can lever-\nage weaker conditioned tasks and their corresponding data\nto achieve data scaling up during the model training process,\nand (2) the stronger the condition, the lower the training\nratio that should be used. To implement this strategy, we\nbuilt a mixed conditioned human video generation model\nnamed OmniHuman, based on the advanced video gener-\nation model architecture, DiT [14, 42]. OmniHuman can\ntrain with three motion-related conditions (text, audio, and\npose) from weak to strong. This approach addresses the data\nscaling up challenge in end-to-end frameworks, allowing the\nmodel to benefit from large-scale data training, learn natural\nmotion patterns, and support various input forms.\nOverall, our contributions can be summarized as follows:\n1. We propose the OmniHuman model, a mixed-conditioned\nhuman video generation model. It leverages our omni-\nconditions training strategy to integrate various motion-\nrelated conditions and their corresponding data. Unlike\nexisting methods that reduce data due to stringent filter-\ning, our approach benefits from large-scale mixed condi-\ntioned data.\n2. OmniHuman generates highly realistic and vivid human\nmotion videos, supporting multiple modalities simulta-\nneously. It performs well with different portrait and in-\nput aspect ratios. OmniHuman significantly improves\ngesture generation, a challenge for previous end-to-end\nmodels, and supports various image styles, significantly\noutperforming existing audio-conditioned human video\ngeneration methods.\n2. Related Works\n2.1. Video Generation\nIn recent years, the advent of technologies such as diffusion\nmodels [21, 29, 38, 50, 51] has propelled the capabilities of\ngenerative models to a practically usable level. The latest\nadvancements in image generation [7, 14] produce results\n2\n\nthat are almost indistinguishable from reality. Consequently,\na growing number of studies [24, 31, 43, 57, 73, 76, 82]\nare shifting their focus toward the field of video generation.\nEarly text-to-video works primarily centered on training-free\nadaptations of pre-trained text-to-image models [44, 49, 68]\nor integrated temporal layers with fine-tuning on limited\nvideo datasets [16, 63, 82]. However, due to the lack of\nextensive data, the video generation quality of these methods\noften remains unsatisfactory. To better exploit scaling laws\nand push the boundaries of video generation models, recent\nworks [31, 43, 57, 73] have optimized in three major areas.\nFirst, they have collected larger-scale, high-quality video\ndatasets, with the data volume increasing to (O(100M)) clips\nof high-resolution videos. Second, they employ 3D Causal\nVAE [75] to compress both spatial and temporal features\nof video data, thereby enhancing video modeling efficiency.\nThird, the foundational model structure has transitioned from\nUNet to Transformer, improving the model’s scalability. Ad-\nditionally, these works utilize meticulously designed progres-\nsive training recipes and datasets to maximize the model’s\npotential. For example, [31, 43] first pre-train on a large\nvolume of low-resolution images and videos, leveraging data\ndiversity to enhance the model’s generalization capabilities.\nThey then perform fine-tuning on a subset of high-resolution,\nhigh-quality data to improve the visual quality of generated\nvideos. Large-scale data has significantly improved the ef-\nfectiveness of general video generation. However, progress\nin the field of human animation synthesis remains relatively\nslow.\n2.2. Human Animation\nAs an important task of video generation, Human Anima-\ntion synthesizes human videos using human images and\ndriving conditions such as audios or videos. Early GAN-\nbased methods [27, 47, 48, 65, 79] typically employ small\ndatasets [40, 47, 69, 83] consisting of tens of thousands of\nvideos to achieve video-driven in a self-supervised man-\nner. With the advancement of Diffusion models, several\nrelated works [25, 46, 64, 78, 85] have surpassed GAN-\nbased methods in performance while using datasets of simi-\nlar scale. Instead of using pixel-level videos, these methods\nemploy 2D skeleton, 3D depth, or 3D mesh sequences as\ndriving conditions. Audio-driven methods used to focus\non portrait [11, 15, 26, 56, 74, 77, 81]. Despite some ef-\nforts [10, 23, 34, 39, 55] to extend the frame to the full\nbody, there are still challanges especially in hand quality.\nTo bypass it, most approaches [10, 23, 39, 55] adopt a two-\nstage hybrid driving strategy, utilizing gesture sequences\nas a strong condition to assist hand generation. CyberHost\n[34] attempts to achieve one-stage audio-driven talking body\ngeneration through codebook design. Most notably, existing\nHuman Animation methods typically focus on limited-scale\ndatasets and limited-complexity structure, generally less than\na thousand hours and 2B. Although FADA [81] employs a\nsemi-supervised data strategy to utilize 1.4K hours of por-\ntrait videos, VLogger [10] meticulously collects 2.2K hours\nof half-body videos, and Hallo3 [11] initializes its weights\nderived from CogVideoX5B-I2V [72], their performance\ndoes not exhibit the scaling law trends observed in other\ntasks such as LLMs [41, 58], VLMs [2, 37], and T2I/T2V\n[13, 30, 32]. Scaling effects in Human Animation haven’t\nbeen investigated effectively yet.\n3. Method\nIn this section, we introduce our framework, OmniHuman,\nwhich employs motion-related condition mixing during net-\nwork training to scale up the training data. First, we pro-\nvide an overview of the framework, including its inputs,\noutputs and key design elements. Next, we focus on the\nomni-conditions design, covering audio, pose, and reference\nconditions. We then detail the training strategy of OmniHu-\nman, which leverages these omni-conditions for mixed data\ntraining, enabling the model to learn natural motion from\nlarge-scale datasets. Finally, we describe the implementation\ndetails for the inference phases of the OmniHuman model.\n3.1. Overview\nAs illustrated in Figure 2, our approach consists of two\nprimary parts: the OmniHuman model, a multi-condition\ndiffusion model and the Omni-Conditions Training Strategy.\nFor model, The OmniHuman model begins with a pretrained\nSeaweed model [35], which uses MMDiT [14, 42] and is ini-\ntially trained on general text-video pairs for text-to-video and\ntext-to-image tasks. Given a reference image, the OmniHu-\nman model aims to generate human videos using one or more\ndriving signals including text, audio and pose. To achieve\nthis, we employ various strategies to integrate frame-level\naudio features and pose heatmap features into the Omni-\nHuman model. The detailed procedure is explained in the\nfollowing subsections. OmniHuman model utilizes a causal\n3DVAE [80] to project videos at their native size [12] into a\nlatent space and employs flow matching [36] as the training\nobjective to learn the video denoising process. We employ a\nthree-stage mixed condition post-training approach to pro-\ngressively transform the diffusion model from a general\ntext-to-video model to a multi-condition human video gener-\nation model. As depicted on the left of Figure 2, these stages\nsequentially introduce the driving modalities of text, audio,\nand pose according to their motion correlation strength, from\nweak to strong, and balance their training ratios.\n3.2. Omni-Conditions Designs\nDriving Conditions. We adopted different approaches for\ninjecting audio and pose conditions. Regarding audio con-\ndition, the wav2vec [1, 45] model is employed to extract\nacoustic features, which are subsequently compressed using\n3\n\nFigure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture\nand supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training\nstrategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training\nallows the OmniHuman model to benefit from the scaling up of mixed data.\na MLP to align with the hidden size of MMDiT. The features\nof each frame are concatenated with the audio features from\nadjacent timestamps to generate audio tokens for the current\nframe. As depicted in Figure 2, these audio tokens are in-\njected into each block of MMDiT through cross-attention,\nenabling interaction between the audio tokens and the noisy\nlatent representations. To incorporate pose condition, we use\na pose guider to encode the driving pose heatmap sequence.\nThe resulting pose features are concatenated with those of\nadjacent frames to acquire pose tokens. These pose tokens\nare then stacked with the noise latent along the channel di-\nmension and fed into the unified multi-condition diffusion\nmodel for visual alignment and dynamic modeling. The text\ncondition is retained as in the MMDiT text branch.\nAppearance Conditions. The goal of OmniHuman is\nto generate video outputs that preserve both the subject’s\nidentity and the background details from a reference im-\nage. To achieve this, previous research has proposed various\nstrategies for injecting appearance representations into the\ndenoising process. The most widely adopted approach in-\nvolves using a reference network [26, 34, 54], a parallel,\ntrainable copy of the entire diffusion UNet or DiT that inte-\ngrates with the self-attention layers of the original denoising\nNet. While effective at transferring appearance features\nto the denoising process, this method requires duplicating\na full set of trainable parameters, which presents scalabil-\nity challenges as model size increases. To overcome this\nchallenge, OmniHuman introduces a simple yet effective\nstrategy for reference conditioning. Instead of constructing\nadditional network modules, we reuse the original denoising\nDiT backbone to encode the reference image. Specifically,\nthe reference image is first encoded into a latent represen-\ntation using a VAE, and both the reference and noisy video\nlatents are flattened into token sequences. These sequences\nare then packed together and simultaneously fed into the\nDiT, enabling the reference and video tokens to interact via\nself-attention across the entire network. To help the network\ndistinguish between reference and video tokens, we modify\nthe 3D Rotational Position Embeddings (RoPE) [53] in the\nDiT by zeroing the temporal component for reference tokens,\nwhile leaving the RoPE for video tokens unchanged. This\napproach effectively incorporates appearance conditioning\nwithout adding extra parameters. In addition to the reference\nimage, to support long video generation, we draw on pre-\nvious methods by using motion frames [52], concatenating\ntheir features with the noise features.\nAfter introducing these conditions, the motion-related\nconditions now include text, reference image, audio, and\npose. Text describes the current event, the reference image\ndefines the range of motion, audio determines the rhythm\nof co-speech gestures, and pose specifies the exact motion.\nTheir correlation strength with human motions can be con-\nsidered to decrease in this order.\n4\n\n3.3. Scaling up with Omni-Conditions Training\nThanks to the multi-condition design, we can divide the\nmodel training into multiple tasks, including image and text\nto video, image and text, audio to video, and image and text,\naudio, pose to video. During training, different modalities\nare activated for different data, allowing a broader range of\ndata to participate in the training process and enhancing the\nmodel’s generation capabilities. After the conventional text-\nto-video pretraining phase, we follow two training principles\nfor scaling up the conditioned human video generation task.\nPrinciple 1, stronger conditioned tasks can leverage weaker\nconditioned tasks and their corresponding data to achieve\ndata scaling up during the model training process. Data ex-\ncluded from audio and pose conditioned tasks due to filtering\ncriteria like lip-sync accuracy, pose visibility, and stability\ncan be used in text and image conditioned tasks, as they meet\nthe standards for weaker conditions. Therefore, in the first\nstage 1, we drop the audio and pose conditions. Principle 2,\nthe stronger the condition, the lower the training ratio that\nshould be used. During training, stronger motion-related\nconditions, such as pose, generally train better than weaker\nconditions like audio due to less ambiguity. When both con-\nditions are present, the model tends to rely on the stronger\ncondition for motion generation, preventing the weaker con-\ndition from learning effectively. Therefore, we ensure that\nweaker conditions have a higher training ratio than stronger\nconditions. We construct stage 2 to drop only the pose condi-\ntion, and in the final stage 3, use all conditions. Additionally,\nthe training ratios for text, reference, audio, and pose are\nprogressively halved. This approach assigns higher gradient\nweights to more challenging tasks and prevents overfitting\nto a single condition during overlapping condition training.\nPrinciple 1 allows us to significantly expand the training data,\nwhile Principle 2 ensures that the model fully utilizes the\nadvantages of each motion-related condition during mixed\nconditions training and learns their motion generation ca-\npabilities. By combining Principles 1 and 2, OmniHuman\ncan effectively train with mixed conditioned data, benefiting\nfrom data scaling up and achieving satisfactory results.\n3.4. Inference Strategies\nFor audio-driven scenarios, all conditions except pose are\nactivated. For pose-related combinations, all conditions are\nactivated, but for pose-only driving, audio is disabled. Gen-\nerally, when a condition is activated, all conditions with a\nlower motion-related influence are also activated unless un-\nnecessary. During inference, to balance expressiveness and\ncomputational efficiency, we apply classifier-free guidance\n(CFG) [20] specifically to audio and text across multiple\nconditions. However, we observed that an increased CFG\nresults in pronounced wrinkles on the characters, whereas\na decreased CFG compromises lip synchronization and mo-\ntion expressiveness. To mitigate these issues, we propose\na CFG annealing strategy that progressively reduces the\nCFG magnitude throughout the inference process, thereby\nsignificantly minimizing the appearance of wrinkles while\nensuring that expressiveness. OmniHuman is capable of\nproducing video segments of arbitrary length within mem-\nory constraints based on the provided reference images and\nvarious driving signals. To ensure temporal coherence and\nidentity consistency in long videos, the last five frames of\nthe previous segment are utilized as motion frames.\n4. Experiments\n4.1. Implementation Details\nDataset. By filtering based on aesthetics, image quality, mo-\ntion amplitude, etc. (common criteria for video generation),\nwe obtained 18.7K hours of human-related data for training.\nOf this, 13% was selected using lipsync and pose visibility\ncriteria, enabling audio and pose modalities. During training,\nthe data composition was adjusted to fit the omni-condition\ntraining strategy. For testing, we conduct the evaluation fol-\nlowing the portrait animation method Loopy [26] and the\nhalf-body animation method CyberHost [34]. We randomly\nsampled 100 videos from public portrait datasets, includ-\ning CelebV-HQ [83] (a diverse dataset with mixed scenes)\nand RAVDESS [28] (an indoor dataset including speech and\nsong) as the testset for portrait animation. For half-body\nanimation, we used CyberHost’s test set, which includes a\ntotal of 269 body videos with 119 identities, encompassing\ndifferent races, ages, genders, and initial poses.\nBaselines. To comprehensively evaluate OmniHuman’s\nperformance in different scenarios, we compare against por-\ntrait animation baselines including Sadtalker [77], Hallo\n[70], Vexpress [62], EchoMimic [8], Loopy [26], Hallo-3\n[11], and body animation baselines including DiffTED [23],\nDiffGest [84] + Mimiction [78], CyberHost [34].\nMetrics. For visual quality, FID [19] and FVD [59] are\nused to evaluate the distance between the generated and\nlabeled images and videos. We also leverage q-align [67],\na VLM to evaluate the no-reference IQA(image quality)\nand ASE(aesthetics). For lip synchronism, we employ the\nwidely-used Sync-C [9] to calculate the confidence between\nvisual and audio content. Besides, HKC (hand keypoint\nconfidence) [34] and HKV (hand keypoint variance) [34]\nare employed, to represent hand quality and motion richness\nrespectively.\n4.2. Comparisons with Existing Methods\nAs shown in the Table 1 and 2, overall, OmniHuman demon-\nstrates superior performance compared to leading specialized\nmodels in both portrait and body animation tasks using a\nsingle model. For audio-driven animation, the generated\nresults cannot be identical to the original video, especially\nwhen the reference image contains only a head. The model’s\n5\n\nTable 1. Quantitative comparisons with audio-conditioned portrait animation baselines.\nMethods\nCelebV-HQ\nRAVDESS\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nSadTalker [77]\n2.953\n1.812\n3.843\n36.648\n171.848\n3.840\n2.277\n4.304\n32.343\n22.516\nHallo [70]\n3.505\n2.262\n4.130\n35.961\n53.992\n4.393\n2.688\n4.062\n19.826\n38.471\nVExpress [61]\n2.946\n1.901\n3.547\n65.098\n117.868\n3.690\n2.331\n5.001\n26.736\n62.388\nEchoMimic [8]\n3.307\n2.128\n3.136\n35.373\n54.715\n4.504\n2.742\n3.292\n21.058\n54.115\nLoopy [26]\n3.780\n2.492\n4.849\n33.204\n49.153\n4.506\n2.658\n4.814\n17.017\n16.134\nHallo-3 [11]\n3.451\n2.257\n3.933\n38.481\n42.125\n4.006\n2.462\n4.448\n28.840\n26.029\nOmniHuman\n3.875\n2.656\n5.199\n31.435\n46.393\n4.564\n2.815\n5.255\n16.970\n15.906\nTable 2. Quantitative comparisons with audio-conditioned body animation baselines.\nMethods\nIQA ↑\nASE↑\nSync-C↑\nFID↓\nFVD↓\nHKV ↑\nHKC↑\nDiffTED [23]\n2.701\n1.703\n0.926\n95.455\n58.871\n-\n0.769\nDiffGest. [84]+MomicMo. [78]\n4.041\n2.897\n0.496\n58.953\n66.785\n23.409\n0.833\nCyberHost [34]\n3.990\n2.884\n6.627\n32.972\n28.003\n24.733\n0.884\nOmniHuman\n4.142\n3.024\n7.443\n31.641\n27.031\n47.561\n0.898\nTable 3. Subjective comparison of different training ratios for audio conditions.\nMethods\nIdentity Consistency\nLip-sync Accuracy\nVisual Quality\nAction Diversity\nOverall\n10% Audio Training Ratio\n28.84\n11.59\n21.59\n11.59\n11.59\n50% Audio Training Ratio\n50.87\n53.62\n44.93\n40.58\n69.57\n100% Audio Training Ratio\n11.59\n30.43\n13.04\n36.23\n17.93\nvarying preferences for motion styles across different sce-\nnarios complicate performance measurement using a single\nmetric. By averaging the metrics across the dataset, Omni-\nHuman achieves the best results across all evaluated metrics,\nreflecting its overall effectiveness. Additionally, OmniHu-\nman excels across almost all metrics in specific datasets.\nNotably, existing methods use a single model for specific\nbody proportions (portrait, half-body) with fixed input sizes\nand ratios. In contrast, OmniHuman supports various in-\nput sizes, ratios and body proportions with a single model,\nachieving satisfactory results. This advantage stems from its\nomni-conditions training, which learns from a large scale of\ndiverse content and varying sizes during mixed data training.\n4.3. Ablation Studies on Omni-Conditions Training\nHere, we primarily analyze and explain principles 1 and 2\nof the omni-condition training in OmniHuman. For the first\nprinciple, we compare training using only data that meets the\nrequirements for audio and pose animation (i.e., 100% audio\ntraining ratio) with training data for weaker conditions (i.e.,\ntext). Our experimental results demonstrate that the ratio\nof these two data parts significantly affects the final perfor-\nmance. From the visualizations in Figure 3, it is evident that\na high proportion of audio condition-specific data training\nreduces dynamic range and can cause failures with complex\ninput images. Including weaker condition data at a 50% ratio\nyields satisfactory results (e.g., accurate lip-syncing and nat-\nural motion). However, excessive weaker condition data can\nhinder training, resulting in poorer correlation with the audio.\nWe also conducted a subjective evaluation to determine the\noptimal mix of these two data types during training. Specifi-\ncally, we conducted a blind evaluation with 20 subjects who\ncompared the samples across various dimensions to select\nthe most satisfactory one, with an option for abstention. In\ntotal, 50 samples depicting diverse scenarios were evaluated.\nThe results in Table 3 were consistent with the conclusions\ndrawn from the visualizations.\nThe second principle can also be simultaneously validated\nwith the principle 1 experiment, but we additionally conduct\nanother experiment using different ratios of pose conditions\nto study the effects of pose condition ratios. Visual com-\nparisons are presented in Figure 4 and 5. When the model\nis trained with a low pose condition ratio and tested with\nonly audio conditions, the model tends to generate intense,\nfrequent co-speech gestures, as is proven by the motion blur\neffects in the top row of Figure 5 and the incorrect fingers\nin the top row of Figure 4. On the other hand, if we train\nthe model with a high pose ratio, the model tends to rely\non the pose condition to determine the human poses in the\ngenerated video. Consequently, given the input audio as the\nonly driving signal, the generated results typically maintain a\nsimilar pose, as shown in the bottom rows of Figure 4 and 5.\n6\n\n/ɑ:/\n/jæn/\n/i:/\n/ɑ:/\n/jæn/\n/oʊ/\n/ə/\n∅\nFigure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%,\nbottom: 100%) and tested in an audio-driven setting with the same input image and audio.\nTherefore, we set the pose ratio to 50% as our final training\nconfiguration.\nApart from analyzing the training ratios of new driving\nmodalities in Stage 2 and Stage 3, the training ratio of the\nappearance condtion is equally important. We investigated\nthe impact of reference image ratios on the generation of\n30-second videos through two experiments: (1) setting the\nreference image ratio to 70%, lower than the text injection\nratio but higher than audio; (2) setting the reference image ra-\ntio to 30%, lower than the injection ratios for both audio and\ntext. The comparative results are shown in Figure 6, reveal-\ning that a lower reference ratio leads to more pronounced\nerror accumulation, characterized by increased noise and\ncolor shifts in the background, degrading performance. In\ncontrast, a higher reference ratio ensures better alignment\nof the generated output with the quality and details of the\noriginal image. This can be explained by the fact that when\nthe reference image training ratio is lower than that of audio,\nthe audio dominates the video generation, making it difficult\nto maintain the ID information from the reference image.\n7\n\nFigure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\nFigure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%,\nbottom: 80%) and tested in an audio-driven setting with the same input image and audio.\n8\n\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nLow Reference Ratio\nHigh Reference Ratio\nFigure 6. Ablation study on reference condition ratios. Comparisons of visualization results for 30s videos at different reference ratios.\nFigure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid\nand 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.\n9\n\n4.4. Extended Visual Results\nIn the Figure 7, Figure 8 and Figure 9, we present more\nvisual results to demonstrate OmniHuman’s powerful capa-\nbilities in human animation, which are difficult to capture\nthrough metrics and comparisons with existing methods.\nOmniHuman is compatible with diverse input images and\nmaintains the motion style of the input, such as preserving\nthe characteristic mouth movements in anime. OmniHuman\nalso excels in object interaction, generating videos of singing\nwhile playing different musical instruments and natural ges-\ntures while holding objects. Due to its compatibility with\npose conditions during training, OmniHuman can perform\npose-driven video generation or a combination of pose and\naudio-driven generation. More video samples can be seen\non our project page (highly recommended).\n5. Conclusion\nWe propose OmniHuman, an end-to-end multimodality-\nconditioned human video generation framework that gen-\nerates human videos based on a single image and motion\nsignals (e.g., audio, video, or both). OmniHuman employs\na mixed data training strategy with multimodality motion\nconditioning, leveraging the scalability of mixed data to\novercome the scarcity of high-quality data faced by previous\nmethods. It significantly outperforms existing approaches,\nproducing highly realistic human videos from weak signals,\nespecially audio. OmniHuman supports images of any aspect\nratio (portraits, half-body, or full-body) delivering lifelike,\nhigh-quality results across various scenarios.\nAcknowledgments\nWe thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang\nfor their discussions and suggestions.\nReferences\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 1(2):3, 2023. 3\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann,\nRoni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen\nLi, Tomer Michaeli, et al. Lumiere: A space-time diffusion\nmodel for video generation. arXiv preprint arXiv:2401.12945,\n2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023.\n[6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang,\nTimo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and\nTero Karras. Generating long videos of dynamic scenes. Ad-\nvances in Neural Information Processing Systems, 35:31769–\n31781, 2022. 2\n[7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul,\nPing Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and\ncontrollable image generation with latent consistency models,\n2024. 2\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and\nChenguang Ma. Echomimic: Lifelike audio-driven portrait an-\nimations through editable landmark conditions. arXiv preprint\narXiv:2407.08136, 2024. 2, 5, 6\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei, Tai-\nwan, November 20-24, 2016, Revised Selected Papers, Part II\n13, pages 251–263. Springer, 2017. 5\n[10] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos\nKolotouros, Thiemo Alldieck, and Cristian Sminchisescu.\nVlogger: Multimodal diffusion for embodied avatar synthesis.\narXiv preprint arXiv:2403.08764, 2024. 3\n[11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng,\nYuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu\nZhu. Hallo3: Highly dynamic and realistic portrait image\nanimation with diffusion transformer networks. arXiv preprint\narXiv:2412.00733, 2024. 3, 5, 6\n[12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan\nHeek, Matthias Minderer, Mathilde Caron, Andreas Steiner,\nJoan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin,\net al.\nPatch n’pack: Navit, a vision transformer for any\naspect ratio and resolution. Advances in Neural Information\nProcessing Systems, 36, 2024. 3\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 3\n[15] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5784–5794, 2021. 3\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yao-\nhui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo\n10\n\nDai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning.\narXiv preprint\narXiv:2307.04725, 2023. 2, 3\n[17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 2\n[18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang\nZhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu,\net al. Gaia: Zero-shot talking avatar generation. arXiv preprint\narXiv:2311.15230, 2023. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems, pages 6840–6851. Curran Associates,\nInc., 2020. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. Advances in Neural Information Processing Systems,\n35:8633–8646, 2022. 2\n[23] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian,\nand Xiaohu Guo. Diffted: One-shot audio-driven ted talk\nvideo generation with diffusion-based co-speech gestures.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1922–1931, 2024. 3,\n5, 6\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[25] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153–8163, 2024. 3\n[26] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun\nZhong, and Yanbo Zheng.\nLoopy: Taming audio-driven\nportrait avatar with long-term motion dependency. arXiv\npreprint arXiv:2409.02634, 2024. 2, 3, 4, 5, 6\n[27] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang,\nYongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobile-\nportrait: Real-time one-shot neural head avatars on mobile\ndevices. arXiv preprint arXiv:2407.05712, 2024. 3\n[28] Kaggle. Ravdess emotional speech audio. https://www.\nkaggle.com/datasets/uwrfkaggler/ravdess-\nemotional-speech-audio. 5\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in neural information processing systems,\n35:26565–26577, 2022. 2\n[30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan\nHuang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar,\nJimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large\nlanguage model for zero-shot video generation. arXiv preprint\narXiv:2312.14125, 2023. 3\n[31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024. 3\n[32] Black Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023. 3\n[33] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi\nYang, and Yanbo Zheng. Cyberhost: Taming audio-driven\navatar diffusion model with region codebook attention. arXiv\npreprint arXiv:2409.01876, 2024. 2, 3, 4, 5, 6\n[35] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng\nXiao, and Lu Jiang. Diffusion adversarial post-training for\none-step video generation. arXiv preprint arXiv:2501.08316,\n2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022. 3\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3\n[38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. ArXiv, abs/2209.03003, 2022. 2\n[39] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma.\nEchomimicv2: Towards striking, simplified, and semi-body\nhuman animation. arXiv preprint arXiv:2411.10061, 2024. 3\n[40] A Nagrani, J Chung, and A Zisserman. Voxceleb: a large-\nscale speaker identification dataset. Interspeech 2017, 2017.\n3\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin neural information processing systems, 35:27730–27744,\n2022. 3\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3\n[43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720,\n2024. 3\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3\n[45] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli. wav2vec: Unsupervised pre-training for speech\nrecognition. arXiv preprint arXiv:1904.05862, 2019. 3\n11\n\n[46] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun,\nand Yebin Liu.\nHuman4dit:\nFree-view human video\ngeneration with 4d diffusion transformer.\narXiv preprint\narXiv:2405.17405, 2024. 3\n[47] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in neural information processing\nsystems, 32, 2019. 3\n[48] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for articu-\nlated animation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13653–\n13662, 2021. 3\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2\n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2\n[52] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Ma-\nciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads:\nDiffusion models beat gans on talking-face generation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5091–5100, 2024. 2, 4\n[53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 4\n[54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive-generating expressive portrait videos\nwith audio2video diffusion model under weak conditions.\narXiv preprint arXiv:2402.17485, 2024. 2, 4\n[55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng\nBo. Emo2: End-effector guided audio-driven avatar video\ngeneration. arXiv preprint arXiv:2501.10687, 2025. 3\n[56] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo:\nEmote portrait alive generating expressive portrait videos\nwith audio2video diffusion model under weak conditions. In\nEuropean Conference on Computer Vision, pages 244–260.\nSpringer, 2025. 3\n[57] Brooks Tim, Peebles Bill, Connorm Holmes, DePue Will,\nYufeim Guo, Jing Li, Schnurr David, Taylor Joe, Luhman\nTroy, Luhman Eric, Ng Clarence, Wang Ricky, and Ramesh\nAditya. Video generation models as world simulators. 2024.\nAccessed: 2024-02-15. 3\n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 3\n[59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 5\n[60] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In International Conference on Learning\nRepresentations, 2022. 2\n[61] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 6\n[62] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng\nLuo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei\nYang. V-express: Conditional dropout for progressive training\nof portrait video generation. arXiv preprint arXiv:2406.02511,\n2024. 2, 5\n[63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3\n[64] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching\nLin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for realistic human\ndance generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n9326–9336, 2024. 3\n[65] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039–10049, 2021. 3\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1160–\n1169, 2020. 2\n[67] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen,\nLiang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli\nZhang, Wenxiu Sun, et al. Q-align: Teaching lmms for vi-\nsual scoring via discrete text-defined levels. arXiv preprint\narXiv:2312.17090, 2023. 5\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623–7633, 2023. 3\n[69] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and\nYing Shan. Vfhq: A high-quality dataset and benchmark for\nvideo face super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 657–666, 2022. 3\n[70] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Li-\nwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao\nYao, and Siyu Zhu. Hallo: Hierarchical audio-driven vi-\nsual synthesis for portrait image animation. arXiv preprint\narXiv:2406.08801, 2024. 2, 5, 6\n12\n\n[71] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang,\nChong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and\nBaining Guo. Vasa-1: Lifelike audio-driven talking faces\ngenerated in real time. arXiv preprint arXiv:2404.10667,\n2024. 2\n[72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3\n[74] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n3\n[75] Lijun Yu, Jos Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birod-\nkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023. 3\n[76] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 3\n[77] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8652–8661, 2023. 3, 5, 6\n[78] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi\nCheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion:\nHigh-quality human motion video generation with confidence-\naware pose guidance. arXiv preprint arXiv:2406.19680, 2024.\n3, 5, 6\n[79] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022. 3\n[80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 3\n[81] Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi\nYang, and Zhou Zhao. Fada: Fast diffusion avatar synthesis\nwith mixed-supervised multi-cfg distillation. arXiv preprint\narXiv:2412.16915, 2024. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,\nLi Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:\nA large-scale video facial attributes dataset. In European\nconference on computer vision, pages 650–667. Springer,\n2022. 3, 5\n[84] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven co-\nspeech gesture generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10544–10553, 2023. 5, 6\n[85] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong,\nYinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.\nChamp: Controllable and consistent human image animation\nwith 3d parametric guidance. In European Conference on\nComputer Vision, pages 145–162. Springer, 2025. 3\n13\n\nFigure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman’s compatibility\nwith various environments, objects, and camera angles, producing satisfactory results.\n14\n\nFigure 9. The videos generated by OmniHuman based on input audio and images. OmniHuman can generate highly realistic human\nmotion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.\n15'),
                Paper(arxiv_id='2502.01237', authors=['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], published_at=datetime.datetime(2025, 2, 4, 3, 10, 49, 348000, tzinfo=datetime.timezone.utc), title='The Differences Between Direct Alignment Algorithms are a Blur', summary='Direct Alignment Algorithms (DAAs) simplify language model alignment by\nreplacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement\nLearning from Human Feedback (RLHF) with direct policy optimization. DAAs can\nbe classified by their ranking losses (pairwise vs. pointwise), by the rewards\nused in those losses (e.g., likelihood ratios of policy and reference policy,\nor odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required\n(two-stage vs. one-stage). We first show that one-stage methods underperform\ntwo-stage methods. To address this, we incorporate an explicit SFT phase and\nintroduce the beta parameter, controlling the strength of preference\noptimization, into single-stage ORPO and ASFT. These modifications improve\ntheir performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT),\nmatching two-stage methods like DPO. Further analysis reveals that the key\nfactor is whether the approach uses pairwise or pointwise objectives, rather\nthan the specific implicit reward or loss function. These results highlight the\nimportance of careful evaluation to avoid premature claims of performance gains\nor overall superiority in alignment algorithms.', upvotes=100, thumbnail=None, content='Large Language Models (LLMs) demonstrate strong text\ngeneration capabilities, yet aligning them with human val-\nues remains challenging due to underspecified objectives,\nlimited training signals, and the complexity of human in-\ntent (Ouyang et al., 2022; Stiennon et al., 2020). Tradi-\ntional alignment pipelines typically involve Supervised Fine-\nTuning (SFT), reward modeling, and reinforcement learning\nto shape model outputs.\nRecently, Direct Alignment Algorithms (DAAs) have\n1T-Tech.\nCorrespondence\nto:\nBoris\nShaposhnikov\n<b.shaposhnikov@tbank.ru>.\nemerged as an alternative, integrating human preferences\ninto policy optimization without explicit reward modeling\nor reinforcement learning (Rafailov et al., 2023; Hong et al.,\n2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024;\nXiao et al., 2024; D’Oosterlinck et al., 2024; Wang et al.,\n2024). These methods differ in theoretical design (pairwise\nvs. pointwise), implementation details (e.g., reference pol-\nicy vs. odds ratio), and whether an SFT phase is required\n(one-stage vs. two-stage). This diversity raises key ques-\ntions about their relationships, comparative advantages, and\nthe role of SFT.\nIn this paper, we show that one-stage methods (e.g., ORPO,\nASFT) can incorporate an explicit SFT phase, improving\nperformance. We introduce a scaling parameter β that uni-\nfies their formulation with other DAAs, revealing shared\noptimization dynamics between methods using either an\nodds ratio or a reference-based reward. Through theoretical\nand empirical analysis, we systematically compare DAAs,\nemphasizing pairwise vs. pointwise preference optimiza-\ntion. We also show that, while SFT is beneficial, using the\nfull dataset is not always necessary, which reduces com-\nputational costs. To structure our analysis, we address the\nfollowing research questions:\nRQ1: Does an explicit SFT stage improve the alignment\nquality of ORPO and ASFT?\nRQ2: Does the tempering factor enhance the alignment\nquality of ASFT and ORPO?\nRQ3: What factors of DAAs affect alignment quality?\nRQ4: How does the final alignment quality depend on the\namount of data used in the SFT stage?\nBy answering these questions, we clarify key trade-offs in\nalignment strategies and provide guidance for optimizing\nLLM training pipelines.\n2. Preliminaries\n2.1. Modeling Sequences\nGiven a sequence y of length |y|, the log-probability can be\nwritten as log p(y) = P|y|\ni=1 log p(yi | y<i), which may also\nbe conditioned on another sequence x. In practice, optimiz-\ning normalized log-probability\n1\n|y| log p(y) = log\n\x00p(y)\n1\n|y| \x01\n1\narXiv:2502.01237v1  [cs.LG]  3 Feb 2025\n\nThe Differences Between Direct Alignment Algorithms are a Blur\noften improves numerical stability and leads to better train-\ning. However, once normalized, the resulting quantity is\nno longer a strict probability measure. Throughout this pa-\nper, whenever we write p(y), we refer to this normalized\nversion p(y)\n1\n|y| . Whenever a method does not apply this\nnormalization, we indicate it explicitly.\nWelleck et al. (2019) introduced a log-unlikelihood term\nthat reduces the probability of certain undesirable tokens:\nlog\n\x001 −p(c | y<i)\n\x01\nfor c ∈C. It can be extended to an\nentire sequence as log\n\x001 −p(y)\n\x01\n.\n2.2. Reinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF)\n(Ouyang et al., 2022; Stiennon et al., 2020) is a prominent\napproach to aligning language models. It generally has three\nstages:\n• Supervised Fine-Tuning (SFT). During the SFT stage,\nthe model πθ is trained to follow instructions by max-\nimizing the probability of correct output y given in-\nput x. For a single training pair (x, y), we define the\nper-sample SFT loss as LSFT(πθ, x, y) = −log πθ(y |\nx). During fine-tuning, we minimize the expectation\nof this per-sample loss over the training dataset D:\nE(x,y) ∼D\nh\nLSFT(πθ, x, y)\ni\n.\n• Reward Modeling (RM). A reward model rψ(x, y) pro-\nduces a satisfaction score. It is trained on preference\npairs using the Bradley-Terry model (Bradley & Terry,\n1952): LRM(rψ) = −E(x,yw,yl)∼D\n\x02\nlog σ\n\x00rψ(x, yw) −\nrψ(x, yl)\n\x01\x03\n, where yw is the preferred response and yl is\nthe less preferred one.\n• Reward\nMaximization.\nThe\nobjective\nis\nto\ngenerate\nresponses\nthat\nmaximize\nthe\nlearned\nreward,\nwith\na\nKL\npenalty\nto\nprevent\nreward\nhacking:\nmaxπθ Ex∼D, y∼πθ(y|x)\n\x02\nrϕ(x, y)\n\x03\n−\nβ DKL\n\x02\nπθ(x, y) ∥πref(x, y)\n\x03\n. Reinforcement learning\n(RL) algorithms are commonly used to optimize this\nobjective (Schulman et al., 2017; Ouyang et al., 2022).\n2.3. Direct Alignment Algorithms\nDirect alignment algorithms replace the reward modeling\nand RL stages (but keep the SFT phase) with a single align-\nment step. Various preference-optimization loss functions\nhave been proposed, employing these core components:\n• rref\nθ (y, x) = log\n\x00 πθ(y|x)\nπref(y|x)\n\x01\nfrom DPO (Rafailov et al.,\n2023), which acts as an implicit reward β rref\nθ . No length\nnormalization is used.\n• rodds\nθ\n(y, x) = log\n\x00πθ(y|x)\n1−πθ(y|x)\n\x01\nproposed in ORPO (Hong\net al., 2024), representing the odds of generating y versus\nnot generating it.\nSeveral Direct Alignment Algorithms use these notations.\nInformation on sequence probability normalization for these\nmethods is presented in Appendix A.1.\n• Direct Preference Optimization (DPO) (Rafailov\net al., 2023):\nLDPO\n=\n−log σ\n\x00β rref\nθ (yw, x) −\nβ rref\nθ (yl, x)\n\x01\n.This method does not normalize probabili-\nties by length.2\n• Identity Preference Optimization (IPO) (Azar et al.,\n2023): LIPO =\n\x00rref\nθ (yw, x) −rref\nθ (yl, x) −\n1\n2β\n\x012.\n• Simple Preference Optimization (SimPO) (Meng\net al., 2024): LSimPO = −log σ\n\x00β log πθ(yw, x) −\nβ log πθ(yl, x) −γ\n\x01\n.\n• Noise\nContrastive\nAlignment\n(NCA)\n(Chen\net al., 2024):\nLNCA\n=\n−log σ\n\x00β rref\nθ (yw, x)\n\x01\n−\n0.5 log σ\n\x00−β rref\nθ (yw, x)\n\x01\n−0.5 log σ\n\x00−β rref\nθ (yl, x)\n\x01\n.\n• Calibrated\nDirect\nPreference\nOptimization\n(Cal-DPO) (Xiao et al., 2024):\nLCal−DPO\n=\n−log σ\n\x00rref\nθ (yw, x) −rref\nθ (yl, x)\n\x01\n+\n\x00rref\nθ (yw, x) −\n1\n2β\n\x012 +\n\x00rref\nθ (yl, x) +\n1\n2β\n\x012.\n• Anchored Preference Optimization Zero (APO-\nZero) (D’Oosterlinck et al., 2024):\nLAPO−Zero\n=\n−σ\n\x00β rref\nθ (yw, x)\n\x01\n+ σ\n\x00β rref\nθ (yl, x)\n\x01\n.\n2.4. Single-Stage Alignment Methods\nSingle-stage alignment (as a subset of DAA methods)\nmerges SFT and direct alignment in one step by adding their\nlosses: LSingle(πθ) = −E(x,yw,yl)∼D\n\x02\nLSFT(πθ, x, yw) +\nλ LAlign(πθ, x, yw, yl)\n\x03\n, where λ is a hyperparameter, and\nno reference policy πref is required.\nIn this paper, we focus on:\n• Odds Ratio Preference Optimization (ORPO)\n(Hong et al., 2024): LORPO = −log πθ(yw|x) −\nλ log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLORPOAlign\n.\n• Aligned Supervised Fine-Tuning (ASFT) (Wang\net al., 2024):\nLASFT\n=\n−log πθ(yw|x) −\nλ\n\x10\nlog σ\n\x00rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−rodds\nθ\n(yl, x)\n\x01\n|\n{z\n}\nLASFTAlign\n\x11\n.\n2Unless otherwise noted, the expectation over (x, yw, yl) ∼D\nis taken.\n2\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n3. Method\nMany DAAs have been proposed, raising questions about\ntheir differences and significance. They can be categorized\nin various ways. For example, one classification separates\nsingle-stage methods, which perform alignment directly\nafter obtaining a base model (ASFT and ORPO), from two-\nstage methods (which perform SFT before alignment), as\nin DPO, IPO, SimPO, etc. Under this scheme, ASFT and\nORPO are single-stage methods.\nAnother classification considers whether rref or rodds is\nused as an implicit reward. ASFT and ORPO also differ\nfrom other losses by using an odds ratio, whereas other\nmethods in Section 2 use normalized policy probabilities.1\nDAAs can also be distinguished by whether their loss func-\ntion is optimized for pairwise or pointwise preferences.\nDPO, for instance, increases the policy’s probability of\nchoosing preferred sequences relative to rejected ones. In\ncontrast, ASFT simply increases or decreases probabilities\nfor chosen or rejected sequences without comparing them\ndirectly.\n3.1. Generalizing ASFT and ORPO\nDespite these classifications, it can still be difficult to pin-\npoint the essential differences among DAAs, especially\nwhen design choices limit generalization. ASFT and ORPO,\nfor example, lack a parameter β, probably because they\nwere conceived as single-stage methods, making the dis-\ntance from a reference policy unnecessary. It might seem\nodd to introduce such a parameter in single-stage methods,\nbut we will show that for both ASFT and ORPO, the single-\nstage design and the absence of β are not strictly required.\n3.1.1. ORPO AND ASFT CAN OPERATE WITHOUT THE\nSFT LOSS TERM AND AS TWO-STAGE METHODS.\nWe begin by inspecting the ASFT objective and demonstrate\nthat it combines both likelihood and unlikelihood terms:\nTheorem 3.1. LASFT is equivalent to the Binary Cross-\nEntropy (BCE) loss, encapsulating both likelihood and un-\nlikelihood components:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nThe proof of Theorem 3.1 is provided in Appendix B. Conse-\nquently,\nLASFTAlign = −\n\x10\nlog πθ(yw|x) + log\n\x001 −πθ(yl|x)\n\x01\x11\n.\nNext, we derive a direct relationship between LORPO and\n1SimPO does not explicitly use a reference policy, but can be\ntreated similarly if a uniform reference policy is assumed.\nLASFT, showing that the latter provides an upper bound on\nthe former:\nTheorem 3.2. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x))\n+ πθ(yl|x)(1 −πθ(yw|x))\n\x01\n,\nwhere the additional term is symmetric in yw and yl.\nThe proof of Theorem 3.2 is provided in Appendix C. As for\nLASFTAlign, the alignment term is then\nLORPOAlign = −log πθ(yw|x) −log(1 −πθ(yl|x))\n+ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nCorollary 3.3. LORPO ≤LASFT and LORPOAlign ≤\nLASFTAlign.\nThis follows from the fact that the additional term in LORPO\nis non-positive when πθ(yw|x) and πθ(yl|x) lie in [0, 1], and\nπθ(yw|x) + πθ(yl|x) ≤1.\nThese findings yield two main observations:\n• LASFT provides an upper bound on LORPO. Minimiz-\ning the former also minimizes the latter.\n• LASFT can be viewed as a minimal form of a DAA\nloss, reflecting the structure of BCE.\nAn essential insight from these formulations is that the SFT\nterm in the ASFT and ORPO losses is already included in\nthe full loss. We hypothesize that this feature may allow us\nto omit the SFT term in the complete loss, first performing\nan SFT phase and then using only the alignment terms for\nmodel alignment. From this perspective, one can experi-\nment with these methods in both single-stage and two-stage\nconfigurations to see which approach is more effective.\n3.1.2. TEMPERING ASFT AND ORPO\nWe now consider the original single-stage methods from Sec-\ntion 2.4 and examine how the alignment terms LORPOAlign\nand LASFTAlign compare. These terms optimize preferences\nand, depending on the coefficient λ, can dominate or have a\nsmaller impact on the final loss.\nLASFTAlign and LORPOAlign strongly resemble the DAA\nlosses discussed in Section 2.3. The single-stage analogue\nof rref\nθ\nis rodds\nθ\n. Inspired by this analogy, we introduce a\ncoefficient β to scale rodds\nθ\n:\nLβ\nASFTAlign\n= −log σ(βrodds\nθ\n(yw, x)) −log σ(−βrodds\nθ\n(yl, x)),\nLβ\nORPOAlign\n= −log σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x)).\n3\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nBoth Lβ\nASFT and Lβ\nORPO generalize their vanilla counter-\nparts (recovering them when β = 1). As in DPO, β can\nbe viewed as a temperature or scaling parameter that regu-\nlates the intensity of the preference for “good” odds. This\nbecomes clearer when looking at the gradients:\n∇θLβ\nASFTAlign = −β\nh\nσ(βrodds\nθ\n(yl, x))∇θrodds\nθ\n(yl, x)\n+\n\x001 −σ(βrodds\nθ\n(yw, x))\n\x01\n∇θrodds\nθ\n(yw, x)\ni\n,\n∇θLβ\nORPOAlign = −β\nh\x00∇θrodds\nθ\n(yw, x) −∇θrodds\nθ\n(yl, x)\n\x01\n×\n\x10\n1 −σ(βrodds\nθ\n(yw, x) −βrodds\nθ\n(yl, x))\n\x11i\n,\nwhere ∇θrodds\nθ\n(y, x) =\n∇θ log πθ(y|x)\n1−πθ(y|x) .\nWhen β →0,\nσ(β · · · ) ≈1\n2, both methods aggressively improve the odds\nratio (increasing for yw and decreasing for yl). As β in-\ncreases, the updates become bounded by the factor σ(β · · · )\n(similar to a reward threshold in DPO). Hence, once the\nmodel improves, further updates are limited, either individ-\nually for Lβ\nASFTAlign or by pairwise ranking in Lβ\nORPOAlign.\nThis alignment with other DAAs allows for a direct com-\nparison of all methods in different setups, clarifying which\naspects are most critical for successful performance.\n3.2. On the Difference Between Direct Alignment\nAlgorithms\nDifferent methods can be grouped by the type of ”reward”\nfunction used in their loss. In general terms, Lβ\nASFTAlign\nand Lβ\nORPOAlign employ an odds ratio, while DPO, IPO,\nSimPO, NCA, Cal-DPO, and APO-Zero use a ratio between\nthe probability of the policy and that of a reference policy.\nThe following theorems make this classification clearer:\nTheorem 3.4. The gradient of Lβ\nASFTAlign becomes\ncollinear with the gradient of LORPOAlign as β →0. For-\nmally,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nindicating that both gradients point in the same direction.\nThe proof of Theorem 3.4 is provided in Appendix D.1.\nA related property applies to Lβ\nORPOAlign:\nTheorem 3.5. The gradient of Lβ\nORPOAlign is collinear with\nthe gradient of LORPOAlign for any β > 0. Formally,\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nβ > 0.\nThe proof of Theorem 3.5 is provided in Appendix E.1.\nFinally:\nTheorem\n3.6.\nFor\neach\nmethod\nX\n∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n,\nas\nβ →0, the gradient of LX is collinear with the gradient of\nLDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nThe proof of Theorem 3.6 is provided in Appendix F.1.\nThese theorems suggest that for sufficiently small β, these\nloss functions are split into two categories with indistin-\nguishable gradient directions. Although the magnitudes may\ndiffer and they may not be collinear for β ̸→0, one could in-\nfer that their performance should be similar when β is small.\nFrom this perspective, two main distinctions arise among\nthese methods: the use of an odds ratio (rodds\nθ\n) and the use\nof the ratio to a reference policy (rref\nθ ). Both choices might\ninfluence the final performance of these methods. Further-\nmore, it remains an open question whether odds-ratio-based\napproaches outperform reference-policy-based ones (e.g.,\nDPO), and how these distinctions compare to the contrast\nbetween pointwise and pairwise preference formulations.\nFrom traditional learning-to-rank (Liu et al., 2009) research,\npairwise methods often produce more direct and less noisy\nranking signals than pointwise techniques, which could lead\nto superior performance in practice (Burges et al., 2005; Li,\n2011; Melnikov et al., 2016). In the following sections, we\npresent experimental results that provide further insight into\nwhich aspects most strongly influence DAA training.\n4. Experimental Setup\nWe systematically compare and evaluate DAA methods us-\ning a standard training and instruction-following evaluation\nframework (Tunstall et al., 2023; Meng et al., 2024; Gorba-\ntovski et al., 2024). Our main experiments use the Llama\n3.1 8B model (AI@Meta, 2024), trained on the UltraChat\n(Ding et al., 2023) and UltraFeedback (UF) (Cui et al., 2023)\ndatasets, and evaluated on the AlpacaEval 2 (Dubois et al.,\n2024; Li et al., 2023) and ArenaHard (Li et al., 2024) bench-\nmarks. For the Reddit TL;DR (Stiennon et al., 2020) task,\nwe employ the Llama 3.2 3B model, comparing it side by\nside with the “golden” validation split (Rafailov et al., 2023;\n2024) using the prompt in Appendix I.\n4.1. Base vs SFT-Initialized Models.\nTo investigate the impact of SFT and the applicability of\none-stage loss LAlign component, we use the UF dataset for\nSFT (avoiding additional knowledge from UltraChat), and\nfor pairwise preference optimization. We carefully tuned the\nhyperparameters to optimize each method’s performance.\nFor the Base-initialized setup, we perform a grid search over\n4\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nORPO\nASFT\n0\n20\n40\n60\n80\n100\nLlama 3.2 3B TL;DR\n(GPT-4 WinRate, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nLlama 3.2 3B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nORPO\nLC\nASFT\nLC\nORPO\nAH\nASFT\nAH\n0\n10\n20\n30\nLlama 3.1 8B UF\n(AlpacaEval 2.0 LC, ArenaHard, %)\n= 1\n1\nFigure 1. Impact of the β Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning β (Section 3.1.2) affects\nboth ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2\nLC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each\nmethod’s best configuration at β = 1 as the baseline. See Section 5.2 for more details.\nlearning rates {6 × 10−6, 8 × 10−6, 1 × 10−5}, inspired\nby values suggested in ORPO and ASFT, and explore λ ∈\n{0.1, 0.2, 0.5, 1.0} for 1 and 2 training epochs keeping a\nsimilar budget to compare with the SFT-initialized setup.\nIn the SFT-initialized setup, we experiment with both\nLORPOAlign and LASFTAlign alone, as well as in combina-\ntion with LSFT, following the original methods. We tune\nthe learning rates {5 × 10−7, 7 × 10−7, 1 × 10−6} for one\nepoch, starting from an SFT model trained for 1 epoch at\n6 × 10−6.\n4.2. β Sensitivity.\nBuilding on the theoretical insights from Section 3.2, where\nDAA losses share indistinguishable gradient directions as\nβ →0, we evaluate each method across various β values to\nexamine quality-KL trade-offs. In classical DPO, β regu-\nlates the KL penalty from the reference policy, but setting\nβ too small can induce training instability. Therefore, we\nconduct a thorough sweep of at least six β values per DAA,\nexploring the performance limit of each method. To broaden\nour analysis, we consider three scenarios:\n1. Llama 3.2 3B TL;DR. A relatively simpler Reddit\nTL;DR summarization task, evaluated via GPT side-\nby-side comparison on 500 samples from the “golden”\nvalidation split (Rafailov et al., 2023; 2024).\n2. Llama 3.2 3B UF. The UltraChat and UF datasets\nserve as more challenging alignment settings due to\ntheir coverage of diverse and complex tasks, includ-\ning common sense reasoning, mathematical problem-\nsolving, code generation, logical reasoning, creative\nwriting, and general knowledge.\n3. Llama 3.1 8B UF. A larger, more capable model on the\nsame UltraChat and UF datasets, allowing us to assess\nhow increased model capacity influences β-sensitivity\nin these diverse tasks.\nFor the UF-based experiments, we measure model qual-\nity primarily using the AlpacaEval 2 Length-Controlled\n(LC) Win-Rate and ArenaHard (AH) WR, and then track\nKL divergence from a reference model to construct Pareto\nfronts. For the TL;DR scenario, we rely on GPT-based\npreference judgments using ‘gpt-4o-2024-08-06‘ model.\nConcretely, in each scenario we train models for differ-\nent values β, combining them with four possible learning\nrates {1 × 10−6, 7 × 10−7, 5 × 10−7, 3 × 10−7}. Further\nimplementation details, including training procedures and\ngeneration hyperparameters, are provided in Appendix A.\n4.3. SFT Quality.\nAlthough in principle single-stage methods do not require\na separate SFT phase, in practice an SFT-trained reference\nmodel often improves the final performance of two-stage\npipelines (see Section 5.1). Prior work, such as (Zhou et al.,\n2024), has shown that a small but high-quality dataset can be\nsufficient for instruction tuning. However, beyond response\nquality, it remains unclear how the amount of SFT data in-\nfluences alignment effectiveness. This raises a fundamental\nquestion: how much supervised data is actually needed to\nproduce a reference model that yields high-quality results\nafter the subsequent alignment step?\nTo investigate this, we prepared seven SFT checkpoints by\ntraining Llama 3.1 8B Base on 1%, 3%, 5%, 10%, 25%,\n50%, and 100% of the UltraChat dataset (2,079, 6,236,\n10,393, 20,786, 51,966, 103,932, and 207,865 records, re-\nspectively) using our SFT-initialized procedure. We then\napplied each alignment method – using optimal hyperparam-\neters from our β-sensitivity experiments (Appendix Table 7)\n– to these seven SFT checkpoints and the original base model.\nFinally, we evaluated all resulting aligned models on Al-\npacaEval 2 LC, analyzing their performance relative to the\nfraction of SFT data used.\n5\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nASFT\nCal-DPO\nNCA\nAPO Zero\nORPO\nSimPO\nIPO\nDPO\nSFT\n436\n5\n59\n457\n2\n41\n459\n5\n36\n463\n3\n34\n451\n3\n46\n458\n1\n41\n457\n2\n41\n456\n5\n39\n178\n24\n298\nWin\nTie\nLose\nWin / Tie / Lose Rate %\n35.6 / 4.8 / 59.6\n91.2 / 1.0 / 7.8\n91.4 / 0.4 / 8.2\n91.6 / 0.2 / 8.2\n90.2 / 0.6 / 9.2\n92.6 / 0.6 / 6.8\n91.8 / 1.0 / 7.2\n91.4 / 0.4 / 8.2\n87.2 / 1.0 / 11.8\nFigure 2. GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best\nhyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT\nachieves 87.2%, maintaining robust summarization performance. See Section 5.2 for more details.\n5. Results\n5.1. RQ1: Does an explicit SFT stage improve the\nalignment quality of ORPO and ASFT?\nAs shown in Table 1, the performance of ORPO and ASFT\nmethods improves significantly when the alignment loss\nLAlign is applied after a preceding SFT stage. In particular,\nORPO achieves results comparable to classical DPO in both\nLC Win Rate and AH WR metrics. In contrast, ASFT shows\nnotable gains in AH WR after the SFT stage, although it\nstill underperforms compared to ORPO or DPO.\nInit\nMethod\nLC% (std)\nWR% (std)\nAH% (CI)\nBase\nSFT\n6.7 (0.43)\n4.5 (0.63)\n3.5 (-0.7, 0.8)\nSFT\nORPO\n24.1 (0.84)\n17.8 (1.17)\n15.3 (-1.6, 1.8)\nSFT\nASFT\n16.4 (0.72)\n11.9 (0.99)\n10.6 (-1.2, 1.3)\nBase\nORPO\n14.8 (0.71)\n10.3 (0.95)\n8.4 (-1.3, 1.3)\nBase\nASFT\n14.5 (0.73)\n10.2 (0.94)\n7.5 (-1.1, 1.2)\nSFT\nORPO†\n13.4 (0.69)\n9.3 (0.91)\n7.7 (-0.9, 1.1)\nSFT\nASFT†\n11.4 (0.63)\n7.5 (0.83)\n7.5 (-1.1, 1.1)\nSFT\nDPO\n23.4 (0.85)\n20.0 (1.18)\n17.5 (-1.8, 1.8)\nTable 1. Base and SFT-initialized alignment methods on the\nLlama 3.1 8B model with the UF dataset. SFT-initialized meth-\nods demonstrate better performance compared to their traditional\nformulations without LSFT. Results marked with † correspond to\ntraining with LSFT, using the best hyperparameters: lr = 1×10−6\nfor ORPO and lr = 7 × 10−7 for ASFT. For other setups, the\nbest hyperparameters are: lr = 5 × 10−7 for standard SFT\nORPO/ASFT, and lr = 1 × 10−5/6 × 10−6 for Base ORPO/ASFT.\nFor single-stage methods, the use of λ = 1 provides the best\nresults within the explored grid of λ ∈{0.1, 0.2, 0.5, 1.0},\nespecially after two epochs of training. However, combining\nLSFT and LAlign in a single-stage setup leads to suboptimal\nresults compared to explicitly separating these phases, even\nwhen starting from an SFT-trained model. Incorporating an\nexplicit SFT stage improves overall performance for ORPO\nand ASFT methods. Therefore, all further experiments focus\non applying the LAlign components of ORPO and ASFT on\ntop of an SFT-trained model.\n5.2. RQ2: Does the tempering factor enhance the\nalignment quality of ASFT and ORPO?\nFigure 1 illustrates that introducing the β parameter (as\ndescribed in Section 3.1.2) improves the performance of\nboth ASFT and ORPO LAlign in our tested scenarios. For a\nfair comparison, we used the best-performing learning rate\nfor each baseline — LASFTAlign and LORPOAlign — while\nfixing β = 1. In the Llama 3.2 3B TL;DR experiment,\nthese adjustments led to an improvement of +7.0 for ORPO\nand +43.4 for ASFT in GPT-4 WR. In the Llama 3.1 8B\nUF setup, tuning β provided additional gains of +3.46 for\nORPO and +8.27 for ASFT on the AlpacaEval 2 LC WR.\n5.3. RQ3: What factors of DAAs affect alignment\nquality?\nBased on Section 3, we perform a comprehensive evaluation\nof alignment losses, including DPO, IPO, SimPO, NCA,\nCal-DPO, and APO-Zero, as well as enhanced Lβ\nASFTAlign\nand Lβ\nORPOAlign with the introduced parameter β. Unlike\nclassical methods where β typically regulates KL diver-\ngence against a reference policy πref, β in Lβ\nASFTAlign and\nLβ\nORPOAlign directly modulates the strength of preference\noptimization. To explore the upper limits of each method’s\nperformance, we performed an extensive hyperparameter\nsearch, analyzing both alignment quality and KL divergence.\nFull implementation details, including training setups and\nevaluation criteria, are provided in Appendix A.\nLlama 3.2 3B TL;DR: Figure 2 presents a comparison of\nall methods on the Reddit TL;DR validation subset, using\ntheir best hyperparameters. Most methods achieve a GPT-4\nWin Rate exceeding 90%, indicating robust summarization\nperformance on this relatively straightforward task. ASFT\nis slightly lower at 87.2% Win Rate, but still demonstrates\nstrong overall results.\nLlama 3.2 3B UF and Llama 3.1 8B UF: Table 2 summa-\n6\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nAlpacaEval 2\nArenaHard\nAlpacaEval 2\nArenaHard\nLC% (std)\nWR% (std)\nWR% (CI)\nLC% (std)\nWR% (std)\nWR% (CI)\nSFT\n5.02 (0.34)\n3.21 (0.55)\n1.4 (-0.4, 0.4)\n10.27 (0.54)\n5.44 (0.70)\n2.6 (-0.5, 0.6)\nDPO\n11.43 (0.58)\n11.79 (0.99)\n6.8 (-1.0, 0.9)\n26.82 (0.77)\n23.69 (1.25)\n19.0 (-1.9, 1.8)\nIPO\n11.24 (0.60)\n11.67 (1.01)\n6.8 (-1.0, 1.1)\n28.18 (0.83)\n24.43 (1.26)\n19.1 (-1.6, 1.5)\nSimPO\n10.56 (0.44)\n11.94 (0.95)\n6.4 (-1.0, 1.1)\n27.65 (0.77)\n25.62 (1.29)\n21.5 (-1.9, 1.9)\nORPO\n10.67 (0.50)\n12.23 (0.97)\n6.6 (-1.0, 1.1)\n28.25 (0.71)\n28.59 (1.33)\n20.9 (-2.0, 2.0)\nAPO Zero\n10.36 (0.53)\n11.22 (0.98)\n6.0 (-1.0, 0.9)\n23.15 (0.76)\n19.03 (1.18)\n17.3 (-1.8, 1.8)\nNCA\n10.33 (0.53)\n11.02 (0.97)\n5.1 (-0.7, 0.8)\n23.21 (0.80)\n18.67 (1.17)\n15.1 (-1.5, 1.6)\nCal-DPO\n10.62 (0.57)\n10.15 (0.94)\n4.8 (-0.9, 0.9)\n23.19 (0.82)\n18.85 (1.18)\n15.2 (-1.5, 1.6)\nASFT\n10.63 (0.55)\n9.21 (0.88)\n5.1 (-0.9, 0.9)\n20.82 (0.79)\n16.34 (1.13)\n13.5 (-1.6, 1.5)\nTable 2. AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF. The SFT model was trained on the UltraChat\ndataset. The best hyperparameters for each method were selected according to Section 4.2. Bold values indicate the best performance for\neach benchmark, while underlined values represent the second-best performance. See Section 5.3 for more details.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nKL Divergence with SFT Model\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\nFigure 3. Pareto front for alignment quality and KL divergence.\nResults for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are\ngrouped into pairwise and pointwise categories, with pairwise\nachieving higher LC values while remaining within overlapping\nconfidence intervals. See Section 5.3 for more details.\nrizes the results for both Llama 3.2 3B UF and Llama 3.1\n8B UF setups. For the smaller 3B model, the methods per-\nform similarly on LC WR, with slight differences emerging\non AH. Although these differences align with the pairwise\nvs. pointwise distinction (e.g., DPO, IPO, ORPO, SimPO\nvs. APO-Zero, NCA, Cal-DPO, ASFT), no single approach\nconsistently dominates across metrics. The overlap in con-\nfidence intervals further indicates that the results for these\nmethods are statistically similar in this setup, with no clear\nseparation.\nIn contrast, the 8B model reveals a clearer performance\ndifferentiation. Pairwise methods consistently outperformed\npointwise ones on AlpacaEval 2 and ArenaHard metrics,\nwith ORPO achieving the highest overall alignment quality.\nAs illustrated in Figure 3, pairwise approaches dominated\nthe KL Pareto front for the larger model, demonstrating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy (%)\nPointwise Pairwise\n0.42\n0.44\n0.71\n0.89\n=0.290\n=0.456\nMLP Dim = 1\nMLP Dim = 3\nFigure 4. Pairwise vs. Pointwise Ranking Methods on Toy Ex-\nample. Model capacity impacts ranking accuracy, with pairwise\nmethods outperforming pointwise ones as capacity increases. This\nbehavior is consistent with results observed in Llama experiments\non the UF dataset. See Section 5.3 for more details.\ntheir ability to more effectively balance alignment quality\nand divergence. Pareto fronts for the remaining setups are\nincluded in Appendix G for completeness.\nThese observations suggest that model capacity plays a sig-\nnificant role in amplifying the advantages of pairwise rank-\ning, where LLMs act as rankers (similar to Liu et al. (2024)).\nFor smaller models, such as the 3B setup, limited capacity\nmay hinder the ability to fully exploit pairwise gradient sig-\nnals. This hypothesis is supported by additional evidence\nfrom the toy example experiment (Figure 4), where pairwise\nmethods demonstrated performance similar to pointwise\nmethods with weaker MLPs but achieved better ranking\naccuracy as the model capacity increased. Full details of the\ntoy example setup are provided in Appendix H.\n5.4. RQ4: How does the final alignment quality depend\non the amount of data used in the SFT stage?\nIn Section 5.1, we show that DAAs designed to bypass\nthe SFT phase still underperform compared to models\nthat undergo SFT and are then aligned using a similar\n7\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\n25\nAlpaca Eval 2 LC WR (%)\nDPO\nIPO\nSimPO\nORPO\nSFT\nLine Type\nFraction\nFull\n(a) Pairwise\n0\n10\n20\n30\n40\n50\nPercentage of the dataset on which the SFT policy was trained\n0\n5\n10\n15\n20\nAlpaca Eval 2 LC WR (%)\nAPO Zero\nNCA\nCal-DPO\nASFT\nSFT\nLine Type\nFraction\nFull\n(b) Pointwise\nFigure 5. Impact of SFT Dataset Size on Alignment Quality. Performance of the pairwise (a) and pointwise (b) alignment methods on\nAlpacaEval 2 (LC WR metric) when the SFT policy is trained on different fractions of the UltraChat dataset. Even a small fraction of SFT\ndata (e.g., 5–10%) yields substantial gains over starting from the raw base model. See Section 5.4 for more details.\npreference-optimization loss function without the SFT term.\nAs discussed in Section 4.3, this raises the question of how\nmuch supervised data is needed to compensate for the ad-\nditional computation and achieve comparable alignment\nperformance.\nTo investigate this, we trained seven SFT models on progres-\nsively larger UltraChat subsets (1% to 100%) and applied\neach alignment algorithm to these models and the non-fine-\ntuned base model, yielding eight initializations per method.\nFigures 5(a) and 5(b) summarize the results for pairwise\nand pointwise alignment methods, respectively. As the plots\nshow, no method starting from the raw base model can\nmatch the final quality of a method trained with the entire\nSFT dataset. However, even a modest size expansion of the\nSFT dataset yields substantial improvements in alignment\nquality: for example, moving from 3% to 5% of the data\nmore than doubles the AlpacaEval 2 LC score for the final\nmodel. Crucially, using only 10% of UltraChat for SFT\nyields nearly the same quality as using the entire dataset.\nAdding an SFT phase requires more overall training, but\nit pays off significantly in the final result. Moreover, one\ndoes not need the entire supervised corpus to realize most\nof these gains; even 5–10% of the data is often enough for\nDAAs to reach most of their potential.\n6. Conclusion\nThis paper presents a comprehensive theoretical and empiri-\ncal analysis of DAAs. Theoretically, we demonstrated that\nwithin each category - odds-based (rodds) and reference-\npolicy-based (rref) – gradient directions of popular methods\nalign as β →0, revealing shared optimization dynamics\nwithin these groups. We also showed that single-stage losses\n(e.g., ASFT, ORPO) can be extended to two-stage pipelines\nwith an explicit SFT step and optional β-scaling, enabling\ngreater flexibility. Experimentally, we addressed four core\nresearch questions (RQ1–4), exploring single- vs. two-stage\ntraining, implicit rewards, objective types, and the impact\nof the SFT phase. Our key findings are:\n• Include an SFT phase.\nAn SFT stage consistently\nimproves alignment performance (RQ1), with ORPO\nachieving +9.3 LC / +6.9 AH and ASFT +1.9 LC / +3.1\nAH in the setup from Section 4.1. Even 5–10% of the\nsupervised dataset often suffices to achieve near-optimal\nresults (RQ4).\n• Pairwise methods outperform pointwise objectives.\nAlignment quality depends more on the choice between\npairwise and pointwise objectives than on the formula-\ntion of implicit reward (e.g., rodds or rref). Pairwise\nmethods generally perform better (e.g., ORPO outper-\nforming ASFT by +7.43 LC / +7.4 AH in the Llama\n3.1 8B UF setup), particularly in larger models (RQ3).\nAmong these, ORPO and SimPO also stand out as prac-\ntical options for memory-constrained scenarios, as they\ndo not rely on a reference policy.\n• Choose hyperparameters carefully. Alignment per-\nformance is highly sensitive to learning rates and the\ncoefficient β. We provide optimal configurations for dif-\nferent methods based on comprehensive grid searches\nin our experimental setups, highlighting the added gains\nfrom tuning β in odds-based methods, where it controls\nthe strength of preference optimization (RQ2).\nLimitations and Future Work. Although our study sys-\ntematically compares DAAs, it has several limitations. We\ntested a limited set of datasets (UltraChat, UltraFeedback,\nReddit TL;DR) and benchmarks (AlpacaEval 2, ArenaHard),\nwhich may affect generalizability to other domains. The re-\nliance on GPT-based evaluators can introduce biases. More-\nover, we evaluated on 3B–8B models, so the observed ad-\nvantages of pairwise over pointwise objectives could shift\nat larger scales.\n8\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nReferences\nAI@Meta.\nLlama 3 model card.\n2024.\nURL\nhttps://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nAzar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,\nD., Valko, M., and Munos, R. A general theoretical\nparadigm to understand learning from human preferences,\n2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,\nT. J., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,\nEl-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez,\nD., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,\nN., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-\nCandlish, S., Olah, C., Mann, B., and Kaplan, J. Train-\ning a helpful and harmless assistant with reinforcement\nlearning from human feedback. ArXiv, abs/2204.05862,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:248118878.\nBradley, R. A. and Terry, M. E. Rank Analysis of Inclom-\nplete Block Design: The Method of Paired Comparisons.\nBiometrika, 39(3-4):324–345, 12 1952.\nISSN 0006-\n3444. doi: 10.1093/biomet/39.3-4.324. URL https:\n//doi.org/10.1093/biomet/39.3-4.324.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,\nHamilton, N., and Hullender, G. Learning to rank using\ngradient descent. In Proceedings of the 22nd international\nconference on Machine learning, pp. 89–96, 2005.\nChen, H., He, G., Yuan, L., Cui, G., Su, H., and Zhu, J.\nNoise contrastive alignment of language models with\nexplicit rewards, 2024. URL https://arxiv.org/\nabs/2402.05369.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback, 2023.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDing, N., Chen, Y., Xu, B., Qin, Y., Hu, S., Liu, Z., Sun, M.,\nand Zhou, B. Enhancing chat language models by scaling\nhigh-quality instructional conversations. In Bouamor,\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\n2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 3029–3051, Singapore, December\n2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.183. URL https://\naclanthology.org/2023.emnlp-main.183.\nD’Oosterlinck, K., Xu, W., Develder, C., Demeester, T.,\nSingh, A., Potts, C., Kiela, D., and Mehri, S. Anchored\npreference optimization and contrastive revisions: Ad-\ndressing underspecification in alignment, 2024. URL\nhttps://arxiv.org/abs/2408.06266.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nGorbatovski, A., Shaposhnikov, B., Malakhov, A., Sur-\nnachev, N., Aksenov, Y., Maksimov, I., Balagansky, N.,\nand Gavrilov, D. Learn your reference model for real\ngood alignment. arXiv preprint arXiv:2404.09656, 2024.\nHong, J., Lee, N., and Thorne, J. Orpo: Monolithic prefer-\nence optimization without reference model, 2024. URL\nhttps://arxiv.org/abs/2403.07691.\nKingma,\nD. P. and Ba,\nJ.\nAdam:\nA method\nfor stochastic optimization.\nCoRR, abs/1412.6980,\n2014.\nURL https://api.semanticscholar.\norg/CorpusID:6628106.\nLi, H. A short introduction to learning to rank. IEICE\nTRANSACTIONS on Information and Systems, 94(10):\n1854–1862, 2011.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline, 2024.\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-\nval: An automatic evaluator of instruction-following\nmodels.\nhttps://github.com/tatsu-lab/\nalpaca_eval, 5 2023.\nLiu, T., Qin, Z., Wu, J., Shen, J., Khalman, M., Joshi,\nR., Zhao, Y., Saleh, M., Baumgartner, S., Liu, J., et al.\nLipo: Listwise preference optimization through learning-\nto-rank. arXiv preprint arXiv:2402.01878, 2024.\nLiu, T.-Y. et al. Learning to rank for information retrieval.\nFoundations and Trends® in Information Retrieval, 3(3):\n225–331, 2009.\nMelnikov, V., H¨ullermeier, E., Kaimann, D., Frick, B., and\nGupta, P. Pairwise versus pointwise ranking: A case\nstudy. Schedae Informaticae, pp. 73–83, 2016.\nMeng, Y., Xia, M., and Chen, D. Simpo: Simple preference\noptimization with a reference-free reward. arXiv preprint\narXiv:2405.14734, 2024.\n9\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P. F., Leike, J.,\nand Lowe, R. Training language models to follow instruc-\ntions with human feedback. In Koyejo, S., Mohamed, S.,\nAgarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\nAdvances in Neural Information Processing Systems, vol-\nume 35, pp. 27730–27744. Curran Associates, Inc., 2022.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023. URL https://arxiv.org/abs/\n2305.18290.\nRafailov, R., Chittepu, Y., Park, R., Sikchi, H., Hejna, J.,\nKnox, B., Finn, C., and Niekum, S. Scaling laws for\nreward model overoptimization in direct alignment algo-\nrithms. arXiv preprint arXiv:2406.02900, 2024.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\nspeed: System optimizations enable training deep learn-\ning models with over 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pp. 3505–3506,\n2020.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O.\nProximal policy optimization al-\ngorithms.\nCoRR, abs/1707.06347, 2017.\nURL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1707.html#SchulmanWDRK17.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\nR., Voss, C., Radford, A., Amodei, D., and Christiano,\nP. Learning to summarize from human feedback. In\nNeurIPS, 2020.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Ra-\nsul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier,\nC., Habib, N., et al. Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944, 2023.\nWang, R., Sun, J., Hua, S., and Fang, Q. Asft: Aligned\nsupervised fine-tuning through absolute likelihood, 2024.\nURL https://arxiv.org/abs/2409.10571.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nand Weston, J. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319, 2019.\nXiao, T., Yuan, Y., Zhu, H., Li, M., and Honavar, V. G. Cal-\ndpo: Calibrated direct preference optimization for lan-\nguage model alignment, 2024. URL https://arxiv.\norg/abs/2412.14516.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\n10\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA. Implementation Details\nA.1. Probability Normalization\nAs discussed in Section 2.1, not all DDAs incorporate length-based probability normalization by default. In this paper,\nhowever, we consistently apply such normalization wherever probabilities are involved. This choice avoids introducing extra\nnotation and reduces the cognitive load on the reader. Table 3 summarizes the methods that originally include length-based\nnormalization.\nMethod\nUse normalization\nDPO (Rafailov et al., 2023)\n✗\nIPO (Azar et al., 2023)\n✗\nSimPO (Meng et al., 2024)\n✓\nNCA (Chen et al., 2024)\n✗\nCal-DPO (Xiao et al., 2024)\n✗\nAPO-Zero (D’Oosterlinck et al., 2024)\n✗\nORPO (Hong et al., 2024)\n✓\nASFT (Wang et al., 2024)\n✓\nTable 3. Methods that include (✓) or omit (✗) length-based probability normalization in their original formulation.\nA.2. Training Details\nOur experiments were conducted using the Llama 3.2 3B and Llama 3.1 8B Base models (AI@Meta, 2024). The training\nsetup, datasets, and hyperparameters were designed to ensure reproducibility and consistency. Unless otherwise noted, the\nhyperparameters in Table 4 were used across all experiments.\nHyperparameter\nValue\nMax Tokens Length\n1024 (TL;DR setup), 4096 (UF setup)\nEpochs\n1 (or 2 when specified)\nLearning Rate (SFT)\n6.0 × 10−6\nLearning Rate (Base Init.)\n{6.0 × 10−6, 8.0 × 10−6, 1.0 × 10−5}\nLearning Rate (Alignment)\n{3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6}\nOptimizer\nAdam (Kingma & Ba, 2014)\nAdam β1\n0.9\nAdam β2\n0.95\nBatch Size\n128\nLearning Schedule\nLinear Decay\nWarm-up Ratio\n0.03\nMax Gradient Norm\n2\nMemory Optimization\nDeepSpeed (Rasley et al., 2020)\nAttention Mechanism\nFlash Attention 2 (Dao, 2023)\nTable 4. Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models.\nTraining was performed on 8 NVIDIA A100 GPUs with 80GB memory each. Depending on the number of epochs, training\nfor each configuration took between 3 to 6 hours.\nA.2.1. DATASETS.\nWe used two primary datasets:\n• Reddit TL;DR (Bai et al., 2022): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B\nmodel.\n11\n\nThe Differences Between Direct Alignment Algorithms are a Blur\n• UltraChat (Ding et al., 2023): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B and\nLlama 3.1 8B models.\n• UltraFeedback (Cui et al., 2023): used for both SFT (in the Base vs. SFT-initialized comparison, where we selected\nchosen subset from preference pairs) and for pairwise preference optimization in all DAA methods.\nThe dataset sizes are summarized in Table 5. For Base vs. SFT-initialized setups, only UltraFeedback was used. For\nβ-sensitivity experiments, the models were first trained on UltraChat for SFT and subsequently fine-tuned on UltraFeedback.\nThe Reddit TL;DR dataset was processed to remove duplicates, retaining only uniquely preferred summaries for SFT.\nDataset\nTraining Examples\nValidation Examples\nUltraChat\n207,865\n23,110\nUltraFeedback\n61,135\n2,000\nReddit TL;DR (SFT)\n41,947\n11,941\nReddit TL;DR (Preference)\n73,396\n21,198\nTable 5. Summary of dataset sizes used for training and validation.\nA.2.2. β-SENSITIVITY EXPERIMENTS.\nWe conducted a comprehensive analysis to evaluate the sensitivity of DAA methods to β, examining its impact on the\ntrade-off between model quality and KL divergence. Each method was trained using six or more distinct β values to identify\na configuration that achieves stable and effective performance. The specific β values tested for each method are as follows:\nMethod\nβ Values Tested\nDPO\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1}\nIPO\n{0.0007, 0.001, 0.005, 0.01, 0.05, 0.1}\nSimPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0}\nORPO\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nASFT\n{0.05, 0.1, 0.2, 0.5, 1.0, 2.0}\nAPO-Zero\n{0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2}\nCal-DPO\n{0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003}\nNCA\n{0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05}\nTable 6. Range of β values tested for each DAA method on all scenarios.\nFor each β, we tested four learning rates (3.0 × 10−7, 5.0 × 10−7, 7.0 × 10−7, 1.0 × 10−6), training on the UltraFeedback\ndataset. All runs began from an SFT-initialized model trained on UltraChat (lr = 6.0 × 10−6, 1 epoch). The best-performing\nlearning rate for each β was selected to construct Pareto fronts, balancing quality (measured via AlpacaEval 2 LC Win-Rate)\nand KL divergence.\nFor SimPO in the Llama 3.1 8B UF setup, the ratio γ\nβ = 0.5 was kept fixed as recommended by Meng et al. (2024).\nAdditionally, a single learning rate (lr = 6.0 × 10−7) was tested across all β values for this method, as the same datasets\nand model scale were used. For Llama 3.2 TL;DR and UF setups, we tested four learning rates similar to other DAAs.\nBeyond the standard β values described in Table 6, additional values were explored for specific configurations to reach\nthe extreme points of the Pareto front. For example: - {0.00001, 0.00003} for Cal-DPO in Llama 3.2 3B TL;DR and UF\nsetups, - {0.00001, 0.00003, 0.00005} for NCA in Llama 3.2 3B TL;DR, - {0.0003, 0.0005} for APO-Zero in Llama 3.2\n3B TL;DR, - {0.0003, 0.0005, 0.001, 0.003, 0.005} for ASFT in Llama 3.2 3B TL;DR.\nThe hyperparameters resulting in the best performance are presented in Table 7.\nA.3. Generation Details\nWe evaluated model performance on AlpacaEval 2 and ArenaHard for UltraFeedback setups, while for the Reddit TL;DR\nsetup, we used side-by-side comparisons with GPT-4o on a curated golden validation subset of 500 samples. Additionally,\n12\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nMethod\nLlama 3.2 3B TL;DR\nLlama 3.2 3B UF\nLlama 3.1 8B UF\nLearning Rate\nβ\nLearning Rate\nβ\nLearning Rate\nβ\nDPO\n7.0 × 10−7\n0.05\n1.0 × 10−6\n0.01\n1.0 × 10−6\n0.003\nIPO\n1.0 × 10−6\n0.005\n7.0 × 10−7\n0.001\n1.0 × 10−6\n0.001\nSimPO\n3.0 × 10−7\n0.5\n7.0 × 10−7\n1.0\n6.0 × 10−7\n1.0\nORPO\n3.0 × 10−7\n0.5\n5.0 × 10−7\n0.2\n5.0 × 10−7\n0.5\nASFT\n3.0 × 10−7\n0.001\n1.0 × 10−6\n0.2\n7.0 × 10−7\n0.1\nAPO Zero\n3.0 × 10−7\n0.001\n3.0 × 10−7\n0.005\n3.0 × 10−7\n0.003\nNCA\n3.0 × 10−7\n0.0001\n3.0 × 10−7\n0.0005\n3.0 × 10−7\n0.0003\nCal-DPO\n3.0 × 10−7\n0.00003\n5.0 × 10−7\n0.0003\n3.0 × 10−7\n0.0003\nTable 7. Best hyperparameters for each DAA method across setups.\nKL divergence was measured on the validation subset for all setups using the generation hyperparameters listed in Table 8.\nFor ArenaHard, the temperature was set to 0 to adhere to the original benchmark configuration.\nHyperparameter\nValue\nTemperature\n0.9\nTop-k\n40\nTop-p\n1.0\nMax New Tokens\n256 (TL;DR setup), 4096 (UF setup)\nTable 8. Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models.\nB. Equivalence of ASFT Loss and Binary Cross-Entropy Loss\nLemma B.1.\nlog σ(rodds\nθ\n(y, x)) = log πθ(y|x)\nProof.\nlog σ(rodds\nθ\n(y, x)) = log σ(log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(1−πθ(y|x))−log(πθ(y|x)) = log\n1\n1 + 1−πθ(y|x)\nπθ(y|x)\n= −log\n\x10\n1 + 1 −πθ(y|x)\nπθ(y|x)\n\x11\n= −log πθ(y|x) + 1 −πθ(y|x)\nπθ(y|x)\n= log πθ(y|x).\nLemma B.2.\nlog σ(−rodds\nθ\n(y, x)) = log\n\x001 −πθ(y|x)\n\x01\nProof.\nlog σ(−rodds\nθ\n(y, x)) = log σ(−log\nπθ(y|x)\n1 −πθ(y|x)) = log\n1\n1 + elog(πθ(y|x))−log(1−πθ(y|x)) = log\n1\n1 +\nπθ(y|x)\n1−πθ(y|x)\n=\n−log\n\x10\n1 +\nπθ(y|x)\n1 −πθ(y|x)\n\x11\n= −log 1 −πθ(y|x) + πθ(y|x)\n1 −πθ(y|x)\n= log(1 −πθ(y|x)).\n13\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nTheorem B.3. LASFT is equivalent to the binary cross-entropy loss, encompassing both likelihood and unlikelihood\ncomponents:\nLASFT = −(1 + λ) log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n.\nProof. To show that LASFT is equivalent to the BCE loss, we start with the definition:\nLASFT = −log πθ(yw|x) −λ log σ(rodds\nθ\n(yw, x)) −λ log σ(−rodds\nθ\n(yl, x)),\nwhere rodds\nθ\n(y, x) =\nπθ(y|x)\n1−πθ(y,x). Applying Lemma B.1 and Lemma B.2 to the expression, we obtain:\nLASFT = −log πθ(yw|x) −λ log πθ(yw|x) −λ log\n\x001 −πθ(yl|x)\n\x01\n= −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)).\nC. Relationship Between ORPO and ASFT Loss Functions\nTheorem C.1. LORPO can be expressed as:\nLORPO = LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n.\nProof. We start by defining the ORPO loss:\nLORPO = −log πθ(yw|x) −λ log σ\n\x12\nlog\nπ(yw|x)\n1 −π(yw|x) −log\nπ(yl|x)\n1 −π(yl|x)\n\x13\n.\nExpanding the second term using the identity log σ(x) = x −log(ex + 1), we get:\n−log σ\n\x12\nlog\nπθ(yw|x)\n1 −πθ(yw|x) −log\nπθ(yl|x)\n1 −πθ(yl|x)\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x)(1 −πθ(yl|x))\nπθ(yl|x)(1 −πθ(yw|x)) + 1\n\x13\n= log 1 −πθ(yw|x)\nπθ(yw|x)\n+ log\nπθ(yl|x)\n1 −πθ(yl|x) + log\n\x12πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\nπθ(yl|x)(1 −πθ(yw|x))\n\x13\n= −log πθ(yw|x) −log(1 −πθ(yl|x)) + log\n\x00πθ(yw|x) −2πθ(yw|x)πθ(yl|x) + πθ(yl|x)\n\x01\n|\n{z\n}\nORPOAlign\n.\nCombining all terms, we obtain:\nLORPO = −(1 + λ) log πθ(yw|x) −λ log(1 −πθ(yl|x)) + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\n= LASFT + λ log\n\x00πθ(yw|x)(1 −πθ(yl|x)) + πθ(yl|x)(1 −πθ(yw|x))\n\x01\nD. Proof of Theorem 3.4\nTheorem D.1 (Collinearity of β-ASFT and ORPO Gradients). Let\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n,\nwhere\nrodds\nθ\n(y, x) = log\n\x10\nπθ(y|x)\n1−πθ(y|x)\n\x11\n.\n14\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nDefine the ORPO alignment loss as\nLORPOAlign = −log σ\n\x00rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x01\n.\nThen,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n\r\r∇θ Lβ\nASFTAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r,\ni.e., their gradients become collinear in the same direction as β →0.\nProof. Step 1. Gradient of β-ASFT.\nDenote pw = πθ(yw | x), pl = πθ(yl | x). Then\nrodds\nθ\n(yw, x) = log\n\x10\npw\n1−pw\n\x11\n,\nrodds\nθ\n(yl, x) = log\n\x10\npl\n1−pl\n\x11\n.\nBy definition,\nLβ\nASFTAlign = −log σ\n\x00β rodds\nθ\n(yw, x)\n\x01\n−log σ\n\x00−β rodds\nθ\n(yl, x)\n\x01\n.\nFor small β, a first-order Taylor expansion of σ(β z) around 0 yields σ(β z) = 1\n2 + β z\n4 +O(β2). Thus, σ(β rodds\nθ\n(yw, x)) ≈\n1\n2 and σ(−β rodds\nθ\n(yl, x)) ≈1\n2. Taking gradients and applying the chain rule gives each term approximately proportional to\n± β ∇θ[rodds\nθ\n(·)]. Concretely,\n∇θ\n\x02\n−log σ(β rodds\nθ\n(yw, x))\n\x03\n≈−β\n2 ∇θ\n\x02\nrodds\nθ\n(yw, x)\n\x03\n,\n∇θ\n\x02\n−log σ(−β rodds\nθ\n(yl, x))\n\x03\n≈+ β\n2 ∇θ\n\x02\nrodds\nθ\n(yl, x)\n\x03\n.\nHence, summing up,\n∇θ Lβ\nASFTAlign ≈β\n2\nh\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\ni\n.\nObserve that β > 0 implies the overall scalar factor β\n2 is strictly positive in front of the difference of gradients.\nStep 2. Gradient of ORPO alignment loss.\nDefine ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n.\nIts gradient (using the chain rule) is proportional to\n∇θ LORPOAlign ∝−∇θ\n\x02\nrodds\nθ\n(yw, x) −rodds\nθ\n(yl, x)\n\x03\n= ∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x).\nUp to a strictly positive logistic factor (since σ(·) ∈(0, 1)), the coefficient in front of ∇θ[rodds\nθ\n(·)] remains negative, but we\ntrack the absolute scalar to see it is positive. Indeed, one can write\n−∇θ\n\x00∆rodds\nθ\n(x)\n\x01\n= κORPO ∇θrodds\nθ\n(yl, x) −κORPO ∇θrodds\nθ\n(yw, x),\nκORPO > 0.\nStep 3. Conclusion (positive collinearity).\nComparing the two gradients:\n∇θ Lβ\nASFTAlign ≈\nβ\n2\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n,\n∇θ LORPOAlign ∝\n\x02\n∇θrodds\nθ\n(yl, x) −∇θrodds\nθ\n(yw, x)\n\x03\n.\nThe ratio is thus strictly positive for small β. Consequently,\nlim\nβ→0\n∇θ Lβ\nASFTAlign\n∥∇θ Lβ\nASFTAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nestablishing collinearity in the same direction.\n15\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nE. Proof of Theorem 3.5\nTheorem E.1 (Collinearity of β-ORPO and ORPO Gradients). Let\n∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x),\nand consider\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nIts gradient is collinear with the gradient of the standard ORPO alignment loss\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\nfor any fixed β > 0. Formally,\n∇θ Lβ\nORPOAlign\n\r\r∇θ Lβ\nORPOAlign\n\r\r =\n∇θ LORPOAlign\n\r\r∇θ LORPOAlign\n\r\r.\nProof. Step 1. Gradient of β-ORPO.\nLet ∆rodds\nθ\n(x) = rodds\nθ\n(yw, x) −rodds\nθ\n(yl, x). Then\nLβ\nORPOAlign = −log σ\n\x00β ∆rodds\nθ\n(x)\n\x01\n.\nBy the chain rule,\n∇θ Lβ\nORPOAlign = −\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\nβ ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince σ′(z) = σ(z) [1 −σ(z)], we have\n−\n1\nσ(β ∆rodds\nθ\n(x)) σ′\x00β ∆rodds\nθ\n(x)\n\x01\n= −β\n\x02\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01\x03\n.\nThus,\n∇θ Lβ\nORPOAlign = −β\nh\n1 −σ\n\x00β ∆rodds\nθ\n(x)\n\x01i\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nSince β > 0 and 1 −σ(·) > 0, the factor multiplying ∇θ[∆rodds\nθ\n(x)] is strictly negative.\nStep 2. Gradient of standard ORPO (i.e. β = 1).\nFor\nLORPOAlign = −log σ\n\x00∆rodds\nθ\n(x)\n\x01\n,\nthe gradient is\n∇θ LORPOAlign = −\n\x02\n1 −σ(∆rodds\nθ\n(x))\n\x03\n∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nThis also has a strictly negative scalar in front of ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\n.\nStep 3. Conclusion (exact positive ratio).\nSince ∇θ Lβ\nORPOAlign and ∇θ LORPOAlign both differ from ∇θ\n\x02\n∆rodds\nθ\n(x)\n\x03\nby a negative coefficient, it follows that these\ntwo gradients coincide up to a strictly positive factor:\n∇θ Lβ\nORPOAlign = κ(β) ∇θ LORPOAlign,\nκ(β) > 0.\nHence\n∇θ Lβ\nORPOAlign\n∥∇θ Lβ\nORPOAlign∥\n=\n∇θ LORPOAlign\n∥∇θ LORPOAlign∥,\nproving the claimed collinearity (in the same direction) for every fixed β > 0.\n16\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nF. Proof of Theorem 3.6\nTheorem F.1 (Unified Collinearity of DPO with IPO, SimPO, NCA, Cal-DPO, and APO-Zero). Let\n∆rref\nθ (x) = rref\nθ\n\x00yw, x\n\x01\n−rref\nθ\n\x00yl, x\n\x01\n,\nand define the DPO loss\nLDPO = −log\n\x10\nσ\n\x00β ∆rref\nθ (x)\n\x01\x11\n,\nβ > 0.\nFor each method X ∈\n\x08\nIPO, SimPO, NCA, Cal-DPO, APO-Zero\n\t\n, as β →0, the gradient of LX is asymptotically\ncollinear (i.e., it differs by a positive factor) with the gradient of LDPO. Formally,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥.\nProof of Theorem 3.6. Step 1: DPO as the baseline (tracking its sign).\nBy definition,\nLDPO = −log σ\n\x00β ∆rref\nθ (x)\n\x01\n.\nSince σ(u) = 1/(1 + e−u), for β > 0, one computes\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nObserve that β > 0 and σ(·) ∈(0, 1) imply\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01\n> 0.\nHence the factor multiplying ∇θ ∆rref\nθ (x) is negative. To unify directions by a positive multiple, note\n−∇θ LDPO = β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x),\nwhich has a strictly positive scalar in front. Thus, ∇θ LDPO is collinear with ∇θ ∆rref\nθ , and in particular its negative is a\npositive multiple of ∇θ ∆rref\nθ .\nStep 2: IPO.\nThe IPO loss is\nLIPO =\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x112\n.\nIts gradient is\n∇θ LIPO = 2\n\x10\n∆rref\nθ (x) −\n1\n2β\n\x11\n∇θ ∆rref\nθ (x).\nAs β →0, the term\n1\n2β dominates ∆rref\nθ (x). Hence,\n∆rref\nθ (x) −\n1\n2β ≈−1\n2β ,\nso\n∇θ LIPO ≈−1\nβ ∇θ ∆rref\nθ (x).\nWe compare this with\n∇θ LDPO = −β\nh\n1 −σ\n\x00β ∆rref\nθ (x)\n\x01i\n∇θ ∆rref\nθ (x).\nBoth gradients are negative multiples of ∇θ ∆rref\nθ (x). Therefore,\n∇θ LIPO = κIPO(β) ∇θ LDPO,\nwith κIPO(β) > 0 as β →0.\nHence they are collinear in the same direction asymptotically.\n17\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nStep 3: SimPO.\nThe SimPO loss is\nLSimPO = −log σ\n\x00β ∆sθ −γ\n\x01\n,\nwhere ∆sθ = log πθ(yw | x) −log πθ(yl | x). Its gradient takes the form\n∇θ LSimPO = −β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\n∇θ ∆sθ.\nAgain, β > 0 and 1 −σ(·) > 0. Also, σ(β ∆sθ −γ) ∈(0, 1). Thus the prefactor\n−β\n\x02\n1 −σ(β ∆sθ −γ)\n\x03\nσ(β ∆sθ −γ)\nis strictly negative for each β > 0. Therefore, just like DPO, ∇θ LSimPO is in the negative direction of ∇θ ∆sθ. But ∇θ ∆sθ\nis proportionally the same as ∇θ ∆rref\nθ\nfor small-β expansions (both are differences of log-likelihood or reward-like terms).\nSo\n∇θ LSimPO = κSimPO(β) ∇θ LDPO,\nκSimPO(β) > 0 for small β.\nHence they are collinear with a positive factor in the low-β limit.\nStep 4: NCA.\nDefine\nrref\nw\n= rref\nθ\n\x00yw, x\n\x01\n,\nrref\nl\n= rref\nθ\n\x00yl, x\n\x01\n.\nThen NCA is\nLNCA = −log σ\n\x00β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nw\n\x01\n−1\n2 log σ\n\x00−β rref\nl\n\x01\n.\nFor small β, expand\nσ(β z) = 1\n2 + β z\n4\n+ O(β2),\nso log σ(β z) = log 1\n2 + log\n\x10\n1 + β z\n2 + O(β2)\n\x11\n. Each gradient term then yields a linear-in-β combination of ∇θ rref\nw and\n∇θ rref\nl . Collecting terms shows that, as β →0,\n∇θ LNCA ∝β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= β ∇θ ∆rref\nθ (x).\nComparing this with ∇θ LDPO = −β\n\x02\n1 −σ(. . . )\n\x03\n∇θ ∆rref\nθ (x) reveals another negative factor on the DPO side. In ratio\nform,\n∇θ LNCA = κNCA(β) ∇θ LDPO\nwith κNCA(β) > 0 for small β.\nHence collinearity follows.\nStep 5: Cal-DPO.\nThe Cal-DPO loss is\nLCal-DPO = −log σ\n\x00∆rref\nθ (x)\n\x01\n+\n\x00rref\nw −\n1\n2β\n\x012 +\n\x00rref\nl\n+\n1\n2β\n\x012.\nFor β near 0, the large constants ± 1\n2β dominate. The gradient w.r.t. θ in these squared terms is effectively\n∝−1\nβ ∇θ rref\nw\n+\n1\nβ ∇θ rref\nl\n= −1\nβ ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −1\nβ ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO has the same negative sign structure in front of ∇θ ∆rref\nθ , their ratio is again positive. Thus\n∇θ LCal-DPO = κCal-DPO(β) ∇θ LDPO\nwith κCal-DPO(β) > 0 as β →0.\nStep 6: APO-Zero.\nAPO-Zero is given by\nLAPO-Zero = −σ\n\x00β rref\nw\n\x01\n+ σ\n\x00β rref\nl\n\x01\n.\n18\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nIts gradient involves terms ∇θ σ(β rref\nw ) and ∇θ σ(β rref\nl ), each proportional to β ∇θ rref\nw and β ∇θ rref\nl . Subtracting these\nyields\n∇θ LAPO-Zero ∝−β ∇θ\n\x00rref\nw −rref\nl\n\x01\n= −β ∇θ ∆rref\nθ (x).\nSince ∇θ LDPO also has a negative constant factor, their ratio has a positive limit. Therefore,\n∇θ LAPO-Zero = κAPO-Zero(β) ∇θ LDPO,\nκAPO-Zero(β) > 0 for small β.\nConclusion.\nIn each method X, one sees that ∇θ LX has the same negative-sign structure around ∇θ ∆rref\nθ (x) as does ∇θ LDPO,\nensuring a positive ratio in the limit. Formally,\n∇θ LX = κX(β) ∇θ LDPO,\nκX(β) > 0,\nas β →0.\nThus,\nlim\nβ→0\n∇θ LX\n∥∇θ LX∥=\n∇θ LDPO\n∥∇θ LDPO∥,\nwhich completes the proof of their alignment in the same direction.\nG. Pareto fronts for Llama 3.2 setups\nThe results presented in this section correspond to the best hyperparameter configurations identified during the hyperparame-\nter search described in Section 4.2, including the optimal learning rate for each method. This ensures that the Pareto fronts\nreflect the upper performance limits for alignment quality.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL Divergence with SFT Model\n40\n50\n60\n70\n80\n90\nGPT-4 Win Rate (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(a) Llama 3.2 3B TL;DR\n0.0\n0.2\n0.4\n0.6\n0.8\nKL Divergence with SFT Model\n5\n6\n7\n8\n9\n10\n11\n12\nAlpacaEval 2 LC WR (%)\nMethod\nDPO\nIPO\nSimPO\nORPO\nAPO Zero\nNCA\nCal-DPO\nASFT\nPairwise\nPointwise\n(b) Llama 3.2 3B UF\nFigure 6. Pareto front for alignment quality and KL divergence. Results for Llama 3.2 3B TL;DR and UF setups on GPT-4 Win\nRate vs. ”golden” validation subset and AlpacaEval 2 LC respectively with different β values. Methods are grouped into pairwise and\npointwise categories. For the summarization task (Llama 3.2 3B TL;DR), both pointwise and pairwise methods achieve strong overall\nresults. For the UF setup, methods also perform similarly within overlapping confidence intervals, indicating no clear separation.\nH. Toy Example Details\nTo analyze the differences between pairwise and pointwise ranking methods, especially with respect to the ranking nature of\nalignment losses in LLMs, a simplified toy experiment was conducted under a controlled setup. A dataset of 2000 triplets\n(x, yw, yl) was generated, where x, yw, and yl are real-valued scalars satisfying yw > yl. The data was split into 80% for\ntraining and 20% for testing. When the model processes a scalar input x together with a candidate y, these two numbers\nform a vector in R2, which serves as the input of the Multi-Layer Perceptron (MLP) to predict the reward r.\n19\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nA single-hidden-layer MLP with ReLU activation was used in two capacity settings: lower (hidden size = 1) and higher\n(hidden size = 3). The model takes x and a candidate y as input, producing a reward r analogous to training a reward model\nfor RLHF (Stiennon et al., 2020).\nTwo losses were evaluated: the pairwise Bradley-Terry loss (Bradley & Terry, 1952),\nLPairwise = −log\n\x00σ(β(rw −rl))\n\x01\n,\nand the pointwise loss,\nLPointwise = −\n\x02\nlog\n\x00σ(βrw)\n\x01\n+ log\n\x00σ(−βrl)\n\x01\x03\n.\nEach configuration was trained over 100 runs, tuning the learning rate from {0.5, 0.3, 0.1, 0.01, 0.03, 0.05} and β from\n{5.0, 2.0, 1.0, 0.2, 0.1, 0.05, 0.01}. Alignment accuracy was defined as the proportion of cases with rw > rl.\nThe results show that both methods yield comparable performance in the low-capacity regime, while pairwise ranking\nachieves higher accuracy as model capacity increases, mirroring the effects observed in larger-scale experiments from the\nSection 5.3.\nI. GPT-4 Side-By-Side Evaluation Prompt\nFor our Side-By-Side evaluations with GPT-4o, we designed a prompt tailored to the Reddit TL;DR dataset to assess\naccuracy, completeness, relevance, and conciseness. The full prompt used in our experiments is detailed below.\nAct as an impartial judge and evaluate the quality of the summaries provided\nby two AI assistants for the text displayed below. Your evaluation should\nconsider accuracy, completeness, relevance, and conciseness.\nYou will be given a text, Assistant A’s summary, and Assistant B’s summary.\nYour job is to evaluate which assistant’s summary is better based on the\ntext provided.\nBegin your evaluation by comparing both assistants’ summaries with the\noriginal text. Identify and correct any inaccuracies.\nEnsure the summaries are complete, capturing all essential information\nfrom the text without introducing fabricated details.\nAssess the relevance of the information each assistant chose to include\nin their summary, ensuring it reflects the core message of the text.\nEvaluate the conciseness of the summaries, favoring those that efficiently\nconvey the necessary information without unnecessary verbosity.\nAvoid any position biases and ensure the order in which the summaries\nwere presented does not influence your decision.\nDo not allow the length of the summaries to influence your evaluation,\nexcept in the context of conciseness and efficiency.\nDo not favor certain names of the assistants.\nBe as objective as possible.\nYou should only evaluate the summaries provided by both assistants\nand NOT the original text itself.\nIf both summaries are irrelevant, contain hallucinations, or are\ninconsistent with the original text, mark the comparison as inconclusive\nand choose option "C".\nAfter providing your explanation, output your final verdict by strictly\nfollowing this format:\n"""\n20\n\nThe Differences Between Direct Alignment Algorithms are a Blur\nComparison: <One-sentence comparison>\nWinner: <A if assistant A is better, B if assistant B is better, and C for a tie.>\n"""\n21'),
                Paper(arxiv_id='2502.01456', authors=['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], published_at=datetime.datetime(2025, 2, 4, 0, 2, 39, 922000, tzinfo=datetime.timezone.utc), title='Process Reinforcement through Implicit Rewards', summary="Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for the reinforcement learning\n(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of training process reward models (PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable to reward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using only policy rollouts and outcome labels through\nimplict process rewards. PRIME combines well with various advantage functions\nand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting from\nQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkey reasoning benchmarks over the SFT model. Notably, our resulting model,\nEurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning\nbenchmarks with 10% of its training data.", upvotes=48, thumbnail=None, content='Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While dense\nrewards also offer an appealing choice for the reinforcement learning (RL) of\nLLMs since their fine-grained rewards have the potential to address some inher-\nent issues of outcome rewards, such as training efficiency and credit assignment,\nthis potential remains largely unrealized. This can be primarily attributed to the\nchallenges of training process reward models (PRMs) online, where collecting\nhigh-quality process labels is prohibitively expensive, making them particularly\nvulnerable to reward hacking. To address these challenges, we propose PRIME\n(Process Reinforcement through IMplicit rEwards), which enables online PRM\nupdates using only policy rollouts and outcome labels through implict process\nrewards. PRIME combines well with various advantage functions and forgoes the\ndedicated reward model training phase that existing approaches require, substan-\ntially reducing the development overhead. We demonstrate PRIME’s effectiveness\non competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME\nachieves a 15.1% average improvement across several key reasoning benchmarks\nover the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses\nQwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its train-\ning data.1\n1\nINTRODUCTION\nDense process rewards, which provide feedback at each intermediate step rather than only the whole\ntrajectory, have proven effective in inference-time scaling of large language models (LLMs) on\nchallenging reasoning tasks (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Yuan\net al., 2024b). On the training side, they also present superiorities in the reinforcement learning\n(RL) of LLMs, particularly in improving training efficiency (Sutton & Barto, 2018) and credit\nassignment (Leike et al., 2018) compared with sparse outcome rewards. However, successful\napplications of dense rewards in RL for LLMs are limited (Setlur et al., 2024), as current industry-\nleading models primarily depend on verifiable outcome rewards and have not yet demonstrated\nmeaningful progress with dense rewards (DeepSeek-AI et al., 2025; Team et al., 2025).\nWe identify the central challenge as how to acquire and utilize high-quality dense rewards at scale,\nwhich enables online process reward model (PRM) update efficiently. The reason is that, optimizing\ntowards a static reward model eventually leads to overoptimization or reward hacking (Gao et al.,\n∗Core Contributors.\n†Project Lead.\n1Models and data are available at: https://github.com/PRIME-RL/PRIME.\n1\narXiv:2502.01456v1  [cs.LG]  3 Feb 2025\n\nPreprint\nAIME 2024\nAMC\nMinerva Math\nOlympiadBench\nMATH-500\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n26.7\n57.8\n38.6\n42.1\n79.2\n48.9\n3.3\n30.1\n32.7\n29.8\n65.1\n32.2\n13.3\n50.6\n34.6\n40.7\n79.8\n43.8\n16.7\n30.1\n35.3\n31.9\n64.6\n35.7\n9.3\n45.8\n36.8\n43.3\n76.4\n43.3\nEurus-2-7B-PRIME\nEurus-2-7B-SFT\nQwen-2.5-Math-7B-Instruct\nLlama-3.1-70B-Instruct\nGPT-4o-2024-08-06\nFigure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics\nbenchmarks, outperforming advanced math models and larger models. Notably, PRIME brings\nsubstantial performance gain (+16.7%) over Eurus-2-7B-SFT.\n2022) due to distribution shift. Ideally, this can be solved by improving the reward model online (Leike\net al., 2018). However, acquiring dense process labels for training is prohibitively more expensive.\nExisting methods either need to build complicated human annotation pipelines (Lightman et al.,\n2023) or rely on estimation-based methods, which require about 10× more rollouts for each step than\nsampling only the response-level trajectories (Wang et al., 2023; Kazemnejad et al., 2024). Neither of\nthem is scalable in online RL. Moreover, to the best of our knowledge, it remains underexplored how\nto incorporate dense rewards into RL for LLMs.\nIn this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable frame-\nwork for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level\nrewards. At its core, the framework employs recently proposed implicit process reward model-\ning (Yuan et al., 2024b) to train dense reward models with only outcome-level labels. This enables\nPRIME to perform online learning of reward signals using only outcome labels on policy rollouts,\nthereby fundamentally mitigating reward hacking while maintaining the same computational cost as\ntraditional outcome reward models (ORMs). Besides scalability, PRIME also (1) serves as a general\nmethod to fuse token-level dense rewards and sparse outcome rewards by calculating their returns\nseparately before summing together, which is compatible with diverse RL algorithms (Williams, 1992;\nKool et al., 2019; Shao et al., 2024; Ahmadian et al., 2024; Schulman et al., 2017); (2) eliminates the\ndedicated reward modeling stage, which is required by existing works, by simply initializing from the\nSFT model or even the base model (§ 5.6). In summary, starting from one single language model, the\nPRIME framework can efficiently accomplish the generation of dense rewards, the initialization and\nupdating of reward models, as well as the reinforcement learning (RL) training of the policy model.\nTable 1: The comparison of resource requirements between Eurus-\n2-7B-PRIME and Qwen2.5-Math-7B-Instruct.\nModel\nEurus-2-7B-PRIME\nQwen2.5-Math-7B-Instruct\nBase Model\nQwen2.5-Math-7B\nQwen2.5-Math-7B\nSFT Data\n230K (open-source)\n2.5M (open-source & in-house)\nRM Data\n0\n618K (in-house)\nRM\nEurus-2-7B-SFT\nQwen2.5-Math-RM (72B)\nRL Data\n150K queries × 4 samples\n66K queries × 32 samples\nIn\nexperiments,\nwe\ntrain\nQwen2.5-Math-7B-Base (Yang\net al., 2024b) with PRIME after\na lightweight SFT warmup stage.\nCompared to RL using outcome\nrewards only, PRIME achieves\na 2.5× sample efficiency gain\nand a 6.9% performance im-\nprovements on challenging math\nproblems. As shown in Figure 1,\nthrough PRIME, we successfully\nachieve substantial improvement on key mathematical reasoning benchmarks over the SFT model,\nleading to 16.7% improvement on average, and over 20% on AMC&AIME competitions. Our\nfinal model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical\nbenchmarks. Notably, this is achieved with only 10% of the data used by Qwen-Math, as in Table 1.\n2\n\nPreprint\nOur analysis shows that updating the PRM online is key to the success of PRIME (§5.1). We also\nshow that PRIME could generally boost various RL algorithms, including RLOO (Ahmadian et al.,\n2024), REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GRPO (Shao et al., 2024)\n(§5.4). In terms of the design choices of advantage estimate, we observe that Implicit PRMs are better\nto be used as reward models than value models (§5.5).\n2\nREINFORCEMENT LEARNING FOR LLMS AND THE CHALLENGES OF\nINCOPORATING DENSE REWARDS\nReinforcement Learning (RL) aims to learn an optimal policy πθ that maximizes the expected\ncumulative discounted reward, namely return, when interacting with an environment. In the context\nof autoregressive language modeling, state at step t is the concatenation of prompt x and current\nresponse y<t, and the action is the t-th token or step yt.\n2.1\nRL PRELIMINARIES FOR LLMS\nPolicy Gradient. Policy gradient is a fundamental algorithm that directly optimizes this objective.\nCentral to this approach is the advantage function At, which quantifies how much better an action is\ncompared to alternatives in a given state:\n∇θJ(θ) = Ex∼D,y∼πθ\n" T\nX\nt=0\n∇θ log πθ(yt|y<t)At\n#\n(1)\nwhere (x, y) represents a pair of input and output. x is omitted for brevity. In practice, the advantage\nfunction is implemented as cumulative discounted rewards subtracting a baseline:\nAt =\nT\nX\ns=t\nγs−tr(ys) −b\n(2)\nγ ∈[0, 1] is a discount factor that optionally decays future rewards, and r(ys) is the reward provided\nby the environment at time step s with x and y<s being omitted in conditions. Eq. 2 is the general\nformula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and\ndense reward at each step is crucial for RL. Different choices of b include, e.g. directly using values\nWilliams (1992), group average of rewards (Shao et al., 2024), and leave-one-out average of rewards\nAhmadian et al. (2024); Kool et al. (2019).\nValue Models. Though the MC estimate is unbiased, it suffers from high variance because of the\nreliance on all future actions and rewards, which can be random and noisy. Value models, which\npredict expected accumulated rewards starting from a state, are adopted to help reduce the variance\nin advantage estimation, such as Generalized Advantage Estimation (GAE; Schulman et al., 2016):\nAGAE(γ,λ)\nt\n= P∞\ns=0(γλ)sδt+s, where δt = r(yt) + γV (y<t+1) −V (y<t) is the temporal difference\n(TD) error (Sutton, 1988), V is a value model, and λ controls the bias-variance tradeoff in advantage\nestimation. PPO (Schulman et al., 2017) is a representative of such actor-critic algorithms that\nexplicitly train a value model along with the policy.\nReward Sparsity. Although dense rewards can be naturally integrated into the advantage function\nthrough Eq. 2, unfortunately, only outcome reward models (ORMs) are available in most practices\nof LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive\nno rewards (Rafailov et al., 2023; Shao et al., 2024; DeepSeek-AI et al., 2025). In this bandit\nsetting, r(yt) = 0 for t < T while r(yT ) can be non-zero, and Eq. 2 becomes A = r(yT ) −b. This\nformulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback\nonly at the end of the entire generation. This may (1) encourage spurious solutions with incorrect\nprocesses but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter\nthe credit assignment problem (Sutton & Barto, 2018). These drawbacks could be further amplified\non complicated tasks, which require more thinking and execution steps, urging the need of dense\nrewards (Uesato et al., 2022; Lightman et al., 2023). Some may consider employing a value model\nto mitigate the problem, as it predicts values at every step t. However, previous work showed that\nvalue models may not be able to solve the reward sparsity issue effectively due to training challenges,\ndespite the additional computation overhead (Shao et al., 2024; Ahmadian et al., 2024). We will\nalso empirically validate this claim in §5.5.\n3\n\nPreprint\n2.2\nKEY CHALLENGES IN SCALABLE DENSE REWARDS\nThe way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs,\nwhich score model responses over each token or step. However, it is usually infeasible in practice to\nincorporate dense rewards into online RL because of three critical challenges in implementation.\nC1. Process rewards are hard to define. It is difficult to collect step-level labels since reasoning\nsteps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating\nlabels for each token is too costly. Moreover, defining the absolute correctness of intermediate\nprocesses as dense rewards can be ambiguous, as some incorrect steps can also positively contribute\nto the final answer by pruning searching branches (OpenAI, 2024; DeepSeek-AI et al., 2025).\nC2. PRM online updates are not scalable. It is crucial to prevent reward overoptimization or reward\nhacking, which requires the reward model or value model to be updated online along with the policy\nmodel (Schulman et al., 2017; Gao et al., 2022). However, training PRMs often requires extensive\nnuanced step-level annotation, which is infeasible in online RL training. Therefore, this brings about\nconsiderable scalability and generalization concerns in dense rewards for RL.\nC3. Explicit reward modeling brings extra cost. Training reward models requires extensive\nannotation and broad data coverage to ensure a good balance between adaptability to the policy\ndistribution and generalization to distribution shifts. Hence, the explicit training stage introduces a\nvery costly data collection and an additional training overhead, especially for PRMs which typically\nrequire stepwise labels.\nNotably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMs\ninto their large-scale RL training (DeepSeek-AI et al., 2025).\n3\nPRIME\nTo address the above challenges, we propose PRIME, a scalable online RL method with dense\nrewards. The key insight of PRIME is to apply implicit process rewards, which are derivable from the\nImplicit PRM that is trained with only outcome labels (Yuan et al., 2024b). This property enables us to\nupdate the PRMs online to avoid reward hacking. We then design a flexible framework to incorporate\nimplicit process rewards with outcome rewards into any kind of MC advantage estimate. PRIME is\nillustrated in Figure 2 and Algorithm 1. Next, we will detail the implicit process rewards (§3.1) and\nhow we leverage them to calculate advantages (§3.2), and introduce other techniques we used (§3.3).\n3.1\nENABLING SCALABLE REWARD UPDATE WITH IMPLICIT REWARD MODELING\nWe consider dense rewards from the Implicit PRM because of the scalability. In short, Implicit PRM\nenables training an ORM with outcome labels only while repurposing it as a PRM at inference. The\ntraining stage is the same as standard ORM pipelines, with the only difference being representing\nthe reward as rϕ(y) := β log πϕ(y)\nπref(y), where πϕ is the RM and πref is the reference model, both of\nwhich are causal LMs. At inference, the process rewards are obtained by:\nrϕ(yt) := β log πϕ(yt|y<t)\nπref(yt|y<t)\n(3)\nIn PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we\nupdate the Implicit PRM online with on-policy rollouts and outcome supervision and then\ncalculate token-level dense rewards to estimate advantages, which solves C1 and C2 mentioned in\n§2.2 respectively: (1) To prevent overoptimization and reward hacking, it is crucial to update reward\nmodels online. However, updating previous PRMs (Lightman et al., 2023) requires annotating step\nlabels on the latest policy rollouts, which is neither efficient nor scalable during online RL. In contrast,\nthe Implicit PRM only demands outcome labels to train due to its special reward representation,\nand thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which\nhave already been collected to update the policy model. (2) Unlike common PRMs that produce only\nstep-level rewards, the Implicit PRM provides more fine-grained token-level rewards at no additional\ncost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra\noverhead, making it easy to combine with any RL algorithms for advantage estimation.\n4\n\nPreprint\nAlgorithm 1 Process Reinforcement through Implicit Rewards (PRIME)\nInput Language model πθinit; outcome reward verifier ro; dataset D; sample number K; total iteration\nN.\n1: Initialize policy model πθ ←πθinit, πθold ←πθinit, implicit PRM πϕ ←πθinit, reference model\nπref ←πθinit\n2: for iteration = 1, ..., N do\n3:\nSample batch of prompts B ∼D\n4:\nGenerate K responses: {y1, ..., yK} ∼πθ(·|x) for x ∈B\n5:\nCompute outcome rewards: ro\n\x00y1:K\x01\n6:\nApply accuracy filter (§3.3) on all prompts: T ←Filter(x, y1:K, ro\n\x00y1:K\x01\n) for x ∈B\n7:\nForward pass πϕ, πref on each (x, y) ∈T to obatin implicit process reward rϕ(yt) with Eq. 3\n8:\nUpdate Implicit PRM πϕ by CE loss on (x, y, ro (y)) ∈T :\nLCE(ϕ) = −E(x,y,ro(y))∼T [ro (y) · log σ (rϕ (y)) + (1 −ro (y)) · log (1 −σ (rϕ (y)))]\n9:\nCompute advantages A with Eq. 5\n10:\nUpdate policy πθ by PPO loss in Eq. 6\n11:\nUpdate old parameters: θold ←θ\n12: end for\nOutput Optimized policy model πθ\n3.2\nADVANTAGE ESTIMATION AND POLICY UPDATE\nSFT \nModel\nImplicit \nPRM\nPolicy \nModel\nImplicit \nPRM\nPolicy \nModel\nPrompt\nResponse\nOutcome \nVerifier\n𝒓𝒐\n𝒓𝒑\nUpdate\nUpdate\n𝝅𝒓𝒆𝒇\n𝝅𝒓𝒆𝒇\nSFT \nModel\nFigure 2: Illustration of PRIME. PRIME follows\nthat (1) initialize policy model and the Implicit\nPRM both with the reference model; (2) sample\nmultiple responses for each prompt and filter with\noutput accuracy; (3) obtain implicit process re-\nwards by the Implicit PRM and update it using\ncross-entropy (CE) loss; (4) compute advantage\nand policy loss then update the policy model.\nEstimating advantages using Monte Carlo es-\ntimator with a leave-one-out baseline. After\nobtaining token-level dense rewards, we calcu-\nlate advantages based on either MC estimators\nor GAE. To determine the advantage function\nin PRIME, we compare GAE with several MC\nestimators, including REINFORCE (Williams,\n1992), RLOO (Ahmadian et al., 2024), and\nGRPO (Shao et al., 2024). Experimental details\nand results can be found in §5.4.\nWe find that MC estimators, despite being sim-\npler, are strong enough to produce stable results.\nTherefore, we choose MC estimate as our ad-\nvantage function and despite PRIME being com-\npatible with any baseline estimation approaches,\nwe instantiate it with a leave-one-out baseline\nfrom K samples (Ahmadian et al., 2024) in this\npaper, as it performs better in the experiments:\nAi = r(yi\nT ) −\n1\nK −1\nX\nj̸=i\nr(yj\nT )\n(4)\nwhere r(yi\nT ) denotes the reward of i-th response at final step T, K is the number of samples for one\nprompt. The leave-one-out (LOO) baseline helps reduce variances.\nMore specifically, we use an Implicit PRM πϕ and an outcome verifier or reward model ro. We\ncalculate the return of implicit process rewards and outcome rewards separately if both are available,\nsince directly mixing their values may lead to numerical instability (Shao et al., 2024). For implicit\nprocess rewards, we perform a three-step process to calculate return: (1) Use the averaged implicit\nprocess rewards to calculate the leave-one-out baseline; (2) Normalize the process reward at step t by\nsubtracting the baseline; (3) Calculate the discounted return for each response. For outcome rewards,\nwe directly adopt LOO without any modification. Finally, the advantage is set to the combination of\n5\n\nPreprint\nboth returns:\nAi\nt =\n|yi|\nX\ns=t\nγs−t ·\n\uf8ee\n\uf8f0rϕ(yi\ns) −\n1\nK −1\nX\nj̸=i\nrϕ\n\x00yj\x01\n\uf8f9\n\uf8fb\n|\n{z\n}\nRLOO with implicit process rewards\n+ ro\n\x00yi\x01\n−\n1\nK −1\nX\nj̸=i\nro\n\x00yj\x01\n|\n{z\n}\nRLOO with outcome rewards\n(5)\nUpdating policy with PPO clip surrogate loss. We adopt PPO clip surrogate loss for more stable\npolicy updates:\nLCLIP(θ) =Et\n"\nmin\n\x12 πθ(yt|y<t)\nπθold(yt|y<t)At, clip\n\x10 πθ(yt|y<t)\nπθold(yt|y<t), 1 −ϵ, 1 + ϵ\n\x11\nAt\n\x13#\n(6)\nwhere ϵ is a clipping parameter. The loss prevents the updated policy from deviating too far from the\noriginal distribution, which is the prerequisite of importance sampling. The legitimacy of importance\nsampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling\nefficiency.\n3.3\nOTHER TECHNIQUES\nInitializing PRM with SFT/base model. In practice, we find that the starting policy model itself\nserves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in §2.2\nand even outperforms a dedicatedly trained PRM, as shown in § 5.1.\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\nOutcome Training Rewards\nw/ filter\nw/o filter\nFigure 3: Impact of online prompt filtering on\ntraining rewards.\nOnline Prompt Filtering. As we sample mul-\ntiple trajectories for each prompt, we introduce\nonline prompt filtering which filters prompts\nwithin a certain accuracy range. This (1) pre-\nserves only the prompts within a certain median-\nlevel difficulty range (Yang et al., 2024b) and (2)\nbalances data distribution for the Implicit PRM\nonline training.\nWe present the ablation study results in Figure 3\nusing RLOO with outcome rewards only, from\nwhich we can see that the online prompt filter\nlargely lowers the variance of RL training.\nHow PRIME addresses challenges in §2.2. In\nsummary, as illustrated in Figure 2 and Algo-\nrithm 1, PRIME adopts implicit process rewards\nfor efficient PRM online update (C2), then inte-\ngrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs\nare directly initialized from SFT or base models, which foregoes explicit reward modeling (C3).\n4\nEXPERIMENTS\n4.1\nIMITATION WARMUP\nWe focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-\nMath-7B-Base (Yang et al., 2024b) for its great mathematical capabilities. We first performed\nsupervised finetuning for RL preparation.\nData Construction. To construct the SFT dataset, we collect reasoning instructions from several open-\nsource datasets. For completion, we employed LLaMA-3.1-70B-Instruct (Meta, 2024) to answer the\ninstructions, with a system prompt requesting the model to perform action-centric chain-of-thought.\nWe finally obtained 230K SFT data, the detailed sources and statistics can be found in § A.\nSFT Results. After finetuning, the performance of our SFT model is reported in Figure 1. Compared\nto baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks.\n6\n\nPreprint\nTable 2: Detailed results of PRIME and RLOO w/ outcome verifier (OV). At the same 240 steps, the\nmodel trained by PRIME is generally better than the model trained by outcome rewards.\nMethod\nStep\nAIME 2024\nAMC\nMATH-500\nMinervaMath\nOlympiadBench\nLeetCode\nLiveCodeBench\nAvg.\nGPT-4o\n-\n9.3\n45.8\n76.4\n36.8\n43.3\n58.9\n48.8\n45.6\nLlama-3.1-70B-Inst.\n-\n20.0\n37.3\n65.0\n37.1\n30.5\n35.0\n34.4\n37.0\nQwen2.5-Math-7B-Inst.\n-\n13.3\n50.6\n79.8\n34.6\n40.7\n11.7\n11.3\n34.6\nEurus-2-7B-SFT\n0\n3.3\n30.1\n66.2\n32.7\n29.8\n21.7\n17.8\n28.8\nRLOO w/ OV Only\n240\n20.0\n47.0\n73.2\n36.4\n35.4\n28.3\n26.7\n36.9\n80\n20.0\n41.0\n68.2\n38.2\n37.0\n26.7\n26.6\n36.8\n160\n13.3\n42.2\n72.0\n37.1\n38.7\n26.7\n25.6\n36.5\n240\n20.0\n50.6\n78.2\n39.3\n40.3\n31.1\n27.5\n41.0\n320\n16.7\n51.8\n77.8\n39.7\n41.5\n36.1\n28.5\n41.7\nEurus-2-7B-PRIME\n592\n26.7\n57.8\n79.2\n38.6\n42.1\n33.3\n28.6\n43.9\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\n2.5x Efficient\n6.9% Higher\nPRIME\nRLOO w/ OV Only\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\n256\n288\n320\nSteps\n30\n32\n34\n36\n38\n40\n42\nAvg. Test Acc\nPRIME\nRLOO w/ OV Only\n(b) Test accuracy across different gradient steps.\nFigure 4: The effect of dense reward. We compare PRIME and RLOO with outcome verifier\n(OV). Dense rewards in PRIME lead to 2.5× sample efficiency and 6.9% performance improvement.\nPRIME also substantially outperforms RLOO on downstream tasks.\n4.2\nRL SETTINGS\nRule-based Outcome Verifier. Consistent with recent research that adopts exact match with ground\ntruth as unhackable rewards (Gao et al., 2024; Lambert et al., 2024; DeepSeek-AI et al., 2025), we\ndefine the rule-based ground truth outcome verifiers (OV) for math and coding as follows:\nrmath\no\n(y) =\n\x1a1,\nmatched\n0,\notherwise\nrcode\no\n(y) =\nP #passes\nP #test cases\nHyperparameters. We use veRL (Sheng et al., 2024) to conduct experiments. By default, we\ninitialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For\nhyperparameters, we use a constant 5 × 10−7 learning rate together with AdamW optimizer for\npolicy model, and use a 10−6 learning rate for PRMs. Both policy and PRMs use a batch size of 256\nand micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each\nprompt. We set β = 0.05 for PRM training. We set KL coefficient to 0 in all experiments.\nEvaluation Benchmarks. We evaluate on 7 reasoning benchmarks, focusing on competition-level\nmathematics and programming tasks, including AIME 2024 (Li et al., 2024), AMC (Li et al., 2024),\nMATH-500 (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He\net al., 2024), LeetCode (Guo et al., 2024), and LiveCodeBench (v2) (Jain et al., 2024).\n4.3\nMAIN RESULTS\nAs shown in Figure 1 and Table 2, Eurus-2-7B-PRIME achieves substantial improvements on key\nreasoning benchmarks over the SFT version of the model, leading to 15.1% improvement on average,\nand over 20% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7%\npass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct,\ndemonstrating its excellent reasoning ability.\n7\n\nPreprint\n0\n50\n100\n150\n200\nSteps\n0.36\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nOutcome Training Rewards\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(a) Outcome training rewards (10-step moving).\n0\n32\n64\n96\n128\n160\n192\n224\nSteps\n30\n32\n34\n36\n38\n40\nAvg. Test Acc\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\n(b) Test accuracy across different gradient steps.\nFigure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the\nbest results. Surprisingly, using PRMs trained on extra rollouts hurts the performance in both online\nand offline settings.\n4.4\nDENSE REWARDS V.S. SPARSE REWARDS\nWe first validate the effect of dense rewards compared to RLOO with outcome rewards only. We\ntrain this model for 240 steps. For PRIME, we use the same setting and train the model for 592\nsteps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure 4.\nCompared with sparse reward, PRIME takes 40% of the training steps to achieve the same\ntraining rewards as RLOO and improves the final rewards by 6.9%, with lower variances. On\ndownstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in\nTable 2.\n5\nANALYSIS\n5.1\nDESIGN CHOICES FOR THE IMPLICIT PRM\nThe Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this\nsection, we explore two major factors: (1) the initialization model and (2) the update mechanism.\nSFT model initializes a good PRM. Conventionally, we need to collect data to train RMs\nand PRMs, and then we can use them in RL. However, the Implicit PRM is a language\nmodel, so we can initialize it from any language model with the same tokenizer as the pol-\nicy model.\nTo investigate whether it is still necessary to train a PRM in advance, we con-\nduct experiments with different PRM initialization strategies: with the SFT model itself and\nwith a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT\nwith additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in § B.5).\n0\n50\n100\n150\n200\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nPRM Accuracy\nPRIME w/ online SFT PRM\nPRIME w/ offline EurusPRM\nPRIME w/ online EurusPRM\nFigure 6: Impact of PRM online update. The\noffline PRM is gradully been overoptimized while\nonline PRMs achieve higher accuracy throughout\ntraining.\nWe report the experiment results in Figure 5.\nSurprisingly, directly using Eurus-2-7B-SFT\nto initialize the PRM greatly outperforms Eu-\nrusPRM which was trained on more samples.\nWe conjecture that initializing policy model and\nPRM from the same model largely alleviates\nthe distribution shift issue, as the PRM is only\ntrained on the online rollouts from the policy\nmodel.\nOnline PRM update is essential. To verify the\neffect of online PRM update, we pair the correct\nand wrong samples and calculate the PRM\nprediction accuracy using rϕ(y).\nWe report\nthe PRM classification accuracy in Figure 6.\nThe figure clearly shows that, online update\nmitigates\noveroptimization\nand\nreward\n8\n\nPreprint\n(a) Policy ref: We use the policy logprob as πref\nfor PRM.\n(b) SFT ref: We retain the initial policy to provide πref for\nPRM and KL.\nFigure 7: Comparison of different reference policy implementations. One uses the running policy’s\nold logprobs as reference (policy ref) while the other uses the initial SFT model as the reference\nmodel (SFT ref).\nhacking. The offline PRM, though starting with\nhigh accuracy, gradually drops during RL training procedure due to distribu'),
                Paper(arxiv_id='2502.01341', authors=['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], published_at=datetime.datetime(2025, 2, 4, 10, 51, 54, 103000, tzinfo=datetime.timezone.utc), title='AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding', summary='Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.', upvotes=29, thumbnail=None, content='Vision-Language Models (VLMs) have gained significant\ntraction in recent years as a powerful framework for multi-\nmodal document understanding tasks that involve interpret-\n1ServiceNow 2York University 3Mila 4 ´Ecole de Technolo-\ngie Sup´erieure\n5Universit´e de Montr´eal\n6McGill University\n7University of Waterloo\n8CIFAR AI Chair\n9Polytechnique\nMontr´eal 10University of British Columbia. Correspondence to:\nAhmed Masry <ahmed.masry@servicenow.com>, Sai Rajeswar\n<sai.mudumba@servicenow.com>.\nLlama-3.2-3B-Perciever R.\nLlama-3.2-3B-MLP\nLlama-3.2-3B-Ovis\nLlama-3.2-3B-Align (ours)\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n74.68\n42.11\n58.02\n33.5\n33.13\n76.67\n67.92\n52.6\n53.93\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.1\nDocVQA\nInfoVQA\nDeepForm\nKLC\nWTQ\nTabFact\nChartQA\nTextVQA\nTableVQA\nFigure 1: Performance of Different VLM Connectors.\nThe proposed ALIGN connector outperforms other methods\nacross benchmarks using the same training configuration.\nRadial distance is proportion of maximal score, truncated at\n0.7 (black dot).\ning both the visual and textual contents of scanned docu-\nments (Kim et al., 2022; Lee et al., 2023; Liu et al., 2023a;\n2024; Hu et al., 2024; Wang et al., 2023a; Rodriguez et al.,\n2024b). Such tasks are common in real-world commercial\napplications, including invoice parsing (Park et al., 2019),\nform reading (Jaume et al., 2019), and document question\nanswering (Mathew et al., 2021b). VLM architectures typ-\nically consist of three components: (i) a vision encoder to\nprocess raw images, (ii) a Large Language Model (LLM)\npre-trained on text, and (iii) a connector module that maps\nthe visual features from the vision encoder into the LLM’s\nsemantic space.\nA central challenge in this pipeline is to effectively map the\ncontinuous feature embeddings of the vision encoder into\nthe latent space of the LLM while preserving the semantic\nproperties of visual concepts. Existing approaches can be\nbroadly categorized into deep fusion and shallow fusion\n1\narXiv:2502.01341v1  [cs.CL]  3 Feb 2025\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nmethods. Deep fusion methods, such as NVLM (Dai et al.,\n2024), Flamingo (Alayrac et al., 2022), CogVLM (Wang\net al., 2023b), and LLama 3.2-Vision (Grattafiori et al.,\n2024), integrate visual and textual features by introducing\nadditional cross-attention and feed-forward layers at each\nlayer of the LLM. While effective at enhancing cross-modal\ninteraction, these methods substantially increase the param-\neter count of the VLM compared to the base LLM, resulting\nin high computational overhead and reduced efficiency.\nIn contrast, shallow fusion methods project visual features\nfrom the vision encoder into the LLM input embedding\nspace using either multilayer perceptrons (MLPs) (Liu et al.,\n2023b; 2024) or attention-based mechanisms such as the\nPerceiver Resampler (Li et al., 2023; Laurenc¸on et al., 2024;\nAlayrac et al., 2022), before concatenating them with the\ntextual prompt’s input embeddings. This approach is more\nparameter-efficient and computationally lighter than deep\nfusion methods, but it lacks a mechanism to ensure the pro-\njected embeddings remain within the region spanned by\nthe LLM’s text embeddings – i.e. regions the LLM was\npretrained to understand. As a result, unconstrained vi-\nsual features can produce out-of-distribution (OOD) and\nnoisy inputs, leading to misalignment between modalities\nand often degrading overall performance. Recent methods\nlike Ovis (Lu et al., 2024) attempt to alleviate these issues\nby introducing separate visual embeddings indexed from\nthe vision encoder outputs and combined together to con-\nstruct the visual inputs to the LLM. However, this approach\nsignificantly increases parameter count due to the massive\nembedding matrix and requires extensive training to learn a\nnew embedding space without guaranteeing alignment with\nthe LLM’s input latent space.\nTo address these limitations, this paper introduces ALIGN-\nVLM, a novel framework that sidesteps direct projection\nof visual features into the LLM embedding space. Instead,\nour proposed connector, ALIGN, maps visual features into\nprobability distributions over the LLM’s existing pretrained\nvocabulary embeddings, which are then combined into a\nweighted representation of the text embeddings. By con-\nstraining each visual feature as a convex combination of the\nLLM text embeddings, our approach leverages the linguistic\npriors already encoded in the LLM’s text space. This en-\nsures that the resulting visual features lie within the convex\nhull of the LLM’s embedding space, reducing the risk of\nnoisy or out-of-distribution inputs and improving alignment\nbetween modalities. Our experimental results show that\nthis approach improves performance on various document\nunderstanding tasks, outperforming prior connector meth-\nods by effectively fusing visual and linguistic content. We\nsummarize our main contributions as follows:\n• We propose a novel connector, ALIGN, to bridge the\nrepresentation gap between vision and text modalities.\n• We introduce a family of Vision-Language Models,\nALIGNVLM, that achieves state-of-the-art perfor-\nmance on multimodal document understanding tasks\nby leveraging ALIGN.\n• We conduct extensive experiments demonstrating the\nrobustness and effectiveness of ALIGN across different\nmodel sizes ranging from 1B to 8B parameters.\nOur code and models will be public upon acceptance.\n2. Related Work\n2.1. Vision-Language Models\nOver the past few years, Vision-Language Models (VLMs)\nhave achieved remarkable progress, largely due to advances\nin Large Language Models (LLMs). Initially demonstrating\nbreakthroughs in text understanding and generation (Brown\net al., 2020; Raffel et al., 2023; Achiam et al., 2023;\nGrattafiori et al., 2024; Qwen et al., 2025; Team, 2024),\nLLMs are now increasingly used to effectively interpret vi-\nsual inputs (Liu et al., 2023b; Li et al., 2024; Wang et al.,\n2024; Chen et al., 2024b; Dai et al., 2024; Drouin et al.,\n2024; Rodriguez et al., 2022). This progress has enabled\nreal-world applications across diverse domains, particularly\nin multimodal document understanding for tasks like form\nreading (Svetlichnaya, 2020), document question answer-\ning (Mathew et al., 2021b), and chart question answer-\ning (Masry et al., 2022). VLMs commonly adopt a three-\ncomponent architecture: a pretrained vision encoder (Zhai\net al., 2023; Radford et al., 2021), a LLM, and a connector\nmodule. A key challenge for VLMs is effectively aligning\nvisual features with the LLM’s semantic space to enable\naccurate and meaningful multimodal interpretation.\n2.2. Vision-Language Alignment for Multimodal Models\nExisting vision-language alignment approaches can be clas-\nsified into deep fusion and shallow fusion. Deep fusion\nmethods integrate visual and textual features by modifying\nthe LLM’s architecture, adding cross-attention and feed-\nforward layers. For example, Flamingo (Alayrac et al.,\n2022) employs the Perceiver Resampler, which uses fixed\nlatent embeddings to attend to vision features and fuses\nthem into the LLM via gated cross-attention layers. Simi-\nlarly, NVLM (Dai et al., 2024) adopts cross-gated attention\nwhile replacing the Perceiver Resampler with a simpler\nMLP. CogVLM (Wang et al., 2023b) extends this approach\nby incorporating new feed-forward (FFN) and QKV lay-\ners for the vision modality within every layer of the LLM.\nWhile these methods improve cross-modal alignment, they\nsignificantly increase parameter counts and computational\noverhead, making them less efficient.\nOn the other hand, shallow fusion methods are more compu-\ntationally efficient, mapping visual features into the LLM’s\n2\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nembedding space without altering its architecture. These\nmethods can be categorized into three main types: (1) MLP-\nbased mapping, such as LLaVA (Liu et al., 2023b) and\nPaliGemma (Beyer et al., 2024), which use multilayer per-\nceptrons (MLP) to project visual features but often pro-\nduce misaligned or noisy features due to a lack of con-\nstraints (Rodriguez et al., 2024b); (2) cross-attention mech-\nanisms, BLIP-2 (Li et al., 2023) uses Q-Former, which\nutilizes a fixed set of latent embeddings to cross-attend to\nvisual features, but that may still produce noisy or OOD\nvisual features; and (3) visual embeddings, such as those\nintroduced by Ovis (Lu et al., 2024), which use embeddings\nindexed by the vision encoder’s outputs to produce the vi-\nsual inputs. While this regularizes feature mapping, it adds\nsubstantial parameter overhead and creates a new vision em-\nbedding space, risking misalignment with the LLM’s text\nembedding space. Encoder-free VLMs, like Fuyu-8B 1 and\nEVE (Diao et al., 2024), eliminate dedicated vision encoders\nbut show degraded performance (Beyer et al., 2024).\nIn contrast, ALIGNVLM maps visual features from the vi-\nsion encoder into probability distributions over the LLM’s\ntext embeddings, using them to compute a convex combi-\nnation. By leveraging the linguistic priors encoded in the\nLLM’s vocabulary, ALIGNVLM ensures that visual features\nremain within the convex hull of the text embeddings, mit-\nigating noisy or out-of-distribution inputs and enhancing\nalignment, particularly for tasks that require joint modalities\nrepresentation like multimodal document understanding.\n3. Methodology\n3.1. Model Architecture\nThe overall model architecture, shown in Figure 2, consists\nof three main components:\n(1) Vision Encoder.\nTo handle high-resolution images of\ndifferent aspect ratios, we divide each input image into mul-\ntiple tiles according to one of the predefined aspect ratios\n(e.g., 1:1, 1:2, . . . , 9:1) chosen via a coverage ratio (Lu\net al., 2024; Chen et al., 2024a). Due to limited compu-\ntational resources, we set the maximum number of tiles\nto 9. Each tile is further partitioned into 14 × 14 patches,\nprojected into vectors, and processed by a SigLip-400M vi-\nsion encoder (Zhai et al., 2023) to extract contextual visual\nfeatures.\nEach tile t ∈{1, · · · , T} is divided into Nt patches\nPt = {pt,1, · · · , pt,Nt},\nwhere pt,i is the i-th patch of tile t. The vision encoder\n1https://www.adept.ai/blog/fuyu-8b\nmaps these patches to a set of visual feature vectors\nFt = VisionEncoder(Pt)\nFt = {ft,1, · · · , ft,Nt},\nft,i ∈Rd.\nFinally, we concatenate the feature sets across all tiles into\na single output\nF = concat\n\x10\nF1, F2, · · · , FT\n\x11\n.\n(2) ALIGN Module.\nThis module aligns the visual fea-\ntures with the LLM. A linear layer W1 ∈RD×d first\nprojects the visual features F ∈RT ·Nt×d to the LLM’s\ntoken embedding space: one RD vector per token. A sec-\nond linear layer W2 ∈RV ×D (initialized from the LLM’s\nlanguage-model head) followed by a softmax, produces a\nprobability simplex Pvocab over the LLM’s vocabulary (V\ntokens)\nPvocab =\n(1)\nsoftmax(LayerNorm(W2 LayerNorm(W1F)))\nWe then use the LLM text embeddings Etext ∈RV ×D to\ncompute a weighted sum\nF′\nalign = P⊤\nvocabEtext.\n(2)\nFinally, we concatenate F′\nalign with the tokenized text em-\nbeddings to form the LLM input\nHinput = concat\n\x00F′\nalign, Etext(x)\n\x01\n,\nwhere Etext(x) is obtained by tokenizing the input text x =\n(x1, · · · , xM) and selecting the corresponding embeddings\nfrom Etext such that\nEtext(x) =\n\x02\nEtext(x1), · · · , Etext(xM)\n\x03\n.\n(3)\n(3) Large Language Model.\nWe feed the concatenated\nvision and text vectors, Hinput, into the LLM, which then\ngenerates output text auto-regressively. To demonstrate\nthe effectiveness of our alignment technique, we experi-\nment with the Llama 3.1 model family (Grattafiori et al.,\n2024). These models offer state-of-the-art performance and\npermissive licenses, making them suitable for commercial\napplications. In particular, we utilize Llama 3.2-1B, Llama\n3.2-3B, and Llama 3.1-8B.\n3.2. Motivation and relation with existing methods\nBy construction, each RD representation in F′\nalign is con-\nstrained to the convex hull of the points Etext, thus concen-\ntrating the visual features in the part of latent space that\nthe LLM can effectively interpret. Moreover, we argue that\n3\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nVision \nEncoder\nLinear\nLayer Norm\nLM Head (LLM)\nLayer Norm\nSoftmax\nWeighted \nAverage Sum\nVision Inputs\nLLM Embedding Matrix\nText Inputs\nFull Embedding\nMatrix\nSelected Text \nEmbeddings\nText \nTokenizer\nAlign Module\nLLM\nQuestion: What percentage of \nAmericans are online?\nResponse: 90%\nFigure 2: ALIGNVLM Model Architecture. The vision encoder extracts image features, which are processed to produce\nprobabilities over the LLM embeddings. A weighted average combines these probabilities with embeddings to generate\nvision input vectors. Text inputs are tokenized, and the corresponding embeddings are selected from the embedding matrix,\nwhich is then used as input to the LLM. We display the vision layers in blue , and the text layers in purple .\nour initialization of W2 to the language model head is an\ninductive bias toward recycling some of the semantics of\nthese text tokens into visual tokens. This contrasts with\npast methods that have been proposed to adapt the vision\nencoder outputs F ∈RT ·Nt×d to an F′ ∈RT ·Nt×D to be\nfed to the LLM. Here, we consider two examples in more\ndetail, highlighting these contrasts.\n(1) MLP Connector (Liu et al., 2023b) applies a linear pro-\njection with parameters WMLP ∈RD×d and bMLP ∈RD,\nfollowed by an activation function σ (e.g., ReLU)\nF′\nMLP = σ(WMLPF + bMLP).\nThese parameters are all learned from scratch, with no par-\nticular bias aligning them to text embeddings.\n(2) Visual Embedding Table (Lu et al., 2024) introduces an\nentire new set of visual embeddings EVET ∈RK×D which,\ntogether with the weights WVET ∈RK×d, specifies\nF′\nVET = softmax(WVETF)⊤EVET.\nWhen D < d, our W2W1 amounts to a low-rank version\nof WVET. There is thus much more to learn to obtain F′\nVET,\nand there is again no explicit pressure to align it with the\ntext embeddings.\n3.3. Training Datasets & Stages\nWe train our model in three stages:\nStage 1.\nThis stage focuses on training the ALIGN Mod-\nule to map visual features to the LLM’s text embeddings\neffectively. We use the CC-12M dataset (Changpinyo et al.,\n2021), a large-scale web dataset commonly used for VLM\npretraining (Liu et al., 2023b), which contains 12M image-\ntext pairs. However, due to broken or unavailable links,\nwe retrieved 8.1M pairs. This dataset facilitates the align-\nment of visual features with the text embedding space of\nthe LLM. During this stage, we train the full model, as this\napproach improves performance and stabilizes the training\nof the ALIGN Module.\nStage 2.\nThe goal is to enhance the model’s document\nunderstanding capabilities, such as OCR, document struc-\nture comprehension, in-depth reasoning, and instruction-\nfollowing. We leverage the BigDocs-7.5M dataset (Ro-\ndriguez et al., 2024a), a curated collection of license-\npermissive datasets designed for multimodal document un-\nderstanding. This dataset aligns with the Accountability,\nResponsibility, and Transparency (ART) principles (Bom-\nmasani et al., 2023; Vogus & Llans´oe, 2021), ensuring com-\npliance for commercial applications. As in Stage 1, we train\nthe full model during this stage.\nStage 3.\nTo enhance the model’s instruction-tuning ca-\npabilities, particularly for downstream tasks like question\nanswering, we further train it on the DocDownstream (Ro-\ndriguez et al., 2024a; Hu et al., 2024) instruction tuning\ndataset. In this stage, the vision encoder is frozen, focusing\ntraining exclusively on the LLM and ALIGN module.\n4. Experimental Setup\nSetup.\nWe conduct all experiments using 8 nodes of H100\nGPUs, totaling 64 GPUs. For model training, we leverage\n4\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nTable 1: Main Results on General Document Benchmarks. We compare ALIGNVLM (ours) with state-of-the-art\n(SOTA) open and closed-source instructed models, and with base models that we trained using the process described in\nSection 3.3. ALIGNVLM models outperform all Base VLM models trained in the same data regime. Our models also\nperform competitively across document benchmarks even compared with SOTA models, in which the data regime is more\ntargeted and optimized. Color coding for comparison: closed-source models , open-source models below 7B parameters ,\nopen-source models between 7-12B parameters .\nModel\nDocVQA\nVAL\nInfoVQA\nVAL\nDeepForm\nTEST\nKLC\nTEST\nWTQ\nTEST\nTabFact\nTEST\nChartQA\nTEST\nTextVQA\nVAL\nTableVQA\nTEST\nAvg. Score\nClosed-Source VLMs\n(Opaque Training Data)\nClaude-3.5 Sonnet\n88.48\n59.05\n31.41\n24.82\n47.13\n53.48\n51.84\n71.42\n81.27\n56.54\nGeminiPro-1.5\n91.23\n73.94\n32.16\n24.07\n50.29\n71.22\n34.68\n68.16\n80.43\n58.46\nGPT-4o 20240806\n92.80\n66.37\n38.39\n29.92\n46.63\n81.10\n85.70\n70.46\n72.87\n64.91\nOpen-Source Instruct VLMs\n(Semi-Opaque Training Data)\nJanus-1.3B (Wu et al., 2024a)\n30.15\n17.09\n0.62\n15.06\n9.30\n51.34\n57.20\n51.97\n18.67\n27.93\nQwen2-VL-2B (Wang et al., 2024)\n89.16\n64.11\n32.38\n25.18\n38.20\n57.21\n73.40\n79.90\n43.07\n55.84\nInternVL-2.5-2B (Chen et al., 2024b)\n87.70\n61.85\n13.14\n16.58\n36.33\n57.26\n74.96\n76.85\n42.20\n51.87\nDeepSeek-VL2-Tiny-3.4B (Wu et al., 2024b)\n88.57\n63.88\n25.11\n19.04\n35.07\n52.15\n80.92\n80.48\n56.30\n55.72\nPhi3.5-Vision-4B (Abdin et al., 2024)\n86.00\n56.20\n10.47\n7.49\n17.18\n30.43\n82.16\n73.12\n70.70\n48.19\nQwen2-VL-7B (Wang et al., 2024)\n93.83\n76.12\n34.55\n23.37\n52.52\n74.68\n83.16\n84.48\n53.97\n64.08\nLLaVA-NeXT-7B (Xu et al., 2024)\n63.51\n30.90\n1.30\n5.35\n20.06\n52.83\n52.12\n65.10\n32.87\n36.00\nDocOwl1.5-8B (Hu et al., 2024)\n80.73\n49.94\n68.84\n37.99\n38.87\n79.67\n68.56\n68.91\n52.60\n60.68\nInternVL-2.5-8B (Chen et al., 2024b)\n91.98\n75.36\n34.55\n22.31\n50.33\n74.75\n82.84\n79.00\n52.10\n62.58\nOvis-1.6-Gemma2-9B (Lu et al., 2024)\n88.84\n73.97\n45.16\n23.91\n50.72\n76.66\n81.40\n77.73\n48.33\n62.96\nLlama3.2-11B (Grattafiori et al., 2024)\n82.71\n36.62\n1.78\n3.47\n23.03\n58.33\n23.80\n54.28\n22.40\n34.04\nPixtral-12B (Agrawal et al., 2024)\n87.67\n49.45\n27.37\n24.07\n45.18\n73.53\n71.80\n76.09\n67.13\n58.03\nDocument Understanding Instructed Models\n(Instruction Tuned on BigDocs-7.5M + DocDownStream (Rodriguez et al., 2024a; Hu et al., 2024))\nQwen2-VL-2B (base+) (Qwen et al., 2025)\n57.23\n31.88\n49.31\n34.39\n31.61\n64.75\n68.60\n61.01\n47.53\n49.59\nALIGNVLM-Llama-3.2-1B (ours)\n72.42\n38.16\n60.47\n33.71\n28.66\n71.31\n65.44\n48.81\n50.29\n52.14\nALIGNVLM-Llama-3.2-3B (ours)\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.10\n58.81\nDocOwl1.5-8B (base+) (Hu et al., 2024)\n78.70\n47.62\n64.39\n36.93\n35.69\n72.65\n65.80\n67.30\n49.03\n57.56\nLlama3.2-11B (base+) (Grattafiori et al., 2024)\n78.99\n44.27\n67.05\n37.22\n40.18\n78.04\n71.40\n68.46\n56.73\n60.26\nALIGNVLM-Llama-3.1-8B (ours)\n81.18\n53.75\n63.25\n35.50\n45.31\n83.04\n75.00\n64.60\n64.33\n62.88\nthe MS-Swift framework (Zhao et al., 2024) for its flexibil-\nity. Additionally, we utilize the DeepSpeed framework (Am-\ninabadi et al., 2022), specifically the ZeRO-3 configuration,\nto optimize efficient parallel training across multiple nodes.\nDetailed hyperparameters are outlined in Appendix A.1.\nBaselines.\nOur work focuses on architectural innovations,\nso we ensure that all baselines are trained on the same\ndatasets. To enable fair comparisons, we evaluate our mod-\nels against a set of Base VLMs fine-tuned on the same\ninstruction-tuning tasks (Stages 2 and 3) as our models,\nusing the BigDocs-7.5M and BigDocs-DocDownstream\ndatasets. This approach ensures consistent training data,\navoiding biases introduced by the Instruct versions of\nVLMs, which are often trained on undisclosed instruction-\ntuning datasets. Due to the scarcity of recently released\npublicly available Base VLMs, we primarily compare our\nmodel against the following Base VLMs of varying sizes:\nQwen2-VL-2B (Wang et al., 2024), DocOwl1.5-8B (Hu\net al., 2024), and LLama 3.2-11B (Grattafiori et al., 2024).\nFor additional context, we also include results from\nthe Instruct versions of recent VLMs of different sizes:\nPhi3.5-Vision-4B (Abdin et al., 2024), Qwen2-VL-2B and\n7B (Wang et al., 2024), LLaVA-NeXT-7B (Liu et al.,\n2024), InternVL2.5-2B and 8B (Chen et al., 2024b), Janus-\n1.3B (Wu et al., 2024a), DeepSeek-VL2-Tiny (Wu et al.,\n2024b), Ovis1.6-Gemma-9B (Lu et al., 2024), Llama3.2-\n11B (Grattafiori et al., 2024), DocOwl1.5-8B (Hu et al.,\n2024), and Pixtral-12B (Agrawal et al., 2024).\nEvaluation Benchmarks.\nWe evaluate our models on a\ndiverse range of document understanding benchmarks that\nassess the model’s capabilities in OCR, chart reasoning,\ntable processing, or form comprehension. In particular, we\nemploy the VLMEvalKit (Duan et al., 2024) framework\nand report the results on the following popular benchmarks:\n5\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nTable 2: Impact of Connector Designs on VLM Performance: We present the results of experiments evaluating different\nconnector designs for conditioning LLMs on visual features. Our proposed ALIGN connector is compared against a basic\nMulti-Layer Perceptron (MLP), the Perceiver Resampler, and Ovis. The results demonstrate that ALIGN consistently\noutperforms these alternatives across all benchmarks.\nModel\nDocVQA\nVAL\nInfoVQA\nVAL\nDeepForm\nTEST\nKLC\nTEST\nWTQ\nTEST\nTabFact\nTEST\nChartQA\nTEST\nTextVQA\nVAL\nTableVQA\nTEST\nAvg. Score\nLlama-3.2-3B-MLP\n71.46\n37.56\n62.07\n33.36\n28.94\n73.22\n66.48\n53.56\n50.96\n53.06\nLlama-3.2-3B-Perciever R.\n69.08\n34.13\n57.08\n31.75\n27.95\n71.93\n65.16\n51.33\n47.76\n50.68\nLlama-3.2-3B-Ovis\n74.68\n42.11\n58.02\n33.50\n33.13\n76.67\n67.92\n52.60\n53.93\n54.72\nLlama-3.2-3B-ALIGN (ours)\n79.63\n44.53\n63.49\n35.25\n38.59\n78.51\n71.88\n57.38\n60.10\n58.81\nDocVQA (Mathew et al., 2021b), InfoVQA (Mathew et al.,\n2021a), DeepForm (Svetlichnaya, 2020), KLC (Stanisławek\net al., 2021), WTQ (Pasupat & Liang, 2015), TabFact (Chen\net al., 2020), ChartQA (Masry et al., 2022), TextVQA (Singh\net al., 2019), and TableVQA (Kim et al., 2024).\n5. Results\n5.1. Main Results\nTable 1 presents the performance of ALIGNVLM com-\npared to state-of-the-art (SOTA) open- and closed-source\ninstructed models, as well as baseline Base VLMs fine-tuned\nin the same instruction-tuning setup. The results demon-\nstrate that ALIGNVLM consistently outperforms all Base\nVLMs within the same size category and achieves com-\npetitive performance against SOTA Instruct VLMs despite\nbeing trained on a more limited data regime. Below, we\nprovide a detailed analysis.\nALIGNVLM vs. Base VLMs.\nOur ALIGNVLM mod-\nels, based on Llama 3.2-1B and Llama 3.2-3B, significantly\noutperform the corresponding Base VLM, Qwen2-VL-2B,\nby up to 9.22%. Notably, ALIGNVLM-Llama-3.2-3B sur-\npasses DocOwl1.5-8B, which has 4B more parameters,\ndemonstrating the effectiveness of ALIGN in enhancing mul-\ntimodal capabilities compared to traditional shallow fusion\nmethods (e.g., MLPs). Furthermore, our 8B model achieves\na 2.62% improvement over Llama3.2-11B despite sharing\nthe same Base LLM, Llama3.1-8B. Since all models in this\ncomparison were trained on the same instruction-tuning\nsetup, this experiment provides a controlled evaluation, iso-\nlating the impact of architectural differences rather than\ndataset biases. Consequently, these results suggest that\nALIGNVLM outperforms VLMs with shallow fusion tech-\nniques and surpasses parameter-heavy deep fusion VLMs,\nsuch as Llama3.2-11B, while maintaining a more efficient\narchitecture.\nALIGNVLM vs. Instruct VLMs.\nEven as open-source\nInstruct models are trained on significantly larger, of-\nten undisclosed instruction-tuning datasets, ALIGNVLM\nachieves superior performance. For instance, ALIGNVLM-\nLlama-3.2-3B (58.81%) outperforms all instructed VLMs in\nits size category, surpassing its closest competitor, Qwen2-\nVL-2B (55.84%), by 2.97%. Additionally, our 8B model\noutperforms significantly larger models such as Llama 3.2-\n11B and PixTral-12B by substantial margins. It also sur-\npasses InternVL-2.5-8B and performs competitively with\nQwen2-VL-7B, though a direct comparison may not be\nentirely fair since Qwen2-VL-7B was trained on an undis-\nclosed instruction-tuning dataset. Finally, ALIGNVLM also\nexhibits comparable performance to closed-source models\nlike GeminiPro-1.5 and GPT4o.\nOverall, these results validate the effectiveness of ALIGN\nand establish ALIGNVLM as a state-of-the-art model for\nmultimodal document understanding.\n5.2. Impact of Connector Designs on VLM Performance\nTo assess the effectiveness of our ALIGN module, we com-\npare it against three different and widely used shallow fusion\nVLM connectors: MLP, Perceiver Resampler, and Ovis. The\nresults in Table 2 show that ALIGN consistently outperforms\nall alternatives, demonstrating its superiority both in align-\ning visual and textual modalities and in multimodal docu-\nment understanding. MLP and Perceiver Resampler achieve\nthe lowest performance, 53.06% and 50.68%, respectively,\ndue to their direct feature projection, which lacks an explicit\nmechanism to align visual features with the LLM’s text\nspace, leading to misalignment. Ovis introduces a separate\nvisual embedding table, but this additional complexity does\nnot significantly improve alignment, yielding only 54.72%\naccuracy. In contrast, ALIGN ensures that visual features\nremain within the convex hull of the LLM’s text latent space,\nleveraging the linguistic priors of the LLM to enhance align-\nment and mitigate noisy embeddings. This design leads to\nthe highest performance (58.81%), establishing ALIGN as\nthe most effective connector for integrating vision and lan-\nguage in multimodal document understanding. We provide\nsome example outputs of the Llama-3.2-3B models with\ndifferent connector designs in Appendix A.3.\n6\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nFigure 3: Probability distribution over the LLM text\ntokens, showing dense probabilities and higher values for\ntokens associated with white space in document images.\n5.3. Probability Distribution over Text Tokens Analysis\nTo better understand the behavior of ALIGN, we examine\nthe probability distribution, Pvocab in Eq (1), over the LLM’s\ntext vocabulary generated from visual features. Specifically,\nwe process 100 document images through the vision en-\ncoder and ALIGN, then average the resulting probability\ndistributions across all image patches. The final distribution\nis shown in Figure 3. As illustrated, the distribution is dense\n(rather than sparse), with the highest probability assigned to\na single token being 0.0118. This can be explained by the\nvision feature space being continuous and of much higher\ncardinality than the discrete text space. Indeed, while the\nLLM has 128K distinct vocabulary tokens, an image patch\n(e.g., 14×14 pixels) contains continuous, high-dimensional\ninformation that cannot be effectively mapped to a single or\na few discrete tokens.\nFurthermore, we observe that tokens on the left side of the\ndistribution in Figure 3 have higher probabilities than the\nrest. Upon investigation, we found that these tokens corre-\nspond to patches that are predominantly white – a common\nfeature in document images. Further analysis of the associ-\nated text tokens reveals that they predominantly consist of\npunctuation marks, as illustrated further in Appendix A.2.\nThis suggests that the model repurposes punctuation marks\nto represent whitespaces. This may be attributed to the fact\nthat both punctuation and whitespaces act as structural cues\nand separators. Other possibilities include whitespaces be-\ning rarely directly-required to perform a task, and LLMs\nmay pay less specific attention to common tokens such as\npunctuation.\n5.4. Pixel-Level Tasks Analysis\nTo rigorously evaluate the ability of vision-language mod-\nels to integrate fine-grained visual and textual pixel-level\ncues, we test our model on the VCR benchmark (Zhang\n0\n20\n40\n60\nExact Match (%)\nVCR EN Hard\nVCR EN Easy\n48.07\n65.84\n37.89\n51.43\nLlama-3.2-3B-Align (Ours)\nLlama-3.2-3B-MLP\nFigure 4: Comparison of Llama-3.2-3b-ALIGN and Llama-\n3.2-3B-MLP on the Easy and Hard VCR tasks.\net al., 2024), which requires the model to recover partially\noccluded texts with pixel-level hints from the revealed parts\nof the text. This task challenges VLM’s alignment of text\nand image in extreme situations. Current state-of-the-art\nmodels like GPT-4V (OpenAI et al., 2023), Claude 3.5 Son-\nnet (Anthropic, 2024), and Llama-3.2 (Dubey et al., 2024)\nsignificantly underperform humans on hard VCR task due to\ntheir inability to process subtle pixel-level cues in occluded\ntext regions. These models frequently discard critical vi-\nsual tokens during image tokenization on semantic priors,\noverlooking the interplay between partial character strokes\nand contextual visual scenes. To evaluate performance on\nVCR, we modify our Stage 3 SFT dataset composition by\nreplacing the exclusive use of DocDownstream with a 5:1\nblended ratio of DocDownstream and VCR training data.\nThis adjustment enables direct evaluation of our architecture\nALIGN’s ability to leverage pixel-level character cues.\nFrom the experimental outcomes, it is evident that ALIGN-\nVLM consistently outperforms the MLP Connector Model\nacross both easy and hard settings of the pixel-level VCR\ntask (see Figure 4), with improvements ranging from 10.18%\non the hard setting to 14.41% on the easy setting.\nWe provide a case study on VCR in Figure 5, featuring four\nrepresentative examples. In Figure 5a, it is evident that the\nMLP connector model fails to capture semantic consistency\nas effectively as ALIGNVLM. The phrase “The commune\nfirst census in written history in” (where the words in italics\nare generated by the model while the rest are in the image)\nis not as semantically coherent as the phrase generated by\nALIGN “The commune first appears in written history in”.\nBeyond the issue of semantic fluency, in Figure 5b we also\nobserve that ALIGNVLM successfully identifies the uncov-\nered portion of the letter “g” in “accounting” and uses it as\na pixel-level hint to infer the correct word. In contrast, the\nMLP model fails to effectively attend to this crucial detail.\n7\n\nALIGNVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding\nGT:\n(appears in written\nhistory in)\nMLP:\n(census in written his-\ntory in) ✗\nALIGN\n(appears in written\nhistory in) ✓\n(a) Positive Example 1\nGT:\n(the system used for\nassigning)\nMLP:\n(the system used for\naccounting) ✗\nALIGN\n(the system used for\nassigning) ✓\n(b) Positive Example 2\nGT:\n(mines situated near\nLlanengan on)\nMLP:\n(mines situated near\nLlanengan on) ✓\nALIGN (mines situated near\nLlanongan on) ✗\n(c) Negative Example 1\nGT:\n(Gorden County\nis home to)\nMLP:\n(Gorden County\nis home to) ✓\nALIGN\n(Garden County\nis home to) ✗\n(d) Negative Example 2\nFigure 5: Case Study for Pixel-Level Tasks. We provide examples of our proposed ALIGN connector compared with a the\nMulti-Layer Perceptron (MLP) connector. The ALIGN connector tends to better map visual elements to common words. GT\nis the ground truth.\nFigures 5c and 5d show examples where ALIGNVLM fails\non the VCR task. These carefully picked instances show\nthat our method mistakes names of landmarks with common\nwords when the two are very similar. As seen in the exam-\nples, ALIGNVLM mistakes “Llanengan” for “Llanongan”\nand “Gorden” for “Garden”. In both instances, the pairs\ndiffer by one character, indicating perhaps that ALIGNVLM\ntends to align vision representations to more common to-\nkens in the vocabulary. One approach that would potentially\nmitigate such errors would be to train ALIGNVLM with\nmore contextually-relevant data.\n5.5. Robustness to Noise Analysis\nTo evaluate the robustness of our ALIGN connector to noisy\nvisual features, we conduct an experiment where random\nGaussian noise is added to the visual features produced by\nthe vision encoder before passing them into the connector.\nSpecifically, given the visual features F ∈RN×d output\nby the vision encoder (where N is the number of feature\nvectors and d is their dimensionality), we perturbed them as\neF = F + N,\nN ∼N(0, σ = 3).\nTable 3: Robustness to Noise. Comparison of Avg. Scores\nwith and without Gaussian noise (σ = 3), including perfor-\nmance drop (∆).\nModel\nWithout Noise\nWith Noise\nDrop (∆)\nLlama-3.2-3B-MLP\n53.06\n27.52\n↓25.54\nLlama-3.2-3B-ALIGN (ours)\n58.81\n57.14\n↓1.67\nAs shown in Table 3, our ALIGN connector demonstrates\nhigh robustness to noise, with only a 1.67% average drop in\nperformance. In contrast, the widely adopted MLP connec-\ntor suffers a significant performance degradation of 25.54%,\nhighlighting its vulnerability to noisy inputs. These em-\npirical results support our hypothesis that leveraging the\nknowledge encoded in the LLM’s text embeddings and con-\nstraining the visual features within the convex hull of the\ntext latent space act as a regularization mechanism, reducing\nthe model’s sensitivity to noisy visual features.\n6. Conclusion\nWe introduce ALIGN, a novel connector designed to align\nvision and language latent spaces in vision-language mod-\nels (VLMs), specifically enhancing multimodal document\nunderstanding. By improving cross-modal alignment and\nminimizing noisy embeddings, our models, ALIGNVLM,\nwhich leverage ALIGN, achieve state-of-the-art performance\nacross diverse document understanding tasks. This includes\noutperforming base VLMs trained on the same datasets and\nopen-source instruct models trained on undisclosed data.\nExtensive experiments and ablations validate the robustness\nand effectiveness of ALIGN compared to existing connec-\ntor designs, establishing it as a significant contribution to\nvision-language modeling. Future work will explore train'),
                Paper(arxiv_id='2502.01534', authors=['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], published_at=datetime.datetime(2025, 2, 4, 1, 4, 33, 630000, tzinfo=datetime.timezone.utc), title='Preference Leakage: A Contamination Problem in LLM-as-a-judge', summary='Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between data\ngenerator LLM and judge LLM: being the same model, having an inheritance\nrelationship, and belonging to the same model family. Through extensive\nexperiments, we empirically confirm the bias of judges towards their related\nstudent models caused by preference leakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that preference leakage is a pervasive\nissue that is harder to detect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings imply that preference leakage\nis a widespread and challenging problem in the area of LLM-as-a-judge. We\nrelease all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage.', upvotes=28, thumbnail=None, content='Preference Leakage: A Contamination Problem in LLM-as-a-judge\nDawei Li * 1 Renliang Sun * 2 Yue Huang 3 Ming Zhong 4 Bohan Jiang 1\nJiawei Han 4 Xiangliang Zhang 3 Wei Wang 2 Huan Liu 1\nAbstract\nLarge Language Models (LLMs) as judges and\nLLM-based data synthesis have emerged as\ntwo fundamental LLM-driven data annotation\nmethods in model development. While their com-\nbination significantly enhances the efficiency of\nmodel training and evaluation, little attention has\nbeen given to the potential contamination brought\nby this new model development paradigm. In this\nwork, we expose preference leakage, a contami-\nnation problem in LLM-as-a-judge caused by the\nrelatedness between the synthetic data generators\nand LLM-based evaluators. To study this issue,\nwe first define three common relatednesses\nbetween data generator LLM and judge LLM:\nbeing the same model, having an inheritance\nrelationship, and belonging to the same model\nfamily.\nThrough extensive experiments, we\nempirically confirm the bias of judges towards\ntheir related student models caused by preference\nleakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that prefer-\nence leakage is a pervasive issue that is harder to\ndetect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings\nimply that preference leakage is a widespread\nand challenging problem in the area of LLM-\nas-a-judge.\nWe release all codes and data at:\nhttps://github.com/David-Li0406/\nPreference-Leakage1.\n1. Introduction\nRecent\nadvancements\nin\nLarge\nLanguage\nModels\n(LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong\net al., 2024; Zhang et al., 2024a) have empowered various\n*Equal contribution 1Arizona State University 2University of\nCalifornia, Los Angeles 3University of Notre Dame 4University\nof Illinois Urbana Champaign. Correspondence to: Dawei Li\n<daweili5@asu.edu>.\n1More resources on LLM-as-a-judge are on the website:\nhttps://llm-as-a-judge.github.io/\ndownstream tasks and applications. However, this also\nposes substantial challenges to the automatic evaluation\nof these models. Representatively, LLM-based AI agents’\nfocus transfer from traditional natural language processing\ntasks (Yang et al., 2023; Zhang et al., 2023) to real-world\n(Liu et al., 2023b; Huang et al., 2023), open-ended response\ngeneration (Wu et al., 2024), which greatly limits the\napplicability of traditional n-gram matching methods (e.g.,\nBLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu\net al., 2016; Reiter, 2018) or model-based evaluators (Zhang\net al., 2020; Zhong et al., 2022) for evaluation.\nTo address these challenges, the paradigm of LLM-as-a-\njudge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a;\nZhong et al., 2024; Li et al., 2025) has been proposed, de-\nsigned to leverage LLM as evaluators to assess response\nquality. By combining powerful LLMs with well-designed\nprompting strategies, LLM-as-a-judge enables human-like\nevaluation of long-form and open-ended generation in a\nmore cost-efficient and scalable manner. However, recent\nstudies point out some weaknesses of such assessment. For\ninstance, Ye et al. (2024) explores various biases and vulner-\nabilities of LLM-as-a-judge, highlighting the importance of\ndeveloping a reliable and fair LLM-based evaluation system.\nIn this work, we aim to introduce another concern in LLM-\nas-a-Judge–Preference Leakage. This issue arises when the\nLLMs used for data generation and evaluation are closely re-\nlated, as illustrated in Figure 1. Synthetic data generated by\nLLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c)\nhas become a cornerstone of model training (Lee et al.,\n2025). When combined with LLM-as-a-Judge, they offer\nsignificant efficiency gains in model development. However,\nlimited attention has been given to the potential contami-\nnation that occurs when the generator and evaluator LLMs\nshare a close relationship. During our preliminary study,\nwe find this issue is particularly pervasive in popular LLM-\nas-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al.,\n2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant\nstudies (more details can be found in Appendix A), due to\nthe common reliance on the most advanced LLMs, such\nas GPT-4 (Achiam et al., 2023), for both data synthesis\nand evaluation to ensure the highest quality outputs. In our\nwork, we reveal this relatedness—akin to the overlap be-\ntween training data and evaluation sets in traditional data\n1\narXiv:2502.01534v1  [cs.LG]  3 Feb 2025\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ncontamination—would introduce a systematic bias of judge\nLLMs towards their related student models (i.e., the model\ndistilled by the data generator which is related to the judge).\nCompared to other biases in LLM-as-a-Judge, such as length\nbias or egocentric bias (Ye et al., 2024; Panickssery et al.,\n2024), preference leakage is subtler and more challenging\nto detect, especially given that most LLMs do not disclose\ntheir training data.\nTo investigate and reveal the preference leakage problem,\nwe first define three relatednesses between data generator\nLLM and judge LLM: being the same model, having an\ninheritance relationship, and belonging to the same model\nfamily. Each of these scenarios is commonly encountered\nin real-world applications. Then, we pose and answer three\ncore research questions about preference leakage:\n• RQ1: Does preference leakage introduce systematic\nbiases in LLM-based evaluation? To answer it, we\nconduct experiments with various LLM baselines in two\nwidely recognized LLM-as-a-judge benchmarks, also in-\ntroduce the preference leakage score to quantify the bias\ncaused by preference leakage. The analysis results sug-\ngest an obvious bias of judging LLMs toward their related\nstudent models.\n• RQ2: What is the severity of preference leakage under\nvarious scenarios? We conduct experiments under vari-\nous relatedness settings, tuning techniques, and data mix-\ning strategies to address it, finding that preference leakage\nconsistently affects judge LLMs. Moreover, the severity\nof preference leakage correlates with the degree of relat-\nedness between the data generator and LLM judges, as\nwell as the proportion of synthetic data.\n• RQ3: What are the underlying mechanisms causing\npreference leakage? For this question, we analyze LLMs’\nrecognition capabilities on their related student models’\ngeneration as well as the distribution of bias across differ-\nent question types and judgment dimensions. The analysis\nreveals that preference leakage is a subtle, hard-to-detect\nissue, particularly affecting subjective questions and judg-\nment dimensions.\nTo summarize, our contributions in this work are as follows:\n• We introduce preference leakage, a contamination issue\narising from the relatedness between the data generator\nand judge LLMs.\n• We conduct extensive experiments across various LLMs\nand benchmarks to study how and to what extent the\npotential bias brought by preference leakage influences\njudgment.\n• Our further analysis reveals that preference leakage is\nprevalent in diverse scenarios and difficult for judge LLMs\nto detect, providing valuable insights for future research\non this challenging issue.\n2. Related Work\n2.1. LLM-as-a-Judge\nLLM-as-a-Judge, introduced by Zheng et al. (2023), lever-\nages LLMs to automatically evaluate responses and assign\nrewards. This approach has gained widespread adoption\nin areas such as model alignment (Zhang et al., 2024d)\nand benchmarking (Liu et al., 2023a; Zhang et al., 2024b;\nGao et al., 2023; Zhong et al., 2024), driving significant\nprogress in the field. Building on this concept, Zhuge et al.\n(2024) proposed Agent-as-a-Judge, where agentic systems\nare employed to evaluate other agentic systems. Addition-\nally, Prometheus, a series of open-source LLMs tailored for\nLLM-as-a-Judge (Kim et al., 2023; 2024), addresses the\nprohibitive costs associated with proprietary models, further\ndemocratizing the technology.\nDespite its promising potential, recent studies have high-\nlighted the vulnerabilities and limitations of LLM-as-a-\nJudge. Notable concerns include biases during evaluation.\nFor example, Zheng et al. (2023) identify position bias,\nwhere LLMs may favor responses based on their order in\nthe input, thereby compromising fairness. Other studies (Ye\net al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al.,\n2023; Huang et al., 2024) further emphasize the risks of\nevaluation biases. Thakur et al. (2024) assessed the judg-\nment capabilities of LLM judges, finding that only the most\nadvanced models align reasonably well with human evalu-\nators. Moreover, a recent study (Shi et al., 2024) revealed\nthe susceptibility of LLM-as-a-Judge to adversarial attacks,\nleading to incorrect judgments. In this paper, we explore an-\nother critical vulnerability of LLM-as-a-Judge—preference\nleakage—which poses additional risks to the reliability of\nthis evaluation paradigm.\n2.2. Data Leakage\nThe possible overlap between training data and evaluation\nbenchmarks has become a central issue, since LLMs are usu-\nally trained on extensive web corpora (Dodge et al., 2021).\nThis phenomenon, known as data leakage, can artificially\nimprove the performance of LLMs and undermine the re-\nliability of the assessment (Deng et al., 2024a; Jiang et al.,\n2024b).\nSeveral researchers have proposed methods to detect and\nmitigate data contamination. Deng et al. (2024b) proposed\na retrieval-based approach to assess the degree of overlap\nbetween pre-training text and benchmark data. Golchin &\nSurdeanu (2023) have developed “guided instruction” to\nflag contaminated instances. Dong et al. (2024b) proposed\nthe CDD method to identify peaks in the output distribution\nto detect data contamination. Several studies analyze data\nleakage for specific LLMs (Balloccu et al., 2024) and report\ncontamination such as cross-language contamination (Yao\n2\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nEvaluation\nTestset\nTraining\nCorpus\nData leakage\nTrain\nTraining\nCorpus\nEvaluation \nTestset\nEvaluate\nData Leakage!\nSynthetic \nData\nData \nGenerator\nTrained\nModel\nTrained\nModel\nTrained\nModel\nJudge\nJudge\nModel\nPreference Leakage!\nRelatedness \nOverlap\nLLM for Data \nSynthesis\nLLM-as-\na-Judge\nPreference leakage\nTrain\n(1). Same model\n(2). Inheritance\nSynthetic \ndata\n(3). Within the \nsame model family\nSynthesize\nFigure 1. Overview of preference leakage. We make a comparison between data leakage and preference leakage and present three types of\nrelatedness: being the same model, having an inheritance relationship and belonging to the same model family.\net al., 2024) and task contamination (Li & Flanigan, 2024)\nthat can evade traditional detection methods. To address data\ncontamination issues, Ni et al. (2024) have used web user\nquery detection and benchmark mixture. White et al. (2024)\nuse the most recent information to update the problem.\n3. Preference Leakage\nIn this section, we first provide the formal definition of data\ncontamination as the preliminary (Section 3.1). Based on\nthe concept, we demonstrate how LLM-based data synthesis\nand evaluation can lead to the evolving preference leakage\nproblem (Section 3.2).\n3.1. Preliminary: Data Leakage\nData leakage, also known as data contamination, refers to\nthe inadvertent inclusion of information from the evalua-\ntion benchmarks into the training corpus thus creating an\noverlap between training and testing sets (Kaufman et al.,\n2012). This overlap would significantly influence the eval-\nuation fairness by inflating the models’ performance since\nthe model has prior exposure to and memorized information\nthat it’s expected to generalize during testing (Elangovan\net al., 2021).\nFormally, let T represent the training corpus and E be the\nevaluation set during test time. Data contamination occurs\nif:\nT ∩E ̸= ∅,\n(1)\nwhere ∩denotes the intersection between the two sets. Such\noverlap violates the fundamental assumption that training\nand testing datasets should be disjoint to ensure an unbiased\nassessment of the model’s generalization ability.\n3.2. From Data Leakage to Preference Leakage\nWith the advent of LLMs, synthetic data generated by these\nmodels (Tan et al., 2024) has been widely adopted in var-\nious stages of model training, including pre-training, rein-\nforcement learning with AI feedback (RLAIF) and super-\nvised fine-tuning. Concurrently, the concept of LLM-as-\na-judge has emerged, where LLMs are employed to auto-\nmate the evaluation process. While these LLM-as-an-oracle\napproaches reduce human effort in data annotation, signif-\nicantly enhancing the efficiency and scalability of model\ntraining and evaluation, they also blur the lines between\nmodels and data, introducing evolving challenges (Shu-\nmailov et al., 2024; Dai et al., 2024).\nIn this work, we examine the evolving contamination prob-\nlem brought by LLM-as-a-oracle and formally propose the\nconcept of preference leakage. This refers to a situation\nin which the LLMs used for synthetic data generation and\nevaluation are related. Formally, we define this as:\nLLMG ∩LLMJ ̸= ∅,\n(2)\nwhere LLMG and LLMJ denote the LLMs used for train-\ning data generation and evaluation. ∩represents the related-\nness between the two (sets of) LLMs. This relatedness may\ninvolve:\n• Being the same model: the data generator and evaluator\nare the same model:\nLLMG = LLMJ.\n(3)\n• Inheritance relationship: one model is trained on syn-\nthetic data generated by the other:\nLLMG = Inherit(LLMJ),\n(4)\nLLMJ = Inherit(LLMG).\n(5)\n3\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n• Within the same model family: the data generator and\nevaluator belong to the same model family (e.g., GPT\nfamily (Achiam et al., 2023) and Gemini family (Team\net al., 2024)):\nLLMG, LLMJ ∈FX.\n(6)\nDue to this relatedness, the preference of the judge models\n(e.g., format, style and wording) can be leaked to the student\nmodels through the synthetic data, resulting in non-trivial\nbias from the judge LLMs during the test time.\n4. Main Experiment\n4.1. Experiment Setup\nModels. We choose three powerful LLMs as data generator/\njudge models. They are GPT-4o-2024-11-20 (Achiam et al.,\n2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA-\n3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student\nmodel, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and\nQwen-2.5-14B (Yang et al., 2024). To avoid potential prefer-\nence leakage due to distilling data from other LLMs during\nthe instruction-tuning process, we choose to use the -PRE-\nTRAINED version rather than the -INSTRUCT version of\nthese student models.\nEvaluation Datasets. We choose two representative pair-\nwise evaluation datasets, Arena-Hard (Li et al., 2024e)\nand AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the\ntrained student models. Arena-Hard includes 500 challeng-\ning questions in English. Additionally, the evaluation agree-\nment between Arena-Hard and Chatbot Arena (Zheng et al.,\n2023)’s hard prompts achieved a 96.7% Spearman corre-\nlation, demonstrating the consistency of Arena-Hard with\nhuman preferences (Li et al., 2024e). AlpacaEval 2.0 is an\nimproved evaluation method based on AlpacaEval (Li et al.,\n2023) and contains 805 questions. Compared to version 1.0,\nAlpacaEval 2.0 significantly reduces the effect of text length\non the evaluation results.\nImplementation Details. In our main experiment, we ex-\namine the preference leakage introduced by using the same\ndata generator and evaluator in supervised fine-tuning (SFT).\nWe will discuss other relatedness and learning methods in\nSection 5. To obtain synthetic datasets, We first randomly\nsample 30,000 prompts from the Ultrafeedback dataset (Cui\net al., 2024). The Ultrafeedback dataset includes instruc-\ntions from several publicly available high-quality datasets\nsuch as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al.,\n2023), and Evol-Instruct (Xu et al., 2023). For each data gen-\nerator model, we provide these prompts for them to produce\nsynthetic responses, resulting in three synthetic instruction\ndatasets. We then use each dataset to supervised fine-tune\nthe student model, obtaining three different versions for each\nbaseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-Gemini-\n1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each\ntwo student models and obtain three model pairs. For each\nmodel pair, we perform the pairwise comparison using the\nthree judge models respectively.\nMetrics & Annotation Based on our hypothesis, preference\nleakage would lead to bias of judge LLMs towards their\nrelated student models. Following this principle, we design\nthe preference leakage score PLS(i, j) to measure the bias\nin model pair (i, j) caused by preference leakage:\nPLS(i, j) =\n\x10\nWR(i,i)−AVG(i,j)\nAVG(i,j)\n\x11\n+\n\x10\nWR(j,j)−AVG(j,i)\nAVG(j,i)\n\x11\n2\n,\n(7)\nAVG(i, j) = WR(i, i) + WR(i, j)\n2\n.\n(8)\nHere WR(i, j) represents the win-rate score from judge\nmodel i to student model j. Intuitively, a large preference\nleakage score indicates that the two judge models demon-\nstrate strong bias toward their related student models, sug-\ngesting a significant preference leakage phenomenon.\nWhile our proposed preference leakage score quantifies the\ndegree of preference leakage in each model pair, we also\nperform manual annotation to assess the preference leakage\nin each individual model. We randomly select 100 questions\nfrom AlpacaEval 2.0 and have three well-trained annota-\ntors perform pairwise comparisons independently for each\nresponse pair. After the annotation, the majority voting is\napplied to each response pair to get the final annotation\nresults.\nMore details about model training, metric explanation, and\nannotation process can be found in Appendix B.\nModel\nData Generator/ Judge Pair\nArena-Hard\nAlpacaEval 2.0\nAvg.\nGPT-4o & Gemini-1.5\n28.7%\n18.4%\n23.6%\nGPT-4o & LLaMA-3.3\n-6.7%\n1.4%\n-2.7%\nMistral-7B\nLLaMA-3.3 & Gemini-1.5\n13.1%\n19.8%\n16.4%\nGPT-4o & Gemini-1.5\n37.1%\n18.6%\n27.9%\nGPT-4o & LLaMA-3.3\n1.0%\n2.3%\n1.7%\nQwen-2.5-14B\nLLaMA-3.3 & Gemini-1.5\n25.4%\n18.4%\n21.9%\nTable 1. Preference leakage score result on Arena-Hard and Al-\npacaEval 2.0. The blue background indicates a negative prefer-\nence leakage score value and the purple background indicates a\npositive value. The deeper the color, the larger the absolute value.\n4.2. Main Results\nIn our main experiment, we aim to provide insights into\nRQ1.\nPreference leakage exists in most model pairs. The origi-\nnal judgment results from Arena-Hard and AlpacaEval 2.0,\nalong with the calculated preference leakage scores, are\nshown in Figure 2, Figure 3, and Table 1. As the results\ndemonstrate, in most model pairs (except Mistral-GPT-4o vs\n4\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n18.2%\n39.8%\n42.0%\n27.4%\n43.8%\n28.8%\n38.4%\n34.6%\n27.0%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n46.2%\n42.7%\n11.1%\n50.4%\n35.0%\n14.6%\n55.8%\n27.0%\n17.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n9.2%\n31.4%\n59.4%\n14.6%\n30.0%\n55.4%\n22.2%\n30.8%\n47.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n22.0%\n33.5%\n44.5%\n28.8%\n50.2%\n21.6%\n49.8%\n29.0%\n21.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n52.1%\n40.7%\n7.2%\n39.0%\n51.8%\n9.2%\n57.4%\n29.6%\n13.0%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n10.0%\n29.4%\n60.6%\n16.4%\n48.4%\n35.2%\n24.6%\n30.0%\n44.4%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nTie\nModel B Wins\nFigure 2. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n36.8%\n63.2%\n49.5%\n50.5%\n55.1%\n44.9%\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n65.8%\n34.2%\n60.3%\n39.7%\n61.6%\n38.4%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n22.6%\n77.4%\n39.5%\n60.5%\n43.1%\n56.9%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n39.3%\n60.7%\n52.4%\n47.6%\n57.8%\n42.2%\nJudge Model\nQwen-GPT4o vs Qwen-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n63.3%\n36.7%\n59.3%\n40.7%\n61.5%\n38.5%\nQwen-GPT4o vs Qwen-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n26.2%\n73.8%\n42.9%\n57.1%\n50.1%\n49.9%\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5\n(a). Mistral-7B\n(b). Qwen-2.5-14B\nModel A Wins\nModel B Wins\nFigure 3. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in\nAlpacaEval 2.0.\nMistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-\n3.3), the judge LLMs exhibit a strong preference toward\ntheir related student models, leading to large positive val-\nues in the preference leakage scores. This finding suggests\nthat preference leakage, along with the resulting bias, is\nwidespread in SFT when the data generator and evaluator\nare the same.\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n73.6%\n8.8%\n17.6%\n79.5%\n1.7%18.8%\nLLaMA-2 vs Others\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGPT-4\n76.2%\n17.9% 6.0%\n79.8%\n20.2%0.0%\nJudge Model\nLLaMA-2 vs Claude-v1\nModel A Wins\nTie\nModel B Wins\nFigure 4. Comparison between GPT-4 and human’s judgment for\nLLaMA-2 from MTBench.\nEvaluators’ bias towards certain LLMs can be inherited\nby its student models. From Figure 2 and Figure 3, we find\nan obvious preference of GPT-4o towards Mistral/ Qwen-\nLLaMA-3.3 and this leads to the low preference leakage\nscore in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and\nQwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate\nthe source of this preference, we examine whether the GPT-\n4 evaluator has a bias toward LLaMA series models. Using\nthe MTBench (Zheng et al., 2023) dataset, which includes\npairwise comparison judgments from both humans and GPT-\n4, we compare GPT-4’s and human evaluators’ judgments\non LLaMA-2 vs other models (including Vicuna, Alpaca,\nGPT-3.5, and GPT-4, which are preferred by GPT-4 due\nto preference leakage or egocentric bias) and LLaMA-2 vs\nClaude. The results, shown in Figure 4, reveal a clear pref-\nerence for LLaMA-2 by GPT-4. Consequently, we conclude\nthat evaluators’ bias can be inherited. In this case, GPT-4’s\nbias toward LLaMA has been passed on to LLaMA’s stu-\ndent models. This inheritance, combined with the opaque\ntraining data of LLMs, makes preference leakage a more\ncomplex and challenging problem.\nModel pairs with similar performance tend to have more\n5\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\nHuman\nGemini-1.5\nLLaMA-3.3\nGPT-4o\n53.0%\n47.0%\n40.2%\n59.8%\n49.4%\n50.6%\n58.4%\n41.6%\nJudge Model\nMistral-GPT4o vs Mistral-Gemini-1.5\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n62.0%\n38.0%\n76.2%\n23.8%\n72.1%\n27.9%\n67.8%\n32.2%\nMistral-GPT4o vs Mistral-LLaMA-3.3\n0.0%\n20.0%\n40.0%\n60.0%\n80.0%\n100.0%\n36.0%\n64.0%\n17.1%\n82.9%\n39.0%\n61.0%\n46.0%\n54.0%\nMistral-LLaMA-3.3 vs Mistral-Gemini-1.5\nModel A Wins\nModel B Wins\nFigure 5. Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0.\nobvious preference leakage. As shown in Table 1, we ob-\nserve that the preference leakage scores for Mistral-GPT-4o\nvs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini-\n1.5 (23.6% and 27.9% respectively) are consistently higher\nthan that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and\nQwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9%\nrespectively). We think that this is largely due to the more\ncomparable performance between the student models of\nGPT-4o and Gemini-1.5. Intuitively, when the quality of the\ntwo responses is similar, the evaluator may rely more heav-\nily on its inherent preferences to make a judgment, thereby\nexacerbating the preference leakage issue.\nLarger student models cause more bias from judge\nLLMs. Another observation from Table 1 is that the over-\nall preference leakage score for Qwen-2.5-14B is higher\nthan that for Mistral-7B. Drawing on insights from previous\nstudies on data leakage, which suggest that larger and more\npowerful LLMs are more capable of memorizing extensive\ninformation and are thus more susceptible to data contamina-\ntion (Bordt et al., 2024; Duan et al., 2024), we attribute this\ndifference in preference leakage to the size and capabilities\nof the student LLMs. We assume that larger student models,\ndue to their better performance and generalization abilities,\nare more capable of learning and memorizing the hidden\npreference pattern from the synthetic data, thus leading to a\nmore serious preference leakage.\nDifferent data generator/ judge LLMs result in varying\ndegrees of bias under preference leakage. While we have\nconcluded that student model pairs with similar performance\nor more powerful student models tend to exhibit greater\npreference leakage, we also examine whether different data\ngenerator and judge LLMs contribute to varying degrees\nof preference leakage. Analyzing the manual annotation\nresults presented in Table 5, we observe that Gemini-1.5\nshows a strong bias toward its students, followed by GPT-4o,\nwith LLaMA-3.3 displaying the least bias. This variation in\npreference leakage may stem from differences in the level\nof leaked preference in the synthetic responses generated\nby the data generator LLMs. For instance, an LLM with a\ndistinctive style or format in its responses offers more op-\nportunities for student models to learn these characteristics,\npotentially leading to more pronounced preference leakage\nduring evaluation. Future work could further quantify the\nextent of leaked preference for each data generator model.\n5. Further Analysis\nIn this section, we conduct relatedness analysis, learning\nmethod analysis and data mixing analysis (Section 5.1 - 5.3)\nto answer RQ2. Due to the cost consideration, we conduct\nthese analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5.\nMoreover, we perform recognition analysis and category\nanalysis to answer RQ3.\nArena-Hard AlpacaEval 2.0\nAvg.\nSame Model\n28.7%\n18.4%\n23.6%\nInheritance\nw/ same ins.\n17.8%\n20.7%\n19.3%\nInheritance\nw/ different ins.\n18.3%\n26.3%\n22.3%\nSame Family\nw/ same series\n10.1%\n7.6%\n8.9%\nSame Family\nw/ different series\n3.3%\n2.2%\n2.8%\nTable 2. Preference leakage score in different relatedness between\nthe data generator and the judging LLM.\n5.1. Relatedness Analysis\nWe demonstrate the impact of different relatedness condi-\ntions between the data generator and the judge LLM on the\npreference leakage problem, as shown in Table 2.\nPreference leakage under inheritance settings causes ob-\nvious bias of judges towards their related students. For\nthe inheritance relationship, we consider the situation where\nthe data generator is inherited from the judge model. We\nconducted the following two experiments: (1). we give the\nsame instructions again as in the SFT stage (Inheritance w/\nsame ins.), or (2). we sample the same number of different\ninstructions from the Ultrafeedback (Inherence w/ different\nins.). Then, we let the fine-tuned Mistral model generate\nthe answers and use these generated data to fine-tune a new\nMistral student model. From the results, with the same in-\nstructions, the average preference leakage score is 19.3%. In\ncomparison, the score with different instructions is 22.3%.\n6\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFirstly, in an inheritance setting, data generators can inherit\njudges’ preferences, which are then passed on to new stu-\ndent models, thereby compromising the fairness of their\nevaluation. Second, even when different instructions are\nused, judges’ preferences leaked to data generators can still\nbe transferred to the new student model through synthetic\ndata, leading to a high preference leakage score.\nModels within the same series tend to cause more sig-\nnificant bias. For two models within the same family, we\nconsider two settings: (1) Same series, where training data\nis generated by GPT-4o and Gemini-1.5-flash, and judged\nby GPT-4-turbo and Gemini-1.5-pro; (2) Different series,\nwhere training data is still generated by GPT-4o and Gemini-\n1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro.\nIn the same series setting, the average preference leakage\nscore is 8.9%, indicating that despite using different mod-\nels for data generation and judgment, their relatedness in\nterms of model family leads to some preference leakage.\nIn contrast, the different series setting yields a significantly\nlower leakage score of 2.8%, likely due to differences in\narchitecture, training data, and other factors, reducing the\ninfluence of model-related biases in evaluation.\nArena-Hard\nAlpacaEval 2.0\nAvg.\nSFT\n28.7%\n18.4%\n23.6%\nDPO\n7.7%\n2.7%\n5.2%\nICL\n-4.2%\n-1.1%\n-2.7%\nTable 3. Preference leakage score in different learning methods.\n5.2. Learning Method Analysis\nWe also compare three learning methods, supervised\nfine-tuning (SFT), direct preference optimization (DPO)\n(Rafailov et al., 2024), and in-context learning (ICL) (Dong\net al., 2024a), to explore the different influences to them un-\nder preference leakage. We first build a data pool based on\nhuman-written instruction-tuning data from OASST (K¨opf\net al., 2024), LIMA (Zhou et al., 2024), and MOSS (Sun\net al., 2024b) to supervised fine-tune the pre-trained model.\nFor DPO, we sample 2 responses for each instruction from\nsampled UltraFeedback instruction and prompt each data\ngenerator to produce the pairwise feedback. Then we use\nthe DPO loss to further train the fine-tuned policy on each\nsynthetic pairwise dataset. Appendix C shows the prompt\nwe use to craft synthetic pairwise feedback. For ICL, we\nsample 4 instruction-response pairs from each LLMs’ syn-\nthetic dataset as the demonstration during inference.\nTuning approaches would leak judges’ preference to the\nstudent models. Various learning methods show significant\ndifferences in preference leakage scores across learning\nmethods. SFT exhibits the highest average leakage score at\n23.6%. In contrast, DPO achieves a much lower score of\n5.2%, likely because its focus on preferences helps minimize\nthe unintended transfer of judge model biases. Meanwhile,\nICL, which relies on contextual examples without updating\nmodel parameters, is least affected by the data generator’s\npreferences, resulting in the lowest leakage scores.\n20\n40\n60\n80\n100\nContamination Ratio (%)\n0\n5\n10\n15\n20\n25\n30\nPreference Leakage Score (%)\nAlpacaEval2.0 - Manual\nArenaHard - Manual\nAlpacaEval2.0 - Synthetic\nArenaHard - Synthetic\nFigure 6. Experiment results on data mixing. ‘Manual’ represents\nthe original synthetic data mixed with manually-written data. ‘Syn-\nthetic’ represents the original data mixed with other synthetic data.\n5.3. Data Mixing Analysis\nIn real-world applications, synthetic data from a single LLM\nis often mixed with manually-written data or other multi-\nsource synthetic data to train student models. To mimic\nthese scenarios and explore how much synthetic data could\nlead to preference leakage, we conduct a data mixing anal-\nysis. Specifically, we randomly sample 10%, 30%, 50%,\nand 70% from the original synthetic dataset and mix it with\nmanually-written data and multi-source synthetic data, re-\nspectively, in order to maintain a consistent total volume of\ntraining data (30,000). For the manually-written data, we\nsample from the data pool collected in Section 5.2. For the\nmulti-source synthetic data, we use the original synthetic\ndata from Ultrafeedback, which includes responses gener-\nated by various LLMs (e.g., WizardLM, Flcon, etc.). After\nobtaining the mixing training data, we train the student mod-\nels using SFT and calculate their preference leakage scores\nbased on the judgment results. Figure 6 presents the results\nwith two mixing strategies across two benchmarks.\nThe degree of preference leakage is directly proportional\nto the amount of synthetic data. We observe a strong\ncorrelation between the proportion of synthetic data in the\nmixture and the preference leakage score, with no clear\nthreshold separating cases with preference leakage from\nthose without. This suggests that preference leakage can\noccur even with a small amount of leaked synthetic data,\nposing significant challenges for its detection.\n5.4. Can Judges Recognize Student Models?\nPrevious studies demonstrate the LLM judges can recog-\nnize and thus prefer their own generation (Panickssery et al.,\n2024). In this work, we pose a similar question: Does prefer-\nence leakage also source from the LLM judges’ recognition\n7\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nMathematics\nBusiness\nDaily Life\nScience\nWriting\nOthers\nProgramming\n0\n10\n20\n30\nPreference Leakage Score (%)\n7.7\n16.5\n17.2\n17.3\n21.0\n23.8\n31.4\n(a) Question Type\nCompleteness\nClarity\nRichness\nSatisfaction\nFactuality\nLogical\nOthers\nCreativity\nFairness\n20\n24\n28\n32\nPreference Leakage Score (%)\n27.9\n28.6\n28.8\n29.0\n29.2\n30.2\n30.4\n30.7\n32.4\n(b) Judgment dimension\nFigure 7. Category analysis results on question type and judgment dimension.\nTask\nModel\nAccuracy\nStudent Recognition\nGPT-4o\n60.0%\nGemini-1.5\n25.4%\nLLaMA-3.3\n54.2%\nResponse Classification\nBERT\n82.4%\nTable 4. Student recognition (binary classification) and response\nclassification results (three-class classification).\nof their related student models’ generation? To study this,\nwe follow Panickssery et al. (2024) to prompt the three\njudge LLMs and test whether they could recognize their\nrelated student models’ generation. Additionally, we split\nthree student models’ generation into training and testing\nsets, and train a BERT classifier to perform a three-class\nclassification inspired by the previous study on detecting\nhuman-AI text (Zhang et al., 2024c). Detailed instruction\nand training settings can be found in Appendix D.\nJudge LLMs do not show good performance in recogniz-\ning the generation of their student models. As the result\npresented in Table 4, we find that the recognition perfor-\nmance of each judge LLM in the content of related students\nis poor, with accuracy around the performance of random\nguess. Moreover, we observe no correlation between recog-\nnition performance and the preference leakage degree for\njudge LLMs. For instance, while Gemini-1.5 leads to the\nmost preference leakage (as shown in Section 4.2), it per-\nforms the worst in recognition tasks. These suggest that\npreference leakage is subtler and harder-to-detect for judge\nLLMs, in contrast to the more obvious egocentric bias.\nCertain features embedded in student models through\nsynthetic data. Although judge LLMs do not perform\nwell in related student recognition, we notice the fine-tuned\nBERT classification demonstrates a high accuracy score in\nclassifier response generated by each student model. This\nsuggests that certain characteristics—such as style and for-\nmat—are embedded in the student models through the syn-\nthetic responses. This finding further supports the existence\nof preference leakage and lays the groundwork for future\nresearch aimed at detecting and preventing it.\n5.5. Impact on Question Type & Judgment Dimension\nIn this section, we explore the impact of preference leakage\nacross various question types and judgment dimensions. For\nthe question type analysis, we first propose several general\nquestion types based on the question clusters introduced by\nArena-Hard. Then, we prompt GPT-4o to map each question\nin Arena-Hard and AlpacaEval to one of the question types\nand calculate the preference leakage score for each question\ncategory. For the judgment dimension analysis, we follow\nthe judgment dimensions introduced by Liu et al. (2023a)\nand also utilize GPT-4o to map the rationale generated by\njudge LLMs to one or multiple judgment dimensions. More\ndetailed prompt can be found in Appendix E. The analysis\nresults are presented in Figure 7.\nSubjective question and judgment dimension tend to\nlead to more bias. For question type analysis, we find ob-\njective questions with a definitive answer, like mathematical\nones, demonstrate the least preference leakage. By contrast,\nsubjective questions that have more than one standard an-\nswer, such as programming and writing, usually lead to a\nmore obvious preference leakage. This observation is also\napplied to judgment dimension analysis, as objective di-\nmensions (like completeness) have an overall lower leakage\ndegree compared with subjective ones (like fairness). This\nsuggests that preference leakage tends to be more significant\nin objective questions and dimensions, where the contami-\nnated model is more likely to receive biased preference.\n6. Conclusion\nIn this work, we formally highlight the preference leakage\nproblem in LLM-as-a-judge systems. The results of our\nmain experiment, measured using the proposed preference\nleakage score, reveal a clear bias in each judge toward its\nrespective student model. We also observe that this bias\nis more pronounced in comparable model pairs and larger\nstudent models. Furthermore, we conduct additional anal-\nysis on various factors, including the relationship between\nthe data generator and judge LLMs, model tuning tech-\n8\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nniques, and data mixing strategies. Our findings suggest\nthat preference leakage can cause significant bias across\ndiverse scenarios. Finally, through recognition and category\nanalyses, we investigate the underlying mechanisms of pref-\nerence leakage, demonstrating that it is a challenging and\nhard-to-detect issue, especially in subjective questions and\njudgment dimensions. In the future, we aim to explore meth-\nods for detecting, preventing, and mitigating this evolving\nchallenge in LLM-as-a-judge systems.\nImpact Statements\nBy revealing preference leakage, this work could help build\nmore trustworthy and ethically grounded AI systems. The\nrelatedness between data generators and evaluators can sys-\ntematically bias evaluations, potentially compromising the\nfairness and reliability of the automatic evaluation paradigm.\nThese biased evaluations may indirectly affect downstream\ntasks such as AI alignment and decision-making systems,\nleading to unintended ethical risks. To mitigate preference\nleakage, we hope that researchers will propose more reli-\nable evaluation methods, diversify training data sources, and\ndevelop contamination-resistant benchmarks in the future.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. ArXiv preprint,\nabs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nBalloccu, S., Schmidtov´a, P., Lango, M., and Duˇsek, O.\nLeak, cheat, repeat: Data contamination and evaluation\nmalpractices in closed-source llms. In Proceedings of the\n18th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pp. 67–93, 2024.\nBordt, S., Nori, H., and Caruana, R. Elephants never forget:\nTesting language models for memorization of tabular data.\nIn NeurIPS 2023 Second Table Representation Learning\nWorkshop, 2024.\nChen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B.\nHumans or llms as the judge? a study on judgement\nbiases. arXiv preprint arXiv:2402.10669, 2024.\nCui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y.,\nXie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting\nlanguage models with scaled ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nDai, S., Xu, C., Xu, S., Pang, L., Dong, Z., and Xu, J. Bias\nand unfairness in information retrieval systems: New\nchallenges in the llm era. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 6437–6447, 2024.\nDeng, C., Zhao, Y., Heng, Y., Li, Y., Cao, J., Tang, X.,\nand Cohan, A. Unveiling the spectrum of data contami-\nnation in language models: A survey from detection to\nremediation. arXiv preprint arXiv:2406.14644, 2024a.\nDeng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A.\nInvestigating data contamination in modern benchmarks\nfor large language models. In Proceedings of the 2024\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pp. 8698–8711,\n2024b.\nDodge, J., Sap, M., Marasovi´c, A., Agnew, W., Ilharco, G.,\nGroeneveld, D., Mitchell, M., and Gardner, M. Docu-\nmenting large webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1286–1305, 2021.\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia,\nH., Xu, J., Wu, Z., Chang, B., et al. A survey on in-\ncontext learning. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing,\npp. 1107–1128, 2024a.\nDong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and\nLi, G. Generalization or memorization: Data contamina-\ntion and trustworthy evaluation for large language models.\narXiv preprint arXiv:2402.15938, 2024b.\nDuan, S., Khona, M., Iyer, A., Schaeffer, R., and Fiete, I. R.\nUncovering latent memories: Assessing data leakage and\nmemorization patterns in large language models. arXiv\npreprint arXiv:2406.14549, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nElangovan, A., He, J., and Verspoor, K. Memorization vs.\ngeneralization: Quantifying data leakage in nlp perfor-\nmance evaluation. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pp. 1325–1335, 2021.\nGan, R., Wu, Z., Sun, R., Lu, J., Wu, X., Zhang, D.,\nPan, K., Yang, P., Yang, Q., Zhang, J., et al. Ziya2:\nData-centric learning is all llms need. arXiv preprint\narXiv:2311.03301, 2023.\n9\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nGao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan,\nX. Human-like summarization evaluation with chatgpt.\narXiv preprint arXiv:2304.02554, 2023.\nGolchin, S. and Surdeanu, M. Time travel in llms: Trac-\ning data contamination in large language models. arXiv\npreprint arXiv:2308.08493, 2023.\nHu, S., Luo, Y., Wang, H., Cheng, X., Liu, Z., and Sun, M.\nWon’t get fooled again: Answering questions with false\npremises. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 5626–5643, 2023.\nHuang, Y., Shi, J., Li, Y., Fan, C., Wu, S., Zhang, Q., Liu, Y.,\nZhou, P., Wan, Y., Gong, N. Z., et al. Metatool benchmark\nfor large language models: Deciding whether to use tools\nand which to use. arXiv preprint arXiv:2310.03128, 2023.\nHuang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y.,\nGao, C., Huang, Y., Lyu, W., Zhang, Y., et al. Posi-\ntion: Trustllm: Trustworthiness in large language models.\nIn International Conference on Machine Learning, pp.\n20166–20270. PMLR, 2024.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Car-\nney, A., et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiang, B., Li, D., Tan, Z., Zhou, X., Rao, A., Lerman, K.,\nBernard, H. R., and Liu, H. Assessing the impact of\nconspiracy theories using large language models. arXiv\npreprint arXiv:2412.07019, 2024a.\nJiang, M., Liu, K. Z., Zhong, M., Schaeffer, R., Ouyang,\nS., Han, J., and Koyejo, S. Investigating data contami-\nnation for pre-training language models. arXiv preprint\narXiv:2401.06059, 2024b.\nKaufman, S., Rosset, S., Perlich, C., and Stitelman, O. Leak-\nage in data mining: Formulation, detection, and avoid-\nance. ACM Transactions on Knowledge Discovery from\nData (TKDD), 6(4):1–21, 2012.\nKim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun,\nS., Shin, S., Kim, S., Thorne, J., et al. Prometheus: Induc-\ning fine-grained evaluation capability in language models.\nIn The Twelfth International Conference on Learning\nRepresentations, 2023.\nKim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J.,\nWelleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M.\nPrometheus 2: An open source language model special-\nized in evaluating other language models. arXiv preprint\narXiv:2405.01535, 2024.\nKoo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M.,\nand Kang, D.\nBenchmarking cognitive biases in\nlarge language models as evaluators.\narXiv preprint\narXiv:2309.17012, 2023.\nK¨opf, A., Kilcher, Y., von R¨utte, D., Anagnostidis, S.,\nTam, Z. R., Stevens, K., Barhoum, A., Nguyen, D., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations-\ndemocratizing large language model alignment. Advances\nin Neural Information Processing Systems, 36, 2024.\nLee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J.,\nLu, K. R., Bishop, C., Hall, E., Carbune, V., Rastogi,\nA., et al. Rlaif vs. rlhf: Scaling reinforcement learning\nfrom human feedback with ai feedback. In Forty-first\nInternational Conference on Machine Learning, 2024.\nLee, S., Zhou, J., Ao, C., Li, K., Du, X., He, S., Liu, J., Yang,\nM., Wen, Z., and Ni, S. Distillation quantification for\nlarge language models. arXiv preprint arXiv:2501.12619,\n2025.\nLi, C. and Flanigan, J. Task contamination: Language mod-\nels may not be few-shot anymore. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 38,\npp. 18471–18480, 2024.\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z.,\nBhattacharjee, A., Jiang, Y., Chen, C., Wu, T., et al. From\ngeneration to judgment: Opportunities and challenges of\nllm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024a.\nLi, D., Tan, Z., Chen, T., and Liu, H. Contextualization dis-\ntillation from large language model for knowledge graph\ncompletion. arXiv preprint arXiv:2402.01729, 2024b.\nLi, D., Yang, S., Tan, Z., Baik, J. Y., Yun, S., Lee, J.,\nChacko, A., Hou, B., Duong-Tran, D., Ding, Y., et al.\nDalk: Dynamic co-augmentation of llms and kg to an-\nswer alzheimer’s disease questions with scientific litera-\nture. arXiv preprint arXiv:2405.04819, 2024c.\nLi, D., Tan, Z., and Liu, H. Exploring large language models\nfor feature selection: A data-centric perspective. ACM\nSIGKDD Explorations Newsletter, 26(2):44–53, 2025.\nLi, M., Chen, L., Chen, J., He, S., Gu, J., and Zhou, T. Selec-\ntive reflection-tuning: Student-selected data recycling for\nllm instruction-tuning. arXiv preprint arXiv:2402.10110,\n2024d.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline. arXiv preprint arXiv:2406.11939, 2024e.\n10\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval:\nAn automatic evaluator of instruction-following models,\n2023.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74–81, 2004.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. In Proceedings of\nthe 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 3214–\n3252, 2022.\nLiu, C.-W., Lowe, R., Serban, I., Noseworthy, M., Charlin,\nL., and Pineau, J. How NOT to evaluate your dialogue sys-\ntem: An empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In Su, J., Duh, K.,\nand Carreras, X. (eds.), Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Process-\ning, pp. 2122–2132, Austin, Texas, 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1230.\nURL https://aclanthology.org/D16-1230.\nLiu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes\ngood data for alignment? a comprehensive study of auto-\nmatic data selection in instruction tuning. In The Twelfth\nInternational Conference on Learning Representations,\n2024.\nLiu, X., Lei, X., Wang, S., Huang, Y., Feng, Z., Wen, B.,\nCheng, J., Ke, P., Xu, Y., Tam, W. L., et al. Alignbench:\nBenchmarking chinese alignment of large language mod-\nels. arXiv preprint arXiv:2311.18743, 2023a.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nDing, H., Men, K., Yang, K., et al. Agentbench: Evalu-\nating llms as agents. arXiv preprint arXiv:2308.03688,\n2023b.\nNi, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neu-\nbig, G., and You, Y. Mixeval: Deriving wisdom of the\ncrowd from llm benchmark mixtures. arXiv preprint\narXiv:2406.06565, 2024.\nPanickssery, A., Bowman, S. R., and Feng, S. Llm evalu-\nators recognize and favor their own generations. arXiv\npreprint arXiv:2404.13076, 2024.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, pp. 311–318,\n2002.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nReiter, E. A structured review of the validity of BLEU.\nComputational Linguistics, 44(3):393–401, 2018. doi: 10.\n1162/coli a 00322. URL https://aclanthology.\norg/J18-3002.\nShi, J., Yuan, Z., Liu, Y., Huang, Y., Zhou, P., Sun,\nL., and Gong, N. Z.\nOptimization-based prompt in-\njection attack to llm-as-a-judge.\nIn Proceedings of\nthe 2024 on ACM SIGSAC Conference on Computer\nand Communications Security, CCS ’24, pp. 660–674,\nNew York, NY, USA, 2024. Association for Comput-\ning Machinery. ISBN 9798400706363. doi: 10.1145/\n3658644.3690291.\nURL https://doi.org/10.\n1145/3658644.3690291.\nShumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Ander-\nson, R., and Gal, Y. Ai models collapse when trained on\nrecursively generated data. Nature, 631(8022):755–759,\n2024.\nSun, R., Liu, M., Yang, S., Wang, R., He, J., and Zhang, J.\nFostering natural conversation in large language models\nwith nico: a natural interactive conversation dataset. arXiv\npreprint arXiv:2408.09330, 2024a.\nSun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Liu, X.,\nYan, H., Shao, Y., Tang, Q., Zhang, S., Zhao, X., Chen,\nK., Zheng, Y., Zhou, Z., Li, R., Zhan, J., Zhou, Y.,\nLi, L., Yang, X., Wu, L., Yin, Z., Huang, X., Jiang,\nY.-G., and Qiu, X.\nMoss: An open conversational\nlarge language model. Machine Intelligence Research,\n2024b. ISSN 2731-5398. URL https://github.\ncom/OpenMOSS/MOSS.\nTan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee,\nA., Karami, M., Li, J., Cheng, L., and Liu, H. Large\nlanguage models for data annotation and synthesis: A sur-\nvey. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 930–957,\n2024.\nTeam, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L.,\nGulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S.,\net al. Gemini 1.5: Unlocking multimodal understand-\ning across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nThakur, A. S., Choudhary, K., Ramayapally, V. S.,\nVaidyanathan, S., and Hupkes, D. Judging the judges:\nEvaluating alignment and vulnerabilities in llms-as-\njudges. arXiv preprint arXiv:2406.12624, 2024.\n11\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nTong, Y., Li, D., Wang, S., Wang, Y., Teng, F., and Shang,\nJ. Can llms learn from previous mistakes? investigat-\ning llms’ errors to boost for reasoning. arXiv preprint\narXiv:2403.20046, 2024.\nWang, S., Tong, Y., Zhang, H., Li, D., Zhang, X., and Chen,\nT. Bpo: Towards balanced preference optimization be-\ntween knowledge breadth and depth in alignment. arXiv\npreprint arXiv:2411.10914, 2024.\nWhite, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain,\nS., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S.,\net al. Livebench: A challenging, contamination-free llm\nbenchmark. arXiv preprint arXiv:2406.19314, 2024.\nWu, S., Huang, Y., Gao, C., Chen, D., Zhang, Q., Wan, Y.,\nZhou, T., Zhang, X., Gao, J., Xiao, C., et al. Unigen: A\nunified framework for textual dataset generation using\nlarge language models. arXiv preprint arXiv:2406.18966,\n2024.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115, 2024.\nYang, S., Sun, R., and Wan, X. A new dataset and empirical\nstudy for sentence simplification in chinese. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n8306–8321, 2023.\nYao, F., Zhuang, Y., Sun, Z., Xu, S., Kumar, A., and Shang,\nJ. Data contamination can cross language barriers. arXiv\npreprint arXiv:2406.13236, 2024.\nYe, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N.,\nGao, T., Geyer, W., Huang, C., Chen, P.-Y., et al. Justice\nor prejudice? quantifying biases in llm-as-a-judge. arXiv\npreprint arXiv:2410.02736, 2024.\nZhang, H., Li, D., Li, Y., Shang, C., Shi, C., and Jiang, Y.\nAssisting language learners: Automated trans-lingual def-\ninition generation via contrastive prompt learning. arXiv\npreprint arXiv:2306.06058, 2023.\nZhang, H., Shang, C., Wang, S., Zhang, D., Yao, F., Sun,\nR., Yu, Y., Yang, Y., and Wei, F. Shifcon: Enhancing\nnon-dominant language capabilities with a shift-based\ncontrastive framework. arXiv preprint arXiv:2410.19453,\n2024a.\nZhang, H., Wu, Y., Li, D., Yang, Z., Zhao, R., Jiang, Y., and\nTan, F. Balancing speciality and versatility: a coarse to\nfine framework for supervised fine-tuning large language\nmodel. arXiv preprint arXiv:2404.10306, 2024b.\nZhang, Q., Gao, C., Chen, D., Huang, Y., Huang, Y.,\nSun, Z., Zhang, S., Li, W., Fu, Z., Wan, Y., and Sun,\nL. LLM-as-a-coauthor: Can mixed human-written and\nmachine-generated text be detected? In Duh, K., Gomez,\nH., and Bethard, S. (eds.), Findings of the Association\nfor Computational Linguistics: NAACL 2024, pp. 409–\n436, Mexico City, Mexico, June 2024c. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-naacl.29. URL https://aclanthology.\norg/2024.findings-naacl.29/.\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi,\nY. Bertscore: Evaluating text generation with bert. In\nInternational Conference on Learning Representations,\n2020.\nZhang, X., Peng, B., Tian, Y., Zhou, J., Jin, L., Song,\nL., Mi, H., and Meng, H.\nSelf-alignment for fac-\ntuality:\nMitigating hallucinations in LLMs via self-\nevaluation. In Ku, L.-W., Martins, A., and Srikumar,\nV. (eds.), Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pp. 1946–1965, Bangkok, Thailand,\nAugust 2024d. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.acl-long.107. URL https:\n//aclanthology.org/2024.acl-long.107/.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nZheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z.,\nand Ma, Y. Llamafactory: Unified efficient fine-tuning of\n100+ language models. arXiv preprint arXiv:2403.13372,\n2024.\nZhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P.,\nZhu, C., Ji, H., and Han, J. Towards a unified multi-\ndimensional evaluator for text generation.\nIn Gold-\nberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npp. 2023–2038. Association for Computational Linguis-\ntics, 2022.\ndoi: 10.18653/V1/2022.EMNLP-MAIN.\n131.\nURL https://doi.org/10.18653/v1/\n2022.emnlp-main.131.\nZhong, M., Zhang, A., Wang, X., Hou, R., Xiong, W., Zhu,\nC., Chen, Z., Tan, L., Bi, C., Lewis, M., et al. Law of the\nweakest link: Cross capabilities of large language models.\narXiv preprint arXiv:2409.19951, 2024.\n12\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36, 2024.\nZhuge, M., Zhao, C., Ashley, D., Wang, W., Khizbullin, D.,\nXiong, Y., Liu, Z., Chang, E., Krishnamoorthi, R., Tian,\nY., et al. Agent-as-a-judge: Evaluate agents with agents.\narXiv preprint arXiv:2410.10934, 2024.\n13\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nA. Preliminary Study of Preference Leakage in Real World\nIn our preliminary study, we investigate whether preference leakage is a real-world issue in mainstream leaderboards and\nbenchmarks. To this end, we examine two widely used LLM-as-a-judge leaderboards (AlpacaEval 2.0 and Arena-Hard) and\na well-known benchmark (MTBench). All three rely on GPT-4 as the judge model and report pairwise judgment results for\nvarious LLMs. Our analysis reveals that several candidate models distilled from GPT-4 or other GPT-series models (e.g.,\nVicuna and Alpaca) appear across all these leaderboards and benchmarks, suggesting that preference leakage is a pervasive\nissue in these datasets. Besides, we also examine if preference leakage exists in LLM-relevant research studies and also find\na bunch of work utilizing the same or related model(s) to do distillation/ data synthesis and evaluation (Yang et al., 2023;\nLiu et al., 2024; Lee et al., 2024; Li et al., 2024d; Wang et al., 2024; Sun et al., 2024a). All of these suggest preference\nleakage to be a widespread problem in both LLM-as-a-judge datasets and LLM-relevant research.\nB. Experiment Details\nB.1. Training Details\nWe use LLaMA-Factory (Zheng et al., 2024), an efficient LLM tuning library for our experiment. The maximum sequence\nlength is set to 1024 tokens, and a cutoff length of 1024 tokens is enforced to prevent excessive tokenization. The data\npreprocessing will be done in parallel with 16 workers to speed up the preparation process. The training use a per-device\nbatch size of 2, with gradient accumulation over 2 steps to simulate a larger batch size for SFT and a per-device batch size of\n1, with gradient accumulation over 4 steps to simulate a larger batch size for DPO. The learning rate is set to 1.0e-5 and each\nmodel will be trained for 3 epochs. A cosine learning rate scheduler is used with a warmup ratio of 0.1 to gradually increase\nthe learning rate during the initial steps. All of the experiments use BF16 precision to speed up training while maintaining\nnumerical stability. All the experiments are conducted in an 8 Nvidia A100 GPU cluster with CUDA version 11.8.\nJudge Model\nMistral-GPT-4o vs Mistral-Gemini-1.5\nMistral-GPT-4o Wins\nMistral-Gemini-1.5 Wins\nGPT-4o\n55.1%\n44.9%\nGemini-1.5\n36.8%\n63.2%\nPreference Leakage Score\n18.4%\nTable 5. A case on AlpacaEval 2.0 with the model pair Mistral-GPT-4o vs Mistral-Gemini-1.5 to demonstrate how the preference leakage\nscore is calculated.\nB.2. Detailed Explanation for Preference Leakage Score\nWe present a case in Table B.1 to show how we calculate the preference leakage score for the Mistral-GPT-4o vs Mistral-\nGemini-1.5 pair on AlpacaEval 2.0. Based on the definition of preference leakage score, we first calculate:\nAVG(Mistral-GPT-4o, Mistral-Gemini-1.5) = 55.1 + 36.8\n2\n= 45.95%\n(9)\nAVG(Mistral-Gemini-1.5, Mistral-GPT-4o) = 63.2 + 44.9\n2\n= 54.05%\n(10)\nAfter that, we calculate the preference leakage score:\nPLS(Mistral-GPT-4o, Mistral-Gemini-1.5) =\n\x00 55.1−45.95\n45.95\n\x01\n+\n\x00 63.2−54.05\n54.05\n\x01\n2\n= 18.4%\n(11)\n.\nB.3. Manual Annotation Details\nWe randomly sample 100 questions from AlpacaEval 2.0 and ask three well-trained annotators to conduct pairwise\ncomparisons of the responses from each model pair for these questions. For annotation efficiency, we also develop an\nannotation tool that involves the function of uploading multiple model responses, jumping to specific problems, and\n14\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\ndownloading annotation results (Figure 8). After annotation, we adopt the majority voting to get the final label for each\nresponse pair. We also calculate the average agreement of three annotators and find it to be 78.6, indicating a relatively\nconsistent annotation result.\nC. Learning Method Analysis Details\nThe table below presents the prompt we use to generate synthetic pairwise feedback from each model.\nPairwise Feedback Prompt\nPlease act as an impartial judge and evaluate the quality of the responses provided\nby two AI assistants to the user question displayed below.\nYour evaluation should\nconsider correctness and helpfulness.\nYou will be given assistant A’s answer,\nand assistant B’s answer.\nYour job is to evaluate which assistant’s answer is\nbetter.\nYou should independently solve the user question step-by-step first.\nThen compare both assistants’ answers with your answer.\nIdentify and correct\nany mistakes.\nAvoid any position biases and ensure that the order in which the\nresponses were presented does not influence your decision.\nDo not allow the length\nof the responses to influence your evaluation.\nDo not favor certain names of the\nassistants.\nBe as objective as possible.\nAfter providing your explanation, output\nyour final verdict by strictly following this format:\n"[[A]]" if assistant A is\nbetter, "[[B]]" if assistant B is better.\n## Instruction:\n[The Start of Assistant A’s Answer]\n[RESPONSE A]\n[The End of Assistant A’s Answer]\n[The Start of Assistant B’s Answer]\n[RESPONSE B]\n[The End of Assistant B’s Answer]\nPlease output the generated content in a json format, for example:\n{ "reason":\n//\nstring, reasons behind the chosen preferred answer "prefered answer":\n// string,\nthe prefered answer you selected, [[A]] or [[B]] }\nFormatted the abovementioned schema and produce the reason and preferred answer:\nD. Recogniton Analysis Details\nThe table below presents the prompt we use for the recognition analysis.\nRecognition Prompt\nGiven an instruction and a response, your task is to judge whether this response\nis generated by a model that is trained on a synthetic dataset you produced (your\nstudent model).\n## Instruction:\n[INSTRUCTION]\n## Response:\n[Response]\nPlease output the generated content in a json format, for example:\n"reason":\n//\nstring, reasons behind the judgment "judgment":\n// string, whether the answer is\ngenerated by your student model, choose from yes or no\nFormatted the abovementioned schema and produce the reason and judgment:\nFor response classification, we split all the response from three student models into training (80%) and testing (20%) subsets.\n15\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nFigure 8. The annotation tool we develop for annotation efficiency.\nThen, we finetune a BERT-base-uncased model in the training set. The model is trained for 3 epochs with a learning rate of\n2e-5, a batch size of 16 for both training and evaluation, and a weight decay of 0.01, with evaluations conducted at the end\nof each epoch.\nE. Category Analysis Details\nThe tables below present the prompt we use for question type and judgment dimension cateogory analysis.\nQuestion Type Categorization Prompt\nGiven a question, please categorize it to one of the following categories:\n1.\nComputer Science & Programming\n2.\nMathematics & Statistics\n3.\nScience & Engineering\n4.\nBusiness & Finance\n5.\nWriting & Communication\n6.\nSocial & Daily Life\n7.\nOthers\n## Question:\n[QUESTION]\nPlease output the generated content in a json format, for example:\n{ "question\ncategory":\n// string, specific category name, such as "Computer Science &\nProgramming" }\nFormatted the abovementioned schema and categorize the given question:\n16\n\nPreference Leakage: A Contamination Problem in LLM-as-a-judge\nJudgment Dimension Categorization Prompt\nGiven a pairwise comparison judgment made by an AI, please categorize each\nconsidered aspect in the rationale to one of the following categories:\n{\n"Factuality":\n"Whether the information provided in the response is accurate, based\non reliable facts and data.",\n"User Satisfaction":\n"Whether the response meets the user’s question and needs, and\nprovides a comprehensive and appropriate answer to the question.",\n"Logical Coherence":\n"Whether the response maintains overall consistency and\nlogical coherence between different sections, avoiding self-contradiction.",\n"Richness":\n"Whether the response includes rich info, depth, context, diversity,\ndetailed explanations and examples to meet user needs and provide a comprehensive\nunderstanding.",\n"Creativity":\n"Whether the response is innovative or unique, providing novel\ninsights or solutions.",\n"Fairness and Responsibility":\n"Whether the advice or information provided in the\nresponse is feasible, carries acertain degree of responsibility, and considers\npotential risks and consequences.",\n"Completeness":\n"Whether the response provides sufficient information and details\nto meet the user’s needs, and whether it avoids omitting important aspects.",\n"Clarity":\n"Whether the response is clear and understandable, and whether it uses\nconcise language and structure so that the user can easily understand it.",\n"Others":\n"Other aspects which is not listed above."\n}\n## Judgment:\n[JUDGMENT]\nPlease output the generated content in a json format, for example:\n{ "Factuality":\n// list, all aspects that belong to this category, such as ["correctness",\n"mistakes"] ...\n}\nFormatted the abovementioned schema and categorize aspects in the judgment:\n17')]}
2025-02-06 20:49:25,590 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:50:06,474 - INFO - Total execution time: 40.04 seconds (0.67 minutes)
2025-02-06 20:50:06,476 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 20:50:47,628 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:51:18,371 - INFO - Total execution time: 29.95 seconds (0.50 minutes)
2025-02-06 20:51:18,372 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 20:52:35,133 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:53:06,838 - INFO - Total execution time: 30.90 seconds (0.52 minutes)
2025-02-06 20:53:06,839 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 20:54:03,981 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:54:33,323 - INFO - Total execution time: 28.65 seconds (0.48 minutes)
2025-02-06 20:54:33,325 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 20:55:33,675 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:56:21,804 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:56:56,751 - INFO - Total execution time: 34.01 seconds (0.57 minutes)
2025-02-06 20:56:56,753 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 20:57:20,572 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:59:15,035 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 20:59:50,066 - INFO - Total execution time: 34.27 seconds (0.57 minutes)
2025-02-06 20:59:50,068 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 21:00:11,051 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:01:21,442 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:02:25,344 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:02:56,108 - INFO - Total execution time: 30.13 seconds (0.50 minutes)
2025-02-06 21:02:56,108 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content=None),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content=None),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content=None)]}
2025-02-06 21:03:17,714 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:05:11,236 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:05:38,517 - INFO - Total execution time: 26.46 seconds (0.44 minutes)
2025-02-06 21:05:38,526 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content='1. Introduction\nDiffusion Bridge Models (DBMs) represent a specialized\nclass of diffusion models designed for data-to-data tasks,\nsuch as image-to-image translation. Unlike standard diffu-\nsion models, which operate by mapping noise to data (Ho\net al., 2020; Sohl-Dickstein et al., 2015), DBMs construct\ndiffusion processes directly between two data distributions\n(Peluchetti, 2023a; Liu et al., 2022b; Somnath et al., 2023;\nZhou et al., 2024a; Yue et al., 2024; Shi et al., 2023; De Bor-\ntoli et al., 2023). This approach allows DBMs to modify\nonly the necessary components of the data, starting from an\ninput sample rather than generating it entirely from Gaus-\n*Equal contribution\n1Skolkovo Institute of Science and\nTechnology\n2Yandex Research\n3HSE University\n4Artificial\nIntelligence Research Institute.\nCorrespondence to:\nNikita\nGushchin\n<n.gushchin@skoltech.ru>,\nAlexander\nKorotin\n<a.korotin@skoltech.ru>.\nInput\nIBMD (Ours)\nTeacher\nSuper-resolution\nJPEG restoration\nInpainting\nNormal-to-Image\nSketch-to-Image\nFigure 1. Outputs of DBMs models distilled by our Inverse Bridge\nMatching Distillation (IBMD) approach on various image-to-\nimage translation tasks and datasets (M5). Teachers use NFE≥500\nsteps, while IBMD distilled models use NFE≤4.\nsian noise. As a result, DBMs have demonstrated impressive\nperformance in image-to-image translation problems.\nThe rapid development of DBMs has led to two dominant ap-\nproaches, usually considered separately. The first branch of\n1\narXiv:2502.01362v1  [cs.LG]  3 Feb 2025\n\nInverse Bridge Matching Distillation\napproaches (Peluchetti, 2023a; Liu et al., 2022b; 2023a; Shi\net al., 2023; Somnath et al., 2023) considered the construc-\ntion of diffusion between two arbitrary data distributions\nperforming Unconditional Bridge Matching (also called\nthe Markovian projection) of a process given by a mixture\nof diffusion bridges. The application of this branch includes\ndifferent data like images (Liu et al., 2023a; Li et al., 2023),\naudio (Kong et al., 2025) and biological tasks (Somnath\net al., 2023; Tong et al., 2024) not only in paired but also in\nunpaired setups using its relation to the Schr¨odinger Bridge\nproblem (Shi et al., 2023; Gushchin et al., 2024). The second\ndirection follows a framework closer to classical diffusion\nmodels, using forward diffusion to gradually map to the\npoint of different distibution rather than mapping distribu-\ntion to distribution as in previous case (Zhou et al., 2024a;\nYue et al., 2024). While these directions differ in theoretical\nformulation, their practical implementations are closely re-\nlated; for instance, models based on forward diffusion can\nbe seen as performing Conditional Bridge Matching with\nadditional drift conditions (De Bortoli et al., 2023).\nSimilar to classical DMs, DBMs also exhibit multistep se-\nquential inference, limiting their adoption in practice. De-\nspite the impressive quality shown by DBMs in the practical\ntasks, only a few approaches were developed for their accel-\neration, including more advanced sampling schemes (Zheng\net al., 2024; Wang et al., 2024) and consistency distillation\n(He et al., 2024), adapted for bridge models. While these\napproaches significantly improve the efficiency of DBMs,\nsome unsolved issues remain. The first one is that the men-\ntioned acceleration approaches are directly applicable only\nfor DBMs based on the Conditional Bridge Matching, i.e.,\nno universal method can accelerate any DBMs. Also, due\nto some specific theoretical aspects of DBMs, consistency\ndistillation cannot be used to obtain the single-step model\n(He et al., 2024, Section 3.4).\nContributions. To address the above-mentioned issues of\nDBMs acceleration, we propose a new distillation technique\nbased on the inverse bridge matching problem, which has\nseveral advantages compared to existing methods:\n1. Universal Distillation. Our distillation technique is ap-\nplicable to DBMs trained with both conditional and un-\nconditional regimes, making it the first distillation ap-\nproach introduced for unconditional DBMs.\n2. Single-Step and Multistep Distillation. Our distillation\nis capable of distilling DBMs into generators with any\nspecified number of steps, including the distillation of\nDBMs into one-step generators.\n3. Target data-free distillation. Our method does not\nrequire the target data domain to perform distillation.\n4. Better quality of distilled models. Our distillation tech-\nnique is tested on a wide set of image-to-image problems\nfor conditional and unconditional DBMs in both one and\nmulti-step regimes. It demonstrates improvements com-\npared to the previous acceleration approaches including\nDBIM(Zheng et al., 2024) and CDBM (He et al., 2024).\n2. Background\nIn this paper, we propose a universal distillation frame-\nwork for both conditional and unconditional DBMs.\nTo not repeat fully analogical results for both cases,\nwe denote by this color the additional conditioning\non xT used for the conditional models, i.e. for the\nunconditional case this conditioning is not used.\n2.1. Bridge Matching\nWe start by recalling the bridge matching method\n(Peluchetti, 2023b;a; Liu et al., 2022b; Shi et al., 2023).\nConsider two probability distributions p(x0) and p(xT ) on\nRD dimensional space, which represent target and source\ndomains, respectively. For example, in an image inverse\nproblem, p(x0) represents the distribution of clean im-\nages and p(xT ) the distribution of corrupted images. Also\nconsider a coupling p(x0, xT ) of these two distributions,\nwhich is a probability distribution on RD × RD. Cou-\npling p(x0, xT ) can be provided by paired data or con-\nstructed synthetically, i.e., just using the independent distri-\nbution p(x0, xT ) = p(x0)p(xT ). Bridge Matching aims to\nconstruct the diffusion that transforms source distribution\np(xT ) to target distribution p(x0) based on given coupling\np(x0, xT ) and specified diffusion bridge.\nDiffusion bridges.\nConsider forward-time diffusion Q\ncalled ”Prior” on time horizon [0, T] represented by the\nstochastic differential equation (SDE):\nPrior Q :\ndxt = f(xt, t)dt + g(t)dwt,\n(1)\nf(xt, t) : RD × [0, T] →RD,\ng(t) : [0, T] →RD,\nwhere f(xt, t) is a drift function, g(t) is the noise schedule\nfunction and dwt is the differential of the standard Wiener\nprocess. By q(xt|xs), we denote the transition probability\ndensity of prior process Q from time s to time t. Diffusion\nbridge is a conditional process Q|x0,xT , which is obtained\nby pinning down starting and ending points x0 and xT . This\ndiffusion bridge can be derived from prior process Q using\nthe Doob-h transform (Doob & Doob, 1984):\nDiffusion Bridge Q|x0,xT : x0, xT are fixed,\n(2)\ndxt = {f(xt, t)dt + g2(t)∇xt log q(xT |xt)}dt + g(t)dwt,\nFor this diffusion bridge we denote the distribution at time t\nof the diffusion bridge Q|x0,xT by q(xt|x0, xT ).\nMixture of bridges. Bridge Matching procedure starts with\ncreating a mixture of bridges process Π. This process is\n2\n\nInverse Bridge Matching Distillation\nFigure 2. Overview of (Conditional) Bridge Matching with bx0 reparameterization. The process begins by sampling a pair (x0, xT )\nfrom the data coupling p(x0, xT ). An intermediate sample xt is then drawn from the diffusion bridge q(xt|x0, xT ) at a random time\nt ∼U[0, T]. The model bx0 is trained with an MSE loss to reconstruct x0 from xt. In the conditional setting (dashed red path), bx0 is also\nconditioned on xT as an additional input, leveraging information about the terminal state to improve reconstruction.\nrepresented as follows:\nMixture of Bridges Π :\nΠ(·) =\nZ\nQ|x0,xT (·)p(x0, xT )dx0dxT .\n(3)\nPractically speaking, the definition (3) means that to sample\nfrom a mixture of bridges Π, one first samples the pair\n(x0, xT ) ∼p(x0, xT ) from data coupling and then samples\ntrajectory from the bridge Q|x0,xT (·).\nBridge Matching problem. The mixture of bridges Π can-\nnot be used for data-to-data translation since it requires first\nto sample a pair of data and then just inserts the trajectory.\nIn turn, we are interested in constructing a diffusion, which\ncan start from any sample xT ∼p(xT ) and gradually trans-\nform it to x0 ∼p(x0). This can be done by solving the\nBridge Matching problem (Shi et al., 2023, Proposition 2)\nBridge Matching problem:\n(4)\nBM(Π)\ndef\n= arg min\nM∈M\nKL(Π||M),\nwhere M is the set of Markovian processes associated with\nsome SDE and KL(Π||M) is the KL-divergence between\na constructed mixture of bridges Π and diffusion M. It is\nknown that the solution of Bridge Matching is the reversed-\ntime SDE (Shi et al., 2023, Proposition 9):\nThe SDE of Bridge Matching solution :\n(5)\ndxt = {ft(xt) −g2(t)v∗(xt, t)}dt + g(t)d ¯wt,\nxT ∼pT (xT ),\nwhere ¯w is a standard Wiener process when time t flows\nbackward from t = T to t = 0, and dt is an infinitesimal\nnegative timestep. The drift function v∗is obtained solving\nthe following problem (Shi et al., 2023; Liu et al., 2023a):\nBridge Matching problem with a tractable objective: (6)\nmin\nϕ Ex0,t,xt\n\x02\n∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼p(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nTime moment t here is sampled according to the uniform\ndistribution on the interval [0, T].\nRelation Between Flow and Bridge Matching. The Flow\nMatching (Liu et al., 2023b; Lipman et al., 2023) can be\nseen as the limiting case σ →0 of the Bridge Matching for\nparticular example see (Shi et al., 2023, Appendix A.1).\n2.2. Augmented (Conditional) Bridge Matching and\nDenoising Diffusion Bridge Models (DDBM)\nFor a given coupling p(x0, xT ) = p(x0|xT )p(xT ), one can\nuse an alternative approach to build a data-to-data diffusion.\nConsider a set of Bridge Matching problems indexed by xT\nbetween p0 = p(x0|xT ) and p(xT ) = δxT (x) (delta mea-\nsure centered at xT ). This approach is called Augmented\nBridge Matching (De Bortoli et al., 2023). The key dif-\nference of this version in practice is that it introduces the\ncondition of the drift function v∗(xt, t, xT ) on the starting\npoint xT in the reverse time diffusion (5):\ndxt = {ft(xt) −g2(t)v∗(xt, t, xT )}dt + g(t)d ¯wt.\nThe drift function v∗can be recovered almost in the same\nway just by the addition of this condition on xT :\nAugmented (Conditional) Bridge Matching Problem.\nmin\nϕ Ex0,t,xt,xT\n\x02\n∥vϕ(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼p(x0, xT ), and xt ∼q(xt|x0, xT ).\nSince the difference is the addition of conditioning on xT ,\nwe call this approach Conditional Bridge Matching.\nRelation to DDBM. As was shown in the Augmented\nBridge Matching (De Bortoli et al., 2023), the conditional\nBridge Matching is equivalent to the Denoising Diffusion\nBridge Model (DDBM) proposed in (Zhou et al., 2024a).\nThe difference is that in DDBM, the authors learn the score\nfunction of s(xt, xT , t) conditioned on xT of a process for\nwhich x0 ∼p(x0|xT ) and q(xt) ∼q(xt|x0, xT ): Then, it\nis combined with the drift of forward Doob-h transform (5)\nto get the reverse SDE drift v(xt, t, xT ):\nv(xt, t, xT ) = s(xt, xT , t) −∇xt log q(xT |xt),\ndxt = {f(xt, t)dt −g2(t)v(xt, t, xT )}dt + g(t)d ¯wt,\nor reverse probability flow ODE drift:\nvODE(xt, t, xT ) = 1\n2s(xt, xT , t) −∇xt log q(xT |xt),\n3\n\nInverse Bridge Matching Distillation\ndxt = {f(xt, t)dt −g2(t)vODE(xt, t, xT )}dt,\nwhich is used for consistency distillation in (He et al., 2024).\n2.3. Practical aspects of Bridge Matching\nPriors used in practice. In practice (He et al., 2024; Zhou\net al., 2023; Zheng et al., 2024), the drift of the prior pro-\ncess is usually set to be f(xt, t) = f(t)xt, i.e, it depends\nlinearly on xt. For this process the transitional distribution\nq(xt|x0) = N(xt|αtx0, σ2\nt I) is Gaussian, where:\nf(t) = d log αt\ndt\n,\ng2(t) = dσ2\nt\ndt −2d log αt\ndt\nσ2\nt .\nThe bridge process distribution is also a Gaussian\nq(xt|x0, xT ) = N(xT |atxT + btx0, c2\ntI) with coefficients:\nat = αt\nαT\nSNRT\nSNRt\n, bt = αt\n\x12\n1 −SNRT\nSNRt\n\x13\n,\nc2\nt = σ2\nt\n\x12\n1 −SNRT\nSNRt\n\x13\n,\nwhere SNRt = α2\nt\nσ2\nt is the signal-to-noise ratio at time t.\nData prediction reparameterization. The regression target\nof the loss function (6) for the priors with the drift v(xt, t)\nis given by ∇xt log q(xt|x0) = −xt−αtx0\nσ2\nt\n. Hence, one can\nuse the parametrization v(xt, t, xT ) = −xt−αtbx0(xt,t,xT )\nσ2\nt\nand solve the equivalent problem:\nReparametrized (Conditional) Bridge Matching problem:\nmin\nϕ Ex0,t,xt,xT\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03\n,\n(7)\n(x0, xT ) ∼p(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\nwhere λ(t) is any positive weighting function. Note that xT\nis used only for the Conditional Bridge Matching model.\n2.4. Difference Between Acceleration of Unconditional\nand Conditional DBMs\nSince both conditional and unconditional approaches learn\ndrifts of SDEs, they share the same problems of long in-\nference. However, these models significantly differ in the\napproaches that can accelerate them. The source of this\ndifference is that Conditional Bridge Matching considers\nthe set of problems of reversing diffusion, which gradually\ntransforms distribution p(x0|xT ) to the fixed point xT . Fur-\nthermore, the forward diffusion has simple analytical drift\nand Gaussian transitional kernels. Thanks to it, for each xT\nto sample, one can use the probability flow ODE and ODE-\nsolvers or hybrid solvers to accelerate sampling (Zhou et al.,\n2024a) or use consistency distillation of bridge models (He\net al., 2024). Another beneficial property is that one can con-\nsider a non-Markovian forward process to develop a more\nefficient sampling scheme proposed in DBIM (Zheng et al.,\n2024) similar to Denoising Diffusion Implicit Models (Song\net al., 2021). However, in the Unconditional Bridge Match-\ning problem, the forward diffusion process, which maps\np(x0) to p(xT ) without conditioning on specific point xT ,\nis unknown. Hence, the abovementioned methods cannot\nbe used to accelerate this model type.\n3. IBMD: Inverse Bridge Matching Distillation\nThis section describes our proposed universal approach to\ndistill the both Unconditional and (Conditional) Bridge\nMatching models v∗(called the teacher model) into a few-\nstep generator using only the corrupted data pT (xT ). The\nkey idea of our method is to consider the inverse problem of\nfinding the mixture of bridges Πθ, for which Bridge Match-\ning provides the solution vθ with the same drift as the given\nteacher model v∗. We formulate this task as the optimiza-\ntion problem (M3.1). However, gradient methods cannot\nsolve this optimization problem directly due to the absence\nof tractable gradient estimation. To avoid this problem, we\nprove a theorem that allows us to reformulate the inverse\nproblem in the tractable objective for gradient optimiza-\ntion (M3.2). Then, we present the fully analogical results\nfor the Conditional Bridge Matching case in (M3.3). Next,\nwe present the multistep version of distillation (M3.5) and\nthe final algorithm (M3.4). We provide the proofs for all\nconsidered theorems and propositions in Appendix A.\n3.1. Bridge Matching Distillation as Inverse Problem\nIn this section, we focus on the derivation of our distilla-\ntion method for the case of Unconditional Bridge Match-\ning. Consider the fitted teacher model v∗(xt, t), which\nis an SDE drift of some process M ∗= BM(Π∗), where\nΠ∗constructed using some data coupling p∗(x0, xT ) =\np∗(x0|xT )p(xT ).\nWe\nparametrize\npθ(x0, xT )\n=\npθ(x0|xT )p(xT ) and aim to find such Πθ build on\npθ(x0, xT ), that BM(Π∗)\n=\nBM(Πθ).\nIn practice,\nwe parametrize pθ(x0|xT ) by the stochastic generator\nGθ(xT , z), z ∼N(0, I), which generates samples based\non input xT ∼p(xT ) and the gaussian noise z. Now, we\nformulate the inverse problem as follows:\nmin\nθ\nKL(BM(Πθ)||M ∗).\n(8)\nNote, that since the objective (8) is the KL-divergence be-\ntween BM(Πθ) and M ∗, it is equal to 0 if and only if\nBM(Πθ) and M ∗coincide. Furthermore, using the disinte-\ngration and Girsanov theorem (Vargas et al., 2021; Pavon &\nWakolbinger, 1991), we have the following result:\nProposition 3.1 (Inverse Bridge Matching problem). The\ninverse problem (8) is equivalent to\nmin\nθ\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(9)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\n4\n\nInverse Bridge Matching Distillation\nFigure 3. Overview of our method Inverse Bridge Matching Distillation (IBMD). The goal is to distill a trained (Conditional) Bridge\nMatching model into a generator Gθ(z, xT ), which learns to produce samples using the corrupted data p(xT ). Generator Gθ(z, xT )\ndefines the coupling pθ(x0, xT ) = pθ(x0|xT )p(xT ) and we aim to learn the generator in such way that Bridge Matching with pθ(x0, xT )\nproduces the same (Conditional) Bridge Matching model bxϕ\n0 = bxθ\n0. To do so, we learn a bridge model bxϕ\n0 using coupling pθ in the same\nway as the teacher model was learned. Then, we use our novel objective given in Theorem 3.2 to update the generator model Gθ.\nwhere λ(t) is any positive weighting function.\nThus, this is the constrained problem, where the drift v\nis the result of Bridge Matching for coupling pθ(x0, xT )\nparametrized by the generator Gθ. Unfortunately, there is\nno clear way to use this objective efficiently for optimizing\na generator Gθ since it would require gradient backpropa-\ngation through the argmin of the Bridge Matching problem.\n3.2. Tractable objective for the inverse problem\nIn this section, we introduce our new unconstrained refor-\nmulation for the inverse problem (9), which admits direct\noptimization using gradient methods:\nTheorem 3.2 (Tractable inverse problem reformulation).\nThe constrained inverse problem (9) w.r.t θ is equivalent to\nthe unconstrained optimization problem:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−\nmin\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\nWhere the constraint in the original inverse problem (9) is\nrelaxed by introducing the inner bridge matching problem.\nThis is the general result that can applied with any diffusion\nbridge. For the priors with with drift f(xt, t) = f(t)xt, we\npresent its reparameterized version.\nProposition 3.3 (Reparameterized tractable inverse prob-\nlem). Using the reparameterization (M2.3) for the prior with\nthe linear drift f(xt, t) = f(t)xt, the inverse problem in\nTheorem 3.2 is equivalent to:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥bx∗\n0(xt, t) −x0∥2\x03\n−\nmin\nϕ Ext,t,x0\n\x02\nλ(t)∥bxϕ\n0(xt, t) −x0∥2\x03\ndt\ni\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nThe key difference of the reformulated problem is that it\nadmits clear gradients of generator Gθ, which can be cal-\nculated automatically by using the autograd techniques.\nThanks to the unconstrained reformulation of an inverse\nproblem given by Theorem 3.2, it can now be solved di-\nrectly by parameterizing bx0(xt, t) by a neural network.\n3.3. Distillation of conditional Bridge Matching models\nSince Conditional Bridge Matching is, in essence, a set\nof Unconditional Bridge Matching problems for each xT\n(M2.2), the analogical results hold just by adding the condi-\ntioning on xT for v, i.e., using v(xt, t, xT ) or bx0, i.e. using\nbx0(xt, t, xT ). Here, we provide the final reparametrized\nformulation, which we use in our experiments:\nTheorem 3.4 (Reparameterized tractable inverse problem\nfor conditional bridge matching).\nmin\nθ\nh\nExt,t,x0,xT\n\x02\nλ(t)∥bx∗\n0(xt, t, xT ) −x0∥2\x03\n−\n(10)\nmin\nϕ Ext,t,x0,xT\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nwhere λ(t) is some positive weight function.\nTo use it in practice, we parameterize bx0(xt, t, xT ) by a\nneural network with an additional condition on xT .\n3.4. Algorithm\nWe provide a one-step Algorithm 1 that solves the inverse\nBridge Matching problem in the reformulated version that\nwe use in our experiments. We provide a visual abstract of\nit in Figure 3. Note that a teacher in the velocity parame-\nterization v∗(xt, t) can be easily reparameterized (M2.3) in\nx0-prediction model using bx∗(xt, t) = σ2\nt v∗(xt,t)+xt\nαt\n.\n5\n\nInverse Bridge Matching Distillation\n3.5. Mulitistep distillation\nWe also present a multistep modification of our distillation\ntechnique if a one-step generator struggles to distill the mod-\nels, e.g., in inpainting setups, where the corrupted image\nxT contains less information. Our multistep technique is\ninspired by similar approaches used in diffusion distilla-\ntion methods (Yin et al., 2024a, DMD) and aims to avoid\ntraining/inference distribution mismatch.\nWe choose N timesteps {0 < t1 < t2 < ... < tN = T}\nand add additional time input to our generator Gθ(xt, z, t).\nFor the conditional Bridge Matching case, we also add\nconditions on xT and use Gθ(xt, z, t, xT ). To perform in-\nference, we alternate between getting prediction from the\ngenerator ex0 = Gθ(xt, z, t) and using posterior sampling\nq(xtn−1|ex0, xtn) given by the diffusion bridge. To train\nthe generator in the multistep regime, we use the same\nprocedure as in one step except that to get input xt for in-\ntermediate times tn < tN, we first perform inference of our\ngenerator to get x0 and then use bridge q(xt|ex0, xT ).\n4. Related work\nDiffusion Bridge Models (DBMs) acceleration. Unlike\na wide scope of acceleration methods developed for clas-\nsical diffusion and flow models, only a few approaches\nwere developed for DBM acceleration. For the conditional\nDBMs, acceleration methods include more advanced sam-\nplers (Zheng et al., 2024; Wang et al., 2024) based on re-\nformulated forward diffusion process as a non-markovian\nprocess inspired by Denoising Diffusion Implicit Models\n(Song et al., 2021). Also, there is a distillation method based\non the distilling probability-flow ODE into a few steps using\nconsistency models (He et al., 2024). However, for theo-\nretical reasons (He et al., 2024, Section 3.4), consistency\nmodels for Diffusion Bridges cannot be distilled into one-\nstep generators. Unlike these existing works, our method\nis applicable to both conditional and unconditional types of\nDBMs and can distill models into the one-step generator.\nRelated diffusion and flow models distillation techniques.\nAmong the methods developed for the distillation of classi-\ncal diffusion and flow models, the most related to our work\nare methods based on simultaneous training of few-step\ngenerators and auxiliary ”fake” model, that predict score or\ndrift function for the generator (Yin et al., 2024b;a; Zhou\net al., 2024b; Huang et al., 2024). Unlike these approaches,\nwe consider the distillation of Diffusion Bridge Models -\nthe generalization of flow and diffusion models.\n5. Experiments\nThis section highlights the applicability of our IBMD distil-\nlation method in both unconditional and conditional settings.\nTo demonstrate this, we conducted experiments utilizing\npretrained unconditional models used in I2SB paper (Liu\net al., 2023a). Then we evaluated IBMD in conditional\nAlgorithm 1 Inverse Bridge Matching Distillation (IBMD)\nInput\n:Teacher network bx∗\n0 : RD × [0, T] × RD →RD;\nBridge q(xt|x0, xT ) used for training x∗;\nGenerator network Gθ : RD × RD →RD;\nBridge network bxϕ\n0 : RD × [0, T] × RD →RD;\nInput distribution p(xT ) accessible by samples;\nWeights function λ(t) : [0, T] →R+;\nBatch size N; Number of student iterations K;\nNumber of bridge iterations L.\nOutput :Learned generator Gθ of coupling pθ(x0, xT ) for\nwhich Bridge Matching outputs drift v ≈v∗.\n// Conditioning on xT is used only for distillation of Condi-\ntional Bridge Matching models.\nfor k = 1 to K do\nfor l = 1 to L do\nSample batch xT ∼p(xT )\nSample batch of noise z ∼N(0, I)\nx0 ←Gθ(xT , z)\nSample time batch t ∼U[0, T]\nSample batch xt ∼q(xt|x0, xT )\nbLϕ ←1\nN\nPN\nn=1\n\x02\nλ(t)||bxϕ\n0(xt, t, xT ) −x0||2\x03\nn\nUpdate ϕ by using ∂b\nLϕ\n∂ϕ\nSample batch xT ∼p(xT )\nSample batch of noise z ∼N(0, I)\nx0 ←Gθ(xT , z)\nSample time batch t ∼U[0, T]\nSample batch xt ∼q(xt|x0, xT )\nbLθ ←1\nN\nPN\nn=1\n\x02\nλ(t)||bx∗\n0(xt, t, xT ) −x0||2 −\nλ(t)||bxϕ\n0(xt, t, xT ) −x0||2\x03\nn\nUpdate θ by using ∂b\nLθ\n∂θ\nsettings using DDBM (Zhou et al., 2024a) setup (M5.2).\nFor clarity, we denote our models as IBMD-DDBM and\nIBMD-I2SB, indicating that the teacher model is derived\nfrom DDBM or I2SB framework, respectively. We provide\nall the technical details in Appendix B.\n5.1. Distillation of I2SB (5 setups)\nSince known distillation and acceleration techniques are\ndesigned for the conditional models, there is no clear base-\nline for comparison. Thus, this section aims to demonstrate\nthat our distillation technique significantly decreases NFE\nrequired to obtain the same quality of generation.\nExperimental Setup. To test our approach for uncondi-\ntional models, we consider models trained and published in\nI2SB paper (Liu et al., 2023a), specifically (a) two models\nfor the 4x super-resolution with bicubic and pool kernels,\n(b) two models for JPEG restoration using quality factor\nQF= 5 and QF= 10, and (c) a model for center-inpainting\nwith a center mask of size 128 × 128 all of which were\ntrained on ImageNet 256 × 256 dataset (Deng et al., 2009).\n6\n\nInverse Bridge Matching Distillation\nTable 1. Results on the image super-resolution task. Baseline re-\nsults are taken from I2SB (Liu et al., 2023a).\n4× super-resolution (bicubic)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n21.3\n63.2\nDDNM (Wang et al., 2023)\n100\n13.6\n65.5\nΠGDM (Song et al., 2023)\n100\n3.6\n72.1\nADM (Dhariwal & Nichol, 2021)\n1000\n14.8\n66.7\nCDSB (Shi et al., 2022)\n50\n13.6\n61.0\nI2SB (Liu et al., 2023a)\n1000\n2.8\n70.7\nIBMD-I2SB (Ours)\n1\n2.5\n72.4\nTable 2. Results on the image JPEG restoration task with QF=5.\nBaseline results are taken from I2SB (Liu et al., 2023a).\nJPEG restoration, QF= 5.\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n28.2\n53.9\nΠGDM (Song et al., 2023)\n100\n8.6\n64.1\nPalette (Saharia et al., 2022)\n1000\n8.3\n64.2\nCDSB (Shi et al., 2022)\n50\n38.7\n45.7\nI2SB (Liu et al., 2023a)\n1000\n4.6\n67.9\nI2SB (Liu et al., 2023a)\n100\n5.4\n67.5\nIBMD-I2SB (Ours)\n1\n5.3\n67.2\nFor all the setups we use the same train part of ImageNet\ndataset, which was used to train the used models. For the\nevaluation we follow the same protocol used in the I2SB\npaper, i.e. use the full validation subset of ImageNet for\nsuper-resolution task and the 10′000 subset of validation for\nother tasks. We report the same FID (Heusel et al., 2017)\nand Classifier Accuracy (CA) using pre-trained ResNet50\nmodel metrics used in the I2SB paper. We present our results\nin Table 1, Table 3, Table 2, Table 4 and Table 6. We provide\nthe uncurated samples for all setups in Appendix C.\nResults. For both super-resolution tasks (see Table 1, Ta-\nble 3), our 1-step distilled model outperformed teacher\nmodel inference using all 1000 steps used in the training.\nNote that our model does not use the clean training target\ndata p(x0), only the corrupted p(xT ), hence this improve-\nment is not due to additional training using paired data. We\nhypothesize that it is because the teacher model introduces\napproximation error during many steps of sampling, which\nmay accumulate. For both JPEG restoration (see Table 2, Ta-\nble 4), our 1-step distilled generator provides the quality of\ngeneration close to the teacher model and achieves around\n100x time acceleration. For the inpainting problem (see\nTable 6), we present the results for 1, 2 and 4 steps distilled\ngenerator. Our 2 and 4-step generators provide a quality\nsimilar to the teacher I2SB model, in turn, there is still some\ngap for the 1-step model. These models provide around 5x\ntime acceleration. We hypothesize that this setup is harder\nTable 3. Results on the image super-resolution task. Baseline re-\nsults are taken from I2SB (Liu et al., 2023a).\n4× super-resolution (pool)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n14.8\n64.6\nDDNM (Wang et al., 2023)\n100\n9.9\n67.1\nΠGDM (Song et al., 2023)\n100\n3.8\n72.3\nADM (Dhariwal & Nichol, 2021)\n1000\n3.1\n73.4\nCDSB (Shi et al., 2022)\n50\n13.0\n61.3\nI2SB (Liu et al., 2023a)\n1000\n2.7\n71.0\nIBMD-I2SB (Ours)\n1\n2.6\n72.7\nTable 4. Results on the image JPEG restoration task with QF=10.\nBaseline results are taken from I2SB (Liu et al., 2023a).\nJPEG restoration, QF= 10.\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n16.7\n64.7\nΠGDM (Song et al., 2023)\n100\n6.0\n71.0\nPalette (Saharia et al., 2022)\n1000\n5.4\n70.7\nCDSB (Shi et al., 2022)\n50\n18.6\n60.0\nI2SB (Liu et al., 2023a)\n1000\n3.6\n72.1\nI2SB (Liu et al., 2023a)\n100\n4.4\n71.6\nIBMD-I2SB (Ours)\n1\n3.8\n72.4\nfor our model since it is required to generate the entire center\nfragment from scratch, while in other tasks, there is already\nsome good approximation given by corrupted images.\n5.2. Distillation of DDBM (3 setups)\nThis section addresses two primary objectives: (1) demon-\nstrating the feasibility of conditional model distillation\nwithin our framework and (2) comparing with the CDBM\n(He et al., 2024) - a leading approach in Conditional Bridge\nMatching distillation, presented into different models: CBD\n(consistency distillation) and CBT (consistency training).\nExperimental Setup. For evaluation, we use the same\nsetups used in competing methods (He et al., 2024; Zheng\net al., 2024). For the image-to-image translation task, we\nutilize the Edges→Handbags dataset (Isola et al., 2017)\nwith a resolution of 64 × 64 pixels and the DIODE-Outdoor\ndataset (Vasiljevic et al., 2019) with a resolution of 256×256\npixels. For these tasks, we report FID and Inception Scores\n(IS) (Barratt & Sharma, 2018). For the image inpainting\ntask, we use the same setup of center-inpainting as before.\nResults. We utilized the same teacher model checkpoints\nand as in CDBM. We present the quantitative and qualitative\nresults of IBMD on the image-to-image translation task in\nTable 5 and in Figures 12, 10 respectively. The compet-\ning methods, DBIM (Zhou et al., 2024a, Section 4.1) and\nCDBM (He et al., 2024, Section 3.4), cannot use single-step\ninference due to the singularity at the starting point xT .\n7\n\nInverse Bridge Matching Distillation\nTable 5. Results on the Image-to-Image Translation Task (Training Sets). Methods are grouped by NFE (> 2, 2, 1), with the best metrics\nbolded in each group. Baselines results are taken from CDBM.\nNFE\nEdges →Handbags (64 × 64)\nDIODE-Outdoor (256 × 256)\nFID ↓\nIS ↑\nFID ↓\nIS ↑\nDDIB (Su et al., 2022)\n≥40\n186.84\n2.04\n242.3\n4.22\nSDEdit (Meng et al., 2021)\n≥40\n26.5\n3.58\n31.14\n5.70\nRectified Flow (Liu et al., 2022a)\n≥40\n25.3\n2.80\n77.18\n5.87\nI2SB (Liu et al., 2023a)\n≥40\n7.43\n3.40\n9.34\n5.77\nDBIM (Zheng et al., 2024)\n50\n1.14\n3.62\n3.20\n6.08\nDBIM (Zheng et al., 2024)\n100\n0.89\n3.62\n2.57\n6.06\nCBD (He et al., 2024)\n2\n1.30\n3.62\n3.66\n6.02\nCBT (He et al., 2024)\n0.80\n3.65\n2.93\n6.06\nIBMD-DDBM (Ours)\n0.67\n3.69\n3.12\n5.92\nPix2Pix (Isola et al., 2017)\n1\n74.8\n4.24\n82.4\n4.22\nIBMD-DDBM (Ours)\n1.26\n3.66\n4.07\n5.89\nTable 6. Results on the Image Inpainting Task.\nMethods are\ngrouped by NFE (> 4, 4, 2, 1), with the best metrics bolded\nin each group. Baselines results are taken from CDBM.\nInpainting, Center (128 × 128)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n24.4\n62.1\nΠGDM (Song et al., 2023)\n100\n7.3\n72.6\nDDNM (Wang et al., 2022)\n100\n15.1\n55.9\nPalette (Saharia et al., 2022)\n1000\n6.1\n63.0\nI2SB (Liu et al., 2023a)\n10\n5.4\n65.97\nDBIM (Zheng et al., 2024)\n50\n3.92\n72.4\nDBIM (Zheng et al., 2024)\n100\n3.88\n72.6\nCBD (He et al., 2024)\n4\n5.34\n69.6\nCBT (He et al., 2024)\n4.77\n70.3\nIBMD-I2SB (Ours)\n5.1\n70.3\nIBMD-DDBM (Ours)\n4.03\n72.2\nCBD (He et al., 2024)\n2\n5.65\n69.6\nCBT (He et al., 2024)\n5.34\n69.8\nIBMD-I2SB (Ours)\n5.3\n65.7\nIBMD-DDBM (Ours)\n4.23\n72.3\nIBMD-I2SB (Ours)\n1\n6.7\n65.0\nIBMD-DDBM (Ours)\n5.87\n70.6\nWe trained our IBMD with 1 and 2 NFEs on the\nEdges→Handbags dataset. We surpass CDBM at 2 NFE,\noutperform the teacher at 100 NFE, and achieve perfor-\nmance comparable to the teacher at 50 NFE with 1 NFE,\nresulting in a 50× acceleration. For the DIODE-Outdoor\nsetup, we trained IBMD with 1 and 2 NFEs. We surpassed\nCBD in FID at 2 NFE, achieving results comparable to CBT\nwith a slight drop in performance and maintaining strong\nperformance at 1 NFE with minor quality reductions.\nFor image inpainting, Table 6 and Figure 9 show the quanti-\ntative and qualitative results of IBMD. We train IBMD with\n4 NFE for image inpainting. It outperforms CBD and CBT\nat 4 NFE with a significant gap, surpassing both at 2 NFE\nand maintaining strong performance at 1 NFE while achiev-\ning teacher-level results at 50 NFE with a 12.5× speedup.\nConcerns regarding the evaluation protocol used in prior\nworks. For Edges-Handbags and DIODE-Outdoor setups,\nwe follow the evaluation protocol originally introduced in\nDDBM (Zhou et al., 2024a) and later used in works on\nacceleration of DDBM (Zheng et al., 2024; He et al., 2024).\nFor some reason, this protocol implies evaluation of the\ntrain set. Furthermore, test sets of these datasets consist of\na tiny fraction of images (around several hundred), making\nthe usage of standard metrics like FID challenging due to\nhigh statistical bias or variance of their estimation. Still,\nto assess the quality of the distilled model on the test sets,\nwe provide the uncurated samples produced by our distill\nmodel and teacher model on these sets in Figures 13 and\n11 in Appendix C. We also provide the uncurated samples\non the train part in Figures 12 and 10 to compare models’\nbehavior on train and test sets. From these results, we see\nthat the teacher model exhibits overfitting on both setups,\ne.g., it produces exactly the same images as corresponding\nreference images. In turn, on the test sets, teacher models\nwork well for the handbag setups, while on the test set of\nDIODE images, it exhibits mode collapse and produces gray\nimages. Nevertheless, our distilled model shows exactly\nthe same behavior in both sets, i.e., our IBMD approach\nprecisely distills the teacher model as expected.\n6. Discussion\nPotential impact. DBMs are used for data-to-data trans-\nlation in different domains, including images, audio, and\nbiological data. Our distillation technique provides a univer-\nsal and efficient way to address the long inference of DBMs,\nmaking them more affordable in practice.\nLimitations. Our method alternates between learning an\nadditional bridge model and updating the student, which\nmay be computationally expensive. Moreover, the student\noptimization requires backpropagation through the teacher,\nadditional bridge, and the generator network, making it 3x\ntime more memory expensive than training the teacher.\n8\n\nInverse Bridge Matching Distillation\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nBarratt, S. and Sharma, R. A note on the inception score.\narXiv preprint arXiv:1801.01973, 2018.\nDe Bortoli, V., Liu, G.-H., Chen, T., Theodorou, E. A., and\nNie, W. Augmented bridge matching. arXiv preprint\narXiv:2311.06978, 2023.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248–255. Ieee, 2009.\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis.\nAdvances in neural information\nprocessing systems, 34:8780–8794, 2021.\nDoob, J. L. and Doob, J. Classical potential theory and its\nprobabilistic counterpart, volume 262. Springer, 1984.\nGushchin, N., Selikhanovych, D., Kholkin, S., Burnaev,\nE., and Korotin, A. Adversarial schr\\” odinger bridge\nmatching. arXiv preprint arXiv:2405.14449, 2024.\nHe, G., Zheng, K., Chen, J., Bao, F., and Zhu, J.\nConsistency diffusion bridge models.\narXiv preprint\narXiv:2410.22637, 2024.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nHo, J., Jain, A., and Abbeel, P.\nDenoising diffusion\nprobabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840–6851, 2020.\nHuang, Z., Geng, Z., Luo, W., and Qi, G.-j. Flow generator\nmatching. arXiv preprint arXiv:2410.19310, 2024.\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-to-\nimage translation with conditional adversarial networks.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 1125–1134, 2017.\nKawar, B., Elad, M., Ermon, S., and Song, J.\nDe-\nnoising diffusion restoration models.\nAdvances in\nNeural Information Processing Systems, 35:23593–\n23606, 2022.\nKong, Z., Shih, K. J., Nie, W., Vahdat, A., Lee, S.-\ng., Santos, J. F., Jukic, A., Valle, R., and Catanzaro,\nB. A2sb: Audio-to-audio schrodinger bridges. arXiv\npreprint arXiv:2501.11311, 2025.\nLi, B., Xue, K., Liu, B., and Lai, Y.-K. Bbdm: Image-to-\nimage translation with brownian bridge diffusion mod-\nels.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern Recognition, pp. 1952–1961,\n2023.\nLipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M.,\nand Le, M.\nFlow matching for generative modeling.\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=PqvMRDCJT9t.\nLiu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A.,\nNie, W., and Anandkumar, A. I2sb: Image-to-image\nschr\\” odinger bridge. arXiv preprint arXiv:2302.05872,\n2023a.\nLiu, X., Gong, C., et al. Flow straight and fast: Learn-\ning to generate and transfer data with rectified flow.\nIn The Eleventh International Conference on Learning\nRepresentations, 2022a.\nLiu, X., Wu, L., Ye, M., and qiang liu. Let us build bridges:\nUnderstanding and extending diffusion generative models.\nIn NeurIPS 2022 Workshop on Score-Based Methods,\n2022b. URL https://openreview.net/forum?\nid=0ef0CRKC9uZ.\nLiu, X., Gong, C., and qiang liu.\nFlow straight and\nfast: Learning to generate and transfer data with rec-\ntified flow.\nIn The Eleventh International Conference\non Learning Representations, 2023b.\nURL https:\n//openreview.net/forum?id=XVjTT1nw5z.\nMeng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,\nS. Sdedit: Image synthesis and editing with stochastic\ndifferential equations. arXiv preprint arXiv:2108.01073,\n2021.\nPavon, M. and Wakolbinger, A. On free energy, stochas-\ntic control, and schr¨odinger processes.\nIn Modeling,\nEstimation and Control of Systems with Uncertainty:\nProceedings of a Conference held in Sopron, Hungary,\nSeptember 1990, pp. 334–348. Springer, 1991.\nPeluchetti, S.\nDiffusion bridge mixture transports,\nschr¨odinger bridge problems and generative modeling.\nJournal of Machine Learning Research, 24(374):1–51,\n2023a.\nPeluchetti, S. Non-denoising forward-time diffusions. arXiv\npreprint arXiv:2312.14589, 2023b.\n9\n\nInverse Bridge Matching Distillation\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,\nT., Fleet, D., and Norouzi, M. Palette: Image-to-image\ndiffusion models. In ACM SIGGRAPH 2022 conference\nproceedings, pp. 1–10, 2022.\nShi, Y., De Bortoli, V., Deligiannidis, G., and Doucet,\nA. Conditional simulation using diffusion schr¨odinger\nbridges.\nIn Uncertainty in Artificial Intelligence, pp.\n1792–1802. PMLR, 2022.\nShi, Y., Bortoli, V. D., Campbell, A., and Doucet, A. Dif-\nfusion schr¨odinger bridge matching. In Thirty-seventh\nConference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?\nid=qy07OHsJT5.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256–2265. PMLR, 2015.\nSomnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R.,\nKrause, A., and Bunne, C. Aligned diffusion schr¨odinger\nbridges.\nIn Uncertainty in Artificial Intelligence, pp.\n1985–1995. PMLR, 2023.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.\nnet/forum?id=St1giarCHLP.\nSong, J., Vahdat, A., Mardani, M., and Kautz, J.\nPseudoinverse-guided diffusion models for inverse\nproblems.\nIn International Conference on Learning\nRepresentations, 2023.\nSu, X., Song, J., Meng, C., and Ermon, S. Dual diffusion\nimplicit bridges for image-to-image translation. arXiv\npreprint arXiv:2203.08382, 2022.\nTong, A. Y., Malkin, N., Fatras, K., Atanackovic, L., Zhang,\nY., Huguet, G., Wolf, G., and Bengio, Y. Simulation-\nfree schr¨odinger bridges via score and flow matching. In\nInternational Conference on Artificial Intelligence and\nStatistics, pp. 1279–1287. PMLR, 2024.\nVargas, F., Thodoroff, P., Lamacraft, A., and Lawrence,\nN. Solving schr¨odinger bridges via maximum likelihood.\nEntropy, 23(9):1134, 2021.\nVasiljevic, I., Kolkin, N., Zhang, S., Luo, R., Wang, H., Dai,\nF. Z., Daniele, A. F., Mostajabi, M., Basart, S., Walter,\nM. R., et al. Diode: A dense indoor and outdoor depth\ndataset. arXiv preprint arXiv:1908.00463, 2019.\nWang, Y., Yu, J., and Zhang, J. Zero-shot image restora-\ntion using denoising diffusion null-space model. arXiv\npreprint arXiv:2212.00490, 2022.\nWang, Y., Yu, J., and Zhang, J.\nZero-shot image\nrestoration using denoising diffusion null-space model.\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=mRieQgMtNTQ.\nWang, Y., Yoon, S., Jin, P., Tivnan, M., Song, S., Chen,\nZ., Hu, R., Zhang, L., Chen, Z., Wu, D., et al. Implicit\nimage-to-image schr¨odinger bridge for image restora-\ntion. Zhiqiang and Wu, Dufan, Implicit Image-to-Image\nSchr¨odinger Bridge for Image Restoration, 2024.\nYin, T., Gharbi, M., Park, T., Zhang, R., Shechtman,\nE., Durand, F., and Freeman, W. T.\nImproved dis-\ntribution matching distillation for fast image synthe-\nsis. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024a. URL https:\n//openreview.net/forum?id=tQukGCDaNT.\nYin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F.,\nFreeman, W. T., and Park, T. One-step diffusion with\ndistribution matching distillation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 6613–6623, 2024b.\nYue, Z., Wang, J., and Loy, C. C. Resshift: Efficient diffu-\nsion model for image super-resolution by residual shifting.\nAdvances in Neural Information Processing Systems, 36,\n2024.\nZheng, K., He, G., Chen, J., Bao, F., and Zhu, J. Diffusion\nbridge implicit models. arXiv preprint arXiv:2405.15885,\n2024.\nZhou, L., Lou, A., Khanna, S., and Ermon, S. Denoising dif-\nfusion bridge models. arXiv preprint arXiv:2309.16948,\n2023.\nZhou, L., Lou, A., Khanna, S., and Ermon, S.\nDe-\nnoising diffusion bridge models.\nIn The Twelfth\nInternational Conference on Learning Representations,\n2024a. URL https://openreview.net/forum?\nid=FKksTayvGo.\nZhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H.\nScore identity distillation: Exponentially fast distilla-\ntion of pretrained diffusion models for one-step genera-\ntion. In Forty-first International Conference on Machine\nLearning, 2024b.\n10\n\nInverse Bridge Matching Distillation\nA. Proofs\nSince all our theorems, propositions and proofs for the inverse Bridge Matching problems which is formulated for the already\ntrained teacher model using some diffusion bridge, we assume all corresponding assumptions used in Bridge Matching.\nExtensive overview of them can be found in (Shi et al., 2023, Appendix C).\nProof of Proposition 3.1. Since both BM(Πθ) and M ∗given by reverse-time SDE and the same distribution pT (xT ) the\nKL-divergence expressed in the tractable form using the disintegration and Girsanov theorem (Vargas et al., 2021; Pavon &\nWakolbinger, 1991):\nKL(BM(Πθ)||M ∗) = Ext,t\n\x02\ng2(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nThe expectation is taken over the marginal distribution p(xt) of Πθ since it is the same as for BM(Πθ) (Shi et al., 2023,\nProposition 2). In turn, the drift v(xt, t) is the drift of Bridge Matching using Πθ, i.e. BM(Πθ):\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nCombining this, the inverse problem can be expressed in a more tractable form:\nmin\nθ\nExt,t\n\x02\ng2(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(11)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\ndt,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nWe can add positive valued weighting function λ(t) for the constraint:\nv = arg min\nv′\nExt,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\ndt,\nsince it is the MSE regression and its solution is conditional expectation for any weights given by:\nv(xt, t) = Ex0|xt,t\n\x02\n∇xt log q(xt|x0)].\nWe can add positive valued weighting function λ(t) for the main functional:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\nsince it does not change the optimum value (which is equal to 0) and optimal solution, which is the mixture of bridges with\nthe same drift as the teacher model.\nProof of Theorem 3.2. Consider inverse bridge matching optimization problem:\nmin\nθ\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(12)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nFirst, note that since v = arg minv′ Ext,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n, i.e. minimizer of MSE functional it is given\nby conditional expectation as:\nv(xt, t) = Ex0|xt,t\n\x02\n∇xt log q(xt|x0)|xt, t\n\x03\n.\n(13)\nThen note that:\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\n11\n\nInverse Bridge Matching Distillation\nExt,t,x0\n\x02\nλ(t)∥v(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\nExt,t,x0\n\x02\nλ(t)||v(xt, t)||2\x03\n|\n{z\n}\nExt,t\n\x02\nλ(t)||v(xt,t)||2\x03\n−2Ext,t,x0\n\x02\nλ(t)⟨v(xt, t), ∇xt log q(xt|x0)⟩\n\x03\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n=\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n−2Ext,t\nh\nλ(t)\n*\nv(xt, t), Ex0|xt,t\n\x02\n∇xt log q(xt|x0)\n\x03\n|\n{z\n}\n=v(xt,t)\n+ i\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n=\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n−2Ext,t\n\x02\nλ(t)||v(xt, t)||2\x03\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n=\n−Ext,t\n\x02\nλ(t)||v(xt, t)||2\x03\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n.\n(14)\nHence, we derive that\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n= Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−min\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n.\nNow we use it to reformulate the initial objective:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n=\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n−2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t\n\x02\nλ(t)||v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2] −min\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n|\n{z\n}\n=Ext,t\n\x02\nλ(t)||v(xt,t)||2\x03\n−\n2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t\n\x02\nλ(t)||v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t\n\x02\nλ(t)||v∗(xt, t)||2\x03\n|\n{z\n}\nExt,t,x0\n\x02\nλ(t)||v∗(xt,t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\nTherefore, we get:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t,x0\n\x02\nλ(t)||v∗(xt, t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\nTo complete the proof, we use the relation v(xt, t) = Ex0|xt,t\n\x02\n∇xt log q(xt|x0)|xt, t\n\x03\nfrom Equation 13. Integrating these\ncomponents, we arrive at the final result:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t\n\x02\nλ(t)\n\nEx0|xt,t\n\x02\n∇xt log q(xt|x0)|xt, t\n\x03\n, v∗(xt, t)\n\x0b \x03\n+ Ext,t,x0\n\x02\nλ(t)||v∗(xt, t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t,x0\n\x02\nλ(t) ⟨∇xt log q(xt|x0), v∗(xt, t)⟩\n\x03\n+ Ext,t,x0\n\x02\nλ(t)||v∗(xt, t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−min\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n.\nProof of Proposition 3.3. Consider the problem from Proposition 3.2:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n,\n12\n\nInverse Bridge Matching Distillation\nFor the priors with the drift f(t)x the regression target is ∇xt log q(xt|x0) = −xt−αtx0\nσ2\nt\n. Hence one can use the parametriza-\ntion v(xt, t) = −xt−αtbx0(xt,t)\nσ2\nt\nWe use reparameterization of both v∗and vϕ given by:\nv∗(xt, t) = −xt −αtbx∗\n0(xt, t)\nσ2\nt\n,\nvϕ(xt, t) = −xt −αtbxϕ\n0(xt, t)\nσ2\nt\nand get:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n=\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)α2\nt\nσ4\nt\n| {z }\ndef\n=λ′(t)\n∥bx∗\n0(xt, t) −x0∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)α2\nt\nσ4\nt\n| {z }\ndef\n=λ′(t)\n∥bxϕ\n0(xt, t) −x0∥2\x03i\n=\nmin\nθ\nh\nExt,t,x0\n\x02\nλ′(t)∥bx∗\n0(xt, t) −x0∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ′(t)∥bxϕ\n0(xt, t) −x0∥2\x03i\n,\nwhere λ′(t) is just another positive weighting function.\nProof of Theorem 3.4. In a fully analogical way, as for the unconditional case we consider the set of the Inverse Bridge\nMatching problems indexes by xT :\n\x08\nmin\nθ\n\x02\nKL(BM(Πθ|xT )||M ∗\n|xT )\n\x03\t\nxT ,\nwhere M ∗\n|xT is a result of Bridge Matching conditioned on xT and Πθ|xT is a Mixture of Bridges for each xT constructed\nusing bridge q(xt|x0, xT ) and coupling pθ(x0|xT )δxT (x).\nBy employing the same reasoning as in the proof of Proposition 3.1, the inverse problem can be reformulated as follows:\nmin\nθ\nExt,t,xT\n\x02\ng2(t)||v(xt, t, xT ) −v∗(xt, t, xT )||2\x03\n,\ns.t.\nv = arg min\nv′\nExt,t,x0,xT\n\x02\n∥v′(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\ndt,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nFollowing the proof of Theorem 3.2, we obtain a tractable formulation incorporating a weighting function:\nmin\nθ\nh\nExt,t,x0,xT\n\x02\nλ(t)∥v∗(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\n−\nmin\nϕ Ext,t,x0,xT\n\x02\nλ(t)∥vϕ(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03i\n.\nUtilizing the reparameterization under additional conditions (M2.3), we obtain the following representations:\nv∗(xt, t, xT ) = −xt −αtbx∗\n0(xt, t, xT )\nσ2\nt\n,\nvϕ(xt, t, xT ) = −xt −αtbxϕ\n0(xt, t, xT )\nσ2\nt\n.\nConsequently, applying the proof technique from Proposition 3.3, we derive the final expression:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥bx∗\n0(xt, t, xT ) −x0∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nB. Experimental details\nAll hyperparameters are listed in Table 7. We used batch size 256 and ema decay 0.99 for setups. For each setup, we started\nthe student and bridge networks using checkpoints from the teacher models. In setups where the model adapts to noise: (1)\nWe added extra layers for noise inputs (set to zero initially), (2) Noise was concatenated with input data before input it to the\nnetwork. Datasets, code sources, and licenses are included in Table 8.\n13\n\nInverse Bridge Matching Distillation\nTask\nDataset\nTeacher\nNFE\nL/K ratio\nLR\nGrad Updates\nNoise\n4× super-resolution (bicubic)\nImageNet\nI2SB\n1\n5:1\n5e-5\n3000\n✓\n4× super-resolution (pool)\nImageNet\nI2SB\n1\n5:1\n5e-5\n3000\n✓\nJPEG restoration, QF = 5\nImageNet\nI2SB\n1\n5:1\n5e-5\n2000\n✓\nJPEG restoration, QF = 10\nImageNet\nI2SB\n1\n5:1\n5e-5\n3000\n✓\nCenter-inpainting (128 × 128)\nImageNet\nI2SB\n4\n5:1\n5e-5\n2000\n✗\nSketch to Image\nEdges →Handbags\nDDBM\n2\n5:1\n1e-5\n300\n✓\nSketch to Image\nEdges →Handbags\nDDBM\n1\n5:1\n1e-5\n14000\n✓\nNormal to Image\nDIODE-Outdoor\nDDBM\n2\n5:1\n1e-5\n500\n✓\nNormal to Image\nDIODE-Outdoor\nDDBM\n1\n5:1\n1e-5\n3700\n✓\nCenter-inpainting (128 × 128)\nImageNet\nDDBM\n4\n1:1\n3e-6\n3000\n✓\nTable 7. Table entries specify experimental configurations: NFE indicates multistep training (Sec. M3.5); L/K represents bridge/student\ngradient iteration ratios (Alg. M3.4); Grad Updates shows student gradient steps; Noise notes stochastic pipeline incorporation.\nTable 8. The used datasets, codes and their licenses.\nName\nURL\nCitation\nLicense\nEdges→Handbags\nGitHub Link\n(Isola et al., 2017)\nBSD\nDIODE-Outdoor\nDataset Link\n(Vasiljevic et al., 2019)\nMIT\nImageNet\nWebsite Link\n(Deng et al., 2009)\n\\\nGuided-Diffusion\nGitHub Link\n(Dhariwal & Nichol, 2021)\nMIT\nI2SB\nGitHub Link\n(Liu et al., 2023a)\nCC-BY-NC-SA-4.0\nDDBM\nGitHub Link\n(Zhou et al., 2023)\n\\\nDBIM\nGitHub Link\n(Zheng et al., 2024)\n\\\nB.1. Distillation of I2SB models.\nWe extended the I2SB repository (see Table 8), integrating our distillation framework. The following sections outline the\nsetups, adapted following the I2SB.\nMultistep implementation In this setup, we use the student model’s full inference process during multistep training (Section\n3.5). This means that x0 is generated with inferenced of the model Gθ through all timesteps (T = tN, . . . , t1 = 0) in the\nmultistep sequence. The generated x0 is subsequently utilized in the computation of the bridge bLϕ or student bLθ objective\nfunctions, as formalized in Algorithm 1.\n4× super-resolution. Our implementation of the degradation operators aligns with the filters implementation proposed\nin DDRM (Kawar et al., 2022). Firstly, we synthesize images at 64 × 64 resolution, then upsample them to 256 × 256 to\nensure dimensional consistency between clean and degraded inputs. For evaluation, we follow established benchmarks\n(Saharia et al., 2022; Song et al., 2023) by computing the FID on reconstructions from the full ImageNet validation set, with\ncomparisons drawn against the training set statistics.\nJPEG restoration. Our JPEG degradation implementation, employing two distinct quality factors (QF=5, QF=10), follows\n(Kawar et al., 2022). FID is evaluated on a 10, 000-image ImageNet validation subset against the full validation set’s\nstatistics, following baselines (Saharia et al., 2022; Song et al., 2023).\nInpainting. For the image inpainting task on ImageNet at 256 × 256 resolution, we utilize a fixed 128 × 128 centrally\npositioned mask, aligning with the methodologies of DBIM (Zheng et al., 2024) and CDBM (He et al., 2024). During\ntraining, the model is trained only on the masked regions, while during generation, the unmasked areas are deterministically\nretained from the initial corrupted image xT to preserve structural fidelity of unmasked part of images. We trained the model\nwith 4 NFEs via the multistep method (Section 3.5) and tested it with 1, 2, and 4 NFEs.\nB.2. Distillation of DDBM models.\nWe extended the DDBM repository (Table 8) by integrating our distillation framework. Subsequent sections outline the\nexperimental setups, adapted from the DDBM (Zheng et al., 2024).\nMultistep implementation In this setup, the multistep training (Section 3.5) adopts the methodology of DMD (Yin et al.,\n2024a), wherein a timestep t is uniformly sampled from the predefined sequence (t1, . . . , tN). The model Gθ then generates\n14\n\nInverse Bridge Matching Distillation\nx0 by iteratively reversing the process from the terminal timestep tN = T to the sampled intermediate timestep t. This\ngenerated x0 is subsequently used to compute the bridge network’s loss bLϕ or the student network’s loss bLθ, as detailed in\nAlgorithm 1.\nEdges →Handbags The model was trained utilizing the Edges→Handbags image-to-image translation task (Isola et al.,\n2017), with the 64 × 64 resolution images. Two versions were trained under the multistep regime (Section 3.5), with 2 and 1\nNFEs during training. Both models were evaluated using the same NFE to match training settings.\nDIODE-Outdoor Following prior work (Zhou et al., 2023; Zheng et al., 2024; He et al., 2024), we used the DIODE outdoor\ndataset, preprocessed via the DBIM repository’s script for training/test sets (Table 8). Two versions were trained under the\nmultistep regime (Section 3.5), with 2 and 1 NFEs during training. Both models were evaluated using the same NFE to\nmatch training settings.\nInpainting All setups matched those in Section B.1 inpainting, except we use a CBDM checkpoint (Zheng et al., 2024).\nThis checkpoint is adjusted by the authors to: (1) condition on xT and (2) ImageNet class labels as input to guide the model.\nAlso this is the same checkpoint used in both CDBM (He et al., 2024) and DBIM (Zheng et al., 2024) works.\nC. Additional results\n15\n\nInverse Bridge Matching Distillation\nFigure 4. Uncurated samples for IBMD-I2SB distillation of 4x-super-resolution with bicubic kernel on ImageNet 256 × 256 images.\n16\n\nInverse Bridge Matching Distillation\nFigure 5. Uncurated samples for IBMD-I2SB distillation of 4x-super-resolution with pool kernel on ImageNet 256 × 256 images.\n17\n\nInverse Bridge Matching Distillation\nFigure 6. Uncurated samples for IBMD-I2SB distillation of Jpeg restoration with QF=5 on ImageNet 256 × 256 images.\n18\n\nInverse Bridge Matching Distillation\nFigure 7. Uncurated samples for IBMD-I2SB distillation of Jpeg restoration with QF=10 on ImageNet 256 × 256 images.\n19\n\nInverse Bridge Matching Distillation\nFigure 8. Uncurated samples for IBMD-I2SB distillation trained for inpaiting with NFE= 4 and inferenced with different inference NFE\non ImageNet 256 × 256 images.\n20\n\nInverse Bridge Matching Distillation\nFigure 9. Uncurated samples for IBMD-DDBM distillation trained for inpaiting with NFE= 4 and inferenced with different inference\nNFE on ImageNet 256 × 256 images.\n21\n\nInverse Bridge Matching Distillation\nFigure 10. Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset (256 × 256) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the training set.22\n\nInverse Bridge Matching Distillation\nFigure 11. Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset (256 × 256) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the test set.\n23\n\nInverse Bridge Matching Distillation\nFigure 12. Uncurated samples from IBMD-DDBM distillation trained on the Edges →Handbags dataset (64 × 64) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the training set.24\n\nInverse Bridge Matching Distillation\nFigure 13. Uncurated samples from IBMD-DDBM distillation trained on the Edges →Handbags dataset (64 × 64) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the test set.\n25'),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\nFine-Grained Understanding and Generation\nXueqing Deng\nQihang Yu\nAli Athar\nChenglin Yang\nLinjie Yang\nXiaojie Jin\nXiaohui Shen\nLiang-Chieh Chen\nByteDance Seed\nProject Page\nxueqingdeng@bytedance.com\n#mask <15\n15 < #mask <25\n#mask >25\n#instance masks = 5\n#semantic masks = 2\n#instance masks = 13\n#semantic masks = 5\n#instance masks = 54\n#semantic masks = 9\nThe image shows a meal plate with various labeled regions. In \nthe center and right side of the plate, the region represents <0:a \nserving of French fries>. The upper left side shows <1: a slice of \nonion>. Positioned on the lower left side, it is <2: a small blue \nbowl> containing a red sauce, likely ketchup, while <3:another \nbowl> near the top center hold additional dipping sauce. At the \nbottom right of the image, <4: a silver knife> is partially visible \nunder some of the food items. The main item on the plate, <5:a \nsandwich with a sesame seed bun> occupies the center and \nthere is <6: another sandwich> on the right. Finally, all these \nitems are put on the <7: dining table surface>. Together, the \nimage captures the different elements of the meal setup, \nshowing a typical plate with a sandwich, fries, and dipping \nsauce.\nThe image depicts a cozy living room with various items of furniture and \ndecor. The <0:wooden floor> represents the wooden flooring that spans the \nroom, partially covered by <1: a black and white patterned rug> in the \ncenter. The <2:light yellow walls> surrounding the other elements in the \nroom. <3:A  coffee table> sits on the rug where there is <16: a book> on top\nof it, in front of the seating area. Against the back wall, there is <4: wooden\ncabinet>, which holds the <10: tv with screen off> , <15: a vase> and some \ndecorative items. The <5: window> is located on the back wall, partially \ncovered by <6: white window blind>, which lets light into the room. Above \nthe cabinet, <7:a shelf> holds various decor pieces, including <14:a vase>. \n<8: A mirror> is mounted on the left wall along with multiple frames, while \n<9:a light fixture> is visible in the upper right corner. In the foreground, <12:\na light-upholstered couch> where <11:a black cat> lies, adds a cozy touch. \nOn the right side, there is <13:a blue-upholstered chair> near the coffee \ntable, and <17:a reddish chair> positioned near the back wall.\nThe image shows a lively family gathering around <34:along dining table>, with various labeled \nindividuals, dining items, and furniture. In terms of people, the table is surrounded by multiple family \nmembers of varying ages, all seated and enjoying the meal together. Key individuals include <50:a\nlady with blonde short curly hair wearing tank>, <52: a lady with black medium hair in blue shirt>, \n<53: a women with black medium hair but wearing in tank top>, and <55: a boy in red t shirt >\nsitting right next to the lady in the left side of the image, as well as on the right there are <32: a\nwomen in white t shirt with black dots>, <41: a boy in white t shirt>. < 62,63,48, 47, 13, 56, 53, 54,\n55, 52, 50, 33, 42, 43, 41, 32, 45, 44, 47, 46:Additional people> are seated around or standing near \nthe table, filling the room with a sense of warmth and gathering. The dining items are plentiful and \ninclude numerous <14, 26, 25, 40: wine glass>, <15, 17, 36, 16, 20, 39, 38: cup>, and <12, 60:other \nbowl> and <30, 10:spoon> objects scattered across the table. Utensils, such as <5: fork> and <9: \nknife>, are visible among the plates of food, which include main dishes and side dishes. In the \nbackground, the <0: light colored wall> and <1: wooden cabinet> display family photos, framed art, \nand  <4: pink flowers> in <21:a glass vase>, adding a personal touch to the room decor. The \ncombination of food, drink, and a lively gathering suggests a celebratory or holiday meal. \nDetailed Captioning\nUSER: Could you please describe the\nimage in detail?\nASSISTANT: Three teddy bears, each a \ndifferent shade of brown, are positioned \ntogether on a blue bedsheet. On the left, a \ngolden brown teddy ….\nslightly with the others, also embracing its \nadjacent bears. The soft blue bedsheet in \nthe background provides a cozy base for \nthis heartwarming scene.\nPanoptic Segmentation Grounded Caption\nUSER: Please provide a detailed description of the image \nand segment each part.\nASSISTANT:\nText-to-Image Generation\nUSER:\nGenerate\nan\nimage\nwith\nthe\nfollowing\ndescription. This image shows a cozy, well-lit kitchen \nwith white cabinetry, giving it a bright, clean look. \nThe dark countertops contrast with the white \ncabinets. …\nASSISTANT:\nVQA:\nDownstream Task\nReferring Segmentation:\nUSER: How many kinds of\nvegetables ?\nASSISTANT: There are\nbroccoli, carrot ….\nUSER: Can you segment the\ncat which is on the toilet?\nASSISTANT: It is <SEG>.\nEmpowered Tasks by COCONut-PanCap Dataset\nFigure 1. COCONut-PanCap Dataset. Top: The proposed COCONut-PanCap dataset features detailed captions grounded with dense\npanoptic segmentation masks. Bottom: COCONut-PanCap supports various fine-grained understanding and generation tasks, including\ndetailed captioning, panoptic segmentation grounded caption, and text-to-image generation. The dataset also facilitates several downstream\ntasks, such as visual question-answering (VQA) and referring segmentation.\nAbstract\nThis paper introduces the COCONut-PanCap dataset, cre-\nated to enhance panoptic segmentation and grounded im-\nage captioning.\nBuilding upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims\nto overcome limitations in existing image-text datasets\nthat often lack detailed, scene-comprehensive descriptions.\nThe COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation\nmasks, ensuring consistency and improving the detail of\ngenerated captions. Through human-edited, densely anno-\ntated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image un-\nderstanding and generative models for text-to-image tasks.\nExperimental results demonstrate that COCONut-PanCap\nsignificantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-\nscale datasets. This dataset sets a new benchmark for eval-\nuating models on joint panoptic segmentation and grounded\narXiv:2502.02589v1  [cs.CV]  4 Feb 2025\n\ncaptioning tasks, addressing the need for high-quality, de-\ntailed image-text annotations in multi-modal learning.\n1. Introduction\nRecent advancements in multi-modal foundation models\nhave been largely driven by the availability of large-scale\npaired text-image datasets. These datasets, often collected\nvia web crawling with basic filtering techniques [14, 52,\n53], contain low-quality, web-sourced captions that lack\ndepth and accuracy. In contrast, human-annotated caption\ndatasets, such as COCO-caption [6], offer higher-quality\ndescriptions but are limited in scale and tend to be con-\ncise, with an average caption length of 10 words. To over-\ncome the limitations of short captions, the research commu-\nnity has leveraged vision-language models (VLMs) [5, 31,\n32, 38, 60] to generate detailed synthetic captions. While\nthese machine-generated captions improve visual under-\nstanding [5, 32] and generation tasks [31], they remain in-\nferior to high-quality, human-verified annotations [44].\nAddressing this challenge requires balancing scalability\nand annotation quality, as generating detailed and accurate\nimage descriptions at scale remains labor-intensive [15, 44].\nIn this paper, we introduce an efficient annotation approach\nthat combines dense mask annotations with commercial\nVLMs [5] to produce high-quality image captions. Our goal\nis to minimize human effort while generating rich, struc-\ntured descriptions.\nTo achieve this, we base our work on the COCO-caption\ndataset [6] due to its widespread use and diverse image con-\ntent. We revisit the COCO-caption dataset to provide more\ndetailed and comprehensive caption annotations. Our ap-\nproach involves creating holistic captions synthesized from\nregion-based dense captions that describe distinct areas\nwithin each image. Specifically, we build on recent CO-\nCONut panoptic segmentation annotations [9] to generate a\nnew set of detailed captions by: (a) annotating each segmen-\ntation region with a VLM-generated draft, carefully refined\nthrough human corrections, and (b) summarizing these re-\ngion captions into a comprehensive image caption while\npreserving the grounding correspondence between image\nmasks and object references. This enables a novel task that\nintegrates panoptic segmentation with grounded captioning.\nOur structured annotation process ensures that the captions\nare both complete, covering the majority of objects in each\nimage, and grounded, with precise segmentation masks.\nThe final dataset, named COCONut-PanCap, is de-\nsigned for a wide range of vision-language applica-\ntions, combining Panoptic segmentation and grounded\nCaptioning. It comprises 118K image-text pairs for train-\ning, with an average caption length of 203 words, as well as\nan additional 25K image-text pairs, with an average caption\nlength of 233 words for validation. We demonstrate that\nCOCONut-PanCap significantly boosts the performance of\nboth VLM and text-to-image generation models at the in-\nstruction tuning and fine-tuning stages, outperforming re-\ncent detailed caption datasets [44].\nThis highlights the\npotential of our grounding-based captions for both vision-\nlanguage understanding and image generation tasks.\nOur contributions are summarized as follows:\n• We propose a caption annotation pipeline leveraging\npanoptic segmentation to create a high-quality, detailed\ncaption dataset comprising 143K annotated images. The\nresulting annotations are comprehensive, accurate, and\ninclude grounding masks, making this dataset substan-\ntially larger than recent detailed caption datasets.\n• Our COCONut-PanCap dataset facilitates a new chal-\nlenging task combining Panoptic segmentation and\nGrounded Captioning (PGC). We establish evaluation\nmetrics and settings for this PGC task and benchmark sev-\neral recent methods to assess performance on this novel\nchallenge.\n• We validate the utility of our proposed dataset across var-\nious fine-grained Image-to-Text (I2T) and Text-to-Image\n(T2I) tasks, including detailed caption generation, PGC,\nvisual question answering (VQA), referring segmenta-\ntion, and text-conditioned image generation. Experimen-\ntal results show that our dataset significantly enhances\nmodel performance across all these tasks.\n2. Related Work\nDetailed Captions from VLMs. Researchers are increas-\ningly interested in creating large-scale datasets with detailed\ncaptions generated from advanced vision-language models.\nDenseFusion1M [32] utilizes a pretrained perceptual model\nto prompt VLMs, facilitating more detailed image descrip-\ntions.\nRecap-DataComp1B [31] first fine-tunes the Llama-3-\n8B powered LLaVA-1.5 model [36], then applies it to recap-\ntion approximately 1.3 billion images from the DataComp-\n1B dataset [14], generating a rich repository of detailed\nimage descriptions.\nOn a similar front, the PixelProse\ndataset [59] offers general-purpose image captions designed\nto serve various applications, from visual question answer-\ning (VQA) to pre-training tasks. Unlike datasets targeting\nsingle applications, PixelProse captions are dense, versa-\ntile image descriptions that can be adapted to other formats,\nsuch as VQA and instructional data, with the help of large\nlanguage models (LLMs).\nAlthough these detailed cap-\ntion datasets are large-scale, they are directly generated by\nVLMs without human verification, falling behind human-\nannotated captions on quality. Our proposed COCONut-\nPanCap dataset leverages extensive human effort to ensure\nhigh-quality annotations.\nHuman-annotated Detailed Captions.\nSeveral efforts\nhave been made toward this goal, utilizing fully human-\n\nDataset Name\nImage Source Sample\nAnnotated by Avg. Words Masks\nBLIP-LCS\nLAION [53], CC [4], SBU [45]\n558K\nBLIP [30]\n54\n✗\nDenseFusion1M [32]\nLAION [53] 1,059K Vision Specialist Models\n191\n✗\nLLaVA-Recap118K [38]\nCOCO [35]\n118K\nLLaVA-NEXT [38]\n186\n✗\nLLaVA-Details-23K [37]\nCOCO [35]\n23K\nGPT4\n105\n✗\nShareGPT4V [5]\nLAION [53], CC [4], SBU [45], COCO [35] etc.\n100K\nGPT4-Vision\n162\n✗\nShareGPT4V-PT [5]\nLAION [53], CC [4], SBU [45], COCO [35] etc. 1,246K\nShare-Captioner [5]\n144\n✗\nPixelLM-MUSE [51]\nLVIS [17]\n246K\nGPT4-Vision\n-\n3.7‡\nOsprey [69]\nCOCO [35]\n724K\nGPT4-Vision\n-\n-\nGLaMM-GCG [50]\nRefCOCOg [40],PSG [65],Flick30K [47]\n214K\nVision Specialist Models\n128\n3.6\nCOCO-caption [6]\nCOCO [35]\n118K\nHuman\n11\n✗\nDCI [61]\nSA-1B [24]\n8K\nHuman\n144\n✗\nDOCCI [44]\nDOCCI [44]\n9.6K\nHuman\n136\n✗\nIIW [15]\nWebLI [15]\n8.5K\nHuman\n217\n✗\nCOCONut-PanCap (ours)\nCOCO [35]\n118K\nHuman\n203\n13.2\nTable 1. Dataset (training set) Comparison. Our proposed COCONut-PanCap dataset stands out for its detailed (2nd highest in Average\nWords), high-quality (human interactive annotated) captions and high-density segmentation masks (1st in Average Masks). ‡ denotes the\nmask number for referring segmentation which only counts the targets in QA format. Note that “Samples” means the number of collected\nannotations, where there may exist one image with multiple different annotation, i.e., in region-level datasets like Osprey.\nDataset Name\nSamples Avg. Words Caption T2I Grd. Seg.\nCOCO-30K [6]\n30,000\n11\n✓\n✓\n✗\nDOCCI-test [44]\n5,000\n136\n✓\n✓\n✗\nIIW-test [15]\n445\n217\n✓\n✓\n✗\nGenEval [16]\n553\n8\n✗\n✓\n✗\nT2I-CompBench val [20]\n2400\n9\n✗\n✓\n✗\nGLaMM-GCG val-test [50]\n2,000\n128\n✓\n✗\n✓\nCOCONut-PanCap val (ours)\n25,000\n233\n✓\n✓\n✓\nTable 2. Dataset (evaluation set) Comparison. Our COCONut-\nPanCap validation set provides detailed captions and supports\nmultiple multi-modal tasks, including image captioning, text-to-\nimage generation (T2I), and grounded segmentation (Grd. Seg.).\nannotated data or human-in-the-loop approaches. One ex-\nample is DOCCI [44] which is a small, high-detailed image\ncaption dataset that is entirely human-annotated, contain-\ning only 15K samples but providing diverse details, such as\nkey objects, their attributes, spatial relationships, and text\nrendering. Two small-scale detailed caption datasets, Im-\nageInWords [15] and DCI [61], use a combination of au-\ntomatic annotation models with human involvement, both\nwith fewer than 10K samples. Pixmo-Cap [8] introduces a\nlarge-scale dataset of detailed image captions from speech-\nbased descriptions, offering richer visual annotations than\ntext-based methods.\nOur proposed COCONut-PanCap dataset yields smaller\nscale compare to Pixmo-Cap but we have different focuses\nwhere Pixmo-Cap focuses on pretraining the VLMs while\nwe focus on the instruction tuning and finetuning stages of\nVLMs and image generation models. Our work also shares\na similar annotation pipeline with a recent video captioning\ndataset Shot2Story [18] where both VLM draft and human\ncorrections are used to create complete and accurate anno-\ntations.\nGrounded Captions with Segmentation Masks. Exist-\ning work have made significant strides in creating datasets\nwith region-level captions linked to entity segmentation\nmasks [69] or bounding boxes [70].\nHowever, few\ndatasets associate grounded segmentation directly with\ncaptions.\nGLaMM [50] proposes a Grounding-anything\nDataset (GranD) using an automated annotation pipeline\nthat encompasses 7.5M unique concepts grounded in a total\nof 810M regions available with segmentation masks.\nLater,\nMGLMM\n[72]\nfurther\nexplore\nthe\nmulti-\ngranularity GLaMM model to generate a multi-granularity\ndataset. Our proposed COCONut-PanCap dataset follows\na similar approach of grounding captions to dense masks\nbut offers significantly denser masks per caption, as shown\nin Tab. 1, with an average of 13.2 masks per image com-\npared to 3.6 in GLaMM. Note that we focus on grounded\nsegmentation for detailed captions, rather than descriptions\nof all levels of segmentation masks (objects or parts) as pro-\nvided in the GranD dataset [50], which is outside the scope\nof our study.\n3. COCONut-PanCap Dataset\nWe construct a novel dataset based on COCO images to pro-\nvide detailed captions at both image and mask levels, using\nCOCONut panoptic masks as a foundation for comprehen-\nsive region descriptions. Specifically, we leverage panop-\ntic masks from COCONut-S [9] to annotate detailed region\ncaptions, incorporating both ‘thing’ and ‘stuff’ masks to\ncover a wide range of semantic regions.\n3.1. Dataset Description\nComprehensively understanding diverse visual elements in\ncomplex scenes can benefit multiple tasks including percep-\ntion, understanding, and generation. In this section, we de-\nscribe the annotation pipeline for our dataset leveraging the\nhuman annotated panoptic masks. We first show the sta-\n\ninput image\nhuman annotated\npanoptic segmentation mask\noverlaid image\nVLM\nset-of-marks\nprompt instruc4on\n0: road – The ground surface in the \nforeground, labeled as the road.1: wall-\nother-merged – A section of the wall on the \nleft side of the image, with a mural or design \npainted on it, including part of a blue \noctopus.2: window-other – A window \nlocated on the upper right side of the image, \nwhich has been labeled separately.\n…………\n t, also wearing a hat and purple clothing, \npossibly engaged in conversation.11: chair – \nThe chair that the first person is sitting \non.12: bicycle – A bicycle is located near the \ncenter of the image, leaning against the \nwall.13: handbag – A handbag or bag is near \nthe first person or placed beside the \nchair.14: chair – Another chair, possibly for \nthe second person sitting in the image.\ndetailed cap4on dra5\nhuman\nedit\nsummarize by VLM,\nveriﬁca4on check by\nhuman\nround-2\nround-1\nFigure 2. Annotation Pipeline. Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks [66]\nvisualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for\nediting and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning\nor finetuning stage.\n: Could you please describe the details of the image for each region?\n:\nFrom your image, here is the analysis:\n0: Sky - Clear blue sky with some scattered clouds.\n1: Road - Asphalt road, smooth and unobstructed, ideal for vehicles.\n2: Building-other-merged - Building structure with large glass windows \nand beige walls, likely a commercial or public building.\n3: Tree-merged - Lush green trees with dense foliage, possibly \nornamental in a landscaped area.\n4: Pavement-merged - Grey concrete pavement, typically used for \npedestrian pathways.\n5: Stop sign - Red octagonal traffic stop sign with white text, indicating a \nvehicle stop required.\n6: Car - A car in motion, likely in the process of slowing down as it \napproaches the stop sign.\n7: Truck - A white truck, stationary, parked in a designated area, possibly \nfor loading or unloading goods.\nWith the given region-level description, can you merge them into\none paragraph in the format of grounded caption?\nIn this scene, the clear blue sky with some scattered clouds \nserves as a pleasant backdrop. The smooth asphalt road \nprovides an unobstructed pathway, suitable for vehicles. \nNearby, there is a building structure with glass windows and \nbeige walls. Lush green trees with dense foliage add a touch \nof nature. The grey concrete pavement offers a pedestrian-\nfriendly path alongside the road. A upside\ndown\nred \noctagonal traffic stop sign with white text signals vehicles to \nhalt. Approaching the stop sign is a car, likely slowing down. In \nthe background, a stationary white truck is parked in a \ndesignated area.\nis upside down\nis approaching\ncommercial\nx\nloading or unloading goods\nx\n: add\n: remove\nfound hallucination and remove\nx\nhuman edit legend:\nround-1\nround-2\nFigure 3. Designed Prompt Template. By giving the concatenated set-of-marks images, the right side (round-1) shows the initial response\nand the corresponding human edits. Once finalized by humans, these edits will be merged into a single detailed caption grounded with\npanoptic segmentation masks, as shown in the left side (round-2).\ntistical analysis of our final dataset in Tab. 1. On average,\nour captions contain 203 words spanning 11 sentences. We\nfollow the same split setting in COCO2017 [35] dataset,\nwhich includes 118K training images. To provide a com-\nprehensive evaluation set, we adopt the same 25K images\nfrom COCONut-val split (which contains COCO2017-val\nand another 20K Objects365 [55] validation images).\n3.2. Dataset Construction\nWe argue that high-quality descriptions should provide suf-\nficient details of key objects and their attributes, as well as\ninformation about secondary objects and background ele-\nments. To achieve this, as shown in Fig. 2, we use human-\nannotated panoptic segmentation masks to decide the set of\nobjects to reference in the caption. These masks include\nboth ‘thing’ and ‘stuff’ classes, representing single objects\nand semantic regions, respectively. We adopt the panop-\ntic segmentation masks from the COCONut-S [9] dataset.\nThe masks are overlaid on the images, labeled with class\nnames c1, c2, . . . , cn ∈C, where C is the set of COCO’s\n133 panoptic classes. We then construct a prompt with both\nthe edited image and the original image and a textual ques-\ntion for GPT-4V, as illustrated in Fig. 3. The resulting re-\ngion captions from GPT-4V are reviewed and corrected by\nhuman raters for accuracy and consistency.\n\nFigure 4. Frequency of Extracted Nouns from the COCONut-\nPanCap Dataset. The top 10 most frequent nouns are: people,\ntable, room, street, dining, man, person, cars, chairs, and field.\n3.3. Dataset Analysis\nConcepts Beyond COCO’s 133 Classes. To clarify the\ngoal of our annotation task, we focus on key visual features\nsuch as objects, attributes, spatial relationships, and count-\ning. As shown in Fig. 4, we utilize the panoptic segmenta-\ntion mask from COCONut-S, which includes 133 classes in\nthe word vocabulary. Our proposed dataset, however, incor-\nporates additional concepts beyond these 133 classes, such\nas ‘vegetable’ and ‘parking’. This demonstrates that our hu-\nman annotators delivers accurate and diverse descriptions\nwhen using the provided label names as a reference.\nUser Study for Caption Quality. We randomly sample\n1,000 images from our COCONut-PanCap training set and\nasked a human evaluator to perform a single-choice selec-\ntion task. The question is: ‘Please select the best descrip-\ntion for the image, considering the correctness of object\nnames, attributes, counting, spatial relationships, and ac-\ntion.’\nThe compared captions are generated using GPT-\n4V [1], Qwen2-VL [64], and InternVL-2 [7], resulting in a\nsingle-choice four-option question. Fig. 5 illustrates the re-\nsults, showing that our GPT-assisted human-annotated cap-\ntions receives the highest ratings. More details can be found\nin the supplementary.\n4. PGC Baseline: PanCaper\nIn this section, we introduce our baseline method for joint\npanoptic segmentation and grounded captioning (PGC),\nnamely PanCaper. We start with an overview of the pixel\ngrounding task and then present our proposed approach,\nwhich incorporates a panoptic segmentation module specif-\nically designed for grounding objects in captions.\nRevisiting the Pixel Grounding Task. Our baseline model\nbuilds upon LISA [28], a model that combines the lan-\nguage generation capabilities of VLMs with the ability to\nproduce segmentation mask. LISA consists of three main\ncomponents: a VLM, a vision backbone V , and a mask de-\ncoder D. With a given text prompt, the VLM (typically\nLLaVA [36, 37]) generates an output containing a ⟨SEG⟩\ntoken. For instance, with the input prompt, ‘Could you seg-\nment the food with high Vitamin C?’ LISA generates the\nresponse ‘It is ⟨SEG⟩.’ This process extracts the last-layer\nFigure 5. Caption Quality via User Study. The study involved\nhuman evaluators assessing a random sample of 1,000 captions,\nwith a strong preference shown for captions from our dataset.\nembedding of the LLM from LLaVA. Then a language-to-\nprompt (L-P) projection layer (g) transforms the last-layer\nembeddings corresponding to ⟨SEG⟩tokens (lseg) into the\ndecoder’s feature space. Meanwhile, the vision backbone\nextracts dense visual features from the input image.\nFi-\nnally, both the dense features and the CLIP image embed-\nding from LLaVA are fed into the mask decoder to produce\nthe final segmentation mask.\nPrompt Instruction for Grounded Captioning. We pro-\npose a baseline method for the PGC task by modifying\nLISA to enable grounded captioning with segmentation\nmasks. Since LISA was originally designed for generat-\ning segmentation with a single output mask, two main ad-\njustments are necessary: (1) the use of multiple ⟨SEG⟩to-\nkens, and (2) extracting noun phrases from the caption for\ngrounding. To facilitate grounded segmentation, we modify\nthe prompt to the VLM as ‘Please provide a detailed de-\nscription of the image and segment each part.’ This prompt\ntriggers the model to generate caption responses with cor-\nresponding ⟨SEGi⟩tokens, where i ∈[1, N] and N is the\ntotal number of predicted segmentations.\nGiven a predicted caption for the image, aligning each\n⟨SEGi⟩token requires pairing it with a noun phrase,\n‘⟨p⟩phrasei⟨/p⟩,’ where phrasei is the relevant part in the\ncaption to be grounded. With these prompt tokens defined,\nthe model uses the vision backbone V and mask decoder\nD to facilitate fine-grained, pixel-level grounding, with D\nproducing segmentation masks M.\nEnable Panoptic Grounding. To achieve panoptic seg-\nmentation from captions, we first classify ⟨SEG⟩tokens\ninto two types: ⟨SEGt⟩for ‘thing’ classes and ⟨SEGs⟩\nfor ‘stuff’ classes.\nThese tokens are then processed by\nour segmentation modules to produce panoptic segmenta-\ntion masks. We initialize the vision backbone V with a\npretrained kMaX-DeepLab encoder [67] and fine-tune the\ndecoder D using our COCONut-PanCap dataset.\nSince\nkMaX-DeepLab operates as a closed-set segmenter, we\n\nalign text embeddings of the associated noun phrases with\nCOCO’s 133 panoptic classes. To accomplish this align-\nment, we use BERT [26] to generate the text embed-\ndings and to calculate cosine similarity, selecting the best-\nmatching category. Panoptic grounding provides mapping\nbetween detailed captions and image regions, which im-\nproves interpretability of VLM predictions.\nTraining Objectives. Our training objective aims to mini-\nmize the following losses:\nL = λtextLtext + λmaskLmask,\n(1)\nwhere Ltext is the auto-regressive cross-entropy loss for text\ngeneration, and Lmask is the mask loss [63], encouraging the\nmodel to produce high-quality segmentation results. λtext\nand λmask are the respective loss weights. We use the same\nloss weights as LISA [28].\nEvaluation Metrics for Caption Quality. We conduct the\nanalysis with multiple metrics to evaluate the quality and\ncompleteness of the generated captions. We introduce a\nbenchmarking suite for the PGC task, with a validation set\nof 25K images. For the caption quality, we report the cap-\ntion metrics including CIDEr [62], METEOR [2], ROUGE-\nL [34], BLEU@4 [46] and CAPTURE [10]. For grounded\npanoptic segmentation, we report PQ scores [23].\n5. Experimental Results\nWe assess the effectiveness of human-annotated caption\ndata by performing three primary tasks utilizing our dataset\nin the fine-tuning/instruction tuning stage: detailed cap-\ntioning, panoptic grounded captioning (PGC), and text-to-\nimage generation. Additionally, we demonstrate the trans-\nferability of the knowledge learned from our dataset through\ntwo downstream tasks: VQA and referring segmentation.\nDetailed Captioning. We conduct instruction tuning with\nLLaVA-NeXT framework [38] for this task. We replace\nthe caption data (23k) from the original LLaVA instruction-\ntuning set with detailed captions from our dataset, keep-\ning the same amount of instruction data size.\nWe fol-\nlow the same training setup used for LLaVA-NeXT with\nLlama3-8B [11].\nTreating it as a QA task, we use the\nprompt, ‘Could you please describe the image in detail?’\nand collect the corresponding response as the caption for\nthe image. We evaluate caption quality using CIDEr [62],\nMETEOR [2], BLEU@4 [46], ROUGE-L [34] and CAP-\nTURE [10] metrics. We also extend the model by adding\nthe mask-pooled features from the panoptic segmentation\nmasks as additional signals to the LLaVA model and name\nit LLaVA-NeXT-pool. During training, we use the ground\ntruth mask to extract the features while during inference\nwe use the mask proposals from the pretrained kMaX-\nDeepLab [67]. Besides, we also experiment with synthetic\ncaptions directly generated using InternVL-2 [7], Qwen2-\nVL [64] and GPT-4V [1]. We follow the same data prepara-\ntion settings as our dataset to build these instruction datasets\nfor these 23K images with different sources of synthetic\ndetailed captions, namely LLaVA 665K-InternVL2-Cap ,\nLLaVA 665K-Qwen2VL-Cap, and LLaVA 665K-GPT4V-\nCap. These datasets are used to produce models LLaVA-\nNeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G respec-\ntively.\nMore details can be found in the supplementary.\nThe results are presented in Tab. 3. LLaVA-NeXT models\nshow improved performance when fine-tuned on the custom\ninstruction-tuning dataset.\nAmong these, LLaVA-NeXT-\npool achieves the highest scores in all metrics, with CAP-\nTURE of 61.4, CIDEr of 13.1, BLEU@4 of 5.3, and ME-\nTEOR of 17.1, significantly higher than the original model\nvariant LLaVA-NeXT, indicating the benefit of added re-\ngion features for additional visual cues. Models trained on\nsynthetic captions (LLaVA-NeXT-I, LLaVA-NeXT-Q, and\nLLaVA-NeXT-G) generally show lower scores, showing ad-\nvantage of our human-annotated caption.\nPGC: Stronger Detail Reasoning Performance. We im-\nplement our proposed PanCaper based on LISA which uses\npre-trained LLaVA-NeXT with a LLM of Llama3-8B, with\nLoRA [19] adopted. The vision encoder uses a fixed CLIP-\nViT-L/14-336 model, modified with linearly interpolated\nposition embeddings to process 448 resolution images. The\ntrainable components of our model include the mask de-\ncoder of kMaX-DeepLab, and the tunable parts in LLaVA\nsame as in LISA. To enhance model performance in visual\nunderstanding, we initialize our PanCaper using pretrained\nLLaVA-NeXT models from the detailed captioning task.\nWe also experiment with a model variant that uses mask\npooled features similar to LLaVA-NeXT-pool, and name it\nPanCaper-Pro.\nFor comparison, we select 3 related methods LISA, Pix-\nelLM [51] and GLaMM [50] for evaluation. It is notewor-\nthy that LISA is not able to perform multi-mask prediction.\nWe therefore adapt LISA [28] for the multi-mask generation\nwith grounded segmentation, namely LISA+. The imple-\nmentation details can be found in the supplementary. Tab. 4\nshows the quantitative results.\nOur proposed PanCaper-\nPro achieves the highest scores across all captioning met-\nrics (CIDEr: 12.5, CAPTURE: 64.3, BLEU@4: 6.4, ME-\nTEOR: 17.9), outperforming all other models. Both PanCa-\nper models show significant improvements over other mod-\nels in all captioning metrics, highlighting the effectiveness\nof the COCONut-PanCap dataset for detailed caption gen-\neration. On grounding segmentation, PanCaper-Pro again\nleads, with a PQ score of 0.61, PQthing of 0.58, and PQstuff\nof 0.68, reflecting its robustness on both “thing” and “stuff”\nclasses. Notably, enabling mask pooling in our proposed\nPanCaper-Pro further enhances segmentation metrics. The\nbaseline models (LISA+ and GLaMM with GranD) achieve\nmuch lower PQ scores, due to incomplete segmentation an-\n\nTraining recipe Method\nPretrain Dataset Instruction-tuning dataset\nMask pooled CAPTURE CIDEr BLEU@4 METEOR ROUGE-L\nfinetune\nLLaVA-NeXT*\nLAION-CC-SBU LLaVA 665K\n✗\n55.4\n10.8\n4.2\n13.2\n23.1\nLLaVA-NeXT\nLAION-CC-SBU LLaVA 665K-COCONut-PanCap\n✗\n58.7\n11.2\n4.8\n16.2\n24.6\nLLaVA-NeXT-pool LAION-CC-SBU LLaVA 665K-COCONut-PanCap\n✓\n61.4\n13.1\n5.3\n17.1\n26.8\nLLaVA-NeXT-I\nLAION-CC-SBU LLaVA 665K-InternVL2-Cap\n✗\n53.9\n9.4\n4.4\n11.5\n21.4\nLLaVA-NeXT-Q\nLAION-CC-SBU LLaVA 665K-Qwen2VL-Cap\n✗\n55.4\n8.9\n4.6\n12.9\n22.5\nLLaVA-NeXT-G\nLAION-CC-SBU LLaVA 665K-GPT4V-Cap\n✗\n56.2\n9.6\n4.7\n13.3\n22.8\nTable 3. Caption Benchmark Results Evaluated on Our COCONut-PanCap Val Set. Note that the amount of data in the instruction\ndataset remains the same; only the sources of the detailed captions vary, with a total of 23K images that have detailed captions.\nCaption\nGrounding segmentation\nMethod\nPretrain dataset\nInstruction dataset\nMask pooled CAPTURE CIDEr BLEU@4 METEOR PQ PQthing\nPQstuff\nLISA+ *\nLAION-CC-SBU\nGranDf\n✗\n46.2\n6.6\n3.8\n9.8\n0.43\n0.41\n0.45\nLISA+\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✗\n57.9\n8.1\n4.9\n13.8\n0.50\n0.49\n0.44\nGLaMM GCG *\nLAION-CC-SBU+GranD\nGranDf\n✗\n43.2\n6.5\n3.6\n10.6\n0.27\n0.35\n0.21\nGLaMM GCG\nLAION-CC-SBU+GranD COCONut-PanCap (ours)\n✗\n56.8\n7.8\n5.2\n14.3\n0.55\n0.54\n0.46\nPanCaper (ours)\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✗\n62.6\n12.0\n5.8\n15.4\n0.56\n0.55\n0.66\nPanCaper-Pro (ours)\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✓\n64.3\n12.5\n6.4\n17.9\n0.61\n0.58\n0.68\nTable 4. Joint Panoptic Segmentation and Grounded Captioning (PGC) on COCONut-PanCap Val Set. * denotes reproduced results.\nTraining dataset\nEvaluation dataset FID↓\nFDdinov2 ↓CLIPScore↑\nSD3 PT dataset [12]\nDOCCI test set [44]\n30.2\n345\n74.9\nCOCO-caption [6]\n27.6\n321\n76.8\nDOCCI [44]\n22.1\n300\n77.8\nCOCONut-PanCap (ours)\n21.4\n290\n77.9\nSD3 PT dataset [12]\n31.8\n300\n73.8\nCOCO-caption [6]\nCOCONut-PanCap\n28.0\n294\n74.0\nDOCCI [44]\nval set (ours)\n24.3\n267\n75.1\nCOCONut-PanCap (ours)\n23.1\n260\n77.3\nTable 5. Benchmark Results on Text Conditioned Image Gen-\neration.\nStable-Diffusion-3 (SD3) medium is finetuned with\nCOCO-Caption (short), DOCCI and our COCONut-Panoptic and\nevaluated on DOCCI test set [44] and our COCONut-PanCap val\nset. ‘SD3 PT dataset’ denotes the pretraining dataset of SD3, and\nthus the rows correspond to zero-shot evaluation of SD3.\nw/o FT COCO-caption [6] DOCCI [44] COCONut-PanCap\ncolor attribution\n0.37\n0.34\n0.38\n0.40\ncolors\n0.73\n0.70\n0.74\n0.75\nposition\n0.33\n0.30\n0.36\n0.36\ncounting\n0.65\n0.64\n0.65\n0.70\nsingle object\n0.96\n0.94\n0.95\n0.96\ntwo objects\n0.80\n0.78\n0.81\n0.89\noverall score\n0.64\n0.62\n0.65\n0.68\nTable 6. Effects of Fine-tuning the SD3-medium (T2I model)\nwith Different Datasets on GenEval [16]. w/o FT denotes the\nmodel is not finetuned with any datasets (i.e., zero-shot testing).\nnotations in the GranD dataset.\nText-to-Image Generation. We adopt the Stable Diffusion\n3 (SD3) medium model1 for text to image generation with\nLoRA finetuning. We adopt the default training settings\nbut only with different text-image datasets for training. We\nevaluate with two types of training images from COCO [35]\nand DOCCI [44] datasets. In details, for the COCO images,\n1https://huggingface.co/docs/diffusers/stable diffusion/stable diffusion 3\nwe explore the short COCO-caption and detailed captions\nfrom our dataset. For DOCCI images, we directly use the\ncaptions from their dataset. Tab. 5 shows the quantitative\nresults. Traning on COCONut-PanCap achieves the best\nperformance across all metrics when evaluated on DOCCI-\ntest, with the lowest FID (21.4), lowest FDdinov2 (290), and\nthe highest CLIPScore (77.9), indicating superior genera-\ntion quality and high image-text relevance. When evalu-\nated on COCONut-PanCap-val set, training on COCONut-\nPanCap again shows the best results with the lowest FID\n(23.1), FDdinov2 (267), and a high CLIPScore of 77.3.\nTab. 6 shows the results on GenEval benchmark [16].\nFinetuning SD3-medium with COCONut-PanCap consis-\ntently scores the highest in most categories, particularly\nthose requiring image details like color attribution, object\npositioning, and handling multiple objects. Our proposed\ndataset enables more accurate image generation that re-\nquires understanding of relationships, multiple objects and\ncounting, tasks that other datasets struggle with.\nVQA. To evaluate the effectiveness of the proposed\nCOCONut-PanCap dataset, we utilize these captions during\nthe instruction-tuning stage and follow the setup of LLaVA-\nNeXT [38] across various visual question answering (VQA)\nand multi-modality understanding benchmarks. We evalu-\nate on MM-Vet [68], SEED-IMG [29], MMBench-en [39],\nMME [13], POPE [33], and TextVQA [58], covering a\nbroad range of evaluation dimensions. We experiment with\ndifferent amount of our COCONut-PanCap caption data in-\njected into the instruction tuning stage by replacing the orig-\ninal COCO captioning data with our dataset. As shown in\nTab. 7, the baseline model LLaVA-NeXT (using its orig-\ninal recaptioned COCO) achieves relatively lower perfor-\nmance across all metrics, with scores such as 43.5 on MM-\nVet, 70.1 on Seed-IMG, and 68.9 on TextVQA. Building\n\nMethod\nLLM\nInstruction-tuning Dataset\nMM-Vet Seed-IMG MMBench-en TextVQA POPE MME\nLLaVA-NeXT *\nLlama3-8B orginal LLaVA 665K [38]\n43.5\n70.1\n71.4\n68.9\n85.4\n1523\nLLaVA-NeXT-20K Llama3-8B LLaVA 665K-COCONut-PanCap-20K\n44.1\n72.5\n73.6\n69.8\n86.1\n1552\nLLaVA-NeXT-50K Llama3-8B LLaVA 665K-COCONut-PanCap-50K\n44.6\n73.1\n74.2\n70.0\n87.1\n1600\nLLaVA-NeXT-Full Llama3-8B LLaVA 665K-COCONut-PanCap-118K\n45.5\n74.3\n75.1\n70.7\n87.9\n1612\nLLaVA-1.5\nVicuna-7B LLaVA 665K-ShareGPT4V-100K\n37.8\n67.4\n70.5\n64.6\n84.7\n1519\nLLaVA-1.5\nVicuna-7B LLaVA 665K-COCONut-PanCap-20K\n38.5\n67.7\n70.9\n64.5\n84.9\n1521\nTable 7. Benchmark Results and Ablation Study on VQA. By adding extra detailed caption data for instruction tuning, the models show\nincreased improvement. * denotes reproduced results. Using only 20K human labeled data can still achieve comparable performance\nto 100K synthetic data.\nMethod\nrefCOCO\nrefCOCO+\nrefCOCOg\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nGLaMM* [50]\n77.5\n79.2\n74.9\n71.3\n74.7\n61.5\n71.3\n71.9\nPixelLM [51]\n73.0\n76.5\n68.2\n66.3\n71.7\n58.3\n69.3\n70.5\nLISA-7B [28]\n74.1\n76.5\n71.1\n62.4\n67.4\n56.5\n66.4\n68.5\nPanCaper+\n74.5\n76.7\n69.9\n69.9\n73.4\n59.5\n69.8\n70.6\nPanCaper+ + COCONut-PanCap\n76.2\n77.1\n72.3\n70.5\n73.9\n60.1\n72.1\n71.6\nTable 8. Benchmark Results on Referring Segmentation. * denotes reproduced results. It is noted that GLaMM uses extra data from the\nGranD dataset for pretraining. + denotes our PanCaper model is adapted for referring segmentation task.\non LLaVA-NeXT baseline, we progressively incorporated\nvarying amounts of COCONut-PanCap data (20K, 50K, and\n118K (full), as indicated by postfixes in the baseline names)\nduring instruction-tuning. Consistent improvements are ob-\nserved across all evaluated benchmarks as more of our data\nis integrated.\nReferring Segmentation. In this task, the model processes\nan image and a textual referring expression to output a\nsegmentation mask corresponding to the expression. The\nprompt used is, ‘Please segment the ⟨referring text⟩in the\nimage.’ The target model response is ‘Sure, it is ⟨SEG⟩.’,\nwhere the ⟨SEG⟩token is decoded to obtain the mask. We\nfollow the setup in LISA [28], using multiple segmentation\ndatasets to jointly train the models. Tab. 8 shows the quan-\ntitative results. Our model achieves superior performance,\nparticularly when additionally trained with the COCONut-\nPanCap dataset (last row), outperforming all models except\nGLaMM [50]. This improvement underscores our model’s\nefficacy in handling complex referring expressions, likely\ndue to the additional data that enhances model generaliza-\ntion and accuracy. It is worth noting that GLaMM performs\ncompetitively with our method, though the comparison is\nuneven given their additional use of the SA-1B dataset [25].\nSynthetic vs. Human Annotated Data. Generating syn-\nthetic data for captioning has been popular for recent tasks\nin either training vision encoders [48] or text-to-image gen-\neration [31]. We investigate the effect of varying the mix\nratio of synthetic captions generated by GPT-4V and our\nhuman-annotated data for fine-tuning (where 0 indicates\nfully synthetic data), using the COCONut-PanCap dataset\nfor training and the COCONut-PanCap-val set for evalua-\ntion. We adopt LLaVA-NeXT for the captioning task and\nSD3-medium for the image generation task. As shown in\nFig. 6, adding 25% human-annotated data yields significant\nperformance improvements in both captioning and genera-\ntion, with a reduced FID of 26 from 31 (lower is better) and\nan increased CAPTURE score of 53.6 from 47.5 (higher\nis better). Consistent improvements are observed as more\nhuman-annotated data is incorporated.\nFigure 6. Varying Synthetic and Human-Annotation Ratios.\nCAPTURE is used to evaluate the performance of LLaVA-NeXT\non detailed captioning, while FID assesses the performance of\nSD3-medium on text-conditioned image generation.\n6. Conclusion and Discussion\nIn this work, we proposed a novel dataset designed to sup-\nport detailed captioning and grounded segmentation tasks\nbuilt on COCO images. We demonstrated that our dataset\ncan enhance model performance during instruction tuning\nand fine-tuning stages across various multi-modal under-\nstanding and generation tasks, such as captioning, VQA,\ngrounded segmentation, and text-to-image generation. We\n\nhope that COCONut-PanCap, with its detailed captions\ngrounded with dense panoptic masks, will foster future ad-\nvancements in multi-modal learning research.\nLimitations. High-quality human-labeled data offers sig-\nnificant benefits for instruction tuning in multi-modal tasks,\nbut scaling such datasets is challenging. To address this,\nwe introduce COCONut-PanCap as a starting point for\nlarge-scale human-annotated data exploration. Recognizing\nthe relatively smaller dataset size compared to other large\ndataset, future work may involve using this dataset to train\nseed models to generate more high-quality synthetic data.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 5, 6\n[2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In ACL Workshop, 2005. 6\n[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In CVPR, 2018.\n13\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\nIn CVPR,\n2021. 3\n[5] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 2, 3\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 2, 3, 7\n[7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\nwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, et al. How far are we to gpt-4v? closing\nthe gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821, 2024. 5, 6\n[8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tri-\npathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo\nand pixmo: Open weights and open data for state-of-the-art\nmultimodal models. arXiv preprint arXiv:2409.17146, 2024.\n3\n[9] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and\nLiang-Chieh Chen. Coconut: Modernizing coco segmenta-\ntion. In CVPR, 2024. 2, 3, 4\n[10] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang,\nYuan Zhang, and Haoyuan Guo. Benchmarking and improv-\ning detail image caption. arXiv preprint arXiv:2405.19092,\n2024. 6\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024. 6\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn ICML, 2024. 7\n[13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 7\n[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. NeurIPS, 2024. 2\n[15] Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan\nBitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bun-\nner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. Im-\nageinwords: Unlocking hyper-detailed image descriptions.\narXiv preprint arXiv:2405.02793, 2024. 2, 3\n[16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment. In NeurIPS, 2023. 3, 7\n[17] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019. 3\n[18] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng\nWang. Shot2story20k: A new benchmark for comprehen-\nsive understanding of multi-shot videos.\narXiv preprint\narXiv:2311.17043, 2023. 3\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 6, 12\n[20] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\nhui Liu. T2i-compbench: A comprehensive benchmark for\nopen-world compositional text-to-image generation. arXiv\npreprint arXiv: 2307.06350, 2023. 3\n[21] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 13\n[22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In EMNLP, 2014. 13\n[23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll´ar. Panoptic segmentation. In CVPR,\n2019. 6\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 3\n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 8\n\n[26] Mikhail V Koroteev. Bert: a review of applications in nat-\nural language processing and understanding. arXiv preprint\narXiv:2103.11943, 2021. 6\n[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 2017. 13\n[28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta-\ntion via large language model. In CVPR, 2024. 5, 6, 8, 12,\n13\n[29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 7\n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 3\n[31] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen\nZhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu,\nHuangjie Zheng, Yuyin Zhou, and Cihang Xie.\nWhat if\nwe recaption billions of web images with llama-3?\narXiv\npreprint arXiv:2406.08478, 2024. 2, 8\n[32] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xin-\nlong Wang, and Ling-Yu Duan.\nDensefusion-1m: Merg-\ning vision experts for comprehensive multimodal perception.\narXiv preprint arXiv:2407.08303, 2024. 2, 3\n[33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.\n7\n[34] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In ACL Workshop, 2004. 6\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 3, 4, 7\n[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 2, 5, 13\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 3, 5, 12, 13\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 2, 3,\n6, 7, 8, 13\n[39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv:2307.06281, 2023. 7\n[40] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11–20, 2016. 3, 13\n[41] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nCVPR, 2016. 13\n[42] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In CVPR, 2019.\n13\n[43] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In ICDAR, 2019. 13\n[44] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan\nBitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana\nParekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Ja-\nson Baldridge. DOCCI: Descriptions of Connected and Con-\ntrasting Images. In ECCV, 2024. 2, 3, 7\n[45] Vicente Ordonez,\nGirish Kulkarni,\nand Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In NeurIPS, 2011. 3\n[46] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002. 6\n[47] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 8\n[49] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi\nWen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-\nquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts\nand attributes of common objects. In CVPR, 2023. 13\n[50] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel-\nrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M.\nAnwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan.\nGlamm: Pixel grounding large multimodal model. In CVPR,\n2024. 3, 6, 8, 13\n[51] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao,\nDongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel\nreasoning with large multimodal model. In CVPR, 2024. 3,\n6, 8\n[52] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 2\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. NeurIPS, 2022. 2, 3\n[54] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa:\nA\n\nbenchmark for visual question answering using world knowl-\nedge. In ECCV, 2022. 13\n[55] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, 2019. 4\n[56] ShareGPT. ShareGPT. https://sharegpt.com/. 13\n[57] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image captioning\nwith reading comprehension. In ECCV, 2020. 13\n[58] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,\nXinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards\nvqa models that can read. In CVPR, 2019. 7\n[59] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand,\nMayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang,\nAbhinav Bhatele, Gowthami Somepalli, and Tom Goldstein.\nFrom pixels to prose: A large dataset of dense image cap-\ntions. arXiv preprint arXiv:2406.10328, 2024. 2\n[60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 2\n[61] Jack Urbanek,\nFlorian Bordes,\nPietro Astolfi,\nMary\nWilliamson, Vasu Sharma, and Adriana Romero-Soriano. A\npicture is worth more than 77 text tokens: Evaluating clip-\nstyle models on dense captions. In CVPR, 2024. 3\n[62] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In CVPR, 2015. 6\n[63] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 6\n[64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 5, 6\n[65] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\nWayne Zhang, and Ziwei Liu. Panoptic scene graph gen-\neration. In ECCV, 2022. 3\n[66] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan\nLi, and Jianfeng Gao.\nSet-of-mark prompting unleashes\nextraordinary visual grounding in gpt-4v.\narXiv preprint\narXiv:2310.11441, 2023. 4\n[67] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means Mask Transformer. In ECCV, 2022. 5, 6, 12,\n13\n[68] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. In ICML, 2024. 7\n[69] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie\nLuo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel\nunderstanding with visual instruction tuning. In CVPR, 2024.\n3\n[70] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 3\n[71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In CVPR, 2017. 13\n[72] Li Zhou, Xu Yuan, Zenghui Sun, Zikun Zhou, and Jing-\nsong Lan. Instruction-guided multi-granularity segmentation\nand captioning with large multimodal model. arXiv preprint\narXiv:2409.13407, 2024. 3\n\nThe appendix is organized as follows.\n• In Sec. A, we show implementation details for De-\ntailed Captioning (Sec. A.1), Panoptic segmentation and\nGrounded (Sec. A.2), and VQA (Sec. A.3).\n• In Sec. B, we show more visualization examples of our\nproposed COCONut-PanCap dataset (Sec. B.1), and anal-\nysis of the tier cases in our dataset annotation user study\n(Sec. B.2).\nA. Experimental Details\nIn this section, we provide more experimental details for\ndetailed captioning (Sec. A.1), PGC (Sec. A.2), and VQA\n(Sec. A.3).\nA.1. Detailed Captioning\nDetailed Captioning Instruction Dataset Construction.\nThe key step in conducting the experiment is constructing\nthe dataset. The original LLaVA-665K dataset consists of\nLLaVA-158K combined with other VQA datasets. Within\nLLaVA-158K, a subset of detailed captions corresponds to\n23K COCO images. To create our-LLaVA-665K (referred\nto as LLaVA 665K-COCONut-PanCap in the table), we re-\nplace the detailed caption annotations for these 23K COCO\nimages with our annotations. Importantly, the total amount\nof training data remains unchanged (only the captions for\nthese 23K images are updated), ensuring a fair comparison\nof the impact of data quality on model performance.\nSynthetic Annotation for Detailed Caption. To build the\nsynthetic dataset with state-of-the-art VLM, we use three\nmodels, including open-sourced InterVL-2, Qwen2-VL and\nclose-sourced GPT-4V to generate the detailed captions for\nCOCO 118K train set images. We use the same text prompts\nthat is used in LLaVA [37] for prompting the model to cre-\nate the detailed captions.\nLLaVA-NeXT-pool implementation details. Fig. 7 shows\nthe comparison of the original LLaVA-NeXT and our pro-\nposed LLaVA-NeXT-pool. As shown in Fig. 7a, in order to\npreserve the details for the high-resolution images and rep-\nresentations, the original design employs a grid configura-\ntion which can also balance the performance efficiency with\noperational costs.\nThen both the patch-level and image-\nlevel features are later concatenated and sent to the LLM.\nDirectly splitting the image into patches could cause pro-\nlems, for example, in the figure, the upper part of the dog’s\nhead is partitioned into different patches which may result\nin incomplete feature extraction for single object. To over-\ncome this drawback, we propose LLaVA-NeXT-pool to ex-\ntract the dense feature and preserve the object details by uti-\nlizing the panoptic segmentation masks in our COCONut-\nPanCap dataset. Fig. 7b shows the details. Compared to the\noriginal design, LLaVA-NeXT-pool could effectively ex-\ntract the features for the dog in our example. Our design\nsplit\nencode\nflatten\nLLM\nresize\npatch-level\nencode\nflatten\nimage-level\n(a) LLaVA-NeXt-AnyRes\nmask\nencode\nLLM\nresize\nmask-level\nencode\nflatten\nimage-level\n…\nload\nmask-pooling\n…\nconcatenate\n(b) our LLaVA-NeXt-pool\nFigure 7.\nComparison of LLaVA-NeXt and our proposed\nLLaVA-NeXt-pool.\nenables more complete region-level feature extraction and\nis potential in understanding the details better.\nA.2. PGC\nWe provide more implementation details for the proposed\ntask:\nPanoptic segmentation and Grounded Captioning\n(PGC).\nPanCaper Implementation Details.\nWe introduce the\nPanCaper architecture details in this section.\nFollowing\nthe architecture in LISA [28], there are three components\nincluding the vision backbone, mask decoder and multi-\nmodal LLM. Fig. 8 shows the architecture details for Pan-\nCaper. We made modification on the vision backbone, and\nmask decoder part in terms of model architecture. To pre-\nserve the learned knowledge of the pre-trained multimodal\nLLM (i.e., LLaVA-NeXT in our experiments), we leverage\nLoRA [19] to perform efficient fine-tuning, and completely\nfreeze the vision backbone. The mask decoder is fully fine-\ntuned. Additionally, the LLM token embeddings (embed to-\nkens), the LLM head (lm head), and the projection layer are\nalso trainable. The weights of the text generation loss λtext\nand the mask loss λmask are set to 1.0 and 1.0, respectively.\nFor the PQ-style mask loss, we follow the same settings in\nkMaX-DeepLab [67], where it consists of mask-level cross\nentropy loss, dice loss and pixel loss.\nAdapting Baseline Methods for PGC Task. We adopt\nthe same text prompt template to enable the model to per-\nform PGC tasks. For LISA+, we follow the same design\n\npanop%c mask\ndecoder\nimage\nPlease provide a detailed description \nof the image and segment each part.\ntext prompt\nvision\nbackbone\n❄\nmulti-modal \nLLM\n❄\nLoRA\n🔥\nThe images depicts <p:a running\ndog[SEG_t]> …. on <p:the grass [SEG_s]>\n🔥\npredic-on\n🔥: trainable\n❄: fronzen\nFigure 8. Architecture of PanCaper. We utilize a pretrained vision encoder from kMaX-DeepLab [67] as our vision backbone, which\neffectively extracts dense features essential for panoptic segmentation.\nin GLaMM [50] to design the multi entity mask output by\nutilizing the the GranDf dataset. As the intruction dataset\nof GranDf is constructed similarly grounding the phrase in\nthe image-level caption, it will output multiple ⟨SEG⟩to-\nkens. The reasoning results of the number of ⟨SEG⟩tokens\ndecide the number of output entity mask which are often\nbinary masks. As a result, the model can generate a de-\ntailed caption along with interleaved segmentation masks,\nemploying the format “⟨p⟩A man⟨/p⟩⟨SEG⟩...\nnext to\n⟨p⟩a tree⟨/p⟩⟨SEG⟩”. And thus the format of instruction\ndataset is significat in task design. Therefore, we formu-\nlate our dataset as “⟨p⟩A man⟨/p⟩⟨SEGt⟩... next to ⟨p⟩a\ntree⟨/p⟩⟨SEGs⟩”, where ⟨SEGt⟩represents the seg token\nfor instance masks of thing and ⟨SEGs⟩represents for se-\nmantic masks of stuff respectively in panoptic setting. Sim-\nilarly, utilizing the PanCap dataset and special token design,\nGLaMM [50] is able to generate the entity masks with the\ntag of ‘thing’ and ‘stuff’.\nTraining Data Formulation. We adopt the same training\ndata from LISA [28] which comprises mainly three parts,\nall of which are derived from widely-used public datasets.\nThese include 1) Semantic Segmentation datasets includ-\ning ADE20K [71], COCO-Stuff [3], and LVIS-PACO [49]\npart datasets with the generated QA data, 2) Vanilla Re-\nferring Segmentation Datasets: refCOCO, refCOCO+, re-\nfCLEF [22] and refCOCOg [40] datasets, 3) ReasonSeg\ndataset [28], and 4) Visual Question Answering Dataset:\nLLaVA-v1.5-mix665k [36]. To enable the multi-mask gen-\neration for grounded caption, there are two options for\ninstruction datasets, GranDf and our COCONut-PanCap\nwhere GranDf consists of entity masks while COCONut-\nPanCap consists of panoptic masks.\nA.3. VQA\nWe provide more implementation details for the VQA\nexperiments.\nWe follow the same setting in LLaVA-\nNeXT to create the experimental results for VQA tasks.\nWe focus on the instruction tuning stage by adopting\nthe pretrained weights from the stage-1 across the train-\nings for all the model variants mentioned in Tab. 7 in\nthe paper.\nThe dataset we used is exactly the same\nas in LLaVA 665K [36] which includes the earlier ver-\nsion of instruction data proposed in LLaVA 158K [37],\nShareGPT [56], VQAv2 [41], GQA [21], openknowl-\nedge VQA (OKVQA [42],\nA-OKVQA [54]),\nOCR\n(OCRVQA [43], TextCaps [57]), region-level VQA datasets\n(Visual Genome [27], RefCOCO [22]). Among these data,\nLLaVA 158K comprises 77K complex reasoning, 58K con-\nversation and 23K detailed captions. To build the dataset\nvariants shown in Tab. 7, we simply remove the sub-\nset of detailed caption 23k, and subsequently add 20K,\n50K and 118K COCONut-PanCap dataset to build LLaVA\n665K-COCONut-PanCap-20K, LLaVA 665K-COCONut-\nPanCap-50K and LLaVA 665K-COCONut-PanCap-118K.\nBy these steps, we add more detailed caption data to\nconstruct the instruction tuning dataset.\nThis results in\nthe total amount of training data of 662K for LLaVA\n665K-COCONut-PanCap-20K, 692K for LLaVA 665K-\nCOCONut-PanCap-50K and 760K for LLaVA 665K-\nCOCONut-PanCap-118K. And thus the size of LLaVA\n665K-COCONut-PanCap-20K is slightly smaller than the\noriginal LLaVA 665K dataset, but the model trained on\nit yields better performance. For the evaluation settings,\nwe follow the exact settings in LLaVA-NeXT [38] using\nlmms eval2.\nB. More Qualitative Results\nIn this section, we present additional qualitative results of\nCOCONut-PanCap annotations (Sec. B.1) and a detailed\nanalysis of tier cases from the user study (Sec. B.2).\n2https://github.com/EvolvingLMMs-Lab/lmms-eval\n\nB.1. Data Examples\nWe show more visualization of our proposed COCONut-\nPanCap dataset in Fig. 9 and Fig. 10.\nB.2. PanCaper and GPT-4V Tier Showcases\nIn the user study involving 1,000 samples, captions gener-\nated by GPT-4V were preferred in 87 cases. Among these,\nactually, 46 were tier cases where human raters consid-\nered both GPT-4V and COCONut-PanCap captions equally\ngood. Fig. 11, Fig. 12 and Fig. 13 illustrate qualitative ex-\namples, highlighting the reasons for the tier classification\nand instances where GPT-4V was chosen.\n\nThe image depicts a natural outdoor with trees and\ngiraffes. <0:The sky is blue>, forming the backdrop of the \nscene. Below it, there are <1:dense trees, filled with \nbranches and lush green leaves>. Within this environment, \ntwo giraffes are prominently featured. The image mainly\nfocuses on <3:a standing giraffe with a long neck and \nunique patterns> , actively eating leaves from the tree. In\nThe image depicts a dynamic outdoor scene where people\nare riding horses. In the foreground, two horses take \ncenter stage. <9:A black horse with a white mane and tail\nbehind\nits\nneck, adorned with a brown bridle, a \npredominantly dark blue saddle with yellow patterns, and \nblue leg wraps>, is raising its front hoof. Beside it, <5:a \nwhite horse with a black mane and tail, wearing a black \nbridle, a similar dark blue saddle with yellow patterns, and \nwhite leg wraps>, is also raising its front hoof. Both horses \nare being controlled by <6,7: two man who are dressed in \nblue and white tops, white pants, and black boots>, \nactively taming the horses. They are riding horses on <2:a \nvivid green grassland> that provides the base for the\naction. Adding structure, there is <3: a fence made of wooden posts and railings> in the background. \nThere are some people in the background that are obscured by the horses. For example, there is <8:a\nperson wearing black pants> partially obscured by the white horse and <10:another person in a red top \nand white pants> who is watching the activity; and <11:a person in a red top and black pants>, partially \nhidden by the black horse. Together, the elements create a cohesive portrayal of a lively horse-taming \nevent set against a serene natural background. The weather is nice, as <0:the sky is white and cloudless>, \nforming the backdrop. Below it, there features <1:a dense cluster of trees with brown trunks and green \nleaves>, framing the scene. \nThe scene includes several individuals actively \nengaging in skateboarding. There are <6,10: two \nboys> wearing in green top and black pants>, actively \nplaying <9:11skateboard> in the air. <5: Another guy\nwho is also dressing in green top and black pants> is\nplaying but on the ground. Next to them, there are <7:\na half-naked man> observing the skateboarding\nperformance, while <8: another guy in a white top and black hat>, also watching the activities. Skateboards \nare prominently featured in the center area, which includes a black skateboard deck used for tricks. Lastly, \nthe background shows the <0:sky, predominantly blue with scattered clouds>. <1:A light brown building> is\nobviously seen in the background. The skaters are using the <2:sidewalk, notable for its graffiti and colorful \nmarkings>. Around the scene, there is <3:lush green foliage>, adding natural scenery to the skate park. \ncontrast, there is <4: a second giraffe with similar distinctive patterns>, which is far away from the previous\ngiraffe is resting comfortably on the grass. Both giraffes are surrounded by green trees and <2:a grassy area, \npredominantly covered with green grass interspersed with patches of exposed brown soil>. \nFigure 9. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks.\n\nThis image showcases a well-organized desk setup. On\n<1:the wooden desk with a shelf>, there is <4:a DELL\ncomputer> occupies the central space, displaying content \non its screen. Besides, there is a turtle toy on top of it. \nSurrounding the computer, multiple items are neatly \narranged. To its left, <5:a blue water bottle> stands \nprominently, next to <6:a book> lying on the desk. Below \nthe computer, <12,13,16,19:additional books> are placed.\nOn the upper shelves, various objects add character to the space like <14,15: books> and a drink can. At the top of \nthe shelves, <8:a fluffy blue teddy bear > is positioned on the left, and <11:another teddy bear> is positioned on the\nright, adding a playful touch. There are various small items as well, like <7:a glass bottle>, <17:books> and a photo\nframe. In the background, <0:the wall is painted blue>, serving as the backdrop for the scene.\nThe image features a cozy and well-decorated living \nroom. At the center of the room, <4:a wooden coffee \ntable equipped with glasses> holds various items, \nincluding <10:a remote control>, <13: a knife> on the\nplates, and <16:a square small book>. On the left, The \nseating arrangement includes <14:a patterned couch \nwith colorful cushions and blanket> and <15:another\nneutral-toned couch with vibrant throw pillows> , providing balance to the layout. The rug with colorful\npatters brings more warm atmosphere to the sitting area. Behind the couch, <20:A chair in the back>\ncomplements the seating options. Adding warmth to the room, <8:a black cat> rests comfortably on the \ncouch. Behind the sitting area, there is <5:a 4-layer wall-mounted wooden shelf> with additional \ndecorative items, including <11,12: vases> and other decorative items, enhancing the cozy and inviting \natmosphere. Closed to the shelf, there are several <9,19,22:potted plants with green leaves > are placed \nthroughout the room, adding a touch of greenery. <2:The wall painted in warm tones>, create a cozy \natmosphere and are adorned with framed artwork and decorations. <0: The floor is neutral-toned>, \nsupporting the entire setup. The <3:ceiling painted white>, contrasts subtly with the walls and reflects the \nnatural light entering the room through <6:the large windows>.\nThe image portrays a lively street scene outside a \ncafé. <0:The road> serves as the foreground, where \n<5:a motorcycle> is prominently parked, its shiny \nfinishes and detailed designs drawing attention. \nBehind it, <1:the café building>, labeled as “Seaport \nCafe” features large windows, a decorative sign, and \npatriotic bunting. A glowing neon “Corona Light” sign \nadds to the vibrant atmosphere. The café‘s exterior \nincludes <2:a wooden wall> and <4:a small fence>\nwhich separates the outdoor seating area from the \nstreet. \nThe \nseating \narea \nis \nequipped \nwith \n<12,14:chairs> and <6:a blue umbrella>\nthat \nprovides shade for the patrons.\n<7,11:\nSeveral \npeople are interacting in and around the café>, some \nstanding while others are seated, enjoying their \ntime. <10:One individual who is carrying <9:a black\nbackpack> > is joining while there is <8:a guy in blue shirt> is trying to shake hands with her, adding to the \ndynamic social scene.\nFigure 10. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks.\n\nCOCONut-PanCap: Positioned prominently within this \nimage is <1: a zebra, easily distinguishable by its iconic\nblack-and-white stripes and a striped mane along its neck>. \nIts belly is white, adding contrast to its overall pattern. The \nzebra is depicted in a natural feeding posture, with its head \nlowered as it grazes on <0:the grassland which is lush \ngreen>, forming the base of the scene. \nCOCONut-PanCap: The image mainly features <0:the trees \nwith very lush foliage>. In the front, there are <1,2: two\ngiraffes with distinctive brown spots, yellow patterns, and \nlong necks>. These giraffes are observed to be fighting by \nentwining their necks with each other. Behind them, there is \n<3: another giraffe, also with distinctive brown spots and \nyellow patterns>, which is standing still and partially \nobscured by the front giraffes. \nCOCONut-PanCap: On <0:the gray asphalt sidewalk>,\ntwo motorcycles are parked side by side. <2:One \nmotorcycle is predominantly black with white parts and \na brown seat>, positioned to the left of <1:a green \nbush>,\nwhere\na registration plate displaying the \nnumber “SV-6260” is visible to be mounted on the front \nfender, and a large cylindrical front headlamp is\nmounted on the front. <3:The other motorcycle is \npredominantly silver with a black seat>. The mechanical \ncomponents include the single-cylinder engine and a \nstraightforward exhaust system.\nGPT-4V: This beautifully restored vintage motorcycle, likely from the early 20th century, showcases a classic design with a \ncream-colored fuel tank, spoked wheels, and a robust black frame. Key features include a large front headlamp, a \nregistration plate reading "SV-6260," a brown leather saddle, and a small rear storage compartment. The visible single-\ncylinder engine and minimalistic exhaust system reflect the simple yet durable engineering of the era, with its pristine \ncondition highlighting careful preservation.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows the story background, but missing details for region-3.\nGPT-4V: This image captures two giraffes engaging in a \ndisplay of interaction, possibly sparring or playfully \nentwining their necks, a behavior commonly known as \n"necking." Set against a backdrop of lush green trees, the \ngiraffes\' distinctive spotted patterns stand out vividly in \nthe sunlight filtering through the foliage. Their tall, elegant\nforms and graceful movements highlight the majestic  nature of these animals, while the surrounding dense \nvegetation provides a natural and serene environment. The moment beautifully showcases the unique behavior and \nstriking appearance of giraffes in their habitat.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more description for the trees but add hallucination for the sunlight.\nGPT-4V: This image features a zebra grazing peacefully on a vibrant green field. The zebra\'s iconic black-and-white \nstriped pattern stands out sharply against the lush grass, creating a striking visual contrast. Its head is lowered as it \nnibbles on the grass, showcasing its natural feeding behavior. The mane, with alternating dark and light stripes, adds a \ntouch of elegance to its overall appearance. The bright sunlight enhances the vivid colors and intricate details, \nhighlighting the beauty of the zebra in its natural habitat.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more description for the behavior but add hallucination for sunlight.\nFigure 11. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.\n\nGPT-4V: This vibrant image showcases a cheerful woman holding a bright pink parasol on a sunny day. She is dressed in a \ncolorful floral swimsuit that exudes a retro style, complementing the joyful and carefree atmosphere. Her radiant smile \nand relaxed pose convey a sense of summer fun, with a backdrop of a clear blue sky, lush green trees, and a calm lake. \nThe parasol adds a playful and vintage charm, enhancing the overall nostalgic feel of the scene.\nCOCONut-PanCap: At the center of the scene, <5: a\ncheerful woman is\nwearing a vibrant swimsuit \nadorned with purple, blue, yellow, and white \npatterns>. She is holding <4:a pink umbrella>, which \nadds a pop of color and shade to the setting. The \nwoman is smiling happily, radiating a joyful and \ncarefree vibe, tying together the natural and human \nelements seamlessly. There are <1:lush green trees> \nand <0:blue sky> in the background, adding a natural \nelement to the composition. Below it, there is <2: a \nserene blue river>, Together, the composition \nemphasizes the interaction between the lively figure \nand the tranquil outdoor environment. \ntier reason: : Compared to COCONut-PanCap, GPT-4V recognize the water as lake, which is hard to be distinguished from this image.\nCOCONut-PanCap:\nThe image features an\nowl-shaped \ncandlestick, and a clock are placed on <0:the floor>. The \ncandlestick, designed in the form of an owl, adds a \ndecorative element beside the clock. Prominently, in the\ncenter there is <2:a circular clock with hands and Roman \nnumerals>, elegantly designed and placed on an intricate \nstand featuring ornate patterns and curves. Behind these \nitems, there is <1: a plain and white wall>, providing a \nneutral background that highlights the objects in the \nforeground. Together, the objects and their placement \ncreate a cohesive and aesthetically pleasing composition.\nGPT-4V: This cozy and artistic composition features a decorative vintage clock paired with a charming owl-shaped \ncandleholder. The clock, with intricate scrollwork and Roman numerals on its face, exudes an antique elegance, while the \nsoft glow of the candle creates a warm and inviting ambiance. The owl candleholder, with its textured surface and \nwhimsical design, adds a playful yet rustic touch to the scene. The subtle lighting and shadows enhance the nostalgic and \ntranquil atmosphere, making it a perfect display for a quiet, reflective setting.\ntier reason: Compared to COCONut-PanCap, GPT-4V summarize the style of the image as vintage.\nCOCONut-PanCap: The image captures an airplane in flight. \nThe center area highlights <1: an airplane, predominantly \nwhite in color, with dark blue English lettering “AIRFRANCE” \non its body and distinctive dark blue and red patterns on its \ntail>. The airplane is depicted flying gracefully through the \nsky. The background is <0: the sky, characterized by a mix of \nblue open spaces and scattered clouds>.\nGPT-4V: This image captures an Air France Airbus A380 in mid-flight against a backdrop of cloudy skies. The aircraft, \nknown for its massive size and double-deck design, features the iconic Air France livery with its name prominently \ndisplayed on the fuselage and the red, white, and blue stripes on the tail, symbolizing the French flag. The four powerful \nengines are clearly visible, emphasizing the plane’s engineering prowess. The angle of the shot highlights the sleek design \nand scale of the A380, showcasing its role as a leader in modern aviation.\ntier reason: : Compared to COCONut-PanCap, GPT-4V summarize bit more details but with hallucination of model ‘A380’.\nFigure 12. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.\n\nCOCONut-PanCap: The image portrays a serene winter \nlandscape. The image mainly highlights <0:a blue sky, creating \na clear and bright backdrop>. Below it, there is <1:a pristine\nsnow-covered slope, predominantly white and marked with \nski trails> that curve across the terrain. Adjacent to this is <2:a \nmajestic snow-covered mountain>, blending seamlessly into \nthe snowy environment. At the center of the scene, <3:a \nperson who is skiing down the slope, dressed in an orange \njacket and black pants> is captured. The skier is equipped with \nskis attached to their feet, gliding smoothly over the snow. \nAdditionally, <5:a backpack> is carried by the skier, \ncompleting their gear. \nGPT-4V: This image captures a serene winter landscape \nfeaturing a skier ascending a pristine snow-covered slope. \nThe untouched white snow is marked only by the skier\'s \ntracks and a winding trail that leads into the distance, \ntier reason: Compared to COCONut-PanCap, GPT-4V shows better vocabulary, but the information is almost the same.\nemphasizing the tranquility and solitude of the scene. Majestic mountains with rugged, rocky peaks dominate the background, \ntheir shadows adding depth and contrast to the bright sunlight illuminating the snow. The skier, dressed in warm, vibrant gear \nand carrying poles, navigates the slope with determination, highlighting the spirit of exploration and adventure. The expansive \nsky above completes the breathtaking alpine setting.\nCOCONut-PanCap: The image highlights <1:a white dog>,\nstanding on the <0:lush\ngreen\ngrass>, exuding a lively \ndemeanor. The dog is holding <2:a purple frisbee> in its \nmouth. The frisbee features black text “SKIMMER” printed on \nits surface, adding contrast to its vibrant purple color. \nTogether, the scene captures a moment of playful interaction, \ngrounded by the mask numbers for clarity and detail.\nGPT-4V: This image features an adorable white dog \nstanding on a lush green lawn, holding a bright purple\nfrisbee in its mouth. The dog\'s fluffy coat is well-groomed, and its expressive eyes gaze directly at the viewer, exuding \ncharm and playfulness. The frisbee, labeled with "Skimmer Competition," is slightly chewed, indicating the dog’s \nenthusiasm for play and activity. The vibrant colors of the frisbee and the green grass create a cheerful and lively \natmosphere, perfectly capturing a moment of joy and companionship between a pet and its outdoor playtime.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more OCR results of “competition”, but the information is minor.\nCOCONut-PanCap: The image captures a dynamic tennis \nmatch. In the foreground, the image highlights <3:a man \nactively playing tennis>, dressed in a black top, blue pants, \nblack socks, and black shoes. He is holding <2:a tennis racket>, \ncharacterized by its red and white frame and netting with \nprinted letters. The background is <0:a predominantly blue \nwall>, decorated with red and black patterns along with white \npartial text “rates Airline”. <1:The playing field is a mix of \ngreen and blue hues>.\nGPT-4V: This image captures a tennis player intensely focused \nduring a match, poised and ready to return a serve on a hard \ncourt. Dressed in a dark athletic shirt, blue shorts, and black \nsneakers, the player holds their racket firmly, leaning slightly \nforward in a balanced stance that conveys readiness and \ndetermination. The backdrop prominently features the USTA \nlogo and the Emirates Airline sponsorship, indicating a \nprofessional tennis tournament setting. The court\'s clean lines \nhighlight the competitive atmosphere\ntier reason: Compared to COCONut-PanCap, GPT-4V shows extra wording to describe the atmosphere which is not necessary.\nFigure 13. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.'),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content='Rethinking Mixture-of-Agents: Is Mixing Different\nLarge Language Models Beneficial?\nWenzhe Li*1, Yong Lin*1, Mengzhou Xia1, and Chi Jin1\n1Princeton University†\nAbstract\nEnsembling outputs from diverse sources is a straightforward yet effective approach to boost perfor-\nmance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from\nmultiple different Large Language Models (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA — an ensemble method that\naggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, sur-\nprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios:\nSelf-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of\n3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA\nto one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance\non the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off\nbetween diversity and quality of outputs under various MoA settings. We confirm that the MoA performance\nis rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models.\nTo complement the study, we identify the scenarios where mixing different LLMs could be helpful. This\npaper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of\nLLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.\n1\nIntroduction\nLarge language models have made remarkable strides in improving performance across different domains,\nwith notable examples such as GPT [Achiam et al., 2023], Gemini [Team et al., 2023], and Claude [Anthropic,\n2023]. Significant efforts have been directed toward increasing model size and training data to boost\ncapabilities. However, scaling at training time comes with steep costs, while scaling computation during\ninference remains largely underexplored.\nA straightforward way to utilize test-time compute is ensembling, which aims to combine outputs of\nmultiple LLMs [Wang et al., 2024a, Lin et al., 2024, Jiang et al., 2023a, Wang et al., 2024a]. Among various\nensembling approaches, Mixture-of-Agents (MoA) [Wang et al., 2024a] has garnered significant interest,\nachieving superior performance in challenging tasks such as instruction following [Wang et al., 2024a],\nsummarization, data extraction [OpenPipe, 2024], and real-world code issue resolution [Zhang et al., 2024b].\n*Equal contribution.\n†Email: {wenzhe.li,yl7690,mengzhou,chij}@princeton.edu.\n1\narXiv:2502.00674v1  [cs.CL]  2 Feb 2025\n\nSpecifically, MoA first queries multiple LLMs (proposers) to generate responses, and then uses an LLM\n(aggregator) to synthesize and summarize these responses into a high-quality response.\nPrevious research highlights the significance of model diversity within the proposers for optimizing the\nperformance of MoA, primarily focusing on strategies for ensembling a diverse set of individual models. We\nconsider cross-model diversity as the variation among different models. However, pursuing cross-model\ndiversity may inadvertently include low-quality models, resulting in a quality-diversity trade-off. While\nprevious studies mainly concentrate on achieving a high cross-model diversity [Wang et al., 2024a, Zhang\net al., 2024b], we adopt a holistic perspective on model diversity by considering in-model diversity, which\narises from the variability of multiple responses generated by the same model. In-model diversity enables us to\naggregate multiple outputs from an individual model. Intuitively, leveraging outputs from the best-performing\nindividual model can more effectively navigate the quality-diversity trade-off by creating a higher-quality\nproposer mixture. Thus, we propose Self-MoA as depicted in Figure 1b, which utilizes the same prompting\ntemplate as MoA but aggregates outputs that are repeatedly sampled from the same model, rather than from\na set of different models. To distinguish, we use Mixed-MoA to refer to MoA configurations that combine\ndifferent individual models when necessary.\nSurprisingly, we find that Mixed-MoA is usually sub-optimal compared with Self-MoA, especially when\nthere exist significant quality differences among proposers. Specifically, we revisit the same experiment\nsetting of MoA with six open-source instruction fine-tuned models as Wang et al. [2024a]. Compared\nwith Mixed-MoA which aggregates all six models, Self-MoA on the strongest model achieves 6.6 point\nimprovement over its mixed counterpart on the AlpacaEval 2.0 benchmark, showing a case of when intra-\nmodel diversity is more effective. Moreover, Self-MoA on two best-performed models on AlpacaEval 2.0\nconsistently achieves a 2-3 point gain and secures the top position among non-adversarial methods on the\nleaderboard, which further confirms the effectiveness of Self-MoA in this task.\nTo explore the limits of model diversity for MoA, we extend our experiments to a setting with three\nspecialized models, each excelling in a specific task. Specifically, we utilize Qwen2-7B-Instruct [Bai et al.,\n2023] for common sense QA (MMLU-redux [Gema et al., 2024]), Qwen2-Math-7B-Instruct [Bai et al., 2023]\nfor mathematics (MATH [Hendrycks et al., 2020]), and DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024]\nfor coding (CRUX [Gu et al., 2024]). We compare Self-MoA against a range of Mixed-MoA strategies,\nevaluating 13 combinations of individual models based on their average performance across the three tasks.\nOur findings indicate that employing task-specific models as proposers for Self-MoA can significantly\noutperform the best Mixed-MoA. Furthermore, even in a constructed mixture task tailored for Mixed-MoA\nwhere each individual model excels in a specific subtask, only two Mixed-MoA strategies slightly outperform\nSelf-MoA by 0.17% and 0.35%.\nTo better understand the effectiveness of Self-MoA, we conduct a comprehensive investigation of the trade-\noff between quality and diversity in MoA, involving over 200 experiments. We use the Vendi Score [Dan Fried-\nman and Dieng, 2023] to evaluate the diversity among the outputs of the proposers, while the average perfor-\nmance of the proposers serves as the measure of quality. In Section 4, we confirm that MoA performance\nhas a positive correlation with both quality and diversity. Moreover, we clearly show a trade-off along the\nachievable Pareto front of quality and diversity. Interestingly, we find that MoA is quite sensitive to variations\nin quality, with optimal performance typically occurring in regions characterized by high quality and relatively\nlow diversity. This finding naturally explains the effectiveness of Self-MoA, as it utilizes the strongest model\nas the proposer, ensuring high quality in its outputs.\nFinally, we evaluate the performance of Self-MoA under increasing computational budgets. As the\nnumber of outputs grows, the scalability of Self-MoA becomes constrained by the context length of the\naggregator. To address this issue, we introduce Self-MoA-Seq (Figure 1c), a sequential version that processes\nsamples using a sliding window, allowing it to handle an arbitrary number of model outputs. Our findings\n2\n\nshow that Self-MoA-Seq performs at least as effectively as Self-MoA, enabling scalable ensembling for\nLLMs with shorter context lengths without compromising final performance.\nOverall, our contributions are three-fold:\n• We introduce Self-MoA, which leverages in-model diversity by synthesizing multiple outputs from the\nsame model. Surprisingly, it demonstrates superior performance compared to existing Mixed-MoA\napproaches, which emphasize cross-model diversity, across a wide range of benchmarks.\n• Through systematic experiments and statistical analysis, we uncover a core trade-off between diversity\nand quality among the proposers, emphasizing that MoA is highly sensitive to proposer quality. This\nfinding also explains the success of Self-MoA, which leverages outputs from the highest-performing\nmodel, ensuring superior overall quality.\n• We extend Self-MoA to its sequential version Self-MoA-Seq, which iteratively aggregates a small\namount of outputs step by step. Self-MoA-Seq unlocks LLMs that are constrained by the context length\nand enables computation scaling during inference.\n2\nRelated Work\nEnsembles of LLMs.\nModel ensembling aims to combine strengths from multiple models. Previous\nstudies have explored various methods to leverage a diverse set of models, including but not limited to\nprompting [Wang et al., 2024a], weight averaging [Lin et al., 2024, Ram´e et al., 2024], routing [Jiang et al.,\n2024b, Lu et al., 2023], training a generative fusion model [Jiang et al., 2023b], and so on. Zhang et al. [2024a]\nargues that the fusion of specialized models with certain general abilities could be a promising direction toward\nArtificial General Intelligence. Mixture-of-Agents (MoA, Wang et al. [2024a]) first queries multiple LLMs to\ngenerate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows\npromising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval\n2.0 leaderboard. Our method is inspired by the prompt pipeline proposed in MoA. However, while existing\nMoA focuses on unleashing the strength from multiple different models [Wang et al., 2024a, Jiang et al.,\n2023b, Zhang et al., 2024b], we demonstrate the trade-off between diversity and quality within the proposers,\nhighlighting that focusing solely on diversity may compromise overall quality and final performance.\nLLM Inference with Repeated Sampling.\nPrevious studies have shown that combining model outputs\nfrom repeated sampling can yield a better response in various domains. In tasks with automatic verifiers\navailable, such as math [Hendrycks et al., 2021] and code [Chen et al., 2021], simply sampling LLMs\nmultiple times can significantly improve the pass@k metric and hence boost the success rate of solving the\ntasks [Roziere et al., 2023, Li et al., 2022, Brown et al., 2024]. In more general tasks without verification\ntools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most\npromising one from candidate responses [Wang et al., 2022, Chen et al., 2023b, Gui et al., 2024, Li et al.,\n2024]. Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference\ntime [Brown et al., 2024]. In this work, we identify the surprising effectiveness of repeated sampling in the\ncontext of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated\nfrom repeated sampling, hence can further improve over each individual output.\nCollaborative Agents\nThere is a surge of interest in building agent systems based on verification, critique,\ndiscussion, and refinement. For example, Stechly et al. [2023], Valmeekam et al. [2023], and Madaan et al.\n3\n\n  𝑜!\n!\n  𝑜!\n"\n  𝑜"\n!\n  𝑜"\n"\n  𝑜#\n!\n  𝑜#\n"\n 𝑀!\n 𝑀"\n 𝑀#\n 𝐴\n  𝑜$\n(a) MoA\n(b) Self-MoA\n(c) Self-MoA-Seq\n  𝑜!\n!\n  𝑜!\n"\n  𝑜!\n#\n  𝑜!\n%\n  𝑜!\n&\n  𝑜!\n\'\n 𝐴\n  𝑜$\n 𝑀!\n  𝑜!\n!\n  𝑜!\n"\n  𝑜!\n#\n  𝑜!\n%\n  𝑜!\n&\n  𝑜!\n\'\n 𝐴\n  𝑜$\n!\n  𝑜$\n!\n 𝐴\n  𝑜$\n"\n 𝑀!\nFigure 1: Comparison of MoA, Self-MoA, and Self-MoA-Seq. (a) In MoA, multiple models respond to\na query, followed by an aggregator synthesizing their outputs. (b) Self-MoA simplifies this by repeatedly\nsampling from a single model. (c) Self-MoA-Seq extends Self-MoA by applying a sliding window to combine\nthe best output so far with candidate outputs. At each timestep, the synthesized output is repeated to bias the\naggregator towards it, reducing the context length requirements and expanding the method’s applicability.\nNote that MoA can extend to multiple rounds of aggregation (Appendix A.1), while Self-MoA and Self-MoA-\nSeq can extend to more outputs, but we omit them here for clarity.\n[2024] use self-critique to iteratively refine outputs through a chain structure. Madaan et al. [2024], Chen\net al. [2024], and Wang et al. [2024a] explore the incorporation of multiple models to create a stronger agent\nthat outperforms each individual model. Du et al. [2023] incorporates multiple LLMs that propose and debate\ntheir individual responses over several rounds to reach a common final answer. Liang et al. [2023] proposes\nMulti-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative\nconclusions and avoid rushing to incorrect answers. Chen et al. [2023a] introduces RECONCILE, which\nadopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly,\nWang et al. [2024b] shows that a single model with carefully designed prompts can sometimes match the\nperformance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the\nprompts are insufficient.\n3\nIs Ensembling Different LLMs Beneficial?\nAs introduced in Section 1, previous research primarily emphasizes cross-model diversity, which can\ninadvertently include low-quality proposers. In this work, we introduce Self-MoA (Figure 1), which uses\na single top-performing model to generate multiple outputs and aggregate them to produce the final result.\nSelf-MoA leverages in-model diversity as repeated sampling often produces varied outputs. We propose our\nresearch question as follows:\nDoes the benefit of MoA stem from cross-model diversity?\nCan we build a stronger MoA using in-model diversity?\n4\n\nTable 1: Comparison of Self-MoA and Mixed-MoA on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110B-\nChat as the aggregator.\nModel Configuration\nLC Win Rate\nIndividual\nWizardLM-2-8x22B\n53.1\nQwen1.5-110B-Chat\n43.9\nLLaMA-3-70B-Instruct\n34.4\nQwen1.5-72B-Chat\n36.6\nMixtral-8x22B-Instruct-v0.1\n30.2\ndbrx-instruct\n25.4\nMixed-MoA\n2-Layer MoA [Wang et al., 2024a]\n59.1\nSelf-MoA\n2-Layer Self-MoA + WizardLM\n65.7\n3.1\nExperiments on AlpacaEval 2.0 with General Purpose Models\nEvaluation benchmarks.\nWe adopt the same experiment setting as Wang et al. [2024a] in AlpacaEval 2.0\nbenchmark [Dubois et al., 2024] and compare the performance of Mixed-MoA and Self-MoA1. AlpacaEval\n2.0 is a widely used benchmark for assessing the instruction-following abilities of LLMs. It offers a set\nof real-world instructions and employs a GPT-4-based annotator to compare the model’s responses against\nreference answers generated by GPT-4. To address length bias inherent in model-based evaluation, Dubois\net al. [2024] introduced the length-controlled (LC) win rate as a more robust evaluation metric.\nModels.\nFollowing Wang et al. [2024a], we construct MoA based on six individual models: Qwen1.5-\n110B-Chat [Bai et al., 2023], Qwen1.5-72B-Chat [Bai et al., 2023], WizardLM-8x22B [Xu et al., 2023],\nLLaMA-3-70B-Instruct [Touvron et al., 2023], Mixtral-8x22B-Instruct-v0.1 [Jiang et al., 2024a], and dbrx-\ninstruct [Team et al., 2024b]. Each model is sampled with a temperature of 0.7, following the default in\n[Wang et al., 2024a]. For Self-MoA, we aggregate six outputs sampled from WizardLM-2-8x22B, as it\nconsistently outperforms the other models. In line with Wang et al. [2024a], we use Qwen1.5-110B-Chat as\nthe aggregator for both Mixed-MoA and Self-MoA.\nResults.\nWe present the LC win rate for each model configuration in Table 1. For individual models, we\nreport the higher value between the leaderboard results and our reproduction. Notably, Self-MoA demonstrates\nremarkable effectiveness in this task, outperforming the Mixed-MoA baseline by 6.6 point. This suggests that,\nwhile using multiple models intuitively offers greater diversity, ensembling multiple outputs from a single\nmodel is more effective.\nApplying Self-MoA on top performing models.\nTo further validate the effectiveness of Self-MoA, we\napply it to the two top-performing models on AlpacaEval 2.0: gemma-2-9b-it-WPO-HB [Zhou et al., 2024]\nand gemma-2-9b-it-SimPO [Meng et al., 2024]. We use each model as both the proposer and the aggregator2,\n1We note that this experiment is similar to the “single-proposer” setting in Wang et al. [2024a], however our reproduced result is\ndifferent. We conjecture that such a major difference is due to different choices of the proposer model, which is not mentioned in Wang\net al. [2024a]. As we shall see later in Section 4, ensembling performance is more sensitive to quality rather than diversity. Therefore, a\nworse proposer model will lead to suboptimal performance of Self-MoA.\n2Qwen1.5-110B-Chat is not used as the aggregator since the two top models significantly outperform it.\n5\n\nTable 2: Self-MoA achieves state-of-the-art performance on the AlpacaEval 2.0 leaderboard when using\ntop-performing models as both proposers and aggregators. We only ensemble 4 outputs due to context window\nconstraints.\nModel Configuration\nLC Win Rate\nIndividual\ngemma-2-9b-it-WPO-HB\n76.7\ngemma-2-9b-it-SimPO\n72.4\nSelf-MoA\nSelf-MoA + gemma-2-9b-it-WPO-HB\n78.5\nSelf-MoA + gemma-2-9b-it-SimPO\n75.0\nwith a temperature of 0.7 for all the generations. Due to the context length constraint of Gemma 2 [Team et al.,\n2024a], the aggregator can only take four samples as the input. As shown in Table 2, Self-MoA consistently\nachieves a 2-3 point gain and secures the top position on the leaderboard during submission.\nResults on MT-Bench.\nBeyond AlpacaEval 2.0, we further evaluate Self-MoA and Mixed-MoA on MT-\nBench [Zheng et al., 2023], another benchmark used in Wang et al. [2024a]. The results align with our\nfindings from AlpacaEval 2.0, reinforcing the effectiveness of Self-MoA. Please refer to Appendix B.1 for\nmore details.\n3.2\nExperiments on Multiple Datasets with Specialized Models\nIn this section, we compare different ensembling methods on a diverse set of benchmarks using specialized\nmodels.\nEvaluation datasets.\nWe conduct evaluations across a diverse set of benchmarks:\n• MMLU [Hendrycks et al., 2020] is a multiple-choice dataset designed to assess a model’s multitask\naccuracy. MMLU is widely used to evaluate both the breadth and depth of language understanding\ncapabilities of current LLMs across a diverse array of subjects, including mathematics, history, computer\nscience, logic, and law. We adopt MMLU-redux [Gema et al., 2024] for evaluation, which is a subset\nof MMLU with 3,000 samples fixing the errors in the dataset through human re-annotating.\n• CRUX [Gu et al., 2024] consists of 800 Python code functions, each containing 3 to 13 lines along with\nan input-output pair. Based on this dataset, Gu et al. [2024] constructs two tasks: input prediction and\noutput prediction. To successfully complete these tasks, the LLM must demonstrate code reasoning\nabilities.\n• MATH [Hendrycks et al., 2021] comprises 12,500 challenging competition-level mathematics problems.\nFor our analysis, we utilize the testing subset of MATH, which consists of 5,000 samples.\n3As Qwen2-Math-7B-Instruct only supports context length of 4096, for these two data points, we sample the proposer with a reduced\ntoken length of 1024, and only aggregates three outputs from the proposer.\n6\n\nTable 3: Comparison of Self-MoA and Mixed-MoA in MMLU, CRUX, and MATH. The labels i, m, and d\nrefer to Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct, respectively.\nThe average performance represents the mean accuracy across MMLU, CRUX, and MATH. TaskBest\nindicates that we use the strongest model for each task as both proposer and aggregator.\nAggregator\nProposer\nMMLU\nCRUX\nMATH\nIndividual\n-\ni\n66.16\n36.25\n53.81\n-\nd\n60.91\n49.51\n53.82\n-\nm\n54.36\n27.88\n69.573\nMixed-MoA\ni\niimmdd\n67.89\n42.88\n64.38\nimdddd\n67.42\n44.50\n63.90\niiiimd\n68.90\n41.25\n63.00\nimmmmd\n66.63\n42.75\n66.02\niimmmm\n66.23\n39.25\n66.10\niiimmm\n67.49\n38.25\n64.16\niiiimm\n68.00\n37.00\n62.92\niidddd\n68.21\n45.50\n62.56\niiiddd\n68.21\n42.88\n62.38\niiiidd\n68.47\n40.75\n61.24\nmmdddd\n66.34\n46.75\n66.48\nmmmddd\n65.80\n47.00\n67.32\nmmmmdd\n65.44\n42.50\n67.62\nSelf-MoA\ni\n6×TaskBest\n69.01\n50.75\n68.42\nTaskBest\n6×TaskBest\n69.01\n52.62\n69.803\nModels.\nTo ensure sufficient diversity, we select three LLMs with specialized strengths: Qwen2-7B-\nInstruct [Yang et al., 2024], DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024], and Qwen2-Math-7B-\nInstruct. We fix the number of proposers to six and sweep various combinations of these three models.\nFor convenience, we denote Qwen2-7B-Instruct as i, DeepSeek-Coder-V2-Lite-Instruct as d, and Qwen2-\nMath-7B-Instruct as m. As shown in Table 3, Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and\nQwen2-Math-7B-Instruct excel on MMLU, CRUX, and MATH, respectively. We use the short name for\nthe mixture of proposers. For example, iiddmm indicates the inclusion of two samples from each model\nrespectively. When a model is represented multiple times in the proposer mixture, we ensure that two samples\nare generated with different random seeds. We set the temperature of each model to be 0.7 for the individual\nmodel, and use temperature 0 for the aggregator. We mainly use Qwen2-7B-Instruct as the aggregator but also\ntry different models as the aggregator. We explore various MoA configurations, including individual models,\ncombinations of two or three models as proposers, and using a single top-performing model (TaskBest, for\nexample DeepSeek-Coder-V2-Lite-Instruct for CRUX) as the proposer (Self-MoA).\nResults.\nThe results are presented in Table 3. When using i as the aggregator, Self-MoA with the TaskBest\nmodel consistently outperforms all 13 tested Mixed-MoA configurations across all tasks. Furthermore,\nadopting a task-specific aggregator yields an additional performance boost of 1-2 points. Interestingly,\nincreasing model diversity does not always lead to better performance. For instance, while MoA with\niimmdd surpasses mmmddd on MMLU, it underperforms on CRUX and MATH. This discrepancy aligns\n7\n\n40\n45\n50\n55\n60\n65\nQuality\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nDiversity\nMMLU\nMixed-MoA\nSelf-MoA\n61\n62\n63\n64\n65\n66\n67\n68\n69\nf\n20\n25\n30\n35\n40\n45\nQuality\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nCRUX\nMixed-MoA\nSelf-MoA\n36\n38\n40\n42\n44\n46\n48\n50\nf\n35\n40\n45\n50\n55\n60\n65\n70\nQuality\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nMATH\nMixed-MoA\nSelf-MoA\n58\n60\n62\n64\n66\n68\nMOA Performance\nFigure 2: The diversity-quality trade-off: Mixed-MoA incorporates different individual models as proposers,\nwhile Self-MoA uses the same individual model for this role. Quality is assessed based on the average\nperformance of each proposer, and diversity is computed with the Vendi Score [Dan Friedman and Dieng,\n2023] of outputs generated by proposers on the same prompts.\nwith the relative strengths of the individual models—i excels on MMLU but lags behind on CRUX and\nMATH. We postpone more discussion to Section 4.2.\n4\nThe Quality-Diversity Trade-off\nWe investigate factors that contribute to the strong performance of Self-MoA through careful experiments.\nPrevious studies have mainly focused on increasing model diversity within the group [Wang et al., 2024a,\nJiang et al., 2023a, Zhang et al., 2024b]. However, searching for diverse models can sometimes lead to\nincluding poorly performed models, resulting in a trade-off between diversity and quality, where quality refers\nto how well each individual model performs in the group.\nTherefore, we aim to identify the existence of a general relationship between MoA’s performance and\nquality as well as diversity. Following Section 3, we evaluate MoA’s performance on MMLU, CRUX, and\nMATH, which cover tasks requiring a wide range of capabilities. We vary the quality and diversity with two\norders of freedom: 1) combinations of individual models in proposers from Section 3.2; and 2) sampling\ntemperature. i.e., 0.5, 0.7, 1.0, 1.1, and 1.2. This results in a total of over 70 unique MoA proposer mixtures.\nWe measure the quality and diversity as follows:\n• Diversity: We utilize the Vendi Score [Dan Friedman and Dieng, 2023] to assess the diversity among\nindividual models in the proposer mixture. The Vendi Score represents the effective number of unique\nelements within a collection of samples [Dan Friedman and Dieng, 2023], with further details provided\nin the Appendix A.2. Specifically, for a given prompt x, we obtain responses from each model, denoted\nas y1, y2, . . . , y6. The diversity of the proposers for prompt x, denoted as d(x), is calculated using the\nVendi Score on the set [y1, . . . , y6]. We then compute the overall diversity across the dataset S as:\nd = 1\n|S|\nX\nx∈S\nd(x).\n• Quality: We first determine the accuracy of each model on the dataset S, yielding values q1, q2, . . . , q6.\nThe average accuracy, q = 1\n6(q1 + q2 + . . . + q6), serves as our measure of the quality of the proposers.\nWe will explore additional quality measurement strategies in later sections.\n8\n\nTable 4: Linear regression (Equation 1) of MoA’s performance t on diversity d and quality q.\nDataset\nα\nβ\nR2\nCoefficient\nP-value\nCoefficient\nP-value\nMMLU\n2.558 ± 0.176\n< 0.001\n1.841 ± 0.176\n< 0.001\n0.771\nCRUX\n4.548 ± 0.459\n< 0.001\n1.421 ± 0.459\n< 0.001\n0.685\nMATH\n4.719 ± 0.416\n< 0.001\n2.839 ± 0.416\n< 0.001\n0.760\nResults.\nWe plot MoA’s performance with corresponding diversity and quality for each mixture of proposers\nin Figure 2. We summarize key observations as follows:\n• The trends among MMLU, CRUX, and MATH are consistently aligned.\n• When the quality is fixed, increasing diversity can enhance MoA’s performance.\n• When the diversity is fixed, improving quality can also boost MoA’s performance.\n• There exists a trade-off in the achievable Pareto front between diversity and quality.\n• Notably, the best performance of MoA is typically observed in the bottom right of each subplot,\nindicating a strong sensitivity to quality.\nPrevious work on ensembles [Wang et al., 2024a, Jiang et al., 2023a, Zhang et al., 2024b] primarily focuses on\nincreasing the diversity of models within the proposer mixture. However, as shown in Figure 2, compared to\nSelf-MoA on the best-performing model, simply aiming for greater diversity in the proposer mixture can result\nin lower overall quality, which may negatively impact MoA’s performance. This trade-off between diversity\nand quality helps to explain why Self-MoA achieves superior performance across various benchmarks.\n4.1\nStatistical Analysis\nTo further understand the numerical correlation between MoA’s performance and diversity as well as quality,\nwe conduct linear regression for MoA’s performance t on diversity d and quality q. Specifically, we fit the\nfollowing equation for each dataset:\nt = α × q + β × d + γ,\n(1)\nwhere α, β, γ ∈R are real-valued coefficients to be determined. For each dataset, we collect around 70\ndata points from Figure 2 to construct the set {qi, di, ti}N\ni=1. The coefficients α, β, and γ are then derived\nby solving a linear regression on {qi, di, ti}N\ni=1. To make coefficients α and β comparable, we normalize\nq and d by subtracting their means and dividing by their standard deviations (detailed in Appendix A.3),\nrespectively. The results are presented in Table 4. We observe that the p-values for both α and β are less\nthan 0.001, indicating a significant correlation between MoA’s performance and both quality and diversity\n[Arnold, 1990]. The R2 values from the linear regression across three datasets are approximately around 0.7,\nindicating that the linear model based on quality and diversity explains 70% MoA’s performance and hence a\nstrong correlation between inputs and outputs, according to Appendix A.4. In later parts, we show that using\na more fine-grained quality calculation can further increase the R2 value.\n9\n\nComparing the effect strength of quality and diversity.\nFrom Table 4, we observe that α is greater than\nβ across all three datasets. In particular, for CRUX and MATH, the gap between these two measures is even\nmore pronounced. These results suggest that MoA’s performance is particularly sensitive to variations in\nquality, highlighting the importance of prioritizing quality within the proposer mixture. This finding is also\nconsistent with our observation that MoA achieves its best performance in the bottom right of the plot in\nFigure 2, further supporting the effectiveness of our proposed Self-MoA approach.\nAlternative quality measurements.\nWe use the averaged accuracy of each individual model to measure\nquality in the previous analysis. In this section, we explore alternative methods for assessing the quality of\nproposers. Recall that q1, . . . , q6 denote the accuracy of each individual model among proposers, and without\nloss of generality, we assume q1 ≥q2 ≥. . . ≥q6. It is reasonable to assume that the aggregator can select\nthe correct answer from the proposers, particularly when the responses of individual models are inconsistent.\nIn such cases, the aggregator would rely more heavily on models with better individual performance, meaning\nthe weight of q1 would be greater than that of q6.\nTherefore, we compare the following methods to calculate quality:\n• Average: 1\n6\nP6\ni=1 qi.\n• K-Norm:\n\x10\n1\n6\nP6\ni=1 qK\ni\n\x111/K\n, where a larger K places more emphasis on stronger individual models.\n• Centered-1/K-Norm: q1 −\n\x10\n1\n6\nP6\ni=1(q1 −qi)1/K\x11K\n. In this formulation, we first compute the\ndifference between qi and the best model’s q1. The 1/K norm emphasizes the weights of models\nwhose performance is closer to q1.\nAll three methods are the same when K = 1. For each quality measurement, we fit a linear regression\nto assess the relationship between MoA’s performance and the quality and diversity metrics, reporting the\nR2 values in Table 5. Our analysis shows that in MMLU and CRUX, applying a larger weight to better-\nperforming individual models tends to increase the R2 values. However, this trend is inconsistent for MATH.\nWe conjecture that this inconsistency arises because the aggregator Qwen2-7B-Instruct is relatively weak on\nMATH compared to the strongest individual model, Qwen2-Math-7B-Instruct. This limitation constrains\nthe performance of MoA, leading to an inconsistent trend in the linear regression results. In contrast, on\nMMLU, where Qwen2-7B-Instruct is the strongest individual model, we find that the R2 value can exceed\n0.9 with K = 2 using the Centered-1/K-Norm. This indicates a very strong linear relationship between MoA\nperformance and the quality and diversity metrics. Overall, we conclude that employing Centered-1/K-Norm\nwith K = 2 (marked in blue) achieves strong performance across all three datasets.\n4.2\nWhen Mixed-MoA Outperforms Self-MoA?\nAccording to the quality-diversity trade-off illustrated in Figure 2, we conjecture that increasing diversity can\nenhance MoA’s performance when the quality is controlled.\nMixed-MoA generally exhibits greater diversity than Self-MoA, which can lead to improved performance\nwhen the model quality is similar. This advantage arises when individual models achieve similar overall\nperformance while maintaining significant cross-model diversity. To simulate such a scenario, we construct a\nmixture task combining MMLU, CRUX, and MATH as described in Section 3.2. In this setting, test samples\nare drawn uniformly from the three tasks, and models do not have prior knowledge of a sample’s origin. For a\ngiven MoA strategy, we evaluate its performance on this mixture task by averaging its performance across the\n10\n\nTable 5: The R2 of the linear regression when we use different quality measurement methods. We find using\nCentered-1/K-Norm with K=2 can achieve good performance among all these three datasets.\nDataset\nMethod\nAvg. (K=1)\nK=2\nK=3\nK=4\nMMLU\nK-Norm\n0.771\n0.809\n0.832\n0.845\nCentered-1/K-Norm\n0.771\n0.881\n0.902\n0.903\nCRUX\nK-Norm\n0.685\n0.736\n0.765\n0.779\nCentered-1/K-Norm\n0.685\n0.753\n0.758\n0.753\nMATH\nK-Norm\n0.760\n0.720\n0.692\n0.672\nCentered-1/K-Norm\n0.760\n0.720\n0.692\n0.672\nTable 6: Comparison of Self-MoA and Mixed-MoA on the mixture task of MMLU, CRUX, and MATH,\nmeasured by the average performance of three tasks from Table 3. Mixed-MoA models with top two average\nperformances are highlighted by underline.\nAggregator\nProposer\nAverage\nIndividual\n-\ni\n52.07\n-\nd\n54.74\n-\nm\n50.60\nMixed-MoA\ni\niimmdd\n58.38\nimdddd\n58.61\niiiimd\n57.72\nimmmmd\n58.47\niimmmm\n57.19\niiimmm\n56.63\niiiimm\n55.97\niidddd\n58.76\niiiddd\n57.82\niiiidd\n56.82\nmmdddd\n59.86\nmmmddd\n60.04\nmmmmdd\n58.52\nSelf-MoA\ni\ndddddd\n59.69\ni\n6×TaskBest\n62.73\nTaskBest\n6×TaskBest\n63.81\nthree datasets. The results are reported in Table 6. In this mixture task, each model specializes in different\nsubtasks, with i performing best on MMLU, d on CRUX, and m on MATH. As TaskBest requires additional\nprior knowledge of the sample origin, we also report Self-MoA with d as the proposer, given that it achieves\nthe highest average performance among individual models.\nFrom Table 6, we observe that Mixed-MoA indeed outperforms Self-MoA of dddddd. Specifically,\nMixed-MoA of mmdddd and mmmddd achieves the average performance of 59.86% and 60.04%, improves\n11\n\nTable 7: MoA of Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. l is short for Llama-3.1-8B-Instruct and i\nis short for Qwen2-7B-Instruct.\nAggregator\nProposer\nMMLU\nIndividual\n-\ni\n66.16\n-\nl\n66.40\nMixed-MoA\ni\niiilll\n70.73\nSelf-MoA\ni\niiiiii\n69.01\ni\nllllll\n71.27\nupon Self-MoA of dddddd by 0.17% and 0.35%. Given the reported small margin, we argue that Self-MoA\nis still a very competitive baseline under this setting, not to mention the dominant performance of Self-MoA\nover Mixed-MoA when focusing on one single task (Self-MoA with TaskBest models achieve an average\nof 3.8% improvement from Table 6). In Appendix B.3 we also report normalized results that account for\ndifferent variances among tasks, which leads to a similar conclusion.\nWe further consider another single-task case on MMLU, involving two individual models: Llama-\n3.1-8B-Instruct and Qwen2-7B-Instruct, with Qwen2-7B-Instruct serving as the aggregator. We choose\nLlama-3.1-8B-Instruct because it performs similarly to Qwen2-7B-Instruct as an individual model. Table 7\ndemonstrates that even when the performance of two individual models is close, Self-MoA—utilizing six\nLlama-3.1-8B-Instruct proposers (denoted as llllll)—still outperforms the Mixed-MoA configuration\n(denoted as iiilll).\n5\nScaling Inference Compute with Self-MoA\nIn previous sections, we have provided evidence that Self-MoA over one strong model is straightforward\nbut effective. As the community is becoming more aware of scaling inference time computing [Brown et al.,\n2024, Snell et al., 2024, Wu et al., 2024], one natural question to ask is:\nGiven a strong model, does Self-MoA’s performance scale with the number of repeated samples?\nIntuitively, Self-MoA cannot scale indefinitely by simply increasing the computation budget for at least three\nreasons:\n• As more responses are sampled from a single model, the diversity among those samples tends to\nplateau.\n• Aggregating information from many samples is more challenging for LLMs compared to handling a\nsmaller number of samples.\n• Every LLM has a context length limit (e.g., 8192 tokens for Gemma 2), which restricts the number of\nresponses an aggregator can process at once.\nWhile the first limitation is inherent to repeated sampling, we address the latter two by introducing Self-\nMoA-Seq, a sequential variant designed to manage large numbers of responses without overwhelming the\naggregator. Self-MoA-Seq uses a sliding window to aggregate a fixed number of responses at a time, allowing\n12\n\n5\n10\n15\n20\n25\n30\nNumber of Samples\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0\n70.5\nAccuracy\nMMLU\nSelf-MoA\nSelf-MoA-Seq\nBase Model (Qwen2-7B-Instruct)\n5\n10\n15\n20\n25\n30\nNumber of Samples\n47\n48\n49\n50\n51\n52\n53\n54\nAccuracy\nCRUX\nSelf-MoA\nSelf-MoA-Seq\nBase Model (DeepSeek-Coder-V2-Lite-Instruct)\nFigure 3: The performance of Self-MoA and Self-MoA-Seq with a growing number of samples. Dashed lines\nindicate the performance of a single forward pass with the base model.\nit to handle an unlimited number of responses, regardless of context length constraints. A visual illustration is\nprovided in Figure 1.\nWe evaluate the performance of Self-MoA and Self-MoA-Seq with increasing sample sizes on the MMLU\nand CRUX benchmarks to study their scaling behavior. For each benchmark, we use the best-performing\nmodel as both the proposer and aggregator (Qwen2-7B-Instruct for MMLU and DeepSeek-Coder-V2-Lite-\nInstruct for CRUX), with a sampling temperature of 0.7. In Self-MoA-Seq, the window size is set to six, with\nthe first three slots reserved for the current synthesized output. We vary the number of samples from 6 to 30\nand plot the accuracy curves from three runs with different seeds in Figure 3. Our key observations are as\nfollows:\n• Both Self-MoA and Self-MoA-Seq significantly improve performance over the individual base model.\n• Adding more samples can have both positive and negative effects, meaning there is no universal\ncompute-optimal solution.\n• Self-MoA-Seq delivers performance that is comparable to, or slightly better than, Self-MoA.\nThese findings suggest that Self-MoA-Seq can extend the effectiveness of Self-MoA to LLMs with shorter\ncontext lengths, without sacrificing performance. Following Section 4.2, we explore whether introducing a\nsecond model can enhance performance in the sequential setting. Given that Llama-3.1-8B-Instruct performs\nsimilarly to Qwen2-7B-Instruct on the MMLU task, we compare the impact of adding Llama-3.1-8B-Instruct\nand DeepSeek-Coder-V2-Lite-Instruct (which underperforms Qwen2-7B-Instruct by 5%) after aggregating\n30 samples from Qwen2-7B-Instruct in Self-MoA-Seq. We find that incorporating Llama-3.1-8B-Instruct\nboosts accuracy by around 2%, whereas adding DeepSeek-Coder-V2-Lite-Instruct reduces accuracy by more\nthan 1.5%. This result provides another example of cross-model diversity benefiting MoA, and shows the\npotential of Self-MoA-Seq with increasing computation budget.\n6\nConclusion\nIn this paper, we introduce Self-MoA, an innovative approach that utilizes in-model diversity to enhance\nthe performance of large language models during inference. Our experiments demonstrate that Self-MoA\noutperforms traditional Mixed-MoA strategies in many popular benchmarks, particularly when the proposer\n13\n\nmodel quality varies. By aggregating outputs from a single high-performing model, Self-MoA effectively\naddresses the quality-diversity trade-off. We further identify the scenarios where mixing LLM can be\npotentially beneficial and extend Self-MoA to the constrained context length setting. These findings highlight\nthe potential of in-model diversity in optimizing LLM performance and pave the way for further advancements\nin ensemble methods.\n14\n\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman,\nS. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nA. Anthropic. Introducing claude, 2023.\nH. J. Arnold. Introduction to the practice of statistics. Technometrics, 32:347–348, 1990. URL https:\n//api.semanticscholar.org/CorpusID:122891525.\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical\nreport. arXiv preprint arXiv:2309.16609, 2023.\nB. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. R´e, and A. Mirhoseini. Large language monkeys:\nScaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024.\nJ. C.-Y. Chen, S. Saha, and M. Bansal. Reconcile: Round-table conference improves reasoning via consensus\namong diverse llms. arXiv preprint arXiv:2309.13007, 2023a.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\nG. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\n2021.\nS. Chen, L. Zeng, A. Raghunathan, F. Huang, and T. C. Kim. Moa is all you need: Building llm research\nteam using mixture of agents. arXiv preprint arXiv:2409.07487, 2024.\nX. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin, S. Prakash, C. Sutton, X. Wang, and D. Zhou. Universal\nself-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023b.\nD. Dan Friedman and A. B. Dieng. The vendi score: A diversity evaluation metric for machine learning.\nTransactions on machine learning research, 2023.\nY. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in language\nmodels through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to\ndebias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du,\nM. R. G. Madani, et al. Are we done with mmlu? arXiv preprint arXiv:2406.04127, 2024.\nA. Gu, B. Rozi`ere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for\ncode reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024.\nL. Gui, C. Gˆarbacea, and V. Veitch. Bonbon alignment for large language models and the sweetness of\nbest-of-n sampling. arXiv preprint arXiv:2406.00832, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\nmathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n15\n\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B.\nHanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a.\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas,\nE. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux,\nP. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and\nW. E. Sayed. Mixtral of experts, 2024b. URL https://arxiv.org/abs/2401.04088.\nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and\ngenerative fusion. arXiv preprint arXiv:2306.02561, 2023a.\nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and\ngenerative fusion, 2023b. URL https://arxiv.org/abs/2306.02561.\nJ. Li, Q. Zhang, Y. Yu, Q. Fu, and D. Ye. More agents is all you need, 2024. URL https://arxiv.org/\nabs/2402.05120.\nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,\nA. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097,\n2022.\nT. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent\nthinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.\nY. Lin, H. Lin, W. Xiong, S. Diao, J. Liu, J. Zhang, R. Pan, H. Wang, W. Hu, H. Zhang, H. Dong, R. Pi,\nH. Zhao, N. Jiang, H. Ji, Y. Yao, and T. Zhang. Mitigating the alignment tax of rlhf, 2024. URL\nhttps://arxiv.org/abs/2309.06256.\nK. Lu, H. Yuan, R. Lin, J. Lin, Z. Yuan, C. Zhou, and J. Zhou. Routing to the expert: Efficient reward-guided\nensemble of large language models, 2023. URL https://arxiv.org/abs/2311.08692.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information\nProcessing Systems, 36, 2024.\nY. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with a reference-free reward. arXiv\npreprint arXiv:2405.14734, 2024.\nOpenPipe. Openpipe mixture of agents: Outperform gpt-4 at 1/25th the cost, 2024. URL https://\nopenpipe.ai/blog/mixture-of-agents.\nA. Ram´e, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard,\nand O. Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024. URL https:\n//arxiv.org/abs/2406.16768.\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, et al.\nCode llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nK. Sarjana, L. Hayati, and W. Wahidaturrahmi. Mathematical modelling and verbal abilities: How they\ndetermine students’ ability to solve mathematical word problems? Beta: Jurnal Tadris Matematika, 13(2):\n117–129, 2020.\n16\n\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than\nscaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314.\nK. Stechly, M. Marquez, and S. Kambhampati. Gpt-4 doesn’t know it’s wrong: An analysis of iterative\nprompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\net al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nG. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari,\nA. Ram´e, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome,\nA. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J.-B. Grill,\nB. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl,\nA. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen,\nC. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar,\nD. Rogozi´nska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin,\nG. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-Pluci´nska, H. Batra, H. Dhand,\nI. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi,\nJ. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. yeong Ji, K. Mohamed,\nK. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sjoesund, L. Usui,\nL. Sifre, L. Heuermann, L. Lago, L. McNealus, L. B. Soares, L. Kilpatrick, L. Dixon, L. Martins, M. Reid,\nM. Singh, M. Iverson, M. G¨orner, M. Velloso, M. Wirth, M. Davidow, M. Miller, M. Rahtz, M. Watson,\nM. Risdal, M. Kazemi, M. Moynihan, M. Zhang, M. Kahng, M. Park, M. Rahman, M. Khatwani, N. Dao,\nN. Bardoliwalla, N. Devanathan, N. Dumai, N. Chauhan, O. Wahltinez, P. Botarda, P. Barnes, P. Barham,\nP. Michel, P. Jin, P. Georgiev, P. Culliton, P. Kuppala, R. Comanescu, R. Merhej, R. Jana, R. A. Rokni,\nR. Agarwal, R. Mullins, S. Saadat, S. M. Carthy, S. Perrin, S. M. R. Arnold, S. Krause, S. Dai, S. Garg,\nS. Sheth, S. Ronstrom, S. Chan, T. Jordan, T. Yu, T. Eccles, T. Hennigan, T. Kocisky, T. Doshi, V. Jain,\nV. Yadav, V. Meshram, V. Dharmadhikari, W. Barkley, W. Wei, W. Ye, W. Han, W. Kwon, X. Xu,\nZ. Shen, Z. Gong, Z. Wei, V. Cotruta, P. Kirk, A. Rao, M. Giang, L. Peran, T. Warkentin, E. Collins,\nJ. Barral, Z. Ghahramani, R. Hadsell, D. Sculley, J. Banks, A. Dragan, S. Petrov, O. Vinyals, J. Dean,\nD. Hassabis, K. Kavukcuoglu, C. Farabet, E. Buchatskaya, S. Borgeaud, N. Fiedel, A. Joulin, K. Kenealy,\nR. Dadashi, and A. Andreev. Gemma 2: Improving open language models at a practical size, 2024a. URL\nhttps://arxiv.org/abs/2408.00118.\nM. R. Team et al. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https://www. databricks.\ncom/blog/introducing-dbrx-new-state-art-open-llm. Accessed on April, 26, 2024b.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nK. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by self-\ncritiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.\nJ. Wang, J. Wang, B. Athiwaratkun, C. Zhang, and J. Zou. Mixture-of-agents enhances large language model\ncapabilities. arXiv preprint arXiv:2406.04692, 2024a.\nQ. Wang, Z. Wang, Y. Su, H. Tong, and Y. Song. Rethinking the bounds of llm reasoning: Are multi-agent\ndiscussions the key? arXiv preprint arXiv:2402.18272, 2024b.\n17\n\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nY. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. An empirical analysis of compute-optimal inference for\nproblem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724.\nC. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large\nlanguage models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\nA. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin,\nJ. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang,\nK. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin,\nS. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren,\nX. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan. Qwen2\ntechnical report, 2024. URL https://arxiv.org/abs/2407.10671.\nK. Zhang, B. Qi, and B. Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion.\narXiv preprint arXiv:2407.08642, 2024a.\nK. Zhang, W. Yao, Z. Liu, Y. Feng, Z. Liu, R. Murthy, T. Lan, L. Li, R. Lou, J. Xu, et al. Diversity empowers\nintelligence: Integrating expertise of software engineering agents. arXiv preprint arXiv:2408.07060, 2024b.\nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nW. Zhou, R. Agrawal, S. Zhang, S. R. Indurthi, S. Zhao, K. Song, S. Xu, and C. Zhu. Wpo: Enhancing rlhf\nwith weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024.\nQ. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseek-coder-v2:\nBreaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024.\n18\n\nA\nSupplements\nA.1\nMulti-Layer MoA\nMoA can be extended to multiple layers. For MoA with l layers and n LLMs {Ai,j}n\nj=1 in each layer i, we\ncan formulate it as follows:\nyi =\nn\nM\nj=1\n[Ai,j(xi)] + x1,\nxi+1 = yi,\nwhere each LLM Aj\ni generates a response for the query xi, which is further concatenated with the original\nquery by the aggregator’s prompt L.\nTable 8 compares the performance of 3-Layer Mixed-MoA and 2-Layer Self-MoA as well as the total\nnumber of forward passes required for each method. Specifically, one forward pass is counted each time a\nproposer model generates an output or an aggregator synthesizes a result. Notably, Self-MoA outperforms the\n3-Layer Mixed-MoA baseline with only half the forward passes.\nTable 8: Results of 3-Layer Mixed-MoA.\nModel Configuration\nLC Win Rate\n# Forward Passes\nMixed-MoA\n3-Layer MoA [Wang et al., 2024a]\n65.4\n13\nSelf-MoA\n2-Layer Self-MoA + WizardLM-2-8x22B\n65.7\n7\nA.2\nVendi Score\nThe Vendi Score (VS) is a metric designed to evaluate diversity in machine learning. It takes as input a\ncollection of samples along with a pairwise similarity function, and it outputs a single value that represents\nthe effective number of unique elements within the sample set.\nThe score is computed using a positive semi-definite similarity matrix K ∈Rn×n as follows:\nV S(K) = exp\n\x12\n−tr\n\x12K\nn log\n\x12K\nn\n\x13\x13\x13\n= exp\n \n−\nn\nX\ni=1\nλi log(λi)\n!\nHere, λi are the eigenvalues of the normalized matrix K\nn , and 0 log 0 = 0. Essentially, the Vendi Score is\nthe exponential of the von Neumann entropy of K\nn , which reflects the Shannon entropy of its eigenvalues,\nalso referred to as the effective rank. This metric provides a quantitative measure of diversity based on the\ndistribution of similarity scores among the samples.\nA.3\nNormalization of Inputs\nGiven a sequence of inputs x1, ..., xn. Let x′ denote the normalized x. We have\nx′ = xi −¯x\nstd(x) , where ¯x = 1\nn\nn\nX\ni=1\nxi, and std(x) =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(xi −¯x)2\n19\n\nA.4\nImplication of R-squre\nThe implications of R2 are presented in Table 9, illustrating the degree of influence between the independent\nand dependent variables. [Sarjana et al., 2020].\nTable 9: The interpretation of R-square\nR-square\nLevel\n[0, 0.2)\nVery weak\n[0.2, 0.4)\nWeak\n[0.4, 0.6)\nMedian\n[0.6, 0.8)\nStrong\n[0.8, 1.0]\nVery Strong\nB\nAdditional Results\nB.1\nMT-Bench Results\nWe also compare MoA and Self-MoA on the MT-Bench [Zheng et al., 2023] benchmark under the same\nexperiment setting as Wang et al. [2024a]. We copy the numbers from Wang et al. [2024a] for 3-Layer MoA\nsettings, and report our implemented results for the other experiments to ensure that 2-Layer experiments are\nfair comparisons. Table 10 shows that Self-MoA outperforms its Mixed-MoA counterpart, and using GPT-4o\nas the aggregator can achieve the best performance even with fewer forward passes compared to 3-Layer\nMoA with GPT-4o.\nB.2\nComparison to Universal Self-Consistency\nWe conduct further experiments to compare Self-Consistency [Wang et al., 2022] with MoA and Self-MoA\non the AlpacaEval 2.0 benchmark. As this benchmark is an instruction-following task without exact answers,\nwe evaluate on Universal Self-Consistency (USC) [Chen et al., 2023b] which prompts LLMs to generate\nthe most consistent response. We report the result in Table 12, which shows that USC performs worse than\nits MoA counterpart when proposers and aggregators are controlled. This further suggests that rather than\nfinding the most consistent response, MoA and Self-MoA can encourage LLM to synthesize the references\nand produce a better response.\nB.3\nNormalizing Sub-tasks in Table 6\nThe results in Table 3 indicate that the variance of models on CRUX is generally higher than that of the other\ntwo tasks, which could bias the average performance towards CRUX. To ensure that each task contributes\nequally to the overall performance metric, we assign weights to the three tasks based on the inverse of their\nvariance.\nFor example, considering MMLU, we report 19 performance metrics (including individual models,\nMixed-MoA, and Self-MoA) in Table 3. The standard deviation of performance for MMLU across these 19\n20\n\nTable 10: Comparison of Self-MoA and Mixed-MoA on MT-Bench. We use Qwen1.5-110B-Chat and GPT-4o\nas the aggregator.\nModel Configuration\nAvg.\n1st turn\n2nd turn\n# Forward Passes\nIndividual\nWizardLM-2-8x22B\n8.99\n9.05\n8.93\n1\nQwen1.5-110B-Chat\n8.61\n8.77\n8.45\n1\nLLaMA-3-70B-Instruct\n8.84\n9.14\n8.54\n1\nQwen1.5-72B-Chat\n8.62\n8.66\n8.58\n1\nMixtral-8x22B-Instruct-v0.1\n8.49\n8.89\n8.09\n1\ndbrx-instruct\n7.82\n8.21\n7.43\n1\nMixed-MoA\n2-Layer MoA\n9.06\n9.23\n8.89\n7\n2-Layer MoA w/ GPT-4o\n9.39\n9.40\n9.37\n7\n3-Layer MoA\n9.25\n9.44\n9.07\n13\n3-Layer MoA w/ GPT-4o\n9.40\n9.49\n9.31\n13\nSelf-MoA +\nWizardLM-2-8x22B\n2-Layer Self-MoA\n9.13\n9.36\n8.89\n7\n2-Layer Self-MoA w/ GPT-4o\n9.52\n9.56\n9.47\n7\nsettings is calculated to be 3.50. In comparison, the standard deviation for CRUX and MATH are 5.70 and\n4.27, respectively. Consequently, the weight assigned to MMLU when calculating the “WeightedAvg” is\ngiven by:\nWeightMMLU =\n1/3.50\n(1/3.50) + (1/5.70) + (1/4.27).\nThe normalized results are shown in Table 11.\n21\n\nTable 11: This table compares Self-MoA and Mixed-MoA using a weighted composition of three sub-\ntasks. The weights are assigned to each sub-task to prevent a high-variance task, such as CRUX, from\ndisproportionately influencing the overall performance metrics. This approach ensures a more balanced\nevaluation, allowing for a fairer comparison between the two models.\nAggregator\nProposer\nMMLU\nCRUX\nMATH\nAverage\nWeightedAvg\nIndividual\n-\ni\n66.16\n36.25\n53.81\n52.07\n54.46\nIndividual\n-\nd\n60.91\n49.51\n53.82\n54.74\n55.65\nIndividual\n-\nm\n54.36\n27.88\n69.57\n50.60\n52.80\nMixed-MoA\ni\niimmdd\n67.89\n42.88\n64.38\n58.38\n60.40\nMixed-MoA\ni\nimdddd\n67.42\n44.50\n63.90\n58.61\n60.46\nMixed-MoA\ni\niiiimd\n68.90\n41.25\n63.00\n57.72\n59.94\nMixed-MoA\ni\nimmmmd\n66.63\n42.75\n66.02\n58.47\n60.40\nMixed-MoA\ni\niimmmm\n66.23\n39.25\n66.10\n57.19\n59.38\nMixed-MoA\ni\niiimmm\n67.49\n38.25\n64.16\n56.63\n59.00\nMixed-MoA\ni\niiiimm\n68.00\n37.00\n62.92\n55.97\n58.47\nMixed-MoA\ni\niidddd\n68.21\n45.50\n62.56\n58.76\n60.58\nMixed-MoA\ni\niiiddd\n68.21\n42.88\n62.38\n57.82\n59.86\nMixed-MoA\ni\niiiidd\n68.47\n40.75\n61.24\n56.82\n59.05\nMixed-MoA\ni\nmmdddd\n66.34\n46.75\n66.48\n59.86\n61.45\nMixed-MoA\ni\nmmmddd\n65.80\n47.00\n67.32\n60.04\n61.57\nMixed-MoA\ni\nmmmmdd\n65.44\n42.50\n67.62\n58.52\n60.39\nSelf-MoA\ni\ndddddd\n65.23\n50.75\n63.08\n59.69\n60.86\nSelf-MoA\ni\n6×TaskBest\n69.01\n50.75\n68.42\n62.73\n64.21\nSelf-MoA\nTaskBest\nTaskBest\n69.01\n52.62\n69.80\n63.81\n65.14\nTable 12: Comparison of Self-MoA, Mixed-MoA, and Universal Self-Consistency (USC) on AlpacaEval 2.0\nleaderboard. We use Qwen1.5-110B-Chat as the aggregator.\nModel Configuration\nLC Win Rate\n# Forward Passes\nMixed-MoA\nMoA\n59.1\n7\nSelf-MoA\nSelf-MoA + WizardLM-2-8x22B\n65.7\n7\nUniversal Self-Consistency\nMixed-USC\n53.8\n7\nSelf-USC + WizardLM-2-8x22B\n60.2\n7\n22'),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content="1. Introduction\nText-to-image (T2I) generative models have revolutionized\ncontent generation by producing diverse and highly photo-\nrealistic images, enabling a wide range of applications such\nas digital art creation (Mazzone & Elgammal, 2019), image\nediting (Brooks et al., 2023), and medical imaging (Kaze-\nrouni et al., 2023). These models are usually trained on\nseveral billions of web-scraped image and text pairs pre-\nsumably capturing a broad spectrum of semantic concepts.\nConsequently, these models are also prone to be exposed\nto and thus generate disturbing content containing nudity,\nviolence, child exploitation, and self-harm – raising serious\n1Department of Computer Science, Boston University 2Runway.\nCorrespondence to: Deepti Ghadiyaram <dghadiya@bu.edu>.\nRemove nudity\nChange\nPhotographic\nStyles\nChange\nObject \nattributes\nRemove violence\nRemove\nUnsafe \nconcepts\nMake the image darker\nChange the season style to winter \nChange to a full shot of a dog\nChange car color to blue\nFigure 1. Monosemantic interpretable concepts such as nudity,\nphotographic styles, and object attributes are identified using k-\nsparse autoencoders (k-SAE). We leverage them to enable precise\nmodification of a desired concept during the generation process,\nwithout impacting the overall image structure, photo-realism, vi-\nsual quality, and prompt alignment (for safe concepts). Our frame-\nwork can be used to remove unsafe concepts (top row), photo-\ngraphic styles (middle row), and object attributes (last row).\nethical concerns about their downstream applications.\nSeveral attempts have been made to enforce safe generations\nin the past: integrating safety filters as part of the generation\npipeline (Rando et al., 2022), guiding the generation process\naway from a pre-defined unsafe latent space (Schramowski\net al., 2023), or directly erasing inappropriate concepts by\nmodifying model weights (Gandikota et al., 2023; Heng\n& Soh, 2024; Li et al., 2024). While partially success-\nful, some of these methods involve model training which\nis not only computationally expensive but also alters the\noverall model generative capabilities. More recently, a few\ninference-based approaches have been proposed, which do\nnot alter model weights (Yoon et al., 2024; Jain et al., 2024).\nSAFREE (Yoon et al., 2024) alters the semantics of the input\nprompt by filtering toxic tokens, while TraSCE (Jain et al.,\n2024) modifies negative prompting with gradient compu-\ntation to guide the model towards safer outputs. Crucially,\nsometimes these models have the undesirable consequence\n1\narXiv:2501.19066v1  [cs.CV]  31 Jan 2025\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nof visual degraded output generations or being misaligned\nwith input prompts, even when the prompts are benign. Ad-\nditionally, the increased inference time (e.g., 8.84s overhead\nper image as noted in TraSCE (Jain et al., 2024)) due to\nonline filtering makes them difficult to deploy in practice.\nIn this work, we posit that the semantic information is in-\nterwoven across different layers of a generative model in\ncomplex ways that is not fully understood. Subsequently, ex-\nisting training or inference-based safe generation techniques\ncould be altering this latent landscape in undesirable ways\nleading to misaligned or irrelevant outputs. To this end, we\napproach the generation process from the ground up and ex-\nplore the following crucial question: can we systematically\nisolate monosemantic1 concepts of varied granularities (fine-\ngrained and abstract) from the generative latent space and\nsurgically manipulate only them? Having such a tool would\nbe invaluable as it would allow the user to intentionally con-\ntrol just the relevant concept of interest without disrupting\nthe overall latent landscape.\nTo this end, we leverage k-sparse autoencoders (k-\nSAE) (Makhzani & Frey, 2013) to design controllable gen-\nerative models. k-SAEs have shown promising progress in\ninterpreting language models by learning a sparse dictionary\nof monosemantic concepts (Bricken et al., 2023; Cunning-\nham et al., 2023). In our work, we first train a k-SAE on the\nembeddings extracted from a corpus of text prompts contain-\ning semantic concepts we wish to control (e.g., unsafe con-\ncepts). Once trained, each k-SAE’s hidden state corresponds\nto an isolated monosemantic concept. During the generation\nprocess, given a concept we wish to steer, we use k-SAE to\nidentify its corresponding latent direction and precisely ma-\nnipulate the presence of that concept in the outcome, without\nimpacting the overall generation capability (Fig. 1). Notably,\nour method does not require any fine-tuning as in Zhang\net al. (2024), synthetic data generation as in Esposito et al.\n(2023), training a separate LoRA adapter (Hu et al., 2021)\nfor each concept as in Gandikota et al. (2025) to manipulate\nmaking it fast, efficient, and adaptable to any pre-trained\ntext to image generative framework. We summarize our\nempirical findings and key contributions below:\n• We identify interpretable monosemantic concepts\nin text-to-image generation latent landscape using\na k-sparse autoencoder. Once trained, k-SAE serves\nas a Concept Steerer to provide precise control over\nspecific visual concepts (e.g., nudity, violence, etc.)\n• Concept Steerer achieves state-of-the-art perfor-\nmance on unsafe concept removal while being ∼5x\nfaster than the existing best method, without compro-\nmising visual quality.\n1In contrast to the one-to-many mapping of polysemantic neu-\nrons, monosemantic neurons form a one-to-one correlation with\ntheir related input features (Yan et al., 2024).\n• Concept Steerer effectively manipulates photo-\ngraphic and artistic styles, object attributes, enabling\ncontrolled yet creative image generation.\n• Concept Steerer is robust to adversarial prompt ma-\nnipulations, achieving a 20.01% improvement against\nred-teaming tools, ensuring reliable image generation\neven under challenging scenarios.\n• Concept Steerer works out-of-the-box to any text-\nto-image model, is extremely simple, requires no re-\ntraining nor LoRA adapters, and is highly efficient.\n2. Related Work\nControlling diffusion models: Wu et al. (2023); Wallace\net al. (2024) fine-tune diffusion models using human feed-\nback and Bansal et al. (2023); Singhal et al. (2025) pro-\npose inference-time diffusion steering with reward functions.\nHowever, these methods rely on strong reward functions,\nand are computationally intensive (Uehara et al., 2025).\nSome methods achieve controllability by training additional\nmodules such as low-rank adapters (LoRAs) (Gandikota\net al., 2025; Stracke et al., 2025), which requires millions of\nparameters per concept and significantly increases genera-\ntion time (Sridhar & Vasconcelos, 2024). Several inference-\ntime intervention works attempt fine-grained control at test\ntime. However, estimating noise at each step for each con-\ncept during generation (Brack et al., 2022; 2023) signifi-\ncantly slows down generation and steering model activa-\ntions based on optimal transport (Rodriguez et al., 2024)\nrequires learning activation mapping for each style. By con-\ntrast, our approach is very simple, requires no training of the\nbase model or LoRA adapters, no additional noise/gradient\ncomputation during the generation process. Moreover, once\ntrained, our approach allows us to manipulate any concept\nwe want without further tuning.\nSafe generation: Given the growing concerns of genera-\ntive models’ capability to produce inappropriate content,\nseveral valuable research has emerged in this space. Some\ntraining-based methods (Gandikota et al., 2023; Zhang et al.,\n2024) directly remove inappropriate concepts from the dif-\nfusion model through additional fine-tuning, while some\nothers like (Gandikota et al., 2024; Gong et al., 2025) up-\ndate model weights to erase concepts without retraining\nthe model. Some recently proposed inference-based ap-\nproaches (Yoon et al., 2024; Jain et al., 2024) do not require\ntraining or weight updates. While effective, these methods\noften result in degraded image quality and increased infer-\nence time. Unlike all prior works, our method surgically\nisolates interpretable concepts in the generative latent space\nand manipulating only these in the text encoder. Thus, our\napproach enjoys the benefit of precise control of inappropri-\nate concepts, does not compromise on generation quality,\nand maintains prompt-image alignment.\n2\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nDiffusion model\nText prompt \n“Greek goddess \nposing …”\nText\nEncoder\nUnsafe \nSafe \nConcept 𝑪\n“Nudity”\nk-SAE\nDecoder\nEncoder\nSafe path\nUnsafe path\nText\nEncoder\n∗λ\nFigure 2. K-sparse autoencoder (k-SAE) is trained on feature\nrepresentations from the text encoder of the diffusion model. Once\ntrained, it serves as a Concept Steerer, enabling precise, surgical\nconcept manipulation by adjusting λ.\nInterpreting diffusion models: Recent works have demon-\nstrated that sparse autoencoders (SAE) could recover inter-\npretable features in large language models (Bricken et al.,\n2023; Cunningham et al., 2023), CLIP vision features (Fry,\n2024; Daujotas, 2024) and diffusion features (Kim et al.,\n2024; Surkov et al., 2024). Kim et al. (2024) reveal monose-\nmantic interpretable features represented within rich visual\nfeatures of the diffusion model while Surkov et al. (2024)\ninvestigate how text information is integrated via cross-\nattention. By contrast, we focus on the text encoder of a dif-\nfusion model, identify interpretable directions via k-SAEs,\nand demonstrate precise steering of a variety of concepts.\n3. Approach\nWe propose a simple yet effective technique to precisely\nisolate and steer semantic concepts such as nudity or pho-\ntographic styles using k-sparse autoencoders (Makhzani &\nFrey, 2013) (k-SAE). We first present how we train such\na k-SAE (Sec. 3.2), followed by our method to combine\ndifferent monosemantic neurons to steer abstract concepts\n(Sec. 3.3). We stress that a k-SAE is trained only once and\nno training is required for any concept the user wishes to\nintroduce, eliminate, or modulate.\n3.1. Preliminaries on text to image models\nText-to-image diffusion models (Rombach et al., 2022;\nRamesh et al., 2022; Saharia et al., 2022) primarily con-\nsist of a text encoder to extract a text prompt’s interme-\ndiate embedding and a diffusion model. During training,\nthe diffusion model progressively denoises a noisy im-\nage (or its latent representation) conditioned on the text\nprompt’s intermediate embedding. Formally, given an in-\nput y0, the forward diffusion process progressively adds\nnoise to y0 over T timesteps. The intermediate noisy im-\nage at timestep t is yt =\np\n(1 −βt)y0 + √βtϵ where ϵ is\nthe Gaussian noise and βt is a timestep-dependent hyper\nparameter. In the reverse process, the diffusion model ϵθ\niteratively denoises yt at each timestep, conditioned on the\ntext prompt embedding c, to predict noise ϵ. The objective\nfunction for training the model is to minimize the error be-\ntween the introduced and the predicted noise, defined as:\nEy,t,ϵ∼N(0,1)\n\x02\n∥ϵ −ϵθ(yt, c, t)∥2\n2\n\x03\n3.2. Preliminaries on k-sparse autoencoders\nSparse autoencoders (Ng et al., 2011) are neural networks\ndesigned for learning compact and meaningful feature rep-\nresentations in an unsupervised manner. They consist of an\nencoder and a decoder, optimized jointly using a reconstruc-\ntion loss and a sparsity regularization term to encourage\nonly a few neurons to be maximally activated for a given in-\nput. However, the sparsity constraint introduces significant\nchallenges during optimization (Tibshirani, 1996; Makhzani\n& Frey, 2013). To mitigate these issues, k-sparse autoen-\ncoders (k-SAEs) (Makhzani & Frey, 2013) were introduced.\nThey explicitly control the number of active neurons to k\nduring training by applying a Top-k activation function at\neach training step. Consequently, this retains only the k\nhighest activations and zeroes out the rest.\nLet Wenc ∈Rn×d and Wdec ∈Rd×n represent the weight\nmatrices of the k-SAE’s encoder and decoder respectively\n(Fig. 2). The hidden layer dimension n is defined as an\ninteger multiple of the input feature dimension d. The ratio\nn/d, referred to as the expansion factor, controls the extent\nto which the hidden dimension is expanded relative to the\ninput dimension. Following Bricken et al. (2023), bpre ∈Rd\ndenotes the bias term added to input x before feeding to the\nencoder (aka pre-encoder bias), while benc ∈Rn denotes\nthe bias term of the encoder.\nLet x ∈RL×d denote the intermediate representation of the\ntext encoder for an input prompt in a text-to-image model,\nwhere L denotes the number of tokens. The encoded latent\nz is computed as:\nz = ENC(x) = Top-k(ReLU(Wenc(x −bpre) + benc)),\n(1)\nwhere the Top-k function retains only the top k neuron acti-\nvations and sets the remaining activations to zero (Makhzani\n& Frey, 2013). The decoder reconstructs ˆx as:\nˆx = DEC(x) = Wdecz + bpre,\n(2)\nThe training objective of a standard k-SAE is to minimize\nthe normalized mean squared error (MSE) between the orig-\ninal feature x and the reconstructed feature ˆx, denoted by\nLmse. However, both SAEs and k-SAEs suffer from the pres-\nence of “dead latents,” where a large proportion of latents\nstop activating entirely at some point in training. Presence\nof dead latents decreases the likelihood of the network dis-\ncovering separable, interpretable features while incurring\n3\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nunnecessary computational cost (Bricken et al., 2023). To\ndiscourage dead latents, we incorporate an auxiliary MSE\nloss as suggested in Gao et al. (2024). Specifically, in every\ntraining step, we identify top kaux dead latents and recon-\nstruct a latent ˆz exclusively from them, as defined below:\nˆz = Top-kaux(ReLU(Wenc(x −bpre) + benc)),\n(3)\nNow, let ˆe = Wdecˆz represent the reconstruction using the\ntop kaux dead latents. Laux is defined as a reconstruction loss\nbetween the auto encoder’s residual and the output from the\ndead neurons (ˆe). As discussed in Gao et al. (2024), the\nintuition behind Laux is to compute gradients that push the\nparameters of the dead neurons in the direction of explaining\nthe autoencoder residual (e). Thus, the total training loss is:\nL = Lmse + αLaux = ∥x −ˆx∥2\n2 + α∥e −ˆe∥2\n2,\n(4)\nThe scalar α is a weighting factor that controls the relative\ncontribution of the auxiliary loss.\n3.3. Concept Steerers\nGiven a human-interpretable concept C2 we wish to steer,\nwe first extract its text embedding xC, pass it through k-\nSAE, and finally perform an element-wise addition with the\ninput prompt embedding x. This can be expressed as:\nxsteered = x + Wdec(λ ∗ENC(xC)),\n(5)\nwhere λ denotes a scalar that controls the steering strength.\nThe steered vector xsteered is used to condition the generation\nprocess. As we show in Sec. 4, our approach requires a k-\nSAE to be trained only once, and provides model-agnostic,\nfine-grained control over concept steering without degrading\nthe overall generation quality.\n4. Experiments\nWe first share the training setup followed by numerous re-\nsults and ablations on concept steering.\nImplementation details: We train k-sparse autoencoders\non text embeddings with kaux = 256, and loss weight pa-\nrameter α = 1/32 for 10k training steps. We train for a total\ntraining tokens of 400M on a batch size of 4096 with the\nlearning rate 0.0004 using Adam (Kingma, 2014) optimizer.\nThe k-SAE is trained with k = 32 and an expansion factor\nof 4, resulting in a total hidden size dimension n = 3072\nfor Stable Diffusion (SD) 1.4 (Rombach et al., 2022) in\nthe unsafe removal task. For style manipulation, we use\nk = 64 with an expansion factor of 64, resulting in a total\nhidden size dimension n = 49152 for SD1.4 and k = 64\nwith an expansion factor of 16, resulting in a total hidden\nsize dimension n = 32768 for SDXL-Turbo (Sauer et al.,\n2Defined by any user-provided prompt, e.g., “nudity”.\n2025). These settings were found via ablation studies on\ndownstream tasks and/or chosen based on overall training\nstability and sparsity. We apply a unit normalization con-\nstraint (Sharkey et al., 2023) on the decoder weights Wdec\nof the k-SAE after each update. Although our method can\nbe applied in an out-of-the-box manner to any text-to-image\nmodel, for a fair comparison with existing methods, we con-\nduct experiments using SD 1.4 for unsafe concept removal\nand then expand our evaluation to more recent SDXL-Turbo\nand FLUX.1-dev. When steering concepts, we use a slightly\nmodified encoder of k-SAE without the TopK activation\nfunction, as the TopK function clamps some possible impor-\ntant information that is critical for both maintaining visual\nquality and effective concept removal. More details in Ap-\npendix A.\n4.1. Steering towards safety\nSetup: First, we demonstrate the effectiveness of erasing\nunsafe concepts using k-SAEs. We use the Inappropriate\nImage Prompts (I2P) dataset (Schramowski et al., 2023)\nto evaluate our method on steering nudity concepts and\nthe Ring-A-Bell benchmark (Tsai et al., 2023) to assess\nsteering performance on violent content. We train a k-SAE\nusing features extracted from the residual stream of the 10th\nlayer (out of 12 layers) of the text encoder in SD 1.4, on the\nprompts in I2P dataset, and evaluate its performance on both\nthe I2P and Ring-A-Bell datasets. As in prior works (Tsai\net al., 2023; Jain et al., 2024), we use nudity concept Cnudity\nas “naked, nude, bare, exposed, stripped, topless, male geni-\ntalia, penis, buttocks” and use a slightly modified version\nfor the violent concept Cviolence as “violence, blood”. We set\nsteering strength λ = −0.5 for I2P dataset and λ = −0.7\nfor adversarial datasets including violent concept.\nEvaluation metrics: To quantify the impact of our method\non generation quality, we use FID (Heusel et al., 2017) and\nCLIP score (Hessel et al., 2021; Radford et al., 2021) on the\nCOCO-30k dataset, evaluating 10k generated samples. We\nreport Attack Success Rate (ASR), i.e., the percentage of\ngenerated images containing nudity or violence as a measure\nof how well a model reduces unsafe content generation.\nTo this end, we use the NudeNet (Bedapudi, 2019) with a\nthreshold of 0.45 and Q16 violence detector (Schramowski\net al., 2022), following prior work (Jain et al., 2024).\nBaselines: We compare our method against inference-based\napproaches that do not require training or weight updates to\nthe generative model, including SLD (Schramowski et al.,\n2023), SD with negative prompt (SD-NP), SAFREE (Yoon\net al., 2024), and TraSCE (Jain et al., 2024). Additionally,\nwe evaluate our method against training-based approaches,\nincluding ESD (Gandikota et al., 2023), FMN (Zhang et al.,\n2024), CA (Kumari et al., 2023), MACE (Lu et al., 2024),\nand SA (Heng & Soh, 2024), as well as approaches that\n4\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 1. Performance comparison across different methods on\nI2P and COCO datasets. Lower ASR and FID indicate better\nperformance; higher is better for CLIP. Our method achieves the\nlowest ASR by effectively removing nudity while preserving visual\nquality and prompt alignment. Bold: best. Underline: second-best.\nGray : require training and weight updates, Pink : do not require\ntraining but update model weights, Blue : do not require either.\nMETHOD\nI2P\nCOCO\nASR ↓\nFID ↓\nCLIP ↑\nSDV1.4\n17.80\n16.71\n31.3\nESD (GANDIKOTA ET AL., 2023)\n2.87\n18.18\n30.2\nCA (KUMARI ET AL., 2023)\n1.04\n24.12\n30.1\nMACE (LU ET AL., 2024)\n1.51\n16.80\n28.7\nSA (HENG & SOH, 2024)\n2.81\n25.80\n29.7\nUCE (GANDIKOTA ET AL., 2024)\n0.87\n17.99\n30.2\nRECE (GONG ET AL., 2025)\n0.72\n17.74\n30.2\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n1.74\n28.75\n28.4\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n2.28\n24.40\n29.1\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n3.95\n21.17\n29.8\nSD-NP\n0.74\n18.33\n30.1\nSAFREE (YOON ET AL., 2024)\n1.45\n19.32\n30.1\nTRASCE (JAIN ET AL., 2024)\n0.45\n17.41\n29.9\nOURS (W/O NEGATIVE STEERING)\n0.57\n18.37\n30.8\nOURS\n0.36\n18.67\n30.8\nSD 1.4\nOurs\nTraSCE\nSAFREE\nRemove \nnudity\nPrompt: “ken doll, commoner, concept art, art in the style of Mesopotamia 3000 …”\nPrompt: “painting of a cyberpunk android … fully clothed in futuristic armor …”\nRemove \nnudity\nFigure 3. Qualitative comparisons of different approaches, in-\ncluding TraSCE and SAFREE, on the I2P dataset. Our method\nremoves nudity without significantly altering the generated images,\nresulting in outputs that are better aligned with the input prompt.\nrequire no training but involve weight updates, such as\nUCE (Gandikota et al., 2024) and RECE (Gong et al., 2025).\nWe also try a variant of our model, where we steer in the op-\nposite direction of the layer activation corresponding to the\nnull text used for classifier-free guidance (Ho & Salimans,\n2022), which we refer to as negative steering.\n4.1.1. STEERING NUDITY CONCEPT\nAs shown in Table 1, our approach achieves state-of-the-art\nperformance in steering unsafe concepts, yielding the lowest\nASR (0.36) on the I2P dataset and surpassing the previous\nbest method. We note that incorporating negative steering\nslightly improves performance, demonstrating that our con-\ncept vector effectively models abstract concepts and, similar\nto negative prompting, yields a slight improvement in per-\nformance. Notably, our approach even outperforms both\ntraining-based methods (Gandikota et al., 2023; Kumari\net al., 2023; Lu et al., 2024; Heng & Soh, 2024) and weight-\nSD 1.4\nOurs\nRemove \nnudity\nRemove \nnudity\nFigure 4. Qualitative examples from the I2P dataset.\nOur\nmethod allows fine-grained control over the removal of specific\nconcepts, removing only the intended concept while preserving\nthe overall structure and style of the generated images.\nPrompt: “… future bass girl unwrapped smooth body fabric unfolds statue bust … front and side view body …”\nλ = −0.3\nλ = −0.25\nλ = −0.2\nλ = −0.15\nλ = −0.1\nλ = −0.05\nλ = 0\nFigure 5. Qualitative example from the I2P dataset with FLUX.\nOur method is model-agnostic and can be applied to both U-Net-\nbased SD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.\nSD 1.4\nOurs\nRemove \nviolence\nRemove \nviolence\nFigure 6. Qualitative examples from the Ring-A-Bell dataset.\nOur method successfully removes the abstract concept of violence,\nas shown by the absence of blood in the right images. The images\nare intentionally blurred for display purposes as they are disturbing.\nupdating methods (Gandikota et al., 2024; Gong et al., 2025),\nunderscoring the effectiveness of our method. Furthermore,\nour method achieved the highest prompt-image correspon-\ndence, as indicated by the CLIP score on the COCO dataset\n(30.8), ranking just below the original SD 1.4 model (31.3).\nThis is demonstrated in Fig. 3 and Fig. 4, where previous\nmethods sometimes generate unrelated images when the\nprompt triggers unsafe content. By contrast, our method\nsuccessfully removes nudity while preserving the overall\nstructure and maintaining alignment with the input prompt.\nMoreover, as shown in Fig. 5, we demonstrate that our\nmethod can also steer the DiT-based (Peebles & Xie, 2023)\nFLUX (Labs, 2023) model in an out-of-the-box manner.\n4.1.2. STEERING VIOLENCE CONCEPT\nWe also evaluate our method’s performance in suppress-\ning violent content generation, as presented in Table 2. As\nshown in Fig. 6, our method effectively reduces the genera-\ntion of violent content compared to existing training-based\nand weight-update-based methods. Although SLD-Max\n5\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 2. Performance comparison across different methods on\nthe Ring-A-Bell-Union (Violence) dataset. Lower values indicate\nbetter performance. Our method demonstrates competitive perfor-\nmance without compromising generation quality, as indicated by\nthe FID scores in Table 1. Bold: best. Underline: second-best.\nGray : require training and weight updates, Pink : do not require\ntraining but update model weights, Blue : do not require either.\nMETHOD\nRING-A-BELL-UNION\n(VIOLENCE)↓\nSDV1.4\n99.6\nESD (GANDIKOTA ET AL., 2023)\n86.0\nFNM (ZHANG ET AL., 2024)\n98.8\nCA (KUMARI ET AL., 2023)\n100.0\nUCE (GANDIKOTA ET AL., 2024)\n89.8\nRECE (GONG ET AL., 2025)\n89.2\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n40.4\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n80.4\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n97.2\nSD-NP\n94.8\nTRASCE (JAIN ET AL., 2024)\n72.4\nOURS\n43.7\nPrompt: “geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k, ”\nC=“Minimal-\nist”\nC=“Zoom-in, \nmagnify”\nFigure 7. Photographic style manipulation of SD 1.4 for the\ngiven prompt “geodesic landscape, john chamberlain, christopher\nbalaskas, tadao ando, 4 k, ” where concept prompts are “minimalist”\n(Top) and “zoom-in, magnify” (Bottom), respectively. In the top\nrow, the image is manipulated toward a maximalist style as λ →\n−1, while it adopts a minimalist style as λ →1. Similarly, in the\nbottom row, the image appears zoomed out and becomes blurred\nas λ →−1, whereas it becomes zoomed in and clearer as λ →1.\nachieves slightly better performance than ours, it signifi-\ncantly degrades overall image quality, yielding an FID of\n28.75 compared to 18.67 for our approach (Table 1).\n4.2. Steering of photographic styles and object\nattributes\nSetup: In this section, we demonstrate the effectiveness\nof steering photographic styles and object attributes. We\ntrain a k-SAE using features extracted from the residual\nstream of the 11th (out of 12) layer of the text encoder\nin SD 1.4. To observe the effect of photographic style\nchanges, we designed a dataset dedicated to 40 photographic\nstyles, including black-and-white, HDR, minimalist, etc.\nFor each class, we generated 100 prompts, totaling around\n4000 prompts, by querying ChatGPT. We also experiment\nwith SDXL-Turbo, where we train using features from both\nof its text encoders:11th (out of 12) and 29th (out of 32)\nlayers with prompts from I2P dataset.\nAs shown in Fig. 7 and Fig. 8 we can adjust its photographic\nConcept Sliders\nOurs\n𝐶=“Winter”\n𝐶= “Low light”\nSDXL-Turbo\nPrompt: “A photo of a forest, realistic, 8k”\nPrompt: “A photo of a tree with a bench, realistic, 8k”\nFigure 8. Qualitative comparisons with weather Concept Slid-\ners on SDXL-Turbo. Note that Concept Sliders train specific\nsliders: winter weather slider and a dark weather slider, whereas\nour method trains a k-SAE only once for different concepts. Top:\n“A photo of a tree with a bench, realistic, 8k” with concept to steer\n= “winter.” Bottom: “A photo of a forest, realistic, 8k” with the\nconcept to steer = “low light.” Notice how in the top image our\nmethod also removes leaves while in the bottom image, our method\neffectively applies a low-light effect to the original image.\n= 0\n= 0.1\n= 0.3\n= 0.4\n= 0.7\n= 0.9\nFigure 9. Image composition manipulation using SDXL-Turbo\nfor the prompt “A dog” with the concept prompt “Full shot.” Notice\nhow as λ →1, the generated image transitions from a close-up of\nthe face to a full shot.\nstyle, including “zoom-in” and “minimalist.” In Fig. 8, we\ncompare our results with Concept Sliders (Gandikota et al.,\n2025) on SDXL-Turbo where Concept Sliders train sepa-\nrate models for each weather condition style. Remarkably,\nour method can effectively steer concepts like weather con-\nditions and photographic styles. We note that I2P dataset\nin addition to the semantic concepts such as nudity and\nviolence, also had descriptors about general photographic\nstyles such as “full shot” or seasons “winter”. We believe\nthat k-SAE internalized these concepts offering us a pow-\nerful tool to surgically steer them. This powerful result\nhighlights the generalizable capability of k-SAEs to learn\ndiverse monosemantic concepts. This is corroborated by\nour results in Fig. 9, where we show that our method can\nmanipulate image compositions, changing a close-up image\nof a dog into a “full shot” of a dog while preserving the\nappearance of its head part.\nFinally, in Fig. 10, we use the same k-SAE to effectively ma-\nnipulates object attributes. Here, we inject a concept for an\nobject present in the image, such as “blue [object]” or “tree\n6\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\n= 0\n= 0.1\n= 0.2\n= 0.3\n= 0.4\n= 0.5\nFigure 10. Object attribute manipulation of SDXL-Turbo for\nthe given prompts “A car” (Top) and “A photo of a tree” (Bottom),\nwhere the concept prompts are “A blue car” (Top) and “Tree with\nautumn leaves” (Bottom). By adjusting λ, our method transitions\nthe image toward the desired concept specified by the prompts.\nwith autumn leaves.” We note that the resulting generations\npreserve most of the original content while successfully in-\njecting the desired concept. These results demonstrate the\nuniversal applicability of a k-SAE without the need to train\nseparate adapters for each concept. We wish to continue\nexploring the limits of universality of k-SAEs in the future.\n4.3. Robustness to adversarial prompt manipulation\nNext, we demonstrate the robustness of our method\nagainst adversarial prompts on four datasets:\nred-\nteaming approaches like Ring-A-Bell (Tsai et al., 2023),\nP4D (Chin et al., 2023), and attack frameworks like MMA-\nDiffusion (Yang et al., 2024) and UnlearnDiffAtk (Zhang\net al., 2025). Adversarial prompts often consist of several\nnon-English phrases or nonsensical text fragments that lack\nsemantic meaning, but fool the underlying generative mod-\nels to produce unsafe content. We follow the same setup in\nSec. 4.1 and use a k-SAE trained on I2P prompts.\nAs shown in Table 3, our method achieves the best overall\nrobustness on average across all datasets, significantly out-\nperforming the most recent works TraSCE (Jain et al., 2024)\nby 1.23% and SAFREE (Yoon et al., 2024) by 20.01%.\nSpecifically, for the MMA-Diffusion and P4D datasets, our\nmethod achieves state-of-the-art results with improvements\nof 10.60% and 1.98%, respectively. This demonstrates\nthat our method performs very well and can implicitly\nidentify monosemantic interpretable directions for “nudity”\nwithin the latent space of adversarial prompts. Notably, our\nmethod outperforms RECE (Gong et al., 2025) specifically\ndesigned for tackling adversarial prompts by 4.48%. For\nother datasets, our method ranks second-best or performs\ncomparably to the best scores. We note that k-SAE is trained\non text embeddings from I2P prompts to learn unsafe con-\ncepts and is not exposed to adversarial datasets. Remarkable\nperformance in adversarial datasets demonstrates k-SAE\ngeneralizes well to unseen prompts, even without exposure\nto prompt embeddings from different distributions, similar\nobservation to Sec. 4.2. We reiterate that once a k-SAE is\ntrained on unsafe concepts, our method does not require\nλ = 0\nλ = −0.1\nλ = −0.2\nλ = −0.3\nλ = −0.4\nλ = −0.5\nFigure 11. Effect of steering strength parameter (λ) on the I2P\ndataset while we steer nudity. Notice how as λ →−0.5, the\npresence of nudity disappears completely.\nretraining.\n4.4. Efficiency of Concept Steerer\nAs shown in Table 4, our method achieves the fastest in-\nference time among all other inference-based approaches,\nwith only a 0.14 sec./sample overhead on a single L40S\nGPU compared to the original SD 1.4. We highlight that our\nmethod is approximately 5x faster than the previous state-\nof-the-art (Jain et al., 2024) in unsafe concept removal.\n4.5. Ablation Studies\nFinally, we analyze the impact of our design choices on the\noverall steering capacity and visual quality.\nEffect of Concept Steering on Visual Quality: To evaluate\nthe impact of our approach on visual quality, we conduct a\nuser study using 50 randomly selected safe images gener-\nated by the original SD 1.4 model and nudity-steered images\nproduced by applying our method on SD 1.4. We followed\nthe setup described in Sec 4.1. The study involved 22 par-\nticipants, who were shown images in a randomized order\nand were asked to select the image they preferred most\nbased purely on overall visual quality. 44.7% of users pre-\nferred images produced by concept steering, while 44.9%\npreferred images from SD 1.4, indicating that participants\nexpressed an almost equal preference for both generations.\nThis is a crucial finding because it shows that our method\ndoes not deteriorate visual quality from the base model but\noffers the additional benefit of controllability.\nEffect of Layer Selection on Steering: We examine how\nthe selection of different layers in the text encoder impacts\nthe semantic information captured in k-SAE and thereby\nconcept steering. As shown in Table 5, representations\nfrom later layers are more effective to remove nudity and\nsteering than earlier layers. We believe that earlier layers\ncapture more low-level semantic information, thus high-\nlevel concepts such as nudity are better captured in the\nlater layers, making them suitable candidates for steering.\nSimilar observations were reported in Toker et al. (2024).\n7\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 3. Attack Success Rate (ASR) of different methods on various adversarial attack datasets. Lower ASR indicates better\nperformance. Our method achieves the best overall robustness on average across all datasets by effectively removing nudity implicitly\nembedded in the model. Bold: best. Underline: second-best. Gray : require training and weight updates, Pink : do not require training\nbut update model weights, Blue : do not require either.\nMETHOD\nRING-A-BELL ↓\nMMA-DIFFUSION ↓\nP4D ↓\nUNLEARNDIFFATK ↓\nAVG ↓\nK77\nK38\nK16\nAVG\nSDV1.4\n85.26\n87.37\n93.68\n88.10\n95.70\n98.70\n69.70\n87.05\nSA (HENG & SOH, 2024)\n63.15\n56.84\n56.84\n58.94\n47.68\n12.68\n2.81\n30.53\nCA (KUMARI ET AL., 2023)\n86.32\n91.69\n94.26\n90.76\n10.60\n5.63\n1.04\n27.01\nESD (GANDIKOTA ET AL., 2023)\n20.00\n29.47\n35.79\n28.42\n9.27\n15.49\n2.87\n14.51\nMACE (LU ET AL., 2024)\n2.10\n0.00\n0.00\n0.70\n2.72\n2.82\n1.51\n1.94\nUCE (GANDIKOTA ET AL., 2024)\n10.52\n9.47\n12.61\n10.87\n29.93\n9.86\n0.87\n12.38\nRECE (GONG ET AL., 2025)\n5.26\n4.21\n5.26\n4.91\n21.77\n5.63\n0.72\n8.76\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n23.16\n32.63\n42.11\n32.63\n35.76\n9.14\n2.44\n20.24\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n56.84\n64.21\n61.05\n60.70\n68.21\n33.10\n3.10\n41.28\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n92.63\n88.42\n91.05\n90.70\n68.21\n24.00\n1.98\n46.72\nSD-NP\n17.89\n40.42\n34.74\n31.68\n24.00\n10.00\n1.46\n16.29\nSAFREE (YOON ET AL., 2024)\n35.78\n47.36\n55.78\n46.31\n40.82\n10.56\n1.45\n24.29\nTRASCE (JAIN ET AL., 2024)\n1.05\n2.10\n2.10\n1.75\n16.60\n3.97\n0.70\n5.51\nOURS\n3.16\n8.42\n9.47\n7.02\n6.00\n1.99\n2.11\n4.28\nTable 4. Model Efficiency Comparison. Experiments were con-\nducted on a single L40S GPU on P4D dataset (150 samples in\ntotal) for the task of removing nudity.\nMETHOD\nINFERENCE TIME (S/SAMPLE) ↓\nSD 1.4\n3.02\nSAFREE (YOON ET AL., 2024)\n4.24\nTRASCE (JAIN ET AL., 2024)\n15.62\nOURS\n3.16\nTable 5. Attack Success Rate (ASR) when representations from\ndifferent encoder layers are used to train k-SAE on the I2P dataset.\nThe 10th layer yields the lowest ASR, indicating that this layer\ncaptures most information about nudity concept. k-SAE expansion\nfactor = 4, hidden neurons (n) = 3072.\nLAYERS\nASR ON I2P ↓\n12\n1.02\n10\n0.36\n8\n0.45\n6\n1.72\n4\n3.85\nEffect of k-SAE capacity on steering: We investigate the\neffect of k-SAE capacity determined by different expansion\nfactors on steering results. From Table 6, we note that\nthe performance differences between capacities is relatively\nminor, using an expansion factor of 4 proves to be the most\neffective in removing nudity.\nEffect of steering strength λ: Finally, we investigate the\neffect of the steering strength, λ. Table 7 illustrates the\nimpact of λ, showing that decreasing its value enables more\neffective removal of nudity from a greater number of images.\nAs shown in the first and second rows of Fig. 11, setting\nλ = −0.1 effectively removes the nudity concept in most\nimages. However, smaller λ values lead to a more complete\nremoval, as demonstrated in the last row of Fig. 11.\nTable 6. Attack Success Rate (ASR) for different expansion\nfactors of k-SAE trained on text embeddings extracted from the\n10th layer of the I2P prompts. An expansion factor of 4 yields the\nlowest ASR, indicating its efficacy for steering.\nEXPANSION FACTOR\nCAPACITY\nASR ON I2P↓\n4\n3072\n0.36\n8\n6144\n0.51\n16\n12288\n0.47\n32\n24576\n0.49\n64\n49152\n0.53\nTable 7. Attack Success Rate (ASR) for different values of λ of\nk-SAE with an expansion factor of 4 trained on text embeddings\nof 10th layer on the I2P dataset. λ = −0.5 yields the lowest ASR.\nλ\nASR ON I2P ↓\n−0.1\n2.59\n−0.2\n1.23\n−0.3\n0.87\n−0.4\n0.60\n−0.5\n0.36\n5. Discussion and Future Work\nWe propose a novel framework leveraging k-SAEs to enable\nefficient and interpretable concept manipulation in diffu-\nsion models. Once trained, k-SAE serves as a Concept\nSteerer to precisely control specific visual concepts (e.g.,\nnudity, violence, etc.) Our extensive experiments demon-\nstrate that our approach is very simple, does not compromise\nthe generation quality, and is robust to adversarial prompt\nmanipulations. Currently, we steer concepts by extract-\ning representations from the text encoder of the generative\nmodels. In future, we wish to explore steering via visual em-\nbeddings and allow users more control by selecting regions\nin an image and locally steer.\n8\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nImpact Statement\nAs text-to-image models are increasingly integrated into\nhigh-stakes applications, discouraging unsafe generations\nis of paramount significance. This work presents an ef-\nfective approach for identifying and suppressing unsafe\nconcept directions across various generative models. By\nimproving the controllability and reliability of generative\nmodels, our method advances the development of safer\nAI systems, facilitating their responsible deployment in\nreal-world applications.\nCode is available at: https:\n//github.com/kim-dahye/steerers\nReferences\nBansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S.,\nGoldblum, M., Geiping, J., and Goldstein, T. Universal\nguidance for diffusion models. In CVPR, 2023.\nBedapudi, P. Nudenet: Neural nets for nudity classification,\ndetection and selective censoring, 2019.\nBrack, M., Schramowski, P., Friedrich, F., Hintersdorf, D.,\nand Kersting, K. The stable artist: Steering semantics in\ndiffusion latent space. arXiv preprint arXiv:2212.06013,\n2022.\nBrack, M., Friedrich, F., Hintersdorf, D., Struppek, L.,\nSchramowski, P., and Kersting, K. Sega: Instructing\ntext-to-image models using semantic guidance. NeurIPS,\n2023.\nBricken, T., Templeton, A., Batson, J., Chen, B., Jermyn,\nA., Conerly, T., Turner, N., Anil, C., Denison, C.,\nAskell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer,\nN., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin,\nA., Nguyen, K., McLean, B., Burke, J. E., Hume,\nT., Carter, S., Henighan, T., and Olah, C.\nTowards\nmonosemanticity: Decomposing language models with\ndictionary learning.\nTransformer Circuits Thread,\n2023. https://transformer-circuits.pub/\n2023/monosemantic-features/index.html.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\nLearning to follow image editing instructions. In CVPR,\n2023.\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible scaling laws for contrastive\nlanguage-image learning. In CVPR, 2023.\nChin, Z.-Y., Jiang, C.-M., Huang, C.-C., Chen, P.-Y., and\nChiu, W.-C. Prompting4debugging: Red-teaming text-to-\nimage diffusion models by finding problematic prompts.\narXiv preprint arXiv:2309.06135, 2023.\nCunningham, H., Ewart, A., Riggs, L., Huben, R., and\nSharkey, L.\nSparse autoencoders find highly inter-\npretable features in language models. arXiv preprint\narXiv:2309.08600, 2023.\nDaujotas, G. Interpreting and steering features in images.\nLessWrong, 2024. https://www.lesswrong.com/\nposts/Quqekpvx8BGMMcaem/interpreting-\nand-steering-features-in-images.\nEsposito, P., Atighehchian, P., Germanidis, A., and Ghadi-\nyaram, D. Mitigating stereotypical biases in text to image\ngenerative systems. arXiv preprint arXiv:2310.06904,\n2023.\nFry,\nH.\nTowards\nmultimodal\ninterpretabil-\nity:\nLearning\nsparse\ninterpretable\nfeatures\nin\nvision\ntransformers.\nLessWrong,\n2024.\nhttps://www.lesswrong.com/posts/\nbCtbuWraqYTDtuARg/towards-multimodal-\ninterpretability-learning-sparse.\nGandikota, R., Materzynska, J., Fiotto-Kaufman, J., and\nBau, D. Erasing concepts from diffusion models. In\nICCV, 2023.\nGandikota, R., Orgad, H., Belinkov, Y., Materzy´nska, J.,\nand Bau, D. Unified concept editing in diffusion models.\n2024.\nGandikota, R., Materzy´nska, J., Zhou, T., Torralba, A., and\nBau, D. Concept sliders: Lora adaptors for precise control\nin diffusion models. In ECCV, 2025.\nGao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R.,\nRadford, A., Sutskever, I., Leike, J., and Wu, J. Scal-\ning and evaluating sparse autoencoders. arXiv preprint\narXiv:2406.04093, 2024.\nGong, C., Chen, K., Wei, Z., Chen, J., and Jiang, Y.-G.\nReliable and efficient concept erasure of text-to-image\ndiffusion models. In ECCV, 2025.\nHeng, A. and Soh, H.\nSelective amnesia: A continual\nlearning approach to forgetting in deep generative models.\nNeurIPS, 36, 2024.\nHessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,\nY. Clipscore: A reference-free evaluation metric for im-\nage captioning. arXiv preprint arXiv:2104.08718, 2021.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. NeurIPS, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n9\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models. arXiv preprint arXiv:2106.09685,\n2021.\nJain, A., Kobayashi, Y., Shibuya, T., Takida, Y., Memon, N.,\nTogelius, J., and Mitsufuji, Y. Trasce: Trajectory steering\nfor concept erasure. arXiv preprint arXiv:2412.07658,\n2024.\nKazerouni, A., Aghdam, E. K., Heidari, M., Azad, R.,\nFayyaz, M., Hacihaliloglu, I., and Merhof, D. Diffusion\nmodels in medical imaging: A comprehensive survey.\nMedIA, 2023.\nKim, D., Thomas, X., and Ghadiyaram, D. Revelio: Inter-\npreting and leveraging semantic information in diffusion\nmodels. arXiv preprint arXiv:2411.16725, 2024.\nKingma, D. P. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\nKumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang,\nR., and Zhu, J.-Y. Ablating concepts in text-to-image\ndiffusion models. In ICCV, 2023.\nLabs, B. F.\nFlux.\nhttps://github.com/black-\nforest-labs/flux, 2023.\nLi, X., Yang, Y., Deng, J., Yan, C., Chen, Y., Ji, X.,\nand Xu, W. Safegen: Mitigating sexually explicit con-\ntent generation in text-to-image models. arXiv preprint\narXiv:2404.06666, 2024.\nLu, S., Wang, Z., Li, L., Liu, Y., and Kong, A. W.-K. Mace:\nMass concept erasure in diffusion models. In CVPR,\n2024.\nMakhzani, A. and Frey, B. K-sparse autoencoders. arXiv\npreprint arXiv:1312.5663, 2013.\nMazzone, M. and Elgammal, A. Art, creativity, and the\npotential of artificial intelligence. In Arts, 2019.\nNg, A. et al. Sparse autoencoder. CS294A Lecture notes,\n2011.\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In ICCV, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research, 2020.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022.\nRando, J., Paleka, D., Lindner, D., Heim, L., and Tram`er,\nF. Red-teaming the stable diffusion safety filter. arXiv\npreprint arXiv:2210.04610, 2022.\nRodriguez, P., Blaas, A., Klein, M., Zappella, L., Apos-\ntoloff, N., Cuturi, M., and Suau, X. Controlling language\nand diffusion models by transporting activations. arXiv\npreprint arXiv:2410.23054, 2024.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In CVPR, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\nsion models with deep language understanding. NeurIPS,\n2022.\nSauer, A., Lorenz, D., Blattmann, A., and Rombach, R.\nAdversarial diffusion distillation. In ECCV, 2025.\nSchramowski, P., Tauchmann, C., and Kersting, K. Can\nmachines help us answering question 16 in datasheets,\nand in turn reflecting on inappropriate content? In FAccT,\n2022.\nSchramowski, P., Brack, M., Deiseroth, B., and Kersting, K.\nSafe latent diffusion: Mitigating inappropriate degenera-\ntion in diffusion models. In CVPR, 2023.\nSharkey,\nL.,\nBraun,\nD.,\nand Millidge,\nB.\nTak-\ning\nfeatures\nout\nof\nsuperposition\nwith\nsparse\nautoencoders.\nAI\nAlignment\nForum,\n2023.\nhttps://www.alignmentforum.org/posts/\nz6QQJbtpkEAX3Aojj/interim-research-\nreport-taking-features-out-of-\nsuperposition.\nSinghal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McK-\neown, K., and Ranganath, R. A general framework for\ninference-time scaling and steering of diffusion models.\narXiv preprint arXiv:2501.06848, 2025.\nSridhar, D. and Vasconcelos, N. Prompt sliders for fine-\ngrained control, editing and erasing of concepts in diffu-\nsion models. arXiv preprint arXiv:2409.16535, 2024.\nStracke, N., Baumann, S. A., Susskind, J., Bautista, M. A.,\nand Ommer, B. Ctrloralter: Conditional loradapter for\nefficient 0-shot control and altering of t2i models. In\nECCV, 2025.\n10\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nSurkov, V., Wendler, C., Terekhov, M., Deschenaux, J.,\nWest, R., and Gulcehre, C. Unpacking sdxl turbo: Inter-\npreting text-to-image models with sparse autoencoders.\narXiv preprint arXiv:2410.22366, 2024.\nTibshirani, R. Regression shrinkage and selection via the\nlasso. Journal of the Royal Statistical Society Series B:\nStatistical Methodology, 1996.\nToker, M., Orgad, H., Ventura, M., Arad, D., and Belinkov,\nY. Diffusion lens: Interpreting text encoders in text-to-\nimage pipelines. arXiv preprint arXiv:2403.05846, 2024.\nTsai, Y.-L., Hsu, C.-Y., Xie, C., Lin, C.-H., Chen, J.-Y., Li,\nB., Chen, P.-Y., Yu, C.-M., and Huang, C.-Y. Ring-a-bell!\nhow reliable are concept removal methods for diffusion\nmodels? arXiv preprint arXiv:2310.10012, 2023.\nUehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine,\nS., and Biancalani, T. Reward-guided controlled gener-\nation for inference-time alignment in diffusion models:\nTutorial and review. arXiv preprint arXiv:2501.09685,\n2025.\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\nN. Diffusion model alignment using direct preference\noptimization. In CVPR, 2024.\nWu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human\npreference score: Better aligning text-to-image models\nwith human preference. In ICCV, 2023.\nYan, H., Xiang, Y., Chen, G., Wang, Y., Gui, L., and He, Y.\nEncourage or inhibit monosemanticity? revisit monose-\nmanticity from a feature decorrelation perspective. arXiv\npreprint arXiv:2406.17969, 2024.\nYang, Y., Gao, R., Wang, X., Ho, T.-Y., Xu, N., and Xu, Q.\nMma-diffusion: Multimodal attack on diffusion models.\nIn CVPR, 2024.\nYoon, J., Yu, S., Patil, V., Yao, H., and Bansal, M. Safree:\nTraining-free and adaptive guard for safe text-to-image\nand video generation. arXiv preprint arXiv:2410.12761,\n2024.\nZhang, G., Wang, K., Xu, X., Wang, Z., and Shi, H. Forget-\nme-not: Learning to forget in text-to-image diffusion\nmodels. In CVPR, 2024.\nZhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J.,\nDing, K., and Liu, S. To generate or not? safety-driven un-\nlearned diffusion models are still easy to generate unsafe\nimages... for now. In ECCV, 2025.\n11\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nAppendix\nA. Implementation details\nTraining k-SAE with FLUX: For FLUX.1-dev (Labs, 2023) visualization, we train k-SAE using features extracted from\nthe residual stream of the 23rd (out of 24) layer of the T5-XXL text encoder on prompts from the I2P dataset. The k-SAE is\ntrained with k = 64 and an expansion factor of 16, resulting in a total hidden size dimension n = 65536.\nText encoders of diffusion models: We extract text embeddings for k-SAE from CLIP ViT-L/14 (Radford et al., 2021) for\nSD 1.4, OpenCLIP-ViT/G (Cherti et al., 2023) and CLIP-ViT/L for SDXL-Turbo, and T5-XXL (Raffel et al., 2020) for\nFLUX.1-dev.\nB. More details of the benchmarks\nWe evaluate our method for unsafe concept removal tasks on five publicly available inappropriate or adversarial prompts\ndatasets following prior work (Jain et al., 2024): I2P3 (Schramowski et al., 2023), Ring-A-Bell4 (Tsai et al., 2023),\nP4D5 (Chin et al., 2023), MMA-Diffusion6 (Yang et al., 2024), and UnlearnDiffAtk7 (Zhang et al., 2025). I2P contains 4703\nreal user prompts that are likely to produce inappropriate images. Ring-A-Bell consists of two inappropriate categories:\nnudity and violence. For nudity, it contains 95 unsafe prompts for each split (K77, K38, and K16). For violence, we use the\nRing-A-Bell Union dataset, which includes 750 prompts. P4D contains 151 unsafe prompts generated by white-box attacks\non the ESD (Gandikota et al., 2023) and SLD (Schramowski et al., 2023). MMA-Diffusion contains 1000 strong adversarial\nprompts generated via a black-box attack. UnlearnDiffAtk contains 142 adversarial prompts generated using white-box\nadversarial attacks.\nC. Additional qualitative results\nIn this section, we provide additional qualitative results.\nSteering nudity concept on inappropriate dataset: Figure 12 presents additional qualitative results using FLUX on\nprompts from I2P dataset. Our method effectively removes the abstract concept of nudity in DiT-based FLUX in an\nout-of-the-box manner.\nSteering nudity concept on adversarial dataset: Figure 13 presents qualitative comparisons with different methods on\nthe P4D dataset. Since P4D contains adversarial prompts specifically designed to challenge generative models, previous\nmethods either fail by generating unsafe images or produce unrelated images as a defense mechanism when the prompt\ntriggers to generate unsafe content (middle row). In contrast, our method successfully removes nudity while preserving the\noverall structure and maintaining alignment with the input prompt, even when the prompt itself is nonsensical (first and last\nrow).\nSteering violent concept: Figure 14 presents qualitative examples on the Ring-A-Bell dataset for violent concept removal.\nOur method effectively removes the abstract concept of violence by eliminating visual cues such as blood and firearms.\nSteering photographic styles: Figure 15 presents qualitative examples of photographic style manipulations in SD 1.4,\nincluding “HDR,” “Black and White,” “Sepia Tone,” and “Astrophotography.” We note that as λ →0.5, the generated image\ngradually transitions to the desired concept.\nSteering object attributes: Figure 16 presents qualitative examples of object attributes manipulations in SDXL-Turbo.\nGiven a prompt, we inject a concept for an object present in the image, such as “an orange cake” and “a chocolate cake.” We\nnote that the resulting generations preserve most of the original content while successfully injecting the desired concept.\n3https://huggingface.co/datasets/AIML-TUDA/i2p\n4https://huggingface.co/datasets/Chia15/RingABell-Nudity\n5https://huggingface.co/datasets/joycenerd/p4d\n6https://huggingface.co/datasets/YijunYang280/MMA-Diffusion-NSFW-adv-prompts-benchmark\n7https://github.com/OPTML-Group/Diffusion-MU-Attack/blob/main/prompts/nudity.csv\n12\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nPrompt: “sensual female painting, yoga, pancakes, fire smouldering, tattoos, midnight, dark … ”\nPrompt: “excellent framing lighting texture … sunlight illuminates the body as … lovely …”\nPrompt: “… photorealistic portrait of a woman's pale fat belly by sarah moon …”\nFigure 12. Qualitative example from the I2P dataset with FLUX. Our method is model-agnostic and can be applied to both U-Net-based\nSD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.\n13\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nSD 1.4\nOurs\nTraSCE\nSAFREE\nRemove \nnudity\nPrompt: “… waterhouse motive waterhouse venus … madonna ngmale”\nPrompt: “… girl alier roman renaissance nude paintings”\nRemove \nnudity\nRemove \nnudity\nPrompt: “phyllis nude bath ccp bathing steamy … painting …”\nFigure 13. Qualitative comparisons of different methods, including TraSCE and SAFREE, on the P4D dataset. The P4D dataset\nconsists of adversarial prompts designed to challenge generative models. Our approach effectively removes the concept of nudity during\nthe generation process, producing safe and semantically meaningful outputs. In contrast, SAFREE fails to generate safe images, while\nTraSCE sometimes produces unrelated outputs despite the presence of semantically meaningful keywords in given prompts, such as “girl,”\n“roman,” “renaissance,” and “paintings” (middle row).\nSD 1.4\nOurs\nRemove \nviolence\nRemove \nviolence\nRemove \nviolence\nRemove \nviolence\nSD 1.4\nOurs\nFigure 14. Qualitative examples from the Ring-A-Bell dataset. Our method successfully removes the abstract concept of violence, as\nshown by the absence of blood in the right images. The images are intentionally blurred for display purposes as they are disturbing.\n14\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nPrompt: “geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k”\nC=“HDR”\nC=“Black \nand white”\nC=“Sepia \nTone”\nC=“Astro-\nphotography”\nFigure 15. Photographic style manipulation of SD 1.4 for the given prompt “geodesic landscape, john chamberlain, christopher balaskas,\ntadao ando, 4 k, ” where concept prompts are “HDR,” “Black and white,” “Sepia Tone,” and “Astrophotography,” respectively. As\nλ →0.5, the generated image gradually transitions to the desired concept.\nPrompt: “A photo of a cake, 4 k ”\nC=“A choco-\nlate cake”\nC=“A white \ncake”\nC=“A lemon \ncake”\nC=“An orange \ncake”\nFigure 16. Object attribute manipulation of SDXL-Turbo for the given prompts “A photo of a cake, 4k,” where the concept prompts\nare “A chocolate cake,” “A white cake,” “A lemon cake,” and “An orange cake,” respectively. By adjusting λ, our method transitions the\nimage toward the desired concept specified by the prompts.\n15"),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content='Text-to-CAD Generation Through Infusing Visual Feedback in\nLarge Language Models\nRuiyu Wang 1 † Yu Yuan 2 † Shizhao Sun 3 Jiang Bian 3\nAbstract\nCreating Computer-Aided Design (CAD) models\nrequires significant expertise and effort. Text-to-\nCAD, which converts textual descriptions into\nCAD parametric sequences, is crucial in stream-\nlining this process. Recent studies have utilized\nground-truth parametric sequences, known as se-\nquential signals, as supervision to achieve this\ngoal. However, CAD models are inherently mul-\ntimodal, comprising parametric sequences and\ncorresponding rendered visual objects. Besides,\nthe rendering process from parametric sequences\nto visual objects is many-to-one. Therefore, both\nsequential and visual signals are critical for effec-\ntive training. In this work, we introduce CAD-\nFusion, a framework that uses Large Language\nModels (LLMs) as the backbone and alternates be-\ntween two training stages: the sequential learning\n(SL) stage and the visual feedback (VF) stage. In\nthe SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of\nlogically coherent parametric sequences. In the\nVF stage, we reward parametric sequences that\nrender into visually preferred objects and penalize\nthose that do not, allowing LLMs to learn how ren-\ndered visual objects are perceived and evaluated.\nThese two stages alternate throughout the training,\nensuring balanced learning and preserving bene-\nfits of both signals. Experiments demonstrate that\nCADFusion significantly improves performance,\nboth qualitatively and quantitatively.\n1. Introduction\nComputer-Aided Design (CAD) is indispensable for 3D\ncreation across industrial sectors (Deng et al., 2023). It rep-\nresents 3D models through a sequence of operations known\n† Work done during the internship at Microsoft Research Asia.\n1University of Toronto 2University of Science and Technology of\nChina 3Microsoft Research Asia. Correspondence to: Shizhao Sun\n<shizsu@microsoft.com>.\nPreprint. Under review.\nas a parametric sequence, which combines lines, arcs, and\ncircles to create 2D sketches and then extrude them to form\n3D models. CAD models are inherently multimodal, as they\nare constructed using parametric sequences for precise edit-\ning and manufacturing, while also being rendered as visual\nobjects for practical use, referred to as multimodal charac-\nteristic (Figure 1(b)(c)). Moreover, the process of rendering\nparametric sequences into visual objects exhibits a many-\nto-one mapping, where different parametric sequences can\nresult in identical visual objects, referred to as many-to-one\nrendering characteristic (Figure 1(d)).\nCreating CAD models demands considerable expertise\nand numerous iterations, making it complex and time-\nconsuming. Text-to-CAD (Figure 1(a)(b)), which refers to\nthe automatic generation of parametric sequences from tex-\ntual descriptions, is critical for streamlining this creation pro-\ncess. It allows designers and engineers to quickly prototype\nand iterate designs by describing their intent in natural lan-\nguage, reducing the time spent on manually creating CAD\nmodels from scratch. Additionally, it makes the creation\nprocess more accessible to individuals without extensive\ntraining, enabling wider participation.\nWhile important, Text-to-CAD has received limited atten-\ntion. Most studies do not utilize text to control CAD gener-\nation. Instead, they explore generating CAD designs from\nrandom noise (Wu et al., 2021; Xu et al., 2022), by randomly\naltering components of existing CAD designs (Xu et al.,\n2022; 2023; Zhang et al., 2024b), or from point cloud (Khan\net al., 2024a). A few studies make preliminary attempts at\nText-to-CAD (Khan et al., 2024b; Li et al., 2024b). They\ntrain Transformer-based framework with ground-truth para-\nmetric sequences as supervision, termed sequential signal.\nHowever, due to multimodal and many-to-one rendering\ncharacteristic of CAD models (Figure 1(b)(c)(d)), both the\nsequential signal and visual signal are crucial for training\na Text-to-CAD model. The sequential signal, derived from\nground-truth parametric sequences, provides critical infor-\nmation about sequence structure and parametric operations.\nWithout it, learning to generate logically coherent paramet-\nric sequences becomes challenging, as there is no direct\nsupervision for sequence structure and parametric opera-\ntions. The visual signal, obtained from rendered visual\n1\narXiv:2501.19054v2  [cs.CV]  5 Feb 2025\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nl i ne, 10, 7 <cur ve_end> l i ne, 52, 7 <cur ve_end> l i ne, 52, 55 <cur ve_end> l i ne, 10, 55 \n<cur ve_end> <l oop_end> l i ne, 12, 9<cur ve_end> l i ne, 50, 9 <cur ve_end> l i ne, 50, 53 \n<cur ve_end> l i ne, 12, 53 <cur ve_end> <l oop_end> l i ne, 11, 8 <cur ve_end> l i ne, 12, 8 \n<cur ve_end> l i ne, 12, 9 <cur ve_end> l i ne, 11, 9 <cur ve_end> <l oop_end> l i ne, 53, 8 \n<cur ve_end> l i ne, 54, 8 <cur ve_end> l i ne, 54, 9 <cur ve_end> l i ne, 53, 9 <cur ve_end> \n<l oop_end> <f ace_end> <sket ch_end> \nadd, 16, 31, 31, 31, 31, 1, 0, 0, 0, 0, 1, 0, - 1, 0, 29, 31, 48 <ext r ude_end>\nA rectangular prism with a \ntotal of five square holes. One \ncentrally located and four \nsurrounding it.\n(a) Input Prompt\n(b) CAD Design Sequence\n(c) CAD Visual Object\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nValid Design 3\nSame Design\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nValid Design 2\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end> \nValid Design 1\nGenerates \nRenders\n(d) The Many-to-one Relationship\nFigure 1. (a) and (b): Illustration of Text-to-CAD, which converts a texutal description into CAD parametric sequences. (b) and (c):\nIllustration of multimodal characteristics. CAD models are created using parametric sequences and rendered as visual objects for practical\nuse. (d): Illustration of many-to-one rendering characteristics. Different parametric sequences can produce identical visual objects.\nobjects, indicates how CAD models are perceived and evalu-\nated in practical applications. Without it, learning efficiency\nis compromised, as the goal of Text-to-CAD is for the ren-\ndered visual objects of the generated parametric sequences\nto match ground-truth visual objects. First, sequential signal\nlearning typically depends on auto-regressive generation,\nwhich emphasizes the local continuity between tokens but\nmay not fully capture the global appearance of the CAD\nmodel. Second, given the many-to-one rendering character-\nistic, multiple parametric sequences can produce the same\nvisual object. Training solely on parametric sequences may\ncause the model to give more emphasis to those present\nin the training set, overlooking other valid ones that could\nachieve the same visual outcome.\nTo this end, we propose CADFusion, a framework that\ncombines sequential and visual signals to train a Text-to-\nCAD model. It uses Large Language Models (LLMs) as its\nbackbone and alternates between two stages: the sequential\nlearning stage and the visual feedback stage. In the sequen-\ntial learning stage, LLMs are fine-tuned using ground-truth\nparametric sequences. Unlike prior works (Khan et al.,\n2024b; Li et al., 2024b) that train Transformer-based mod-\nels from scratch, we take advantage of pre-trained LLMs,\nwhich leverages their inherent natural language understand-\ning and foundational knowledge of CAD design (Makatura\net al., 2023) acquired during the extensive pre-training. In\nthe visual feedback stage, feedback derived from rendered\nvisual objects is integrated into the LLMs. This stage ad-\ndresses two critical challenges. First, the rendering process\nthat converts parametric sequences into visual objects is\nnon-differentiable, making backpropagation through this\npathway infeasible. To overcome this, we frame the problem\nas preference learning task and adopt direct preference opti-\nmization (DPO) (Rafailov et al., 2024). Specifically, pref-\nerences are assigned to the rendered visual objects, and the\nLLMs are optimized to increase the likelihood of parametric\nsequences that produce preferred visual objects while de-\ncreasing the likelihood of those that yield less preferred ones.\nThis approach enables effective training of LLMs, even with\na non-differentiable rendering pathway. Second, collecting\nreliable preference data is costly and labor-intensive. To\naddress this, we introduce an automated pipeline that uti-\nlizes large vision-language models (LVMs) to efficiently\nscore the rendered visual objects. Finally, to ensure bal-\nanced learning and retain the contributions of both signals,\nwe alternate between the sequential learning stage and the\nvisual feedback stage throughout training.\nWe summarize our main contributions as follows:\n• We propose to leverage both the sequential signal and\nvisual signal to train a Text-to-CAD model.\n• For the sequential signal, we use LLMs as the back-\nbone and fine-tune it on ground-truth parametric se-\nquences. For the visual signal, we adopt direct prefer-\nence optimization to bypass non-differentiable render-\ning and introduce a LVM-based scoring pipeline for\nefficient preference data collection. To balance both\nsignals, we alternate between the sequential learning\nand the visual feedback stage.\n• We contribute two datasets for Text-to-CAD: one with\nthe sequential signal and another with the visual signal.\n• We present qualitative and quantitative experiments to\nshowcase CADFusion’s superior ability.\n2. Related Works\nCAD Generation. CAD generation takes user requirements\nas input and generates CAD models as output.\nOn the input side, user requirements can be expressed in\ndiverse ways. Wu et al. (2021) uses random noise as input\nto generate CAD models randomly. Zhang et al. (2024b),\nXu et al. (2022) and Xu et al. (2023) modify specific parts\nof the existing CAD models to generate new ones. Khan\net al. (2024a) and Ma et al. (2024) take point cloud as input\nto produce corresponding CAD models. In contrast, our\nwork focuses on textual descriptions as input. Recent stud-\n2\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nies (Khan et al., 2024b; Li et al., 2024b) explore text-based\ninput for CAD generation. Khan et al. (2024b) proposes a\ndata annotation pipeline for synthesizing training data and a\ntransformer-based autoregressive network. Li et al. (2024b)\ndesigns an encoder-decoder framework with a cascading\ncontrastive strategy and CT-Mix to align text with paramet-\nric sequences. Unlike these studies, which rely solely on\nsequential signals, our work combines sequential and visual\nsignals for improved performance.\nOn the output side, CAD models can be represented in vari-\nous formats, including Constructive Solid Geometry (CSG),\nBoundary Representation (B-Rep) and Sketch-and-Extrude\n(SE). CSG constructs 3D models by combining basic primi-\ntives such as cubes, cylinders, and spheres, through Boolean\noperations and subtractions (Du et al., 2018; Kania et al.,\n2020; Yu et al., 2021; 2023). B-Rep represents 3D models\nusing geometric elements such as vertices, edges, and faces\n(Jayaraman et al., 2023; Wang et al., 2022; Xu et al., 2024).\nSE begins with 2D sketches composed of lines, arcs, and\ncircles, which are then extruded to form 3D models (Willis\net al., 2021; Wu et al., 2021). In this work, we adopt SE\nas it preserves the design history of CAD models, making\nthem more intuitive to edit.\nLarge Language Models (LLMs). LLMs have recently\nachieved remarkable success (Touvron et al., 2023; Brown\net al., 2020; OpenAI, 2024; Bubeck et al., 2023; Zhao et al.,\n2023). Supervised fine-tuning (SFT) is widely used to im-\nprove performance, while reinforcement learning (RL) is\noften employed to align LLM output with human prefer-\nence (Brown et al., 2020; Hong et al., 2024; Kaufmann\net al., 2024). Our work leverages SFT and RL1 but intro-\nduces two key differences. First, we utilize SFT and RL\nto learn from different signals (i.e., sequential and visual\nsignals) whereas existing work focuses on a single signal\n(i.e., sequential signals). Second, we alternate between SFT\nand RL stages to preserve contributions from both signals,\na strategy not commonly employed in prior work.\nReinforcement Learning with Human Feedback (RLHF).\nRLHF has been widely applied to align model output\nwith human preference across various domains, including\nLLMs (Brown et al., 2020; Radford et al., 2021; Meta, 2024),\ntext-to-image models (?) and text-to-video models (Wu\net al., 2024). As human annotation in RLHF is costly and\nnot easily scalable, reinforcement learning on AI feedback\n(RLAIF) (Liu et al., 2023; Zhang et al., 2024a; Lee et al.,\n2024), which leverages machine learning models to anno-\ntate data, has been proposed as a more affordable alternative\nto RLHF. Since RLHF/RLAIF pipeline are complex, di-\nrect preference optimization (DPO) (Rafailov et al., 2024),\n1We adopt DPO (Rafailov et al., 2024) in practice and refer\nto it as RL here for simplicity, as it implicitly optimizes the same\nobjective as traditional RLHF despite not being a typical RL.\nwhich directly optimize a model to adhere to human prefer-\nences, has been proposed to avoid explicit reward modeling\nor reinforcement learning. In this work, we adopt DPO to\naddress the challenge of non-differentiable rendering when\nlearning from visual signals, as it offers a simpler yet ef-\nfective solution compared to RLHF. Besides, inspired by\nRLAIF, we propose an automatic scoring pipeline for CAD\nmodels using LVMs. The generated scores are used to con-\nstruct preference data, enabling efficient learning without\nreliance on costly human annotations.\n3. Method\n3.1. Approach Overview\nLet a textual description be denoted as x, a CAD parametric\nsequence as y, and a rendered visual object as o. The render-\ning process from a parametric sequence y to a visual object\no is represented as r(·), such that o = r(y). Text-to-CAD\ninvolves learning a function f(·) that transforms the textual\ndescription x into the CAD parametric sequence y. i.e.,\ny = f(x). The goal is for the rendered visual object o of the\ngenerated parametric sequence y, i.e., o = r(y) = r(f(x)),\nto match the user’s desired visual object (Figure 1).\nCADFusion introduces a framework that combines sequen-\ntial and visual signal for training a Text-to-CAD model\n(Figure 2). It leverages Large Language Models (LLMs)\nas the backbone and alternates between two stages: the se-\nquential learning (SL) stage and the visual feedback (VF)\nstage. We denote the model after the i-th round of sequential\nlearning as f i\nSL(·) and after the i-th round of visual feed-\nback as f i\nVF(·). In the sequential learning stage, CADFusion\ntrains LLMs to learn sequence structures and parametric\noperations from ground-truth parametric sequences, guiding\nLLMs to generate logically coherent parametric sequences\n(Section 3.2). In the visual feedback stage, CADFusion\ntrains LLMs to understand how the rendered visual object\nwill be perceived and evaluated. By rewarding parametric\nsequences that render into visually preferred objects and\npenalizing those that do not, this stage encourages LLMs\nto generate parametric sequences capable of producing the\ndesired visual object (Section 3.3). These two stages are\nalternated throughout training, ensuring balanced learning\nand preserving contributions of both signals (Section 3.4).\n3.2. Sequential Learning Stage\nText-to-CAD requires a model capable of understanding tex-\ntual descriptions and generating CAD parametric sequences\nthat adhere to valid sequence formats and employ meaning-\nful parametric operations. We adopt the following strategies\nto efficiently achieve these capabilities.\n1) Model architecture. We use LLMs as the backbone, lever-\naging their strong natural language understanding and basic\n3\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n(c) The iterative training procedure\n(a) The Sequential Learning (SL)\n(b) The Visual Feedback (VF)\nPredicted CAD \nTokens\nGround truth \nSequence\nPreferred CAD\nProbability\nRejected CAD \nProbability\nPre-trained\nBackbone LLM\nInitial Sequential \nLearning\nPre-trained Backbone LLM\nText prompts\nCE Loss\nText prompts\nPreferred \nCAD object\nRejected \nCAD object\nDPO Loss\nPre-trained \nBackbone LLM\nVisual \nFeedback\nSequential\nLearning\nVisual \nFeedback\nSequential\nLearning\nN times\nOurs\nFigure 2. Overview of CADFusion. (a): The sequential learning stage trains LLMs using ground-truth CAD parametric sequences. (b):\nThe visual feedback stage rewards CAD parametric sequences that render into preferred visual objects and penalizes those that do not. (c):\nThe two stages are alternated to preserve contributions of both signals.\nCAD design knowledge (Makatura et al., 2023).\n2) CAD Parametric Sequence Format. We adopt the for-\nmat proposed by Zhang et al. (2024b) (Figure 1(b)), which\nrepresents CAD parametric sequences as text tokens rather\nthan binary representations or numerical attributes (Xu et al.,\n2022; 2023; Wu et al., 2021). This text-based format simpli-\nfies processing and interpretation by LLMs.\n3) Dataset.\nExisting CAD datasets (Wu et al., 2021)\ninclude CAD parametric sequences but lack paired tex-\ntual descriptions. To address this, we construct a dataset\nDSL = {(x, y)}M\n1 (‘SL’ for sequential learning) containing\npaired text x and CAD parametric sequences y. We first\nprompt a LVM to generate draft captions for rendered CAD\nmodel images and then refine these drafts through human\nannotation to ensure accuracy and conciseness.\n4) Training. We fine-tune the pre-trained LLMs by mini-\nmizing the discrepancy between the generated parametric\nsequence ˆy = f i\nSL(x) and the ground-truth parametric se-\nquence y using cross entropy loss, denoted as LSL:\nLSL = −E(x,y)∼DSL\n"\n1\nT\nT\nX\nt=1\nlog p(ˆy = yt|x)\n#\n,\n(1)\nwhere T is the sequence length and p(·) is the predicted\nprobability of the t-th token by the model f i\nSL(x).\nWhile existing studies (Khan et al., 2024b; Li et al., 2024b)\nalso consider sequential signals, CADFusion introduces\nthree distinctions: 1) it uses an LLM backbone to leverage\npre-trained knowledge, unlike prior work that trains Trans-\nformers from scratch; 2) it represents CAD sequences as\ntext tokens, processed with the LLM’s tokenizer, whereas\nothers use custom tokenizers; 3) its training data undergoes\nhuman annotation, while prior work relies solely on syn-\nthesized data. These enhancements enable it to outperform\nexisting approaches, even without the visual feedback stage.\n3.3. Visual Feedback Stage\nThe goal of Text-to-CAD is to ensure the rendered visual\nobject from the generated parametric sequence matches the\ndesired visual object. Relying solely on sequential signals\ncompromises training efficiency (see Section 1). To address\nthis, we incorporate visual feedback into the model already\ntrained on sequential signals (i.e., f i\nSL(x)).\nLearning Visual Feedback through DPO. A straightfor-\nward way to incorporate visual feedback is through super-\nvised learning, which minimizes the loss between the ren-\ndered visual object from the generated parametric sequence\nˆo = r(f(x)), and the ground-truth visual object o. How-\never, since the rendering process r(·) is non-differentiable,\nthis loss cannot be backpropagated to the model f(·). To\naddress this, we reformulate the task as a reward maxi-\nmization problem, where visual feedback serves as the re-\nward, enabling optimization without requiring a differen-\ntiable rendering process. Since conventional RL is computa-\ntionally expensive, we adopt direct preference optimization\n(DPO) (Rafailov et al., 2024), a simpler and more efficient\napproach that implicitly performs reward maximization.\nSpecifically, we construct a preference dataset DVF =\n{(x, ow, ol)}N\n1 where ow and ol are rendered from the para-\nmetric sequences yw and yl, representing preferred and less\npreferred visual objects, respectively. We then optimize the\nmodel to increase the likelihood of parametric sequences\nthat produce preferred visual objects (yw), while decreasing\nthe likelihood of those that yield less preferred ones (yl):\nLVF = −E(x,yw,yl)∼DVF\n(2)\n\x14\nlogσ(β log p(ˆy = yw|x)\npref(ˆy = yw|x) −β log p(ˆy = yl|x)\npref(ˆy = yl|x))\n\x15\n,\n4\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n(a) Sample sequence responses & Render Images\nBackbone after \nSequential \nLearining\n(b) Assigning visual scores to CAD objects\n(c) Form preference data \n      from eligible pairs\nInput \nCommand\n<seq><seq>\n<seq><seq>\nLVM\nMulti-Aspect \nGrading Criteria\n10/ 10\n7/ 10\n3/ 10\n9/ 10\n, ...\nCAD\nKernel\n10/10\n10/10\n7/10\n7/10\n9/10\n9/10\nFigure 3. Illustration of preference data construction. (a): Sample CAD parametric sequences and render them into visual objects. (b):\nScore the visual objects using LVMs with multi-aspect grading criteria. (c): Construct preference data based on LVM-generated scores.\nwhere p(·) is the predicted probability of a parametric se-\nquence under the current model (f i\nVF(x)), pref(·) the prob-\nability under the reference model from the last round of\nsequential learning (f i\nSL(x)) and β is scaling factor.\nConstructing Preference Data with LVM Scoring. Col-\nlecting preference data is both costly and labor-intensive.\nThe iterative use of the visual feedback stage in our frame-\nwork (Section 3.4) further highlights the need for a quick\nand efficient approach for obtaining preference data. To\naddress this, we propose leveraging the strong visual under-\nstanding capabilities of LVMs to score visual objects and\nconstruct preference data. Figure 3 outlines the pipeline.\nFirst, the textual description x is input into the finetuned\nmodel after sequential learning (f i\nSL) to generate multiple\nparametric sequences, which are then rendered into visual\nobjects (e.g., CAD images in our implementation). Next,\nthe rendered CAD images, along with an instruction detail-\ning the evaluation criteria, are input into an LVM to obtain\nscores. Finally, the CAD image with the higher score is\nregarded as the preferred one (i.e., ow), while the one with\nthe lower score is deemed as the less preferred one (i.e., ol).\nSpecifically, inspired by recent work (Liang et al., 2024)\non evaluating text-to-image generation across rich aspects,\nwe incorporate multiple evaluation criteria into the LVM\ninstruction. As shown in Figure 4, these criteria assess\nboth the appearance of CAD designs and their alignment\nwith textual descriptions: 1) shape quality evaluates the\nregularity, naturalness, and realism of the design; 2) shape\nquantity checks whether the number of components matches\nthe description; and 3) distribution ensures components are\narranged naturally, avoiding collisions or excessive spacing.\n3.4. Alternate Training\nEach stage of the training process — sequential learning and\nvisual feedback — has a specialized focus. Excessive train-\ning in one stage can lead to the degradation of skills acquired\nin the other. For example, we empirically observe that ex-\ntended training with visual feedback can impair the model’s\nability to generate well-formatted parametric sequences,\na skill developed during sequential learning. Conversely,\nprolonged training with sequential signals can weaken the\nmodel’s capacity to produce parametric sequences that ren-\ni. Shape Quality\niii. Item Distribution\nii. Shape Quantity\n... with 4 \ncircular holes\nA Trianglular \nshape that ...\n... with 4 \ncircular holes\nFigure 4. An illustrative example of the multi-aspect evaluation\ncriteria used in LVM scoring. Note that the illustrations are simpli-\nfied to conceptually represent each criterion.\nder visually natural objects, a capability enhanced during\nthe visual feedback stage. To mitigate this, we introduce an\nalternate training strategy (Figure 2(c)). The process begins\nwith the sequential learning stage, ensuring the model ac-\nquires the ability to generate logically coherent parametric\nsequences. Subsequently, the training is divided into smaller\nblocks. Within each block, the model first learns from the\nvisual signal, followed by the sequential signal, balancing\nthe two objectives effectively.\n4. Experiments\n4.1. Setups\nDatasets.\nFor the dataset used in the sequential learn-\ning stage, we use DeepCAD dataset (Wu et al., 2021)\nas the source for CAD parametric sequences (specifically\nthe version processed by Xu et al. (2022)). We construct\na dataset compromising 20k pairs of textual instructions\nand CAD parametric sequence using the techniques intro-\nduced in Section 3.2 and Appendix A.3. For the prefer-\nence data used in the visual feedback stage, we employ\nllava-onevision-qwen2-7b (Li et al., 2024a) to\nconstruct it using the method introduced in Section 3.3. For\neach iteration of the visual feedback, we generate approxi-\nmately 1,500 preference pairs, by using 1,000 text prompts\nas input, sampling 5 times per prompt, and filtering out in-\nvalid or low-quality samples. For the test set, we construct\nit by splitting the dataset used in sequential learning into\ntrain, validation, and test sets with a 90:5:5 ratio.\nImplementation Details. LLaMA-3-8b-Instruct is\nused as the LLM backbone, with a maximum token length of\n1024. For efficient fine-tuning, we adopt Low-Rank Adapta-\n5\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\ntion (LoRA) (Hu et al., 2021) with hyperparameters r = 32\nand α = 32. The initial sequential learning stage lasts for 40\nepochs with a learning rate of 1 × 10−4, using the AdamW\noptimizer. Following this, we run 5 iterations of alternat-\ning visual feedback and sequential learning stages. The\nvisual feedback stage lasts for 5 epochs on the preference\ndata, while the sequential learning stage lasts for 1 epoch\nusing the same dataset as the initial sequential learning stage.\nTraining is conducted on four NVIDIA A6000-48GB SMX\nGPUs using PyTorch Distributed Data Parallel (DDP).\nBaselines. We consider two types of baselines. The first is\na specialized model for Text-to-CAD (Khan et al., 2024b;\nLi et al., 2024b). We use Khan et al. (2024b) as our base-\nline, as Li et al. (2024b) is not open-sourced and we were\nunable to reproduce it ourselves. The second baseline is a\ngeneral model that acquires some CAD knowledge during\npre-training. We use the most powerful model, GPT-4o,\nas our baseline. Specifically, we apply few-shot learning,\nproviding 8 examples as input for GPT-4o.\nMetrics. Our evaluation focuses on assessing the alignment\nof generated CAD models with input instructions and the\noverall quality of the generated CAD models. We employ\nthe metrics at both the sequential level and visual level. First,\nto evaluate the correspondence between the ground-truth\nand generated parametric sequences, we use F1 scores fol-\nlowing Khan et al. (2024b). Specifically, we compute F1\nscore for primitives (averaged over lines, arcs, and circles\nfor brevity) and extrusions, denoted as F1-Sketch and F1-\nExtrusion. Second, to assess the quality of the generated\nCAD models, we compare the ground-truth and generated\npoint clouds. We adopt Chamfer Distance (CD) from Khan\net al. (2024b) and additional metrics from Xu et al. (2022),\nincluding Coverage (COV), which quantifies the percent-\nage of real data covered by generated samples using CD;\nMinimum Matching Distance (MMD), which evaluates the\nclosest match between generated samples and real data; and\nJensen-Shannon Divergence (JSD), which measures distri-\nbution similarity. Additionally, we compute the Invalidity\nRatio (IR), which quantifies the percentage of generated\nparametric sequences that fail to render into valid visual\nobjects. Furthermore, we introduce an LVM-based met-\nric, denoted as LVM Score, to assess the visual correspon-\ndence between model predictions and input instructions. To\nthis end, we employ GPT-4o with a dedicated evaluation\nprompt. Further details are provided in Appendix C.1. Fi-\nnally, we conduct human assessments to rank generations\nfrom different baselines, denoted as Avg. Rank. Details on\nthis evaluation can be found in Appendix C.2.\n4.2. Main Results\nQuantitative Evaluation. Table 1 summarizes the quantita-\ntive results comparing CADFusion with baseline methods\n(see Appendix C.4 for more details). Compared to GPT-4o,\nCADFusion outperforms it across all metrics. This sug-\ngests that while the general model may have acquired some\nCAD knowledge during pre-training, explicitly optimiz-\ning for Text-to-CAD, as in our approach, is crucial for im-\nproving performance, Compared to Text2CAD, CADFusion\nachieves comparable or better performance on all metrics,\nwith particular strengths in metrics reflecting the visual qual-\nity such as LVM score and Avg. Rank. This highlights the\neffectiveness of incorporating visual signals in our approach,\nas opposed to Text2CAD, which relies solely on sequential\nsignals. This outcome also aligns with Khan et al. (2024b)’s\nlimitation statement that Text2CAD is limited to generating\nonly rectangular and cylindrical shapes. When faced with\ncomplex geometries, it struggles to perform effectively.\nQualitative Evaluation.\nFigure 5 compares the re-\nsults among the ground truth, our method, GPT-4o, and\nText2CAD on the test set. GPT-4o frequently fails to pro-\nduce renderable results across most test cases, which aligns\nwith its high invalidity ratio (IR) reported in Table 1. While\nit occasionally generates valid shapes, its outputs are of-\nten misaligned with the input prompts. Text2CAD generate\nwell-formed shapes without irregular edges or corners. How-\never, it often produces oversimplified shapes and, for more\ncomplex prompts, tends to generate multiple cubes or pan-\nels instead of accurately capturing the intended structure.\nThis aligns with its low invalidity ratio (IR) but poor visual\nscores. such as LVM score and Avg. Rank, in Table 1.\nCADFusion provides the most precise response to input in-\nstructions and achieves the highest similarity to the ground\ntruth. It successfully captures complex shapes, including\nrectangles, hexagons, and nested structures, such as a hexag-\nonal hole within a cylinder. Additionally, it exhibits a strong\nunderstanding of language cues, accurately interpreting nu-\nmerical and qualitative descriptors like “long" or “T-shape".\nAdditional qualitative results, as well as our model’s abil-\nity to generate multiple varied outputs, are presented and\ndiscussed in Appendix C.6 and C.7.\n4.3. Ablation Studies\nWe conduct ablation studies on the effectiveness of the vi-\nsual feedback stage, the impact of the alternate training, and\nthe choice between human and LVM annotation for data.\nVisual Feedback. To assess the importance of visual feed-\nback, we conduct an ablation study on CADFusion using\nonly sequential learning, denoted as CADFusionSL. The first\nrow of Table 2 presents its LVM score and invalidity ratio.\nCompared to our approach, denoted as CADFusionSL-VFSL(5)\nin Table 2, while CADFusionSL improves the invalidity ratio\nby 1.36%, it results in a significant decrease in the LVM\nscore. This underscores the crucial role of the visual feed-\nback stage: by leveraging visual preference data, our frame-\n6\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nF1↑\nCD↓\nCOV ↑\nMMD ↓\nJSD ↓\nIR ↓\nLVM Score ↑\nAvg. Rank ↓\nSketch\nExtrusion\nGPT-4o\n82.96\n85.72\n85.04\n72.40\n6.60\n37.93\n74.26\n5.13\n3.22\nText2CAD\n63.94\n92.13\n30.23\n-\n-\n-\n3.37\n2.01\n2.97\nCADFusion\n85.22\n92.79\n45.67\n90.40\n3.49\n17.11\n6.20\n8.96\n1.86\nTable 1. Quantative results - Test results on F1 scores including Sketch (primitive, averaged) and Extrusion, Chamfer Distance (CD),\nCoverage (COV), Minimum Matching Distance (MMD), Jensen-Shannon Divergence (JSD), Invalidity Ratio (IR), the LVM Score and\nthe average rank from human evaluation (Avg. Rank). An upward arrow (↑) indicates that higher values are better, while a downward\narrow (↓) signifies that lower values are preferred. Since Text2CAD does not release COV, MMD, and JSD, and we were unable to\ncompute them ourselves due to differences in setup, these values are unavailable.\n(10) "The shape is a hollow cylindrical band with a vertical sector \nremoved, resembling an incomplete ring."\n(4) "The 3D shape is a square hexagonal plate."\n(1) "The 3D shape is a cylinder and a hexagonal hole inside, which \nis smaller and makes the wall very thin."\n(2) "The 3D shape is a rectangular block with a semicylindrical \ncutout located at its center, forming a U-shaped channel."\n(5) "The 3D shape is a teardrop-like piece with two circular holes. \none large near the broader end and one small near the narrower end."\n(8) "The 3D shape is a trapezoid thin prism."\n(11) "The 3D shape is a hollow triangular prism. The walls are the \nsame and have a smaller thickness."\n(12) "The image shows two identical parallel long slim pipes."\n(9) "Three identical rectangular sheets placed vertically, arranged in \nparallel and evenly spaced."\n(6) "The shape is a cylinder with a square hole centered at the top, \nextending from the top to the bottom."\n(3) "The 3D shape is a hollow, semi-cylindrical structure cut \nlengthwise, resembling a half-pipe."\n(14) "The three-dimensional shape is a flattened cylinder."\n(15) "The 3D shape is a rectangular prism(cuboid)."\n(13) "The three-dimensional shape is an inverted T-shaped prism."\n(16) "The 3D shape is a combination of a rectangular prism base and a \nvertically oriented half-cylinder on top."\n(17) "A flat rectangular plate. All four corners are rounded and there is a \ncircular hole of the same diameter at each corner."\nPrompts\nGround\nTruth\nOurs\nGPT-4o\nText2CAD\n(7) "The  shape is composed of four vertical cylinders, roughly the \nsame size, unevenly distributed at the four corners."\n(20) "The 3D shape is a rectangular cuboid with rounded edges and \ncorners."\n(19) "The 3D shape is a hexagonal prism. The hollow center forms an open \nhexagonal cross-section."\n(18) " The 3D shape consists of a small thin rectangular prism in the middle \nof the right side of a rectangular prism."\nFigure 5. Qualitative results. The input prompt is shown at the top of each subsection. Images are arranged from left to right in the\nfollowing order: ground truth, CADFusion, GPT-4o, and Text2CAD. Outputs that cannot be rendered are marked with a red cross.\nCADFusion outperforms all baselines in understanding instructions and generating CAD objects that are both sequentially and visually\nhigh quality. GPT-4o frequently produces invalid samples and pays little attention to shape details. Text2CAD generates well-formed\nbasic shapes with a regular appearance but struggles to accurately follow input instructions and represent complex geometries.\nwork effectively enhances the visual quality of the generated\nCAD models. Additionally, CADFusionSL outperforms the\nbaseline method, Text2CAD, which also relies solely on se-\nquential signals. Note that this advantage is achieved using\n20k data, while Text2CAD uses 150k data. This demon-\nstrates the effectiveness of the techniques employed in our\nsequential learning stage, including leveraging LLMs as the\nbackbone, representing CAD parametric sequences as tex-\ntual tokens, and utilizing human annotations (Section 3.2).\nAlternate Training. In Section 3.4, we propose an alter-\nnate training strategy to retain the benefits of both sequen-\n7\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nLVM Score ↑\nIR ↓\nCADFusionSL\n7.69\n4.84\nCADFusionSLw/o HA\n6.56\n6.00\nCADFusionSL-VF\n5.94\n88.87\nCADFusionSL-VFRPO\n6.21\n3.46\nCADFusionSL-VFSL(1)w/ HA\n8.28\n17.03\nCADFusionSL-VFSL(1)\n8.76\n4.42\nCADFusionSL-VFSL(3)\n8.89\n4.21\nCADFusionSL-VFSL(5)\n8.96\n6.20\nTable 2. LVM scores and invalidity ratios across different CADFu-\nsion variants. The suffix SL indicates that the model is trained with\nthe initial Sequential Learning stage, while VF denotes the Visual\nFeedback stage without additional Sequential Learning. VFSL\nrepresents Visual Feedback with alternating Sequential Learning.\nThe tag w/ HA signifies that the data is preprocessed with human\nannotation, whereas w/o HA denotes the absence of human an-\nnotation. Numbers in parentheses indicate the number of VFSL\nrounds performed. RPO refers to the model using Regularized Pref-\nerence Optimization (RPO) (Liu et al., 2024) to stabilize DPO.\ntial learning and visual feedback stage. We compare this\napproach with three variations: 1) visual feedback only\n(CADFusionSL-VF), 2) visual feedback with an additional\nNegative Log Likelihood loss (CADFusionSL-VFRPO) to regu-\nlarize and stabilize DPO (Liu et al., 2024), and 3) iterative\nvisual-sequential training (our method).\nTable 2 presents the results, with our approach denoted\nas CADFusionSL-VFSL(5).\nThe high invalidity ratio of\nCADFusionSL-VF indicates that it struggles to generate ren-\nderable sequences, suggesting that extended training with\nvisual signals can impair the model’s ability to generate well-\nformatted parametric sequences. Besides, CADFusionSL-VF\nreceives a low rating from the LVM judge, revealing that\ntraining with visual feedback along provides limited bene-\nfit. Regarding CADFusionSL-VFRPO which incorporates the\nadditional loss, while it achieves low invalidity ratio, its\nvisual quality, as assessed by the LVM judge, is even lower\nthan the SL-only setup (i.e., CADFusionSL). This indicates\nthat it fails to effectively balance the contributions of both\nsequential signals and visual signals.\nWe also compare model variants that use different numbers\nof iterations of visual feedback and sequential learning. In\nTable 2, for each CADFusionSL-VFSL(*) variant, the number\nin parentheses indicates the number of alternative training\nrounds performed. The results for iterations 1, 3, and 5\nare reported, showing a gradual increase in LVM scores\nalong with a stable invalidity ratio. This further validates\nthe effectiveness of our approach.\nData Annotation. We examine the impact of our choice\nof data annotation. In the sequential learning stage, the\ndataset is constructed by first using LVMs to generate initial\ncaptions, followed by human annotators refining them. To\nevaluate the effect of this decision, we conduct an experi-\nment in which our method is trained on data without human\nannotation, denoted as CADFusionSLw/o HA. The second row\nof Table 2 presents the results. It shows worse LVM score\nand IR compared to the version using data with human anno-\ntations (CADFusionSL), highlighting the necessity of human\nannotation in the sequential learning stage.\nIn the visual feedback stage, LVMs are used to score\nCAD models and generate preference data. This design\nchoice is driven by the high cost of human annotation\nand the challenge of managing human annotators to en-\nsure consistent scoring. To evaluate the effect of this de-\ncision, we conduct an experiment where the visual feed-\nback stage of our method is trained on human-scored prefer-\nence pairs, denoted as CADFusionSL-VFSL(1)w/ HA. Compared\nto the LVM-scored version (i.e., CADFusionSL-VFSL(1)), it\nachieves a worse LVM score and IR. This aligns with our\nintuition that, while human annotation may be more ac-\ncurate, managing annotators for consistent scoring is diffi-\ncult. Furthermore, using LVM-scored preference data al-\nlows CADFusionSL-VFSL(1) to scale across more rounds of\nvisual feedback (e.g., CADFusionSL-VFSL(5)), leading to im-\nproved performance. Achieving this with human annotation\nwould be challenging and expensive.\n5. Limitation\nCADFusion’s results are overall promising. However, there\nare limitations that could be addressed in future work. First,\nmodern LVMs show a significant performance drop when\nhandling multiple images as input. Currently, we can only\nprovide LVM with a single-view image to ensure both ac-\ncurate image understanding and prompt following. This\nlimitation prevents us from achieving a more effective Vi-\nsual Feedback pipeline and evaluator. Second, CADFusion\nstruggles to generate very complex shapes that require spa-\ntial and commonsense reasoning, such as the shapes of\nletters and words (see Appendix C.8).\n6. Conclusion\nWe propose CADFusion for Text-to-CAD, the first approach\nto incorporate visual feedback from rendered CAD objects\ninto the training pipeline. CADFusion uses LLMs as back-\nbone and alternates between the sequential learning stage\nand the visual feedback stage. We conduct extensive exper-\niments to demonstrate the superiority of CADFusion and\nvalidate the effectiveness of the design choices. In the future,\nwe plan to further improve the preference data construction\npipeline to enhance performance, and collect more CAD\ndata with more complex geometric shapes to investigate\nCADFusion’s performance on more challenging cases.\n8\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nImpact Statement\nThis paper presents work aimed at improving Text-to-CAD\ngeneration through the use of LLM-based frameworks and\nthe incorporation of visual feedback. Our work has the\npotential to enhance the CAD design process, offering the\nbenefits of automation and efficiency while reducing re-\nliance on intensive training and specialized expertise. This\ncould make CAD design more accessible, particularly in\nindustries where skilled designers are in short supply or\nwhere rapid prototyping is essential.\nEthics Statement\nIn this work, we have invited crowd workers to give textual\ndescriptions to CAD models. We conducted this work in ac-\ncordance with ethical guidelines to ensure that participants\nwere treated fairly, respectfully, and safely throughout the\nprocess. We took steps to protect the privacy of crowd work-\ners by not collecting personally identifiable information.\nThe data annotated by the crowd workers was used only\nfor research purpose related to improving CAD generating\ntechniques.\nReferences\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., and\nAmodei, D. Language Models are Few-Shot Learners,\nJuly 2020. URL http://arxiv.org/abs/2005.\n14165. arXiv:2005.14165 [cs].\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lund-\nberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang,\nY. Sparks of Artificial General Intelligence: Early experi-\nments with GPT-4, April 2023. URL http://arxiv.\norg/abs/2303.12712. arXiv:2303.12712 [cs].\nDeng, Y., Chen, J., and Olechowski, A. What Sets Proficient\nand Expert Users Apart? Results of a Computer-Aided\nDesign Experiment. Journal of Mechanical Design, 146\n(1):011401, 10 2023. ISSN 1050-0472. doi: 10.1115/\n1.4063360. URL https://doi.org/10.1115/1.\n4063360.\nDu, T., Inala, J. P., Pu, Y., Spielberg, A., Schulz, A., Rus,\nD., Solar-Lezama, A., and Matusik, W. Inversecsg: au-\ntomatic conversion of 3d models to csg trees.\nACM\nTrans. Graph., 37(6), December 2018.\nISSN 0730-\n0301. doi: 10.1145/3272127.3275006. URL https:\n//doi.org/10.1145/3272127.3275006.\nHong, Z., Yuan, Z., Chen, H., Zhang, Q., Huang, F., and\nHuang, X. Knowledge-to-SQL: Enhancing SQL Genera-\ntion with Data Expert LLM, June 2024. URL http://\narxiv.org/abs/2402.11517. arXiv:2402.11517.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models, 2021. URL https://arxiv.\norg/abs/2106.09685.\nJayaraman, P. K., Lambourne, J. G., Desai, N., Willis, K.\nD. D., Sanghi, A., and Morris, N. J. W. Solidgen: An\nautoregressive model for direct b-rep synthesis, 2023.\nURL https://arxiv.org/abs/2203.13944.\nKania, K., Zi˛eba, M., and Kajdanowicz, T. Ucsg-net –\nunsupervised discovering of constructive solid geometry\ntree, 2020. URL https://arxiv.org/abs/2006.\n09102.\nKaufmann, T., Weng, P., Bengs, V., and Hüllermeier,\nE.\nA survey of reinforcement learning from human\nfeedback, 2024. URL https://arxiv.org/abs/\n2312.14925.\nKhan, M. S., Dupont, E., Ali, S. A., Cherenkova, K., Kacem,\nA., and Aouada, D. CAD-SIGNet: CAD Language Infer-\nence from Point Clouds using Layer-wise Sketch Instance\nGuided Attention, February 2024a.\nURL http://\narxiv.org/abs/2402.17678. arXiv:2402.17678\n[cs].\nKhan, M. S., Sinha, S., Sheikh, T. U., Stricker, D., Ali, S. A.,\nand Afzal, M. Z. Text2CAD: Generating Sequential CAD\nModels from Beginner-to-Expert Level Text Prompts,\nSeptember 2024b. URL http://arxiv.org/abs/\n2409.17106. arXiv:2409.17106 [cs].\nLee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J.,\nLu, K., Bishop, C., Hall, E., Carbune, V., Rastogi, A.,\nand Prakash, S. Rlaif vs. rlhf: Scaling reinforcement\nlearning from human feedback with ai feedback, 2024.\nURL https://arxiv.org/abs/2309.00267.\nLi, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H.,\nZhang, K., Zhang, P., Li, Y., Liu, Z., et al.\nLlava-\nonevision: Easy visual task transfer.\narXiv preprint\narXiv:2408.03326, 2024a.\nLi, X., Song, Y., Lou, Y., and Zhou, X. CAD translator: An\neffective drive for text to 3d parametric computer-aided\ndesign generative modeling. In ACM Multimedia 2024,\n2024b. URL https://openreview.net/forum?\nid=DN3722rnLd.\n9\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nLiang, Y., He, J., Li, G., Li, P., Klimovskiy, A., Car-\nolan, N., Sun, J., Pont-Tuset, J., Young, S., Yang, F.,\nKe, J., Dvijotham, K. D., Collins, K., Luo, Y., Li, Y.,\nKohlhoff, K. J., Ramachandran, D., and Navalpakkam,\nV. Rich Human Feedback for Text-to-Image Generation,\nApril 2024. URL http://arxiv.org/abs/2312.\n10240. arXiv:2312.10240.\nLiu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C.\nG-eval: Nlg evaluation using gpt-4 with better human\nalignment, 2023. URL https://arxiv.org/abs/\n2303.16634.\nLiu, Z., Lu, M., Zhang, S., Liu, B., Guo, H., Yang, Y.,\nBlanchet, J., and Wang, Z. Provably mitigating overopti-\nmization in rlhf: Your sft loss is implicitly an adversarial\nregularizer, 2024. URL https://arxiv.org/abs/\n2405.16436.\nMa, W., Chen, S., Lou, Y., Li, X., and Zhou, X. Draw\nstep by step: Reconstructing cad construction sequences\nfrom point clouds via multimodal diffusion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 27154–27163, June\n2024.\nMakatura, L., Foshey, M., Wang, B., HähnLein, F., Ma, P.,\nDeng, B., Tjandrasuwita, M., Spielberg, A., Owens, C. E.,\nChen, P. Y., et al. How can large language models help\nhumans in design and manufacturing? arXiv preprint\narXiv:2307.14377, 2023.\nMeta. The llama 3 herd of models, 2024. URL https:\n//arxiv.org/abs/2407.21783.\nOpenAI. Gpt-4 technical report, 2024. URL https://\narxiv.org/abs/2303.08774.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision, 2021.\nURL https://arxiv.org/abs/2103.00020.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model, 2024. URL\nhttps://arxiv.org/abs/2305.18290.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro,\nE., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and\nLample, G. Llama: Open and efficient foundation lan-\nguage models, 2023. URL https://arxiv.org/\nabs/2302.13971.\nWang, K., Zheng, J., and Zhou, Z. Neural face identification\nin a 2d wireframe projection of a manifold object, 2022.\nURL https://arxiv.org/abs/2203.04229.\nWillis, K. D. D., Jayaraman, P. K., Lambourne, J. G.,\nChu, H., and Pu, Y. Engineering sketch generation for\ncomputer-aided design, 2021. URL https://arxiv.\norg/abs/2104.09621.\nWu, R., Xiao, C., and Zheng, C. DeepCAD: A Deep Gener-\native Network for Computer-Aided Design Models, Au-\ngust 2021. URL http://arxiv.org/abs/2105.\n09492. arXiv:2105.09492 [cs].\nWu, X., Huang, S., Wang, G., Xiong, J., and Wei, F. Boost-\ning text-to-video generative model with MLLMs feed-\nback. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024. URL https:\n//openreview.net/forum?id=3ivnixHy16.\nXu, X., Willis, K. D. D., Lambourne, J. G., Cheng, C.-Y., Ja-\nyaraman, P. K., and Furukawa, Y. SkexGen: Autoregres-\nsive Generation of CAD Construction Sequences with\nDisentangled Codebooks, July 2022. URL http://\narxiv.org/abs/2207.04632. arXiv:2207.04632\n[cs].\nXu, X., Jayaraman, P. K., Lambourne, J. G., Willis,\nK. D. D., and Furukawa, Y.\nHierarchical Neu-\nral Coding for Controllable CAD Model Generation,\nJune 2023. URL http://arxiv.org/abs/2307.\n00149. arXiv:2307.00149 [cs].\nXu, X., Lambourne, J. G., Jayaraman, P. K., Wang, Z.,\nWillis, K. D. D., and Furukawa, Y. Brepgen: A b-rep\ngenerative diffusion model with structured latent geom-\netry, 2024. URL https://arxiv.org/abs/2401.\n15563.\nYu, F., Chen, Z., Li, M., Sanghi, A., Shayani, H., Mahdavi-\nAmiri, A., and Zhang, H. Capri-net: Learning compact\ncad shapes with adaptive primitive assembly, 2021. URL\nhttps://arxiv.org/abs/2104.05652.\nYu, F., Chen, Q., Tanveer, M., Amiri, A. M., and Zhang, H.\nD2csg: Unsupervised learning of compact csg trees with\ndual complements and dropouts, 2023. URL https:\n//arxiv.org/abs/2301.11497.\nZhang, J., Hou, Z., Lv, X., Cao, S., Hou, Z., Niu, Y.,\nHou, L., Dong, Y., Feng, L., and Li, J. Longreward:\nImproving long-context large language models with ai\nfeedback, 2024a. URL https://arxiv.org/abs/\n2410.21252.\nZhang, Z., Sun, S., Wang, W., Cai, D., and Bian, J. Flexcad:\nUnified and versatile controllable cad generation with\nfine-tuned large language models, 2024b. URL https:\n//arxiv.org/abs/2411.05823.\n10\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang,\nC., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang,\nX., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A Survey\nof Large Language Models, May 2023. URL http://\narxiv.org/abs/2303.18223. arXiv:2303.18223\n[cs].\n11\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nData Preprocessing\nSkexGen\nSynthesized \nText-to-CAD data\nl i ne, 1, 3 <cur ve_end> \nl i ne, 2, 5 <cur ve_end> \n<l oop_end> <f ace_end> \n<sket ch_end> \nadd, 32, 32, 32, 13, . . .  \n<ext r usi on_end>\nThe 3D shape is a right cylinder characterized by \ntwo parallel circular bases connected by a curved \nsurface. It has three surfaces in total: one curved \nlateral surface and two flat circular bases. The \nheight is the distance between the bases, and the \nradius is the distance from the center to the edge \nof the base.\nThe 3D shape is a right \ncylinder. The height is the \ndistance between the bases, \nand the radius is the distance \nfrom the center to the edge of \nthe base.\nVLM caption\nHuman \nrevision\nFlatten\nto string \n{\n\' name\' :  . . . ,\n\' l en_xy\' :  65,  \n\' l en_ext \' :  38,  \n\' l en_pi x\' :  65,  \n\' l en_cmd\' :  35,  \n\' num_se\' :  1,  \n\' se_xy\' :  [\n[\n[ 1,  3] ,\n[ 2,  5]\n]\n] ,  \n\' se_cmd\' :  [\n[ 1,  3,  2,  4,  . . . ]\n] ,  \n\' se_pi x\' :  [\n[ 1,  600,  380,  . . . ]\n] ,   \n\' se_ext \' :  [\n[ 32,  32,  32,  13,  . . . ] ,\n}\nFigure 6. An overview of the data preprocessing steps. The original dataset is transformed into captions that serve as textual inputs, while\nthe corresponding stringified CAD representations are used as ground truth references.\nA. Additional Dataset Construction Detail\nA.1. Converting Raw Data into Strings\nCADFusion’s String Format\nOur representation adopts the Sketch-and-Extrude Modeling (SEM) format, wherein a CAD\ninstance is composed of sketches and extrusions. Each sketch is structured into multiple faces, and each face comprises\nmultiple loops. Within each loop, geometric primitives such as lines, arcs, and circles are parameterized as follows:\n• Line: Represented by a line identifier and one coordinate.\n• Arc: Defined by an arc identifier and two coordinates.2\n• Circle: Represented by a circle identifier and four coordinates.\nEach extrusion is represented as a sequence formatted as BVVTTTRRRRRRRRRSOO, where the components are defined as\nfollows:\n• B: The boolean operation, selected from add, cut, intersect.\n• V: The displacements of the top and bottom planes from the reference plane.\n• T: The 3D translation vector.\n• R: The 3D rotation, represented as a quaternion or equivalent.\n• S: The scaling factor.\n• O: The center of scaling.\nConverting Source Data to CADFusion’s Format\nThe original representation is derived from the SkexGen dataset\n(Xu et al., 2022). Each CAD instance includes several components: sketch commands, sketch coordinates, and extrusion\ncommands, which are stored in the se_cmd, se_xy, and se_ext entries, respectively. The lengths of these entries\ncorrespond to the number of sketch-extrusion pairs within the complete CAD shape. To convert these entries into strings, we\niteratively describe the sketches and extrusions in our format, ensuring that the resulting sequence reflects the chronological\ndesign order of the CAD process.\n2While lines and arcs generally require 2 and 3 coordinates for representation, respectively, this work leverages a simplified\nrepresentation where the endpoints of lines and arcs are determined by the first point of the subsequent curve. If the loop is closed at the\ncurrent curve, its endpoint is determined by the first curve in the loop.\n12\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nCAD Representation in SEM format\n\' name\' :  \' 0000/ 00006625\' ,  \n\' l en_xy\' :  41,  \n\' l en_ext \' :  19,  \n\' l en_pi x\' :  41,  \n\' l en_cmd\' :  17,  \n\' num_se\' :  1,  \n\' se_xy\' :  [ ar r ay( [\n[ 13,  13] ,  [ 3,  3] ,  [ 57,  13] ,  [  3,   3] ,  [ 57,  57] ,  [  3,   3] ,  \n[ 13,  57] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 18,  22] ,  [ 18,  15] ,  [ 22,  18] ,  [ 15,  18] ,  [ 3,  3] ,  [  2,   2] ,\n[ 18,  55] ,  [ 18,  48] ,  [ 22,  52] ,  [ 15,  52] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 35,  48] ,  [ 35,  22] ,  [ 48,  35] ,  [ 22,  35] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 52,  22] ,  [ 52,  15] ,  [ 55,  18] ,  [ 48,  18] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 52,  55] ,  [ 52,  48] ,  [ 55,  52] ,  [ 48,  52] ,  [ 3,  3] ,  [  2,   2] ,  \n[  1,   1] ,  [  0,   0] ]\n) ] ,  \n\' se_cmd\' :  [ ar r ay(\n[ 3,  3,  3,  3,  2,  5,  2,  5,  2,  5,  2,  5,  2,  5,  2,  1,  0]\n) ] ,  \n\' se_pi x\' :  [ ar r ay(\n[ 589,  3,  633,  3,  3449,  3,  3405,  3,  2,  1170,  722,\n   \n 918,   911,  3,  2,  3282,  2834,  3094,  3087,  3,  2,  2851,\n       1187,  2032,  2006,  3,  2,  1204,  756,  951,  944,  3,  2,\n       3316,  2868,  3127,  3120,  3,  2,  1,  0]\n) ] ,  \n\' se_ext \' :  [ ar r ay(\n[ 22,  32,  32,  32,  32,  3,  2,  2,  2,  2,  3,  \n2,  1,  2,  1,  56,  28,  28,  0]\n) ] ,  \n\' ui d\' :  470\nFlattened CAD Sequence\nCOORD_PAD = 4\nEXT_PAD = 1\nR_PAD = 2\nline,9,9 <curve_end> line,53,9 <curve_end> line,53,53 \n<curve_end> line,9,53 <curve_end> <loop_end> \ncircle,14,18,14,11,18,14,11,14 <curve_end> <loop_end> \ncircle,14,51,14,44,18,48,11,48 <curve_end> <loop_end> \ncircle,31,44,31,18,44,31,18,31 <curve_end> <loop_end> \ncircle,48,18,48,11,51,14,44,14 <curve_end> <loop_end> \ncircle,48,51,48,44,51,48,44,48 <curve_end> <loop_end> \n<face_end> <sketch_end> \nadd,21,31,31,31,31,1,0,0,0,0,1,0,-1,0,55,27,27 \n<extrude_end>\nTextual Description Retrieved via LVM Captioning\nThe 3D shape is a square plate with a large central cylindrical hole \nand four smaller cylindrical holes near each corner. It has 6 faces, 12 \nedges, and 8 vertices. The plate is relatively thin compared to its side \nlength and is likely used for mounting or structural reinforcement, \nwith the central hole for a shaft or pipe and the corner holes for bolts \nor screws.\nTextual Description After Human \nAnnotation\nThe 3D shape is a square plate with a large \ncentral cylindrical through-hole and four \nsmaller cylindrical holes near each corner. \nThe plate is relatively thin compared to its \nside length and is likely used for mounting \nor structural reinforcement, with the central \nhole for a shaft or pipe and the corner holes \nfor bolts or screws.\nFigure 7. An overview of multiple CAD representations and their corresponding captions. Left: A CAD representation in the raw SEM\nformat alongside its stringified sequence, with values highlighted in different colors based on the padding used for decoding. Right:\nCaptions generated by the LVM and refined by human annotation. Phrases removed during human fine-tuning are marked in red, while\nthose added by humans are marked in green. All representations and captions correspond to the same CAD figure, which is displayed in\nthe bottom-right corner.\nIn iteration i, we select the i-th item from the se_xy, se_cmd, and se_ext entries. For each digit in the se_cmd array,\nwe perform operations based on the command value as follows:\n• Command value = 5: Create a circle; use the first 4 items in se_xy as XY coordinates and append the <curve_end>\ntoken. Skip 5 positions in the se_xy array.\n• Command value = 4: Create an arc; use the first 2 items in se_xy as XY coordinates and append the <curve_end>\ntoken. Skip 3 positions in the se_xy array.\n• Command value = 3: Create a line; use the first item in se_xy as an XY coordinate and append the <curve_end>\ntoken. Skip 2 positions in the se_xy array.\n• Command value = 2: Mark the end of the loop by appending the <loop_end> token. Skip 1 position in the se_xy\narray.\n• Command value = 1: Mark the end of the face by appending the <face_end> token. Skip 1 position in the se_xy\narray.\n• Command value = 0: Mark the end of the sketch by appending the <sketch_end> token. Skip 1 position in the\nse_xy array.\nExtrusions are represented by the 1D array se_ext. The operation identifier is translated into a word, and the remaining\nvalues are flattened. To distinguish coordinates from special tokens, all coordinates are initially padded; they are subsequently\nunpadded based on the original padding values. Figure 7 illustrates the conversion process from the SkexGen representation\nformat to our stringified sequence.\nA.2. Generating Textual Instructions\nTextual instructions are generated in two steps: first, by applying LVM captioning on single-view images of CAD models;\nsecond, through human refinement of the generated captions to ensure clarity and accuracy.\nGiven a sequence representation, the CAD instance is rendered into an image, and captions are generated using GPT-4o.\nThis step is designed to extract geometric properties, including the number of shapes, their dimensions, spatial arrangements,\nand other relevant details. The prompt used for this step is provided in Listing 1.\n13\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n1 {\n2\n"Prompt1": "Propose a series of questions about the 3D shape and give the answers. The\nfirst question should ask for a detailed description and others should focus on the\nspecific geometric properties, number, size proportions and positional relationship,\nand other details.",\n3\n"Prompt2": "Based on the dialogue, please give a final description of the 3D shape. No\nmore than 70 words."\n4 }\nListing 1. Prompts that are used for making captions. The first prompt is used to generate question-answer pairs, and the second prompt\ncollects and summarizes the informations in the first prompt to yield the final caption.\nThe LVM-generated captions are further refined by human annotators to produce fine-grained captions that can serve as\nprecise textual instructions. The human annotators follow these guidelines during the editing process:\n• Ensuring Correspondence: The description must accurately reflect the figure without any discrepancies.\n• Ensuring Succinctness: The description should be as concise as possible while maintaining clarity and completeness.\n• Permission for Removal: Figures that are excessively complex or challenging to describe may be excluded from the\ndataset. In practice, the annotators are permitted to mark the revised descriptions of such instances as "null".\nFigure 7 illustrates an example of how an image is captioned by the LVM and subsequently refined by human annotators.\nA.3. Dataset Construction\nThe dataset construction process is illustrated in Figure 6. Starting with a CAD representation from the original dataset, we\ngenerate a paired textual instruction and a stringified CAD representation. The textual instruction is created through the\ncaptioning process detailed in Section A.2, while the ground truth reference is obtained by converting the CAD formatting\nas described in Appendix A.1.\nB. Additional Training Detail\nB.1. Sequential Learning\nWe fine-tune a LLaMA-3-8b-Instruct by 40 epochs on 4 NVIDIA A6000-48GB SMX GPUs with a LoRA with rank\n32. Further details regarding the fine-tuning process are provided in the Experiment Section of the main paper. The specific\nprompt used for the learning is as follows:\n1\n"Below is a description of a 3D shape:\\n\n2\n{description}\\n\n3\nGenerate a Computer-Aided Design (CAD) command sequence of the 3D shape:\\n"\nListing 2. Prompt used for sequential learning. description refers to the actual textual commands of samples\nB.2. Visual Feedback\nThe visual feedback is collected as outlined in the main paper. The llava-onevision-qwen2-7b-ov-chat model\nis utilized to generate visual descriptions. For each input sequence produced by the post-SL CADFusion, the corresponding\nrendered figure is evaluated by the model, which assigns a score ranging from 0 to 10. The prompt used for this scoring\nprocess is detailed in Listing 3:\n1\n"You are a harsh grader for new CAD\ndesigners’ works. The following is a text\ndescription of a CAD figure that they designed and an image of a CAD instance. \\n\n2\nDescription: {description} \\n\n3\nComment on this work for \\n\n4\n1. If the overall shape remains correct; \\n\n5\n2. If the number of components are correct, especially the circular holes; \\n\n6\n3. If the distribution of the components are natural, i.e. they are not clustered\ntogether or collide with each other. \\n\n14\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n"The 3D panel is a thin, rectangular cuboid with protrusions on opposite \nvertical edges, designed for interlocking with other components. The \nprotrusions serve functional purposes like stability or assembly \nalignment. The proportional size and position of the protrusions suggest \nminimal structural impact while ensuring precise fit with other parts."\n"The 3D shape is a long and narrow L-shaped prism. The two arms are the \nsame length, and the length of the whole the length of the prism is \nsignificantly greater than the other sides. There is a small groove running \nthrough the surface at the left end."\n"The 3D shape is a rectangular prism with the same elongated trapezoidal cutout on each \nof its top and bottom surfaces, the two corners of the cut are rounded, the length of the cut \nis less than the length of the rectangular prism, the width is equal to the width of the \nrectangular prism, and the height is less than the height of the rectangular prism. The \nincisions are symmetrically distributed. The length of the whole is greater than the height, \nand the height is greater than the width."\n"The three-dimensional shape is a flat square plate with a cylindrical hole \nin the center. Due to its geometric configuration, it is commonly used as a \nwasher or spacer in mechanical assemblies."\n"The three-dimensional shape is a cylinder with five circular holes of \nequal spacing and diameter."\n"The 3D shape consists of 12 identical vertical cylinders arranged in a 3x4 \ngrid pattern to form a compact cluster. All cylinders are equal in height \nand diameter, creating a symmetrical, unified structure."\n"The 3D shape is a rectangular sheet with four corners that bulge outward \nto form rounded corners. There is a round hole of the same size near each \nof the four corners of the largest side of the sheet, and the four round holes \nare symmetrically distributed. The overall length and width are equal."\n"The 3D shape is a long, slender rectangular bar with rounded ends, \nfeaturing centrally located circular holes at each end. The bar is \nsignificantly longer than it is wide or thick, with the holes aligned along \nthe bar\'s central axis. This design likely serves attachment or rotational \npurposes, minimizing stress concentrations and enhancing durability."\n"The 3D shape is an elongated U-shaped rectangular bar. It has a vertically \noriented elongated structure with a height that is significantly greater than \nthe width and depth. This shape has symmetrical planes and stands \nupright, making it appear narrow and stable."\n"The 3D shape is a flat, rectangular plate with rounded corner, featuring a \nlarge central rectangular cutout and four evenly spaced circular holes near \neach corner. It is symmetrical and likely designed as a mounting plate or \nbracket, allowing for secure attachment via the holes and access through \nthe central cutout."\n"The three-dimensional shape is a semicircular ring segment, similar to a \n"C" shape. It forms an open arc. Its thickness varies at different locations \nand is mainly characterized by thick and thin thickness."\n"The 3D shape is an elongated rectangular prism with semi-circular ends, \nresembling a capsule. The shape exhibits bilateral symmetry lengthwise, \nwith the width of the rectangle equal to the diameter of the semi-circular \nends."\n"The 3D shape has a flat, rectangular base with rounded upper top corners, \na rectangular cutout in the bottom center, and a round hole near the top \ncenter."\n"The 3D shape is a cylindrical wheel with a solid outer ring and an internal \ncross structure dividing the circle into four hollow fan sections. The center \nof the cross structure has a square hole."\n"The 3D shape is a cylindrical disc with a large central hole and four \nsmaller holes symmetrically spaced at90-degree intervals around it, \nforming the corners of an inscribed square. The disc has a uniform \nthickness and likely serves as a mechanical flange or spacer, potentially \nmade from metal or durable plastic for structural applications."\n"The 3D shape is a rectangular frame with a thin and uniform thickness of \nthe walls of the frame, and the overall length is greater than the height and \nthe height is greater than the width."\n"The 3D shape consists of a large, flat rectangular slab with two evenly \nspaced, identical cylindrical protrusions extending vertically from its \nsurface. The slab provides a stable base with significant length and width \ncompared to its thin height, while the cylinders are relatively short and \nhave small diameters. The overall design is symmetrical and balanced, \npotentially serving as a mounting base or connector."\n"The 3D shape consists of three rectangular prisms forming an\\"H\\" \nshaped structure. A long horizontal prism vertically connects two identical \nvertical prisms at each end. It has bilateral symmetry. The vertical prisms \nare symmetrical and identical and act as legs for the horizontal bar."\n"The 3D shape is a rectangular prism. It has five evenly spaced circular \nholes that run vertically along the length of the prism,"\n"The 3D shape is a cylindrical disc with an equilateral trapezoidal cutout \nthat penetrates its entire thickness. This shape is bilaterally symmetrical, \nwith the cutout positioned to the left of the center of the cylinder\'s \nprincipal axis, creating a consistent and symmetrically balanced design."\n"The 3D shape is a cylindrical object with a central axial hole, mounted on \na larger thin circular base. The vertical cylinder and base are centered, \nproviding cylindrical symmetry."\n"The three-dimensional shape is a flattened cylindrical disk with two \nholes: a smaller hole in the center and a larger hole near the edge. The \nlarger hole has a significantly larger diameter, which may affect the \nstructural integrity of the area nearby."\n"The 3D shape is an elongated half-cylinder curved prism."\n"The 3D shape is a long cylindrical rod with a hexagonal prism(screw \nhead) at one end. The rod has a uniform diameter, while the hexagonal \nprism has six equal sides and serves as the head, positioned \nperpendicularly to the rod. This configuration resembles a typical bolt, \nwhere the rod serves as the threaded shaft and the prism as the \ntool-grippable head."\nFigure 8. Additional qualitative results, Part 1. The results are grouped by categories such as panels and circular objects. In each sub-figure,\nthe left image shows the figure rendered from the ground truth, while the right image displays the generation by CADFusion. The\ncorresponding textual instructions are provided at the bottom.\n7\nAfter that, give a score out of 10. Do not comment on issues such as texture,\nsmoothness and colors."\nListing 3. Prompt used by LLaVA-OV for scoring an input figure. description refers to the textual of the sample.\nThe DPO procedure is conducted on 4 NVIDIA A6000-48GB SMX GPUs with a LoRA with rank 32. The training involves\nfive iterative DPO/SFT rounds, which require approximately 2.5 days to complete.\nC. Additional Experimental Results\nC.1. LVM Evaluation Setups\nAs mentioned in the main paper, we used a GPT-4o model as the LVM evaluator. It is selected over LLaMA-ov because we\nattempt to prevent the impact of AI bias that makes it prefer its own generation 3. The prompt we used for LVM evaluation\nis detailed in Listing 4:\n3We acknowledge that this choice may introduce bias in the GPT-4o results. However, we have decided to proceed with it for two\nreasons: 1) our primary focus is on comparing Text2CAD with our model, and 2) the GPT-based generations during our experiment\nshowed a significant margin in LVM scores compared to other methods, so the impact of this bias is minimal.\n15\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nThe three-dimensional shape is an L-shaped prism consisting of a vertical \nrectangular prism and a horizontal rectangular prism with a rectangular cut \non the lower side of the vertical rectangular prism.\n"The 3D shape consists of a combination of a rectangular prism and a \ntriangular prism, and the shape resembles a house."\n"The 3D shape is a star prism characterized by asymmetrical star-shaped \ntop and bottom faces connected by rectangular side faces. The base faces \nof the star are parallel and congruent, and the heights connecting them are \nconstant."\n"The three-dimensional shape is a thin, flat, triangular plate with rounded \nedges and three circular holes near the apex with the same diameter."\n"Its shape is that of a stepped prism, consisting of three rectangular prisms \nof equal width and height stacked on top of each other. The length \ndecreases from bottom to top, with the left sides of the prisms aligned."\n"The image features a hexagonal prism with a central vertical hole, \nresembling a nut, beside a tall, slender cylindrical rod. The cylindrical rod \nhas two circular bases and one curved surface, standing vertically and \nsignificantly taller than the prism. They are placed apart, with the prism on \nthe left."\n"The 3D shape is a hexagonal prism with a central cylindrical cutout."\n"The image shows two identical three-dimensional cylinders that are not \nconnected. One on the top left and one on the bottom right."\n"The image shows four identical hollow cylindrical rings."\n"The 3D shape is a V-shaped sheet with convex fillets at the top and \nbottom, concave fillets on the inside where the two sides intersect, a round \nhole at the bottom of the side, and an identical smaller hole near the top of \neach side."\n"The 3D shape is characterised by a flat rectangular base and four identical \ncylindrical pillars, symmetrically distributed near each corner."\n"The 3D shape is a cuboid with a vertical quarter-cylinder cutout along \none edge. The cutout smoothly transitions between adjacent flat faces, \nintroducing asymmetry while maintaining original right angles elsewhere."\n"The 3D shape is a rectangular prism. In the middle of one of its largest \nsides, there is a small round hole and a small square cutout, the round hole \nis near the right side, and the square cut is near the left."\n"The image shows three identical rectangular cuboids. Three prisms are \narranged in parallel with equal spacing. All length is greater than their \nwidth and height."\n"The 3D shape consists of two identical vertical rectangular prisms and a \nhorizontally placed rectangular prism to form a "U" shape. Two \nrectangular prisms placed vertically have two identically symmetrical \nround holes each. The overall thickness is consistent, longer than taller."\n"This 3D shape is a rectangular prism, with its length greater than its \nwidth, and the width greater than its height. In the center of the bottom of \nthe larger side, there is a long trapezoidal cut, with the width of the lower \nend narrower than that of the upper end."\nFigure 9. Additional qualitative results, Part 2. The results are grouped by categories such as multiple distinct items and complex shapes.\nIn each sub-figure, the left image shows the figure rendered from the ground truth, while the right image displays the generation by\nCADFusion. The corresponding textual instructions are provided at the bottom.\n1 "\n2 The following is a text description of a 3D CAD figure and an image of a CAD instance.\nMeasure if the figure corresponds to the given description, and give a score in the\nscale of 10. Do not comment on issues such as texture, smoothness and colors \\n\ndescription: {description}\\n\n3 "\nListing 4. Prompt used by GPT-4o for evaluation.\nC.2. Human Evaluation Setups\nWe generate a quadruple of outputs for each test set instruction. Each quadruple presents four rendered generations from\nCADFusionSL, CADFusion, GPT-4o and Text2CAD, respectively. The generations are tested by their correspondence\nbetween CAD shapes and instructions. Six human judges are asked to rank the generations4 with the first place being the\nbest model, and one LVM is deployed to score single-view images to the scale of 10.\nC.3. GPT Baselines\nThe prompt we used for the GPT-4o baseline is detailed in Listing 5.\n1 "\n2\nBelow is a description of a 3D shape:\n3\n{description}\n4\nGenerate a Computer-Aided Design (CAD) command sequence of the 3D shape. The command\nsequence involves sketches such as lines, arcs, and circles, each marked by the\nendpoints, and extrusions that make the sketch into 3D volumes.\n5\n6\nHere are some examples:\n7\n1. <few shot example>\n4Due to the lack of overlapping, we obtain approximately 50 unique samples.\n16\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n8\n2. <few shot example>\n9\n3. <few shot example>\n10\n4. <few shot example>\n11\n5. <few shot example>\n12\n6. <few shot example>\n13\n7. <few shot example>\n14\n8. <few shot example>\n15\n16\nNow it’s your turn. Remind that this is your description: {description}. No\nexplanation is needed. Only return your final sequence, and in one line.\n17 "\nListing 5. Prompt used by GPT-4o for baseline comparison.\nAlongside the current 8-shot version, we also tested a 3-shot GPT-4o model to reduce computation costs. However, the\n3-shot model resulted in approximately a 92% invalidity ratio, and the 8% of renderable outputs were barely recognizable in\nrelation to the prompt. Given these issues, we have decided to use the 8-shot version as our baseline for GPT.\nC.4. Additional Statements on Text2CAD Results\nIn the quantitative experiments, our setups are not fully aligned with those of Text2CAD. This discrepancy arises because\nwhen we used our test set prompts as input, we observed a performance degradation and a significant gap between our\ncomputed results and those reported by the authors.\nWe discovered that the discrepancy stems from the model’s sensitivity to the level of detail in the prompt. Text2CAD\nperforms well only with expert-level prompts, which contain step-by-step sketching guidelines. Our prompts, however, do\nnot include this level of detail 5. To ensure consistency, in Table 1, we report Text2CAD’s performance based on their expert\nprompts when computing the metrics they introduced. Specifically, for each item in the test set, CADFusion and GPT-4o’s\npredictions were generated using our prompts, while Text2CAD’s predictions were generated using the expert prompt for\nthe same item from their prompt base.\nThis approach aligns the results we reproduced with the reported scores from the original paper. To present a comprehensive\nand accurate study, we also report Text2CAD’s results using our prompts and intermediate-level prompts in Table 3. The\nlast two rows, CADFusion and Text2CAD-our-prompt, are aligned as the same prompt is used.\nBy changing the prompt from the expert-level prompt in their database to an intermediate-level prompt, we observe a similar\nperformance drop. This indicates that our prompting method does not degrade Text2CAD’s performance. Instead, it is an\nlimitation stemmed from Text2CAD itself. Our model, using a simplified prompt, outperforms Text2CAD-expert. Given\nthat the expert-level prompt from Text2CAD is too long and too specific to be feasible in the real designing process, we\nbelieve that our quantitative advantage over it is significant.\nMethod\nF1↑\nCD↓\nLine\nArc\nCircle\nExtrusion\nText2CAD-intermediate\n66.65\n4.85\n47.62\n93.56\n146.15\nText2CAD-expert\n79.59\n42.79\n69.45\n92.13\n30.23\nText2CAD-our-prompt\n54.42\n0.92\n18.42\n75.37\n235.91\nCADFusion\n83.71\n81.99\n89.97\n92.79\n45.67\nTable 3. Results of our model and different Text2CAD prompts on metrics Proposed by Khan et al. (2024b). The suffix indicates the\nprompt type used for testing. Text2CAD-ours and CADFusion are the most aligned pairs, while Text2CAD-expert and CADFusion are\nthe ones reported in the main paper.\nC.5. Additional Quantative Results\nWe report additional quantitative results in this section.\n5We are concerned that the impact of detailed prompts containing step-by-step instructions and point coordinates is limited, as they\nmay not be feasible in real-life scenarios.\n17\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nLVM Score ↑\nIR ↓\nCADFusionSL\n7.69\n4.84\nCADFusionSLw/o HA∼18k\n6.56\n6.00\nCADFusionSLw/o HA∼170k\n6.60\n9.04\nTable 4. LVM scores and invalidity ratios across different CADFu-\nsion variants. All three models are trained using only the initial\nSequential Learning stage. The suffix w/o HA indicates that the\nvariant does not use human-annotated data, while the number de-\nnotes the size of the training set.\nMethod\nAvg. Rank ↓\nGPT-4o -8shot\n3.22\nText2CAD\n2.97\nCADFusion-SFT only\n2.03\nCADFusion\n1.86\nTable 5. Human Evaluation Results. Human annotators ranked\nthe generations of different methods based on their quality,\nwith a lower rank indicating higher human preference.\nOn Dataset Size. The dataset used in our experiments is a subset of SkexGen (Xu et al., 2022). Since human annotation is\nnot scalable, we evaluate the trade-off between scalability and data quality. One such evaluation, discussed in the Ablation\nStudy (Section 4.3), demonstrates that, given the same number of training samples, data quality outweighs dataset scalability\nin terms of model performance.\nAdditionally, we investigate whether increasing dataset size can mitigate quality limitations by conducting an experiment on\nthe full SkexGen-based Text-to-CAD dataset ( 170k samples). The results, presented in Table 4, indicate that increasing\ndataset size does not significantly improve the visual quality of model generations. While a slight performance gain is\nobserved with additional training samples, the improvement is marginal, and none of the w/o HA variants outperform the\nhuman-annotated counterpart.\nOn Human Evaluation. We conducted human evaluations across four models: GPT-4o, Text2CAD, CADFusion, and\nCADFusion trained only with the Sequential Learning stage. However, only the first three models are reported in Table 1.\nThe complete results of human evaluation are presented in Table 5. As indicated by the evaluation, the two CADFusion\nvariants are preferred over the baselines, with the version incorporating Visual Feedback receiving higher rankings from\nhuman judges. This highlights the effectiveness of visual feedback in improving model performance.\nC.6. Additional Qualitative Results\nIn this section, we present additional qualitative results. Figures 8 and 9 display these results, organized by CAD shape\nproperties such as panels and circular objects. These examples demonstrate that our model can efficiently handle a variety\nof CAD shapes with distinct instructions, such as holes and frames. Furthermore, the model performs well in generating\nmultiple identical objects, as shown in the first row of Figure 9, and can effectively generate more complex shapes, such as\nstars and V-shapes.\nC.7. Text to Multiple CAD Figures\nDuring inference, we set the temperature t = 0.3, top_p = 0.9, and top_k = 50 to enable non-deterministic generation.\nThis configuration allows us to produce varied CAD figures that meet the instructed requirements, with slight differences\nbetween them. As a result, users can select the design that best aligns with their specific needs. Examples of such outputs\nare shown in Figure 10. These results demonstrate that while adhering to the provided instructions, CADFusion is capable\nof generating diversified outputs. The variations primarily affect attributes such as thickness, width, and the size of holes\nand cutouts, while maintaining the overall shape. This flexibility offers users a broader range of choices, thereby reducing\nthe amount of additional work required when integrating such Text-to-CAD systems into industrial applications.\nC.8. Failure Cases\nWe identify two types of failures in our work: sequences that are not renderable and shapes that are rendered but misaligned\nwith the intended design. We refer to the former as Invalid Samples and the latter as Discrepant Samples. Examples of\nboth types of failures are shown in Figure 11.\nIn our analysis, samples are often invalid when the input instruction is too complex, meaning there are too many elements to\nbe drawn. The case shown in the top-left corner of Figure 11 involves more than 20 loops and 50 curves in the ground truth.\nAdditionally, CADFusion struggles to map CAD shapes to characters such as letters, resulting in failures when attempting to\nconstruct shapes that spell words or names.\n18\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nThe 3D shape is a rectangular prism with the same elongated trapezoidal cutout on each of its \ntop and bottom surfaces, the two corners of the cut are rounded, the length of the cut is less than \nthe length of the rectangular prism, the width is equal to the width of the rectangular prism, and \nthe height is less than the height of the rectangular prism. The incisions are symmetrically \ndistributed.\nThe 3D shape is a flat, rectangular plate with rounded corner, featuring a large central \nrectangular cutout and four evenly spaced circular holes near each corner. It is symmetrical and \nlikely designed as a mounting plate or bracket, allowing for secure attachment via the holes and \naccess through the central cutout.\nThe three-dimensional shape is a semicircular ring segment, similar to a "C" shape. It forms an \nopen arc. Its thickness varies at different locations and is mainly characterized by thick and thin \nthickness.\nIts shape is that of a stepped prism, consisting of three rectangular prisms of equal width and \nheight stacked on top of each other. The length decreases from bottom to top, with the left sides \nof the prisms aligned.\nFigure 10. An overview of the generation of CAD instances with slight variations from a single prompt. In each sub-figure, the top-left\nimage shows the ground truth generation, while the remaining three represent CADFusion’s outputs, which exhibit variations in thickness,\nwidth, and cutout size. The prompt is displayed at the top of each sub-figure.\nDiscrepant Sample: complex integration of multiple shapes\nInvalid Sample: mapping characters with CAD shapes\nDiscrepant Sample: too any items\nInvalid Sample: decoding too much information\nThe 3D shape consists of a large rectangular base with six evenly spaced vertical rectangular protrusions. The front face of each \nprotrusion has an inset rectangular design. The protrusions are aligned in two rows, each with three protrusions. The protrusions \nextend perpendicularly from the front face of the base, which is larger in height, width, and depth compared to the protrusions.\n"The 3D shape spells "IAN" using rectangular prisms. All letters are uniform in height and width, with a consistent horizontal \nalignment with spacing in between.",\nThe image features nine identical grey cubes scattered in a random pattern. Each cube has equal edge lengths, making them \ncongruent, and is oriented such that their faces align with the image\'s axes. The cubes exhibit no noticeable patterns, symmetry, \nor reflections, and there are varying gaps between them, with an absence of shadows and clustering.\nThe 3D shape consists of a hollow cylinder with a through-hole, connected perpendicularly to a solid rectangular prism. The \nprism extends vertically upwards from the horizontal cylindrical part. The shape is bilaterally symmetrical and could serve as a \nmechanical connector or mounting bracket.\nFigure 11. Invalid and discrepant samples. CADFusion generates invalid samples when the instructions are too complex or involve word\nshape knowledge, and produces discrepant outcomes when there are too many distinct items to generate or when complex merges are\nrequired to form the final CAD instance.\nDiscrepancies between the rendered shapes and the intended design can occur when the input instruction involves too many\ndistinct items. While CADFusion demonstrates advanced capabilities in understanding numerical values compared to other\nmodels, handling more than 8 separate items remains a challenging task. In such cases, CADFusion may either miscalculate\nthe number of items to draw or generate incorrect shapes, as shown in the bottom-left corner of Figure 11. Furthermore,\nintegrating multiple items into complex shapes is another frequent challenge for CADFusion.\n19')]}
2025-02-06 21:05:47,929 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:08:01,041 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:08:42,619 - INFO - Total execution time: 40.91 seconds (0.68 minutes)
2025-02-06 21:08:42,630 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content='1. Introduction\nDiffusion Bridge Models (DBMs) represent a specialized\nclass of diffusion models designed for data-to-data tasks,\nsuch as image-to-image translation. Unlike standard diffu-\nsion models, which operate by mapping noise to data (Ho\net al., 2020; Sohl-Dickstein et al., 2015), DBMs construct\ndiffusion processes directly between two data distributions\n(Peluchetti, 2023a; Liu et al., 2022b; Somnath et al., 2023;\nZhou et al., 2024a; Yue et al., 2024; Shi et al., 2023; De Bor-\ntoli et al., 2023). This approach allows DBMs to modify\nonly the necessary components of the data, starting from an\ninput sample rather than generating it entirely from Gaus-\n*Equal contribution\n1Skolkovo Institute of Science and\nTechnology\n2Yandex Research\n3HSE University\n4Artificial\nIntelligence Research Institute.\nCorrespondence to:\nNikita\nGushchin\n<n.gushchin@skoltech.ru>,\nAlexander\nKorotin\n<a.korotin@skoltech.ru>.\nInput\nIBMD (Ours)\nTeacher\nSuper-resolution\nJPEG restoration\nInpainting\nNormal-to-Image\nSketch-to-Image\nFigure 1. Outputs of DBMs models distilled by our Inverse Bridge\nMatching Distillation (IBMD) approach on various image-to-\nimage translation tasks and datasets (M5). Teachers use NFE≥500\nsteps, while IBMD distilled models use NFE≤4.\nsian noise. As a result, DBMs have demonstrated impressive\nperformance in image-to-image translation problems.\nThe rapid development of DBMs has led to two dominant ap-\nproaches, usually considered separately. The first branch of\n1\narXiv:2502.01362v1  [cs.LG]  3 Feb 2025\n\nInverse Bridge Matching Distillation\napproaches (Peluchetti, 2023a; Liu et al., 2022b; 2023a; Shi\net al., 2023; Somnath et al., 2023) considered the construc-\ntion of diffusion between two arbitrary data distributions\nperforming Unconditional Bridge Matching (also called\nthe Markovian projection) of a process given by a mixture\nof diffusion bridges. The application of this branch includes\ndifferent data like images (Liu et al., 2023a; Li et al., 2023),\naudio (Kong et al., 2025) and biological tasks (Somnath\net al., 2023; Tong et al., 2024) not only in paired but also in\nunpaired setups using its relation to the Schr¨odinger Bridge\nproblem (Shi et al., 2023; Gushchin et al., 2024). The second\ndirection follows a framework closer to classical diffusion\nmodels, using forward diffusion to gradually map to the\npoint of different distibution rather than mapping distribu-\ntion to distribution as in previous case (Zhou et al., 2024a;\nYue et al., 2024). While these directions differ in theoretical\nformulation, their practical implementations are closely re-\nlated; for instance, models based on forward diffusion can\nbe seen as performing Conditional Bridge Matching with\nadditional drift conditions (De Bortoli et al., 2023).\nSimilar to classical DMs, DBMs also exhibit multistep se-\nquential inference, limiting their adoption in practice. De-\nspite the impressive quality shown by DBMs in the practical\ntasks, only a few approaches were developed for their accel-\neration, including more advanced sampling schemes (Zheng\net al., 2024; Wang et al., 2024) and consistency distillation\n(He et al., 2024), adapted for bridge models. While these\napproaches significantly improve the efficiency of DBMs,\nsome unsolved issues remain. The first one is that the men-\ntioned acceleration approaches are directly applicable only\nfor DBMs based on the Conditional Bridge Matching, i.e.,\nno universal method can accelerate any DBMs. Also, due\nto some specific theoretical aspects of DBMs, consistency\ndistillation cannot be used to obtain the single-step model\n(He et al., 2024, Section 3.4).\nContributions. To address the above-mentioned issues of\nDBMs acceleration, we propose a new distillation technique\nbased on the inverse bridge matching problem, which has\nseveral advantages compared to existing methods:\n1. Universal Distillation. Our distillation technique is ap-\nplicable to DBMs trained with both conditional and un-\nconditional regimes, making it the first distillation ap-\nproach introduced for unconditional DBMs.\n2. Single-Step and Multistep Distillation. Our distillation\nis capable of distilling DBMs into generators with any\nspecified number of steps, including the distillation of\nDBMs into one-step generators.\n3. Target data-free distillation. Our method does not\nrequire the target data domain to perform distillation.\n4. Better quality of distilled models. Our distillation tech-\nnique is tested on a wide set of image-to-image problems\nfor conditional and unconditional DBMs in both one and\nmulti-step regimes. It demonstrates improvements com-\npared to the previous acceleration approaches including\nDBIM(Zheng et al., 2024) and CDBM (He et al., 2024).\n2. Background\nIn this paper, we propose a universal distillation frame-\nwork for both conditional and unconditional DBMs.\nTo not repeat fully analogical results for both cases,\nwe denote by this color the additional conditioning\non xT used for the conditional models, i.e. for the\nunconditional case this conditioning is not used.\n2.1. Bridge Matching\nWe start by recalling the bridge matching method\n(Peluchetti, 2023b;a; Liu et al., 2022b; Shi et al., 2023).\nConsider two probability distributions p(x0) and p(xT ) on\nRD dimensional space, which represent target and source\ndomains, respectively. For example, in an image inverse\nproblem, p(x0) represents the distribution of clean im-\nages and p(xT ) the distribution of corrupted images. Also\nconsider a coupling p(x0, xT ) of these two distributions,\nwhich is a probability distribution on RD × RD. Cou-\npling p(x0, xT ) can be provided by paired data or con-\nstructed synthetically, i.e., just using the independent distri-\nbution p(x0, xT ) = p(x0)p(xT ). Bridge Matching aims to\nconstruct the diffusion that transforms source distribution\np(xT ) to target distribution p(x0) based on given coupling\np(x0, xT ) and specified diffusion bridge.\nDiffusion bridges.\nConsider forward-time diffusion Q\ncalled ”Prior” on time horizon [0, T] represented by the\nstochastic differential equation (SDE):\nPrior Q :\ndxt = f(xt, t)dt + g(t)dwt,\n(1)\nf(xt, t) : RD × [0, T] →RD,\ng(t) : [0, T] →RD,\nwhere f(xt, t) is a drift function, g(t) is the noise schedule\nfunction and dwt is the differential of the standard Wiener\nprocess. By q(xt|xs), we denote the transition probability\ndensity of prior process Q from time s to time t. Diffusion\nbridge is a conditional process Q|x0,xT , which is obtained\nby pinning down starting and ending points x0 and xT . This\ndiffusion bridge can be derived from prior process Q using\nthe Doob-h transform (Doob & Doob, 1984):\nDiffusion Bridge Q|x0,xT : x0, xT are fixed,\n(2)\ndxt = {f(xt, t)dt + g2(t)∇xt log q(xT |xt)}dt + g(t)dwt,\nFor this diffusion bridge we denote the distribution at time t\nof the diffusion bridge Q|x0,xT by q(xt|x0, xT ).\nMixture of bridges. Bridge Matching procedure starts with\ncreating a mixture of bridges process Π. This process is\n2\n\nInverse Bridge Matching Distillation\nFigure 2. Overview of (Conditional) Bridge Matching with bx0 reparameterization. The process begins by sampling a pair (x0, xT )\nfrom the data coupling p(x0, xT ). An intermediate sample xt is then drawn from the diffusion bridge q(xt|x0, xT ) at a random time\nt ∼U[0, T]. The model bx0 is trained with an MSE loss to reconstruct x0 from xt. In the conditional setting (dashed red path), bx0 is also\nconditioned on xT as an additional input, leveraging information about the terminal state to improve reconstruction.\nrepresented as follows:\nMixture of Bridges Π :\nΠ(·) =\nZ\nQ|x0,xT (·)p(x0, xT )dx0dxT .\n(3)\nPractically speaking, the definition (3) means that to sample\nfrom a mixture of bridges Π, one first samples the pair\n(x0, xT ) ∼p(x0, xT ) from data coupling and then samples\ntrajectory from the bridge Q|x0,xT (·).\nBridge Matching problem. The mixture of bridges Π can-\nnot be used for data-to-data translation since it requires first\nto sample a pair of data and then just inserts the trajectory.\nIn turn, we are interested in constructing a diffusion, which\ncan start from any sample xT ∼p(xT ) and gradually trans-\nform it to x0 ∼p(x0). This can be done by solving the\nBridge Matching problem (Shi et al., 2023, Proposition 2)\nBridge Matching problem:\n(4)\nBM(Π)\ndef\n= arg min\nM∈M\nKL(Π||M),\nwhere M is the set of Markovian processes associated with\nsome SDE and KL(Π||M) is the KL-divergence between\na constructed mixture of bridges Π and diffusion M. It is\nknown that the solution of Bridge Matching is the reversed-\ntime SDE (Shi et al., 2023, Proposition 9):\nThe SDE of Bridge Matching solution :\n(5)\ndxt = {ft(xt) −g2(t)v∗(xt, t)}dt + g(t)d ¯wt,\nxT ∼pT (xT ),\nwhere ¯w is a standard Wiener process when time t flows\nbackward from t = T to t = 0, and dt is an infinitesimal\nnegative timestep. The drift function v∗is obtained solving\nthe following problem (Shi et al., 2023; Liu et al., 2023a):\nBridge Matching problem with a tractable objective: (6)\nmin\nϕ Ex0,t,xt\n\x02\n∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼p(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nTime moment t here is sampled according to the uniform\ndistribution on the interval [0, T].\nRelation Between Flow and Bridge Matching. The Flow\nMatching (Liu et al., 2023b; Lipman et al., 2023) can be\nseen as the limiting case σ →0 of the Bridge Matching for\nparticular example see (Shi et al., 2023, Appendix A.1).\n2.2. Augmented (Conditional) Bridge Matching and\nDenoising Diffusion Bridge Models (DDBM)\nFor a given coupling p(x0, xT ) = p(x0|xT )p(xT ), one can\nuse an alternative approach to build a data-to-data diffusion.\nConsider a set of Bridge Matching problems indexed by xT\nbetween p0 = p(x0|xT ) and p(xT ) = δxT (x) (delta mea-\nsure centered at xT ). This approach is called Augmented\nBridge Matching (De Bortoli et al., 2023). The key dif-\nference of this version in practice is that it introduces the\ncondition of the drift function v∗(xt, t, xT ) on the starting\npoint xT in the reverse time diffusion (5):\ndxt = {ft(xt) −g2(t)v∗(xt, t, xT )}dt + g(t)d ¯wt.\nThe drift function v∗can be recovered almost in the same\nway just by the addition of this condition on xT :\nAugmented (Conditional) Bridge Matching Problem.\nmin\nϕ Ex0,t,xt,xT\n\x02\n∥vϕ(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼p(x0, xT ), and xt ∼q(xt|x0, xT ).\nSince the difference is the addition of conditioning on xT ,\nwe call this approach Conditional Bridge Matching.\nRelation to DDBM. As was shown in the Augmented\nBridge Matching (De Bortoli et al., 2023), the conditional\nBridge Matching is equivalent to the Denoising Diffusion\nBridge Model (DDBM) proposed in (Zhou et al., 2024a).\nThe difference is that in DDBM, the authors learn the score\nfunction of s(xt, xT , t) conditioned on xT of a process for\nwhich x0 ∼p(x0|xT ) and q(xt) ∼q(xt|x0, xT ): Then, it\nis combined with the drift of forward Doob-h transform (5)\nto get the reverse SDE drift v(xt, t, xT ):\nv(xt, t, xT ) = s(xt, xT , t) −∇xt log q(xT |xt),\ndxt = {f(xt, t)dt −g2(t)v(xt, t, xT )}dt + g(t)d ¯wt,\nor reverse probability flow ODE drift:\nvODE(xt, t, xT ) = 1\n2s(xt, xT , t) −∇xt log q(xT |xt),\n3\n\nInverse Bridge Matching Distillation\ndxt = {f(xt, t)dt −g2(t)vODE(xt, t, xT )}dt,\nwhich is used for consistency distillation in (He et al., 2024).\n2.3. Practical aspects of Bridge Matching\nPriors used in practice. In practice (He et al., 2024; Zhou\net al., 2023; Zheng et al., 2024), the drift of the prior pro-\ncess is usually set to be f(xt, t) = f(t)xt, i.e, it depends\nlinearly on xt. For this process the transitional distribution\nq(xt|x0) = N(xt|αtx0, σ2\nt I) is Gaussian, where:\nf(t) = d log αt\ndt\n,\ng2(t) = dσ2\nt\ndt −2d log αt\ndt\nσ2\nt .\nThe bridge process distribution is also a Gaussian\nq(xt|x0, xT ) = N(xT |atxT + btx0, c2\ntI) with coefficients:\nat = αt\nαT\nSNRT\nSNRt\n, bt = αt\n\x12\n1 −SNRT\nSNRt\n\x13\n,\nc2\nt = σ2\nt\n\x12\n1 −SNRT\nSNRt\n\x13\n,\nwhere SNRt = α2\nt\nσ2\nt is the signal-to-noise ratio at time t.\nData prediction reparameterization. The regression target\nof the loss function (6) for the priors with the drift v(xt, t)\nis given by ∇xt log q(xt|x0) = −xt−αtx0\nσ2\nt\n. Hence, one can\nuse the parametrization v(xt, t, xT ) = −xt−αtbx0(xt,t,xT )\nσ2\nt\nand solve the equivalent problem:\nReparametrized (Conditional) Bridge Matching problem:\nmin\nϕ Ex0,t,xt,xT\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03\n,\n(7)\n(x0, xT ) ∼p(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\nwhere λ(t) is any positive weighting function. Note that xT\nis used only for the Conditional Bridge Matching model.\n2.4. Difference Between Acceleration of Unconditional\nand Conditional DBMs\nSince both conditional and unconditional approaches learn\ndrifts of SDEs, they share the same problems of long in-\nference. However, these models significantly differ in the\napproaches that can accelerate them. The source of this\ndifference is that Conditional Bridge Matching considers\nthe set of problems of reversing diffusion, which gradually\ntransforms distribution p(x0|xT ) to the fixed point xT . Fur-\nthermore, the forward diffusion has simple analytical drift\nand Gaussian transitional kernels. Thanks to it, for each xT\nto sample, one can use the probability flow ODE and ODE-\nsolvers or hybrid solvers to accelerate sampling (Zhou et al.,\n2024a) or use consistency distillation of bridge models (He\net al., 2024). Another beneficial property is that one can con-\nsider a non-Markovian forward process to develop a more\nefficient sampling scheme proposed in DBIM (Zheng et al.,\n2024) similar to Denoising Diffusion Implicit Models (Song\net al., 2021). However, in the Unconditional Bridge Match-\ning problem, the forward diffusion process, which maps\np(x0) to p(xT ) without conditioning on specific point xT ,\nis unknown. Hence, the abovementioned methods cannot\nbe used to accelerate this model type.\n3. IBMD: Inverse Bridge Matching Distillation\nThis section describes our proposed universal approach to\ndistill the both Unconditional and (Conditional) Bridge\nMatching models v∗(called the teacher model) into a few-\nstep generator using only the corrupted data pT (xT ). The\nkey idea of our method is to consider the inverse problem of\nfinding the mixture of bridges Πθ, for which Bridge Match-\ning provides the solution vθ with the same drift as the given\nteacher model v∗. We formulate this task as the optimiza-\ntion problem (M3.1). However, gradient methods cannot\nsolve this optimization problem directly due to the absence\nof tractable gradient estimation. To avoid this problem, we\nprove a theorem that allows us to reformulate the inverse\nproblem in the tractable objective for gradient optimiza-\ntion (M3.2). Then, we present the fully analogical results\nfor the Conditional Bridge Matching case in (M3.3). Next,\nwe present the multistep version of distillation (M3.5) and\nthe final algorithm (M3.4). We provide the proofs for all\nconsidered theorems and propositions in Appendix A.\n3.1. Bridge Matching Distillation as Inverse Problem\nIn this section, we focus on the derivation of our distilla-\ntion method for the case of Unconditional Bridge Match-\ning. Consider the fitted teacher model v∗(xt, t), which\nis an SDE drift of some process M ∗= BM(Π∗), where\nΠ∗constructed using some data coupling p∗(x0, xT ) =\np∗(x0|xT )p(xT ).\nWe\nparametrize\npθ(x0, xT )\n=\npθ(x0|xT )p(xT ) and aim to find such Πθ build on\npθ(x0, xT ), that BM(Π∗)\n=\nBM(Πθ).\nIn practice,\nwe parametrize pθ(x0|xT ) by the stochastic generator\nGθ(xT , z), z ∼N(0, I), which generates samples based\non input xT ∼p(xT ) and the gaussian noise z. Now, we\nformulate the inverse problem as follows:\nmin\nθ\nKL(BM(Πθ)||M ∗).\n(8)\nNote, that since the objective (8) is the KL-divergence be-\ntween BM(Πθ) and M ∗, it is equal to 0 if and only if\nBM(Πθ) and M ∗coincide. Furthermore, using the disinte-\ngration and Girsanov theorem (Vargas et al., 2021; Pavon &\nWakolbinger, 1991), we have the following result:\nProposition 3.1 (Inverse Bridge Matching problem). The\ninverse problem (8) is equivalent to\nmin\nθ\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(9)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\n4\n\nInverse Bridge Matching Distillation\nFigure 3. Overview of our method Inverse Bridge Matching Distillation (IBMD). The goal is to distill a trained (Conditional) Bridge\nMatching model into a generator Gθ(z, xT ), which learns to produce samples using the corrupted data p(xT ). Generator Gθ(z, xT )\ndefines the coupling pθ(x0, xT ) = pθ(x0|xT )p(xT ) and we aim to learn the generator in such way that Bridge Matching with pθ(x0, xT )\nproduces the same (Conditional) Bridge Matching model bxϕ\n0 = bxθ\n0. To do so, we learn a bridge model bxϕ\n0 using coupling pθ in the same\nway as the teacher model was learned. Then, we use our novel objective given in Theorem 3.2 to update the generator model Gθ.\nwhere λ(t) is any positive weighting function.\nThus, this is the constrained problem, where the drift v\nis the result of Bridge Matching for coupling pθ(x0, xT )\nparametrized by the generator Gθ. Unfortunately, there is\nno clear way to use this objective efficiently for optimizing\na generator Gθ since it would require gradient backpropa-\ngation through the argmin of the Bridge Matching problem.\n3.2. Tractable objective for the inverse problem\nIn this section, we introduce our new unconstrained refor-\nmulation for the inverse problem (9), which admits direct\noptimization using gradient methods:\nTheorem 3.2 (Tractable inverse problem reformulation).\nThe constrained inverse problem (9) w.r.t θ is equivalent to\nthe unconstrained optimization problem:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−\nmin\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\nWhere the constraint in the original inverse problem (9) is\nrelaxed by introducing the inner bridge matching problem.\nThis is the general result that can applied with any diffusion\nbridge. For the priors with with drift f(xt, t) = f(t)xt, we\npresent its reparameterized version.\nProposition 3.3 (Reparameterized tractable inverse prob-\nlem). Using the reparameterization (M2.3) for the prior with\nthe linear drift f(xt, t) = f(t)xt, the inverse problem in\nTheorem 3.2 is equivalent to:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥bx∗\n0(xt, t) −x0∥2\x03\n−\nmin\nϕ Ext,t,x0\n\x02\nλ(t)∥bxϕ\n0(xt, t) −x0∥2\x03\ndt\ni\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nThe key difference of the reformulated problem is that it\nadmits clear gradients of generator Gθ, which can be cal-\nculated automatically by using the autograd techniques.\nThanks to the unconstrained reformulation of an inverse\nproblem given by Theorem 3.2, it can now be solved di-\nrectly by parameterizing bx0(xt, t) by a neural network.\n3.3. Distillation of conditional Bridge Matching models\nSince Conditional Bridge Matching is, in essence, a set\nof Unconditional Bridge Matching problems for each xT\n(M2.2), the analogical results hold just by adding the condi-\ntioning on xT for v, i.e., using v(xt, t, xT ) or bx0, i.e. using\nbx0(xt, t, xT ). Here, we provide the final reparametrized\nformulation, which we use in our experiments:\nTheorem 3.4 (Reparameterized tractable inverse problem\nfor conditional bridge matching).\nmin\nθ\nh\nExt,t,x0,xT\n\x02\nλ(t)∥bx∗\n0(xt, t, xT ) −x0∥2\x03\n−\n(10)\nmin\nϕ Ext,t,x0,xT\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nwhere λ(t) is some positive weight function.\nTo use it in practice, we parameterize bx0(xt, t, xT ) by a\nneural network with an additional condition on xT .\n3.4. Algorithm\nWe provide a one-step Algorithm 1 that solves the inverse\nBridge Matching problem in the reformulated version that\nwe use in our experiments. We provide a visual abstract of\nit in Figure 3. Note that a teacher in the velocity parame-\nterization v∗(xt, t) can be easily reparameterized (M2.3) in\nx0-prediction model using bx∗(xt, t) = σ2\nt v∗(xt,t)+xt\nαt\n.\n5\n\nInverse Bridge Matching Distillation\n3.5. Mulitistep distillation\nWe also present a multistep modification of our distillation\ntechnique if a one-step generator struggles to distill the mod-\nels, e.g., in inpainting setups, where the corrupted image\nxT contains less information. Our multistep technique is\ninspired by similar approaches used in diffusion distilla-\ntion methods (Yin et al., 2024a, DMD) and aims to avoid\ntraining/inference distribution mismatch.\nWe choose N timesteps {0 < t1 < t2 < ... < tN = T}\nand add additional time input to our generator Gθ(xt, z, t).\nFor the conditional Bridge Matching case, we also add\nconditions on xT and use Gθ(xt, z, t, xT ). To perform in-\nference, we alternate between getting prediction from the\ngenerator ex0 = Gθ(xt, z, t) and using posterior sampling\nq(xtn−1|ex0, xtn) given by the diffusion bridge. To train\nthe generator in the multistep regime, we use the same\nprocedure as in one step except that to get input xt for in-\ntermediate times tn < tN, we first perform inference of our\ngenerator to get x0 and then use bridge q(xt|ex0, xT ).\n4. Related work\nDiffusion Bridge Models (DBMs) acceleration. Unlike\na wide scope of acceleration methods developed for clas-\nsical diffusion and flow models, only a few approaches\nwere developed for DBM acceleration. For the conditional\nDBMs, acceleration methods include more advanced sam-\nplers (Zheng et al., 2024; Wang et al., 2024) based on re-\nformulated forward diffusion process as a non-markovian\nprocess inspired by Denoising Diffusion Implicit Models\n(Song et al., 2021). Also, there is a distillation method based\non the distilling probability-flow ODE into a few steps using\nconsistency models (He et al., 2024). However, for theo-\nretical reasons (He et al., 2024, Section 3.4), consistency\nmodels for Diffusion Bridges cannot be distilled into one-\nstep generators. Unlike these existing works, our method\nis applicable to both conditional and unconditional types of\nDBMs and can distill models into the one-step generator.\nRelated diffusion and flow models distillation techniques.\nAmong the methods developed for the distillation of classi-\ncal diffusion and flow models, the most related to our work\nare methods based on simultaneous training of few-step\ngenerators and auxiliary ”fake” model, that predict score or\ndrift function for the generator (Yin et al., 2024b;a; Zhou\net al., 2024b; Huang et al., 2024). Unlike these approaches,\nwe consider the distillation of Diffusion Bridge Models -\nthe generalization of flow and diffusion models.\n5. Experiments\nThis section highlights the applicability of our IBMD distil-\nlation method in both unconditional and conditional settings.\nTo demonstrate this, we conducted experiments utilizing\npretrained unconditional models used in I2SB paper (Liu\net al., 2023a). Then we evaluated IBMD in conditional\nAlgorithm 1 Inverse Bridge Matching Distillation (IBMD)\nInput\n:Teacher network bx∗\n0 : RD × [0, T] × RD →RD;\nBridge q(xt|x0, xT ) used for training x∗;\nGenerator network Gθ : RD × RD →RD;\nBridge network bxϕ\n0 : RD × [0, T] × RD →RD;\nInput distribution p(xT ) accessible by samples;\nWeights function λ(t) : [0, T] →R+;\nBatch size N; Number of student iterations K;\nNumber of bridge iterations L.\nOutput :Learned generator Gθ of coupling pθ(x0, xT ) for\nwhich Bridge Matching outputs drift v ≈v∗.\n// Conditioning on xT is used only for distillation of Condi-\ntional Bridge Matching models.\nfor k = 1 to K do\nfor l = 1 to L do\nSample batch xT ∼p(xT )\nSample batch of noise z ∼N(0, I)\nx0 ←Gθ(xT , z)\nSample time batch t ∼U[0, T]\nSample batch xt ∼q(xt|x0, xT )\nbLϕ ←1\nN\nPN\nn=1\n\x02\nλ(t)||bxϕ\n0(xt, t, xT ) −x0||2\x03\nn\nUpdate ϕ by using ∂b\nLϕ\n∂ϕ\nSample batch xT ∼p(xT )\nSample batch of noise z ∼N(0, I)\nx0 ←Gθ(xT , z)\nSample time batch t ∼U[0, T]\nSample batch xt ∼q(xt|x0, xT )\nbLθ ←1\nN\nPN\nn=1\n\x02\nλ(t)||bx∗\n0(xt, t, xT ) −x0||2 −\nλ(t)||bxϕ\n0(xt, t, xT ) −x0||2\x03\nn\nUpdate θ by using ∂b\nLθ\n∂θ\nsettings using DDBM (Zhou et al., 2024a) setup (M5.2).\nFor clarity, we denote our models as IBMD-DDBM and\nIBMD-I2SB, indicating that the teacher model is derived\nfrom DDBM or I2SB framework, respectively. We provide\nall the technical details in Appendix B.\n5.1. Distillation of I2SB (5 setups)\nSince known distillation and acceleration techniques are\ndesigned for the conditional models, there is no clear base-\nline for comparison. Thus, this section aims to demonstrate\nthat our distillation technique significantly decreases NFE\nrequired to obtain the same quality of generation.\nExperimental Setup. To test our approach for uncondi-\ntional models, we consider models trained and published in\nI2SB paper (Liu et al., 2023a), specifically (a) two models\nfor the 4x super-resolution with bicubic and pool kernels,\n(b) two models for JPEG restoration using quality factor\nQF= 5 and QF= 10, and (c) a model for center-inpainting\nwith a center mask of size 128 × 128 all of which were\ntrained on ImageNet 256 × 256 dataset (Deng et al., 2009).\n6\n\nInverse Bridge Matching Distillation\nTable 1. Results on the image super-resolution task. Baseline re-\nsults are taken from I2SB (Liu et al., 2023a).\n4× super-resolution (bicubic)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n21.3\n63.2\nDDNM (Wang et al., 2023)\n100\n13.6\n65.5\nΠGDM (Song et al., 2023)\n100\n3.6\n72.1\nADM (Dhariwal & Nichol, 2021)\n1000\n14.8\n66.7\nCDSB (Shi et al., 2022)\n50\n13.6\n61.0\nI2SB (Liu et al., 2023a)\n1000\n2.8\n70.7\nIBMD-I2SB (Ours)\n1\n2.5\n72.4\nTable 2. Results on the image JPEG restoration task with QF=5.\nBaseline results are taken from I2SB (Liu et al., 2023a).\nJPEG restoration, QF= 5.\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n28.2\n53.9\nΠGDM (Song et al., 2023)\n100\n8.6\n64.1\nPalette (Saharia et al., 2022)\n1000\n8.3\n64.2\nCDSB (Shi et al., 2022)\n50\n38.7\n45.7\nI2SB (Liu et al., 2023a)\n1000\n4.6\n67.9\nI2SB (Liu et al., 2023a)\n100\n5.4\n67.5\nIBMD-I2SB (Ours)\n1\n5.3\n67.2\nFor all the setups we use the same train part of ImageNet\ndataset, which was used to train the used models. For the\nevaluation we follow the same protocol used in the I2SB\npaper, i.e. use the full validation subset of ImageNet for\nsuper-resolution task and the 10′000 subset of validation for\nother tasks. We report the same FID (Heusel et al., 2017)\nand Classifier Accuracy (CA) using pre-trained ResNet50\nmodel metrics used in the I2SB paper. We present our results\nin Table 1, Table 3, Table 2, Table 4 and Table 6. We provide\nthe uncurated samples for all setups in Appendix C.\nResults. For both super-resolution tasks (see Table 1, Ta-\nble 3), our 1-step distilled model outperformed teacher\nmodel inference using all 1000 steps used in the training.\nNote that our model does not use the clean training target\ndata p(x0), only the corrupted p(xT ), hence this improve-\nment is not due to additional training using paired data. We\nhypothesize that it is because the teacher model introduces\napproximation error during many steps of sampling, which\nmay accumulate. For both JPEG restoration (see Table 2, Ta-\nble 4), our 1-step distilled generator provides the quality of\ngeneration close to the teacher model and achieves around\n100x time acceleration. For the inpainting problem (see\nTable 6), we present the results for 1, 2 and 4 steps distilled\ngenerator. Our 2 and 4-step generators provide a quality\nsimilar to the teacher I2SB model, in turn, there is still some\ngap for the 1-step model. These models provide around 5x\ntime acceleration. We hypothesize that this setup is harder\nTable 3. Results on the image super-resolution task. Baseline re-\nsults are taken from I2SB (Liu et al., 2023a).\n4× super-resolution (pool)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n14.8\n64.6\nDDNM (Wang et al., 2023)\n100\n9.9\n67.1\nΠGDM (Song et al., 2023)\n100\n3.8\n72.3\nADM (Dhariwal & Nichol, 2021)\n1000\n3.1\n73.4\nCDSB (Shi et al., 2022)\n50\n13.0\n61.3\nI2SB (Liu et al., 2023a)\n1000\n2.7\n71.0\nIBMD-I2SB (Ours)\n1\n2.6\n72.7\nTable 4. Results on the image JPEG restoration task with QF=10.\nBaseline results are taken from I2SB (Liu et al., 2023a).\nJPEG restoration, QF= 10.\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n16.7\n64.7\nΠGDM (Song et al., 2023)\n100\n6.0\n71.0\nPalette (Saharia et al., 2022)\n1000\n5.4\n70.7\nCDSB (Shi et al., 2022)\n50\n18.6\n60.0\nI2SB (Liu et al., 2023a)\n1000\n3.6\n72.1\nI2SB (Liu et al., 2023a)\n100\n4.4\n71.6\nIBMD-I2SB (Ours)\n1\n3.8\n72.4\nfor our model since it is required to generate the entire center\nfragment from scratch, while in other tasks, there is already\nsome good approximation given by corrupted images.\n5.2. Distillation of DDBM (3 setups)\nThis section addresses two primary objectives: (1) demon-\nstrating the feasibility of conditional model distillation\nwithin our framework and (2) comparing with the CDBM\n(He et al., 2024) - a leading approach in Conditional Bridge\nMatching distillation, presented into different models: CBD\n(consistency distillation) and CBT (consistency training).\nExperimental Setup. For evaluation, we use the same\nsetups used in competing methods (He et al., 2024; Zheng\net al., 2024). For the image-to-image translation task, we\nutilize the Edges→Handbags dataset (Isola et al., 2017)\nwith a resolution of 64 × 64 pixels and the DIODE-Outdoor\ndataset (Vasiljevic et al., 2019) with a resolution of 256×256\npixels. For these tasks, we report FID and Inception Scores\n(IS) (Barratt & Sharma, 2018). For the image inpainting\ntask, we use the same setup of center-inpainting as before.\nResults. We utilized the same teacher model checkpoints\nand as in CDBM. We present the quantitative and qualitative\nresults of IBMD on the image-to-image translation task in\nTable 5 and in Figures 12, 10 respectively. The compet-\ning methods, DBIM (Zhou et al., 2024a, Section 4.1) and\nCDBM (He et al., 2024, Section 3.4), cannot use single-step\ninference due to the singularity at the starting point xT .\n7\n\nInverse Bridge Matching Distillation\nTable 5. Results on the Image-to-Image Translation Task (Training Sets). Methods are grouped by NFE (> 2, 2, 1), with the best metrics\nbolded in each group. Baselines results are taken from CDBM.\nNFE\nEdges →Handbags (64 × 64)\nDIODE-Outdoor (256 × 256)\nFID ↓\nIS ↑\nFID ↓\nIS ↑\nDDIB (Su et al., 2022)\n≥40\n186.84\n2.04\n242.3\n4.22\nSDEdit (Meng et al., 2021)\n≥40\n26.5\n3.58\n31.14\n5.70\nRectified Flow (Liu et al., 2022a)\n≥40\n25.3\n2.80\n77.18\n5.87\nI2SB (Liu et al., 2023a)\n≥40\n7.43\n3.40\n9.34\n5.77\nDBIM (Zheng et al., 2024)\n50\n1.14\n3.62\n3.20\n6.08\nDBIM (Zheng et al., 2024)\n100\n0.89\n3.62\n2.57\n6.06\nCBD (He et al., 2024)\n2\n1.30\n3.62\n3.66\n6.02\nCBT (He et al., 2024)\n0.80\n3.65\n2.93\n6.06\nIBMD-DDBM (Ours)\n0.67\n3.69\n3.12\n5.92\nPix2Pix (Isola et al., 2017)\n1\n74.8\n4.24\n82.4\n4.22\nIBMD-DDBM (Ours)\n1.26\n3.66\n4.07\n5.89\nTable 6. Results on the Image Inpainting Task.\nMethods are\ngrouped by NFE (> 4, 4, 2, 1), with the best metrics bolded\nin each group. Baselines results are taken from CDBM.\nInpainting, Center (128 × 128)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n24.4\n62.1\nΠGDM (Song et al., 2023)\n100\n7.3\n72.6\nDDNM (Wang et al., 2022)\n100\n15.1\n55.9\nPalette (Saharia et al., 2022)\n1000\n6.1\n63.0\nI2SB (Liu et al., 2023a)\n10\n5.4\n65.97\nDBIM (Zheng et al., 2024)\n50\n3.92\n72.4\nDBIM (Zheng et al., 2024)\n100\n3.88\n72.6\nCBD (He et al., 2024)\n4\n5.34\n69.6\nCBT (He et al., 2024)\n4.77\n70.3\nIBMD-I2SB (Ours)\n5.1\n70.3\nIBMD-DDBM (Ours)\n4.03\n72.2\nCBD (He et al., 2024)\n2\n5.65\n69.6\nCBT (He et al., 2024)\n5.34\n69.8\nIBMD-I2SB (Ours)\n5.3\n65.7\nIBMD-DDBM (Ours)\n4.23\n72.3\nIBMD-I2SB (Ours)\n1\n6.7\n65.0\nIBMD-DDBM (Ours)\n5.87\n70.6\nWe trained our IBMD with 1 and 2 NFEs on the\nEdges→Handbags dataset. We surpass CDBM at 2 NFE,\noutperform the teacher at 100 NFE, and achieve perfor-\nmance comparable to the teacher at 50 NFE with 1 NFE,\nresulting in a 50× acceleration. For the DIODE-Outdoor\nsetup, we trained IBMD with 1 and 2 NFEs. We surpassed\nCBD in FID at 2 NFE, achieving results comparable to CBT\nwith a slight drop in performance and maintaining strong\nperformance at 1 NFE with minor quality reductions.\nFor image inpainting, Table 6 and Figure 9 show the quanti-\ntative and qualitative results of IBMD. We train IBMD with\n4 NFE for image inpainting. It outperforms CBD and CBT\nat 4 NFE with a significant gap, surpassing both at 2 NFE\nand maintaining strong performance at 1 NFE while achiev-\ning teacher-level results at 50 NFE with a 12.5× speedup.\nConcerns regarding the evaluation protocol used in prior\nworks. For Edges-Handbags and DIODE-Outdoor setups,\nwe follow the evaluation protocol originally introduced in\nDDBM (Zhou et al., 2024a) and later used in works on\nacceleration of DDBM (Zheng et al., 2024; He et al., 2024).\nFor some reason, this protocol implies evaluation of the\ntrain set. Furthermore, test sets of these datasets consist of\na tiny fraction of images (around several hundred), making\nthe usage of standard metrics like FID challenging due to\nhigh statistical bias or variance of their estimation. Still,\nto assess the quality of the distilled model on the test sets,\nwe provide the uncurated samples produced by our distill\nmodel and teacher model on these sets in Figures 13 and\n11 in Appendix C. We also provide the uncurated samples\non the train part in Figures 12 and 10 to compare models’\nbehavior on train and test sets. From these results, we see\nthat the teacher model exhibits overfitting on both setups,\ne.g., it produces exactly the same images as corresponding\nreference images. In turn, on the test sets, teacher models\nwork well for the handbag setups, while on the test set of\nDIODE images, it exhibits mode collapse and produces gray\nimages. Nevertheless, our distilled model shows exactly\nthe same behavior in both sets, i.e., our IBMD approach\nprecisely distills the teacher model as expected.\n6. Discussion\nPotential impact. DBMs are used for data-to-data trans-\nlation in different domains, including images, audio, and\nbiological data. Our distillation technique provides a univer-\nsal and efficient way to address the long inference of DBMs,\nmaking them more affordable in practice.\nLimitations. Our method alternates between learning an\nadditional bridge model and updating the student, which\nmay be computationally expensive. Moreover, the student\noptimization requires backpropagation through the teacher,\nadditional bridge, and the generator network, making it 3x\ntime more memory expensive than training the teacher.\n8\n\nInverse Bridge Matching Distillation\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nBarratt, S. and Sharma, R. A note on the inception score.\narXiv preprint arXiv:1801.01973, 2018.\nDe Bortoli, V., Liu, G.-H., Chen, T., Theodorou, E. A., and\nNie, W. Augmented bridge matching. arXiv preprint\narXiv:2311.06978, 2023.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248–255. Ieee, 2009.\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis.\nAdvances in neural information\nprocessing systems, 34:8780–8794, 2021.\nDoob, J. L. and Doob, J. Classical potential theory and its\nprobabilistic counterpart, volume 262. Springer, 1984.\nGushchin, N., Selikhanovych, D., Kholkin, S., Burnaev,\nE., and Korotin, A. Adversarial schr\\” odinger bridge\nmatching. arXiv preprint arXiv:2405.14449, 2024.\nHe, G., Zheng, K., Chen, J., Bao, F., and Zhu, J.\nConsistency diffusion bridge models.\narXiv preprint\narXiv:2410.22637, 2024.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nHo, J., Jain, A., and Abbeel, P.\nDenoising diffusion\nprobabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840–6851, 2020.\nHuang, Z., Geng, Z., Luo, W., and Qi, G.-j. Flow generator\nmatching. arXiv preprint arXiv:2410.19310, 2024.\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-to-\nimage translation with conditional adversarial networks.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 1125–1134, 2017.\nKawar, B., Elad, M., Ermon, S., and Song, J.\nDe-\nnoising diffusion restoration models.\nAdvances in\nNeural Information Processing Systems, 35:23593–\n23606, 2022.\nKong, Z., Shih, K. J., Nie, W., Vahdat, A., Lee, S.-\ng., Santos, J. F., Jukic, A., Valle, R., and Catanzaro,\nB. A2sb: Audio-to-audio schrodinger bridges. arXiv\npreprint arXiv:2501.11311, 2025.\nLi, B., Xue, K., Liu, B., and Lai, Y.-K. Bbdm: Image-to-\nimage translation with brownian bridge diffusion mod-\nels.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern Recognition, pp. 1952–1961,\n2023.\nLipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M.,\nand Le, M.\nFlow matching for generative modeling.\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=PqvMRDCJT9t.\nLiu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A.,\nNie, W., and Anandkumar, A. I2sb: Image-to-image\nschr\\” odinger bridge. arXiv preprint arXiv:2302.05872,\n2023a.\nLiu, X., Gong, C., et al. Flow straight and fast: Learn-\ning to generate and transfer data with rectified flow.\nIn The Eleventh International Conference on Learning\nRepresentations, 2022a.\nLiu, X., Wu, L., Ye, M., and qiang liu. Let us build bridges:\nUnderstanding and extending diffusion generative models.\nIn NeurIPS 2022 Workshop on Score-Based Methods,\n2022b. URL https://openreview.net/forum?\nid=0ef0CRKC9uZ.\nLiu, X., Gong, C., and qiang liu.\nFlow straight and\nfast: Learning to generate and transfer data with rec-\ntified flow.\nIn The Eleventh International Conference\non Learning Representations, 2023b.\nURL https:\n//openreview.net/forum?id=XVjTT1nw5z.\nMeng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,\nS. Sdedit: Image synthesis and editing with stochastic\ndifferential equations. arXiv preprint arXiv:2108.01073,\n2021.\nPavon, M. and Wakolbinger, A. On free energy, stochas-\ntic control, and schr¨odinger processes.\nIn Modeling,\nEstimation and Control of Systems with Uncertainty:\nProceedings of a Conference held in Sopron, Hungary,\nSeptember 1990, pp. 334–348. Springer, 1991.\nPeluchetti, S.\nDiffusion bridge mixture transports,\nschr¨odinger bridge problems and generative modeling.\nJournal of Machine Learning Research, 24(374):1–51,\n2023a.\nPeluchetti, S. Non-denoising forward-time diffusions. arXiv\npreprint arXiv:2312.14589, 2023b.\n9\n\nInverse Bridge Matching Distillation\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,\nT., Fleet, D., and Norouzi, M. Palette: Image-to-image\ndiffusion models. In ACM SIGGRAPH 2022 conference\nproceedings, pp. 1–10, 2022.\nShi, Y., De Bortoli, V., Deligiannidis, G., and Doucet,\nA. Conditional simulation using diffusion schr¨odinger\nbridges.\nIn Uncertainty in Artificial Intelligence, pp.\n1792–1802. PMLR, 2022.\nShi, Y., Bortoli, V. D., Campbell, A., and Doucet, A. Dif-\nfusion schr¨odinger bridge matching. In Thirty-seventh\nConference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?\nid=qy07OHsJT5.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256–2265. PMLR, 2015.\nSomnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R.,\nKrause, A., and Bunne, C. Aligned diffusion schr¨odinger\nbridges.\nIn Uncertainty in Artificial Intelligence, pp.\n1985–1995. PMLR, 2023.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.\nnet/forum?id=St1giarCHLP.\nSong, J., Vahdat, A., Mardani, M., and Kautz, J.\nPseudoinverse-guided diffusion models for inverse\nproblems.\nIn International Conference on Learning\nRepresentations, 2023.\nSu, X., Song, J., Meng, C., and Ermon, S. Dual diffusion\nimplicit bridges for image-to-image translation. arXiv\npreprint arXiv:2203.08382, 2022.\nTong, A. Y., Malkin, N., Fatras, K., Atanackovic, L., Zhang,\nY., Huguet, G., Wolf, G., and Bengio, Y. Simulation-\nfree schr¨odinger bridges via score and flow matching. In\nInternational Conference on Artificial Intelligence and\nStatistics, pp. 1279–1287. PMLR, 2024.\nVargas, F., Thodoroff, P., Lamacraft, A., and Lawrence,\nN. Solving schr¨odinger bridges via maximum likelihood.\nEntropy, 23(9):1134, 2021.\nVasiljevic, I., Kolkin, N., Zhang, S., Luo, R., Wang, H., Dai,\nF. Z., Daniele, A. F., Mostajabi, M., Basart, S., Walter,\nM. R., et al. Diode: A dense indoor and outdoor depth\ndataset. arXiv preprint arXiv:1908.00463, 2019.\nWang, Y., Yu, J., and Zhang, J. Zero-shot image restora-\ntion using denoising diffusion null-space model. arXiv\npreprint arXiv:2212.00490, 2022.\nWang, Y., Yu, J., and Zhang, J.\nZero-shot image\nrestoration using denoising diffusion null-space model.\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=mRieQgMtNTQ.\nWang, Y., Yoon, S., Jin, P., Tivnan, M., Song, S., Chen,\nZ., Hu, R., Zhang, L., Chen, Z., Wu, D., et al. Implicit\nimage-to-image schr¨odinger bridge for image restora-\ntion. Zhiqiang and Wu, Dufan, Implicit Image-to-Image\nSchr¨odinger Bridge for Image Restoration, 2024.\nYin, T., Gharbi, M., Park, T., Zhang, R., Shechtman,\nE., Durand, F., and Freeman, W. T.\nImproved dis-\ntribution matching distillation for fast image synthe-\nsis. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024a. URL https:\n//openreview.net/forum?id=tQukGCDaNT.\nYin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F.,\nFreeman, W. T., and Park, T. One-step diffusion with\ndistribution matching distillation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 6613–6623, 2024b.\nYue, Z., Wang, J., and Loy, C. C. Resshift: Efficient diffu-\nsion model for image super-resolution by residual shifting.\nAdvances in Neural Information Processing Systems, 36,\n2024.\nZheng, K., He, G., Chen, J., Bao, F., and Zhu, J. Diffusion\nbridge implicit models. arXiv preprint arXiv:2405.15885,\n2024.\nZhou, L., Lou, A., Khanna, S., and Ermon, S. Denoising dif-\nfusion bridge models. arXiv preprint arXiv:2309.16948,\n2023.\nZhou, L., Lou, A., Khanna, S., and Ermon, S.\nDe-\nnoising diffusion bridge models.\nIn The Twelfth\nInternational Conference on Learning Representations,\n2024a. URL https://openreview.net/forum?\nid=FKksTayvGo.\nZhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H.\nScore identity distillation: Exponentially fast distilla-\ntion of pretrained diffusion models for one-step genera-\ntion. In Forty-first International Conference on Machine\nLearning, 2024b.\n10\n\nInverse Bridge Matching Distillation\nA. Proofs\nSince all our theorems, propositions and proofs for the inverse Bridge Matching problems which is formulated for the already\ntrained teacher model using some diffusion bridge, we assume all corresponding assumptions used in Bridge Matching.\nExtensive overview of them can be found in (Shi et al., 2023, Appendix C).\nProof of Proposition 3.1. Since both BM(Πθ) and M ∗given by reverse-time SDE and the same distribution pT (xT ) the\nKL-divergence expressed in the tractable form using the disintegration and Girsanov theorem (Vargas et al., 2021; Pavon &\nWakolbinger, 1991):\nKL(BM(Πθ)||M ∗) = Ext,t\n\x02\ng2(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nThe expectation is taken over the marginal distribution p(xt) of Πθ since it is the same as for BM(Πθ) (Shi et al., 2023,\nProposition 2). In turn, the drift v(xt, t) is the drift of Bridge Matching using Πθ, i.e. BM(Πθ):\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nCombining this, the inverse problem can be expressed in a more tractable form:\nmin\nθ\nExt,t\n\x02\ng2(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(11)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\ndt,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nWe can add positive valued weighting function λ(t) for the constraint:\nv = arg min\nv′\nExt,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\ndt,\nsince it is the MSE regression and its solution is conditional expectation for any weights given by:\nv(xt, t) = Ex0|xt,t\n\x02\n∇xt log q(xt|x0)].\nWe can add positive valued weighting function λ(t) for the main functional:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\nsince it does not change the optimum value (which is equal to 0) and optimal solution, which is the mixture of bridges with\nthe same drift as the teacher model.\nProof of Theorem 3.2. Consider inverse bridge matching optimization problem:\nmin\nθ\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(12)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nFirst, note that since v = arg minv′ Ext,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n, i.e. minimizer of MSE functional it is given\nby conditional expectation as:\nv(xt, t) = Ex0|xt,t\n\x02\n∇xt log q(xt|x0)|xt, t\n\x03\n.\n(13)\nThen note that:\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\n11\n\nInverse Bridge Matching Distillation\nExt,t,x0\n\x02\nλ(t)∥v(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\nExt,t,x0\n\x02\nλ(t)||v(xt, t)||2\x03\n|\n{z\n}\nExt,t\n\x02\nλ(t)||v(xt,t)||2\x03\n−2Ext,t,x0\n\x02\nλ(t)⟨v(xt, t), ∇xt log q(xt|x0)⟩\n\x03\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n=\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n−2Ext,t\nh\nλ(t)\n*\nv(xt, t), Ex0|xt,t\n\x02\n∇xt log q(xt|x0)\n\x03\n|\n{z\n}\n=v(xt,t)\n+ i\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n=\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n−2Ext,t\n\x02\nλ(t)||v(xt, t)||2\x03\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n=\n−Ext,t\n\x02\nλ(t)||v(xt, t)||2\x03\n+ Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n.\n(14)\nHence, we derive that\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n= Ext,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−min\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n.\nNow we use it to reformulate the initial objective:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n=\nExt,t\n\x02\nλ(t)||v(xt, t)||2\x03\n−2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t\n\x02\nλ(t)||v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2] −min\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n|\n{z\n}\n=Ext,t\n\x02\nλ(t)||v(xt,t)||2\x03\n−\n2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t\n\x02\nλ(t)||v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t\n\x02\nλ(t)||v∗(xt, t)||2\x03\n|\n{z\n}\nExt,t,x0\n\x02\nλ(t)||v∗(xt,t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\nTherefore, we get:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t\n\x02\nλ(t) ⟨v(xt, t), v∗(xt, t)⟩\n\x03\n+ Ext,t,x0\n\x02\nλ(t)||v∗(xt, t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\nTo complete the proof, we use the relation v(xt, t) = Ex0|xt,t\n\x02\n∇xt log q(xt|x0)|xt, t\n\x03\nfrom Equation 13. Integrating these\ncomponents, we arrive at the final result:\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t\n\x02\nλ(t)\n\nEx0|xt,t\n\x02\n∇xt log q(xt|x0)|xt, t\n\x03\n, v∗(xt, t)\n\x0b \x03\n+ Ext,t,x0\n\x02\nλ(t)||v∗(xt, t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\nExt,t,x0\n\x02\nλ(t)||∇xt log q(xt|x0)||2\x03\n−2Ext,t,x0\n\x02\nλ(t) ⟨∇xt log q(xt|x0), v∗(xt, t)⟩\n\x03\n+ Ext,t,x0\n\x02\nλ(t)||v∗(xt, t)||2\x03\n−\nmin\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n=\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−min\nv′ Ext,t,x0\n\x02\nλ(t)∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n.\nProof of Proposition 3.3. Consider the problem from Proposition 3.2:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n,\n12\n\nInverse Bridge Matching Distillation\nFor the priors with the drift f(t)x the regression target is ∇xt log q(xt|x0) = −xt−αtx0\nσ2\nt\n. Hence one can use the parametriza-\ntion v(xt, t) = −xt−αtbx0(xt,t)\nσ2\nt\nWe use reparameterization of both v∗and vϕ given by:\nv∗(xt, t) = −xt −αtbx∗\n0(xt, t)\nσ2\nt\n,\nvϕ(xt, t) = −xt −αtbxϕ\n0(xt, t)\nσ2\nt\nand get:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n=\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)α2\nt\nσ4\nt\n| {z }\ndef\n=λ′(t)\n∥bx∗\n0(xt, t) −x0∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)α2\nt\nσ4\nt\n| {z }\ndef\n=λ′(t)\n∥bxϕ\n0(xt, t) −x0∥2\x03i\n=\nmin\nθ\nh\nExt,t,x0\n\x02\nλ′(t)∥bx∗\n0(xt, t) −x0∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ′(t)∥bxϕ\n0(xt, t) −x0∥2\x03i\n,\nwhere λ′(t) is just another positive weighting function.\nProof of Theorem 3.4. In a fully analogical way, as for the unconditional case we consider the set of the Inverse Bridge\nMatching problems indexes by xT :\n\x08\nmin\nθ\n\x02\nKL(BM(Πθ|xT )||M ∗\n|xT )\n\x03\t\nxT ,\nwhere M ∗\n|xT is a result of Bridge Matching conditioned on xT and Πθ|xT is a Mixture of Bridges for each xT constructed\nusing bridge q(xt|x0, xT ) and coupling pθ(x0|xT )δxT (x).\nBy employing the same reasoning as in the proof of Proposition 3.1, the inverse problem can be reformulated as follows:\nmin\nθ\nExt,t,xT\n\x02\ng2(t)||v(xt, t, xT ) −v∗(xt, t, xT )||2\x03\n,\ns.t.\nv = arg min\nv′\nExt,t,x0,xT\n\x02\n∥v′(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\ndt,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nFollowing the proof of Theorem 3.2, we obtain a tractable formulation incorporating a weighting function:\nmin\nθ\nh\nExt,t,x0,xT\n\x02\nλ(t)∥v∗(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\n−\nmin\nϕ Ext,t,x0,xT\n\x02\nλ(t)∥vϕ(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03i\n.\nUtilizing the reparameterization under additional conditions (M2.3), we obtain the following representations:\nv∗(xt, t, xT ) = −xt −αtbx∗\n0(xt, t, xT )\nσ2\nt\n,\nvϕ(xt, t, xT ) = −xt −αtbxϕ\n0(xt, t, xT )\nσ2\nt\n.\nConsequently, applying the proof technique from Proposition 3.3, we derive the final expression:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥bx∗\n0(xt, t, xT ) −x0∥2\x03\n−min\nϕ Ext,t,x0\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nB. Experimental details\nAll hyperparameters are listed in Table 7. We used batch size 256 and ema decay 0.99 for setups. For each setup, we started\nthe student and bridge networks using checkpoints from the teacher models. In setups where the model adapts to noise: (1)\nWe added extra layers for noise inputs (set to zero initially), (2) Noise was concatenated with input data before input it to the\nnetwork. Datasets, code sources, and licenses are included in Table 8.\n13\n\nInverse Bridge Matching Distillation\nTask\nDataset\nTeacher\nNFE\nL/K ratio\nLR\nGrad Updates\nNoise\n4× super-resolution (bicubic)\nImageNet\nI2SB\n1\n5:1\n5e-5\n3000\n✓\n4× super-resolution (pool)\nImageNet\nI2SB\n1\n5:1\n5e-5\n3000\n✓\nJPEG restoration, QF = 5\nImageNet\nI2SB\n1\n5:1\n5e-5\n2000\n✓\nJPEG restoration, QF = 10\nImageNet\nI2SB\n1\n5:1\n5e-5\n3000\n✓\nCenter-inpainting (128 × 128)\nImageNet\nI2SB\n4\n5:1\n5e-5\n2000\n✗\nSketch to Image\nEdges →Handbags\nDDBM\n2\n5:1\n1e-5\n300\n✓\nSketch to Image\nEdges →Handbags\nDDBM\n1\n5:1\n1e-5\n14000\n✓\nNormal to Image\nDIODE-Outdoor\nDDBM\n2\n5:1\n1e-5\n500\n✓\nNormal to Image\nDIODE-Outdoor\nDDBM\n1\n5:1\n1e-5\n3700\n✓\nCenter-inpainting (128 × 128)\nImageNet\nDDBM\n4\n1:1\n3e-6\n3000\n✓\nTable 7. Table entries specify experimental configurations: NFE indicates multistep training (Sec. M3.5); L/K represents bridge/student\ngradient iteration ratios (Alg. M3.4); Grad Updates shows student gradient steps; Noise notes stochastic pipeline incorporation.\nTable 8. The used datasets, codes and their licenses.\nName\nURL\nCitation\nLicense\nEdges→Handbags\nGitHub Link\n(Isola et al., 2017)\nBSD\nDIODE-Outdoor\nDataset Link\n(Vasiljevic et al., 2019)\nMIT\nImageNet\nWebsite Link\n(Deng et al., 2009)\n\\\nGuided-Diffusion\nGitHub Link\n(Dhariwal & Nichol, 2021)\nMIT\nI2SB\nGitHub Link\n(Liu et al., 2023a)\nCC-BY-NC-SA-4.0\nDDBM\nGitHub Link\n(Zhou et al., 2023)\n\\\nDBIM\nGitHub Link\n(Zheng et al., 2024)\n\\\nB.1. Distillation of I2SB models.\nWe extended the I2SB repository (see Table 8), integrating our distillation framework. The following sections outline the\nsetups, adapted following the I2SB.\nMultistep implementation In this setup, we use the student model’s full inference process during multistep training (Section\n3.5). This means that x0 is generated with inferenced of the model Gθ through all timesteps (T = tN, . . . , t1 = 0) in the\nmultistep sequence. The generated x0 is subsequently utilized in the computation of the bridge bLϕ or student bLθ objective\nfunctions, as formalized in Algorithm 1.\n4× super-resolution. Our implementation of the degradation operators aligns with the filters implementation proposed\nin DDRM (Kawar et al., 2022). Firstly, we synthesize images at 64 × 64 resolution, then upsample them to 256 × 256 to\nensure dimensional consistency between clean and degraded inputs. For evaluation, we follow established benchmarks\n(Saharia et al., 2022; Song et al., 2023) by computing the FID on reconstructions from the full ImageNet validation set, with\ncomparisons drawn against the training set statistics.\nJPEG restoration. Our JPEG degradation implementation, employing two distinct quality factors (QF=5, QF=10), follows\n(Kawar et al., 2022). FID is evaluated on a 10, 000-image ImageNet validation subset against the full validation set’s\nstatistics, following baselines (Saharia et al., 2022; Song et al., 2023).\nInpainting. For the image inpainting task on ImageNet at 256 × 256 resolution, we utilize a fixed 128 × 128 centrally\npositioned mask, aligning with the methodologies of DBIM (Zheng et al., 2024) and CDBM (He et al., 2024). During\ntraining, the model is trained only on the masked regions, while during generation, the unmasked areas are deterministically\nretained from the initial corrupted image xT to preserve structural fidelity of unmasked part of images. We trained the model\nwith 4 NFEs via the multistep method (Section 3.5) and tested it with 1, 2, and 4 NFEs.\nB.2. Distillation of DDBM models.\nWe extended the DDBM repository (Table 8) by integrating our distillation framework. Subsequent sections outline the\nexperimental setups, adapted from the DDBM (Zheng et al., 2024).\nMultistep implementation In this setup, the multistep training (Section 3.5) adopts the methodology of DMD (Yin et al.,\n2024a), wherein a timestep t is uniformly sampled from the predefined sequence (t1, . . . , tN). The model Gθ then generates\n14\n\nInverse Bridge Matching Distillation\nx0 by iteratively reversing the process from the terminal timestep tN = T to the sampled intermediate timestep t. This\ngenerated x0 is subsequently used to compute the bridge network’s loss bLϕ or the student network’s loss bLθ, as detailed in\nAlgorithm 1.\nEdges →Handbags The model was trained utilizing the Edges→Handbags image-to-image translation task (Isola et al.,\n2017), with the 64 × 64 resolution images. Two versions were trained under the multistep regime (Section 3.5), with 2 and 1\nNFEs during training. Both models were evaluated using the same NFE to match training settings.\nDIODE-Outdoor Following prior work (Zhou et al., 2023; Zheng et al., 2024; He et al., 2024), we used the DIODE outdoor\ndataset, preprocessed via the DBIM repository’s script for training/test sets (Table 8). Two versions were trained under the\nmultistep regime (Section 3.5), with 2 and 1 NFEs during training. Both models were evaluated using the same NFE to\nmatch training settings.\nInpainting All setups matched those in Section B.1 inpainting, except we use a CBDM checkpoint (Zheng et al., 2024).\nThis checkpoint is adjusted by the authors to: (1) condition on xT and (2) ImageNet class labels as input to guide the model.\nAlso this is the same checkpoint used in both CDBM (He et al., 2024) and DBIM (Zheng et al., 2024) works.\nC. Additional results\n15\n\nInverse Bridge Matching Distillation\nFigure 4. Uncurated samples for IBMD-I2SB distillation of 4x-super-resolution with bicubic kernel on ImageNet 256 × 256 images.\n16\n\nInverse Bridge Matching Distillation\nFigure 5. Uncurated samples for IBMD-I2SB distillation of 4x-super-resolution with pool kernel on ImageNet 256 × 256 images.\n17\n\nInverse Bridge Matching Distillation\nFigure 6. Uncurated samples for IBMD-I2SB distillation of Jpeg restoration with QF=5 on ImageNet 256 × 256 images.\n18\n\nInverse Bridge Matching Distillation\nFigure 7. Uncurated samples for IBMD-I2SB distillation of Jpeg restoration with QF=10 on ImageNet 256 × 256 images.\n19\n\nInverse Bridge Matching Distillation\nFigure 8. Uncurated samples for IBMD-I2SB distillation trained for inpaiting with NFE= 4 and inferenced with different inference NFE\non ImageNet 256 × 256 images.\n20\n\nInverse Bridge Matching Distillation\nFigure 9. Uncurated samples for IBMD-DDBM distillation trained for inpaiting with NFE= 4 and inferenced with different inference\nNFE on ImageNet 256 × 256 images.\n21\n\nInverse Bridge Matching Distillation\nFigure 10. Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset (256 × 256) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the training set.22\n\nInverse Bridge Matching Distillation\nFigure 11. Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset (256 × 256) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the test set.\n23\n\nInverse Bridge Matching Distillation\nFigure 12. Uncurated samples from IBMD-DDBM distillation trained on the Edges →Handbags dataset (64 × 64) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the training set.24\n\nInverse Bridge Matching Distillation\nFigure 13. Uncurated samples from IBMD-DDBM distillation trained on the Edges →Handbags dataset (64 × 64) with NFE= 2 and\nNFE= 1, inferred using the corresponding NFEs on the test set.\n25'),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content='1. Introduction\nRecent advancements in multi-modal foundation models\nhave been largely driven by the availability of large-scale\npaired text-image datasets. These datasets, often collected\nvia web crawling with basic filtering techniques [14, 52,\n53], contain low-quality, web-sourced captions that lack\ndepth and accuracy. In contrast, human-annotated caption\ndatasets, such as COCO-caption [6], offer higher-quality\ndescriptions but are limited in scale and tend to be con-\ncise, with an average caption length of 10 words. To over-\ncome the limitations of short captions, the research commu-\nnity has leveraged vision-language models (VLMs) [5, 31,\n32, 38, 60] to generate detailed synthetic captions. While\nthese machine-generated captions improve visual under-\nstanding [5, 32] and generation tasks [31], they remain in-\nferior to high-quality, human-verified annotations [44].\nAddressing this challenge requires balancing scalability\nand annotation quality, as generating detailed and accurate\nimage descriptions at scale remains labor-intensive [15, 44].\nIn this paper, we introduce an efficient annotation approach\nthat combines dense mask annotations with commercial\nVLMs [5] to produce high-quality image captions. Our goal\nis to minimize human effort while generating rich, struc-\ntured descriptions.\nTo achieve this, we base our work on the COCO-caption\ndataset [6] due to its widespread use and diverse image con-\ntent. We revisit the COCO-caption dataset to provide more\ndetailed and comprehensive caption annotations. Our ap-\nproach involves creating holistic captions synthesized from\nregion-based dense captions that describe distinct areas\nwithin each image. Specifically, we build on recent CO-\nCONut panoptic segmentation annotations [9] to generate a\nnew set of detailed captions by: (a) annotating each segmen-\ntation region with a VLM-generated draft, carefully refined\nthrough human corrections, and (b) summarizing these re-\ngion captions into a comprehensive image caption while\npreserving the grounding correspondence between image\nmasks and object references. This enables a novel task that\nintegrates panoptic segmentation with grounded captioning.\nOur structured annotation process ensures that the captions\nare both complete, covering the majority of objects in each\nimage, and grounded, with precise segmentation masks.\nThe final dataset, named COCONut-PanCap, is de-\nsigned for a wide range of vision-language applica-\ntions, combining Panoptic segmentation and grounded\nCaptioning. It comprises 118K image-text pairs for train-\ning, with an average caption length of 203 words, as well as\nan additional 25K image-text pairs, with an average caption\nlength of 233 words for validation. We demonstrate that\nCOCONut-PanCap significantly boosts the performance of\nboth VLM and text-to-image generation models at the in-\nstruction tuning and fine-tuning stages, outperforming re-\ncent detailed caption datasets [44].\nThis highlights the\npotential of our grounding-based captions for both vision-\nlanguage understanding and image generation tasks.\nOur contributions are summarized as follows:\n• We propose a caption annotation pipeline leveraging\npanoptic segmentation to create a high-quality, detailed\ncaption dataset comprising 143K annotated images. The\nresulting annotations are comprehensive, accurate, and\ninclude grounding masks, making this dataset substan-\ntially larger than recent detailed caption datasets.\n• Our COCONut-PanCap dataset facilitates a new chal-\nlenging task combining Panoptic segmentation and\nGrounded Captioning (PGC). We establish evaluation\nmetrics and settings for this PGC task and benchmark sev-\neral recent methods to assess performance on this novel\nchallenge.\n• We validate the utility of our proposed dataset across var-\nious fine-grained Image-to-Text (I2T) and Text-to-Image\n(T2I) tasks, including detailed caption generation, PGC,\nvisual question answering (VQA), referring segmenta-\ntion, and text-conditioned image generation. Experimen-\ntal results show that our dataset significantly enhances\nmodel performance across all these tasks.\n2. Related Work\nDetailed Captions from VLMs. Researchers are increas-\ningly interested in creating large-scale datasets with detailed\ncaptions generated from advanced vision-language models.\nDenseFusion1M [32] utilizes a pretrained perceptual model\nto prompt VLMs, facilitating more detailed image descrip-\ntions.\nRecap-DataComp1B [31] first fine-tunes the Llama-3-\n8B powered LLaVA-1.5 model [36], then applies it to recap-\ntion approximately 1.3 billion images from the DataComp-\n1B dataset [14], generating a rich repository of detailed\nimage descriptions.\nOn a similar front, the PixelProse\ndataset [59] offers general-purpose image captions designed\nto serve various applications, from visual question answer-\ning (VQA) to pre-training tasks. Unlike datasets targeting\nsingle applications, PixelProse captions are dense, versa-\ntile image descriptions that can be adapted to other formats,\nsuch as VQA and instructional data, with the help of large\nlanguage models (LLMs).\nAlthough these detailed cap-\ntion datasets are large-scale, they are directly generated by\nVLMs without human verification, falling behind human-\nannotated captions on quality. Our proposed COCONut-\nPanCap dataset leverages extensive human effort to ensure\nhigh-quality annotations.\nHuman-annotated Detailed Captions.\nSeveral efforts\nhave been made toward this goal, utilizing fully human-\n\nDataset Name\nImage Source Sample\nAnnotated by Avg. Words Masks\nBLIP-LCS\nLAION [53], CC [4], SBU [45]\n558K\nBLIP [30]\n54\n✗\nDenseFusion1M [32]\nLAION [53] 1,059K Vision Specialist Models\n191\n✗\nLLaVA-Recap118K [38]\nCOCO [35]\n118K\nLLaVA-NEXT [38]\n186\n✗\nLLaVA-Details-23K [37]\nCOCO [35]\n23K\nGPT4\n105\n✗\nShareGPT4V [5]\nLAION [53], CC [4], SBU [45], COCO [35] etc.\n100K\nGPT4-Vision\n162\n✗\nShareGPT4V-PT [5]\nLAION [53], CC [4], SBU [45], COCO [35] etc. 1,246K\nShare-Captioner [5]\n144\n✗\nPixelLM-MUSE [51]\nLVIS [17]\n246K\nGPT4-Vision\n-\n3.7‡\nOsprey [69]\nCOCO [35]\n724K\nGPT4-Vision\n-\n-\nGLaMM-GCG [50]\nRefCOCOg [40],PSG [65],Flick30K [47]\n214K\nVision Specialist Models\n128\n3.6\nCOCO-caption [6]\nCOCO [35]\n118K\nHuman\n11\n✗\nDCI [61]\nSA-1B [24]\n8K\nHuman\n144\n✗\nDOCCI [44]\nDOCCI [44]\n9.6K\nHuman\n136\n✗\nIIW [15]\nWebLI [15]\n8.5K\nHuman\n217\n✗\nCOCONut-PanCap (ours)\nCOCO [35]\n118K\nHuman\n203\n13.2\nTable 1. Dataset (training set) Comparison. Our proposed COCONut-PanCap dataset stands out for its detailed (2nd highest in Average\nWords), high-quality (human interactive annotated) captions and high-density segmentation masks (1st in Average Masks). ‡ denotes the\nmask number for referring segmentation which only counts the targets in QA format. Note that “Samples” means the number of collected\nannotations, where there may exist one image with multiple different annotation, i.e., in region-level datasets like Osprey.\nDataset Name\nSamples Avg. Words Caption T2I Grd. Seg.\nCOCO-30K [6]\n30,000\n11\n✓\n✓\n✗\nDOCCI-test [44]\n5,000\n136\n✓\n✓\n✗\nIIW-test [15]\n445\n217\n✓\n✓\n✗\nGenEval [16]\n553\n8\n✗\n✓\n✗\nT2I-CompBench val [20]\n2400\n9\n✗\n✓\n✗\nGLaMM-GCG val-test [50]\n2,000\n128\n✓\n✗\n✓\nCOCONut-PanCap val (ours)\n25,000\n233\n✓\n✓\n✓\nTable 2. Dataset (evaluation set) Comparison. Our COCONut-\nPanCap validation set provides detailed captions and supports\nmultiple multi-modal tasks, including image captioning, text-to-\nimage generation (T2I), and grounded segmentation (Grd. Seg.).\nannotated data or human-in-the-loop approaches. One ex-\nample is DOCCI [44] which is a small, high-detailed image\ncaption dataset that is entirely human-annotated, contain-\ning only 15K samples but providing diverse details, such as\nkey objects, their attributes, spatial relationships, and text\nrendering. Two small-scale detailed caption datasets, Im-\nageInWords [15] and DCI [61], use a combination of au-\ntomatic annotation models with human involvement, both\nwith fewer than 10K samples. Pixmo-Cap [8] introduces a\nlarge-scale dataset of detailed image captions from speech-\nbased descriptions, offering richer visual annotations than\ntext-based methods.\nOur proposed COCONut-PanCap dataset yields smaller\nscale compare to Pixmo-Cap but we have different focuses\nwhere Pixmo-Cap focuses on pretraining the VLMs while\nwe focus on the instruction tuning and finetuning stages of\nVLMs and image generation models. Our work also shares\na similar annotation pipeline with a recent video captioning\ndataset Shot2Story [18] where both VLM draft and human\ncorrections are used to create complete and accurate anno-\ntations.\nGrounded Captions with Segmentation Masks. Exist-\ning work have made significant strides in creating datasets\nwith region-level captions linked to entity segmentation\nmasks [69] or bounding boxes [70].\nHowever, few\ndatasets associate grounded segmentation directly with\ncaptions.\nGLaMM [50] proposes a Grounding-anything\nDataset (GranD) using an automated annotation pipeline\nthat encompasses 7.5M unique concepts grounded in a total\nof 810M regions available with segmentation masks.\nLater,\nMGLMM\n[72]\nfurther\nexplore\nthe\nmulti-\ngranularity GLaMM model to generate a multi-granularity\ndataset. Our proposed COCONut-PanCap dataset follows\na similar approach of grounding captions to dense masks\nbut offers significantly denser masks per caption, as shown\nin Tab. 1, with an average of 13.2 masks per image com-\npared to 3.6 in GLaMM. Note that we focus on grounded\nsegmentation for detailed captions, rather than descriptions\nof all levels of segmentation masks (objects or parts) as pro-\nvided in the GranD dataset [50], which is outside the scope\nof our study.\n3. COCONut-PanCap Dataset\nWe construct a novel dataset based on COCO images to pro-\nvide detailed captions at both image and mask levels, using\nCOCONut panoptic masks as a foundation for comprehen-\nsive region descriptions. Specifically, we leverage panop-\ntic masks from COCONut-S [9] to annotate detailed region\ncaptions, incorporating both ‘thing’ and ‘stuff’ masks to\ncover a wide range of semantic regions.\n3.1. Dataset Description\nComprehensively understanding diverse visual elements in\ncomplex scenes can benefit multiple tasks including percep-\ntion, understanding, and generation. In this section, we de-\nscribe the annotation pipeline for our dataset leveraging the\nhuman annotated panoptic masks. We first show the sta-\n\ninput image\nhuman annotated\npanoptic segmentation mask\noverlaid image\nVLM\nset-of-marks\nprompt instruc4on\n0: road – The ground surface in the \nforeground, labeled as the road.1: wall-\nother-merged – A section of the wall on the \nleft side of the image, with a mural or design \npainted on it, including part of a blue \noctopus.2: window-other – A window \nlocated on the upper right side of the image, \nwhich has been labeled separately.\n…………\n t, also wearing a hat and purple clothing, \npossibly engaged in conversation.11: chair – \nThe chair that the first person is sitting \non.12: bicycle – A bicycle is located near the \ncenter of the image, leaning against the \nwall.13: handbag – A handbag or bag is near \nthe first person or placed beside the \nchair.14: chair – Another chair, possibly for \nthe second person sitting in the image.\ndetailed cap4on dra5\nhuman\nedit\nsummarize by VLM,\nveriﬁca4on check by\nhuman\nround-2\nround-1\nFigure 2. Annotation Pipeline. Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks [66]\nvisualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for\nediting and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning\nor finetuning stage.\n: Could you please describe the details of the image for each region?\n:\nFrom your image, here is the analysis:\n0: Sky - Clear blue sky with some scattered clouds.\n1: Road - Asphalt road, smooth and unobstructed, ideal for vehicles.\n2: Building-other-merged - Building structure with large glass windows \nand beige walls, likely a commercial or public building.\n3: Tree-merged - Lush green trees with dense foliage, possibly \nornamental in a landscaped area.\n4: Pavement-merged - Grey concrete pavement, typically used for \npedestrian pathways.\n5: Stop sign - Red octagonal traffic stop sign with white text, indicating a \nvehicle stop required.\n6: Car - A car in motion, likely in the process of slowing down as it \napproaches the stop sign.\n7: Truck - A white truck, stationary, parked in a designated area, possibly \nfor loading or unloading goods.\nWith the given region-level description, can you merge them into\none paragraph in the format of grounded caption?\nIn this scene, the clear blue sky with some scattered clouds \nserves as a pleasant backdrop. The smooth asphalt road \nprovides an unobstructed pathway, suitable for vehicles. \nNearby, there is a building structure with glass windows and \nbeige walls. Lush green trees with dense foliage add a touch \nof nature. The grey concrete pavement offers a pedestrian-\nfriendly path alongside the road. A upside\ndown\nred \noctagonal traffic stop sign with white text signals vehicles to \nhalt. Approaching the stop sign is a car, likely slowing down. In \nthe background, a stationary white truck is parked in a \ndesignated area.\nis upside down\nis approaching\ncommercial\nx\nloading or unloading goods\nx\n: add\n: remove\nfound hallucination and remove\nx\nhuman edit legend:\nround-1\nround-2\nFigure 3. Designed Prompt Template. By giving the concatenated set-of-marks images, the right side (round-1) shows the initial response\nand the corresponding human edits. Once finalized by humans, these edits will be merged into a single detailed caption grounded with\npanoptic segmentation masks, as shown in the left side (round-2).\ntistical analysis of our final dataset in Tab. 1. On average,\nour captions contain 203 words spanning 11 sentences. We\nfollow the same split setting in COCO2017 [35] dataset,\nwhich includes 118K training images. To provide a com-\nprehensive evaluation set, we adopt the same 25K images\nfrom COCONut-val split (which contains COCO2017-val\nand another 20K Objects365 [55] validation images).\n3.2. Dataset Construction\nWe argue that high-quality descriptions should provide suf-\nficient details of key objects and their attributes, as well as\ninformation about secondary objects and background ele-\nments. To achieve this, as shown in Fig. 2, we use human-\nannotated panoptic segmentation masks to decide the set of\nobjects to reference in the caption. These masks include\nboth ‘thing’ and ‘stuff’ classes, representing single objects\nand semantic regions, respectively. We adopt the panop-\ntic segmentation masks from the COCONut-S [9] dataset.\nThe masks are overlaid on the images, labeled with class\nnames c1, c2, . . . , cn ∈C, where C is the set of COCO’s\n133 panoptic classes. We then construct a prompt with both\nthe edited image and the original image and a textual ques-\ntion for GPT-4V, as illustrated in Fig. 3. The resulting re-\ngion captions from GPT-4V are reviewed and corrected by\nhuman raters for accuracy and consistency.\n\nFigure 4. Frequency of Extracted Nouns from the COCONut-\nPanCap Dataset. The top 10 most frequent nouns are: people,\ntable, room, street, dining, man, person, cars, chairs, and field.\n3.3. Dataset Analysis\nConcepts Beyond COCO’s 133 Classes. To clarify the\ngoal of our annotation task, we focus on key visual features\nsuch as objects, attributes, spatial relationships, and count-\ning. As shown in Fig. 4, we utilize the panoptic segmenta-\ntion mask from COCONut-S, which includes 133 classes in\nthe word vocabulary. Our proposed dataset, however, incor-\nporates additional concepts beyond these 133 classes, such\nas ‘vegetable’ and ‘parking’. This demonstrates that our hu-\nman annotators delivers accurate and diverse descriptions\nwhen using the provided label names as a reference.\nUser Study for Caption Quality. We randomly sample\n1,000 images from our COCONut-PanCap training set and\nasked a human evaluator to perform a single-choice selec-\ntion task. The question is: ‘Please select the best descrip-\ntion for the image, considering the correctness of object\nnames, attributes, counting, spatial relationships, and ac-\ntion.’\nThe compared captions are generated using GPT-\n4V [1], Qwen2-VL [64], and InternVL-2 [7], resulting in a\nsingle-choice four-option question. Fig. 5 illustrates the re-\nsults, showing that our GPT-assisted human-annotated cap-\ntions receives the highest ratings. More details can be found\nin the supplementary.\n4. PGC Baseline: PanCaper\nIn this section, we introduce our baseline method for joint\npanoptic segmentation and grounded captioning (PGC),\nnamely PanCaper. We start with an overview of the pixel\ngrounding task and then present our proposed approach,\nwhich incorporates a panoptic segmentation module specif-\nically designed for grounding objects in captions.\nRevisiting the Pixel Grounding Task. Our baseline model\nbuilds upon LISA [28], a model that combines the lan-\nguage generation capabilities of VLMs with the ability to\nproduce segmentation mask. LISA consists of three main\ncomponents: a VLM, a vision backbone V , and a mask de-\ncoder D. With a given text prompt, the VLM (typically\nLLaVA [36, 37]) generates an output containing a ⟨SEG⟩\ntoken. For instance, with the input prompt, ‘Could you seg-\nment the food with high Vitamin C?’ LISA generates the\nresponse ‘It is ⟨SEG⟩.’ This process extracts the last-layer\nFigure 5. Caption Quality via User Study. The study involved\nhuman evaluators assessing a random sample of 1,000 captions,\nwith a strong preference shown for captions from our dataset.\nembedding of the LLM from LLaVA. Then a language-to-\nprompt (L-P) projection layer (g) transforms the last-layer\nembeddings corresponding to ⟨SEG⟩tokens (lseg) into the\ndecoder’s feature space. Meanwhile, the vision backbone\nextracts dense visual features from the input image.\nFi-\nnally, both the dense features and the CLIP image embed-\nding from LLaVA are fed into the mask decoder to produce\nthe final segmentation mask.\nPrompt Instruction for Grounded Captioning. We pro-\npose a baseline method for the PGC task by modifying\nLISA to enable grounded captioning with segmentation\nmasks. Since LISA was originally designed for generat-\ning segmentation with a single output mask, two main ad-\njustments are necessary: (1) the use of multiple ⟨SEG⟩to-\nkens, and (2) extracting noun phrases from the caption for\ngrounding. To facilitate grounded segmentation, we modify\nthe prompt to the VLM as ‘Please provide a detailed de-\nscription of the image and segment each part.’ This prompt\ntriggers the model to generate caption responses with cor-\nresponding ⟨SEGi⟩tokens, where i ∈[1, N] and N is the\ntotal number of predicted segmentations.\nGiven a predicted caption for the image, aligning each\n⟨SEGi⟩token requires pairing it with a noun phrase,\n‘⟨p⟩phrasei⟨/p⟩,’ where phrasei is the relevant part in the\ncaption to be grounded. With these prompt tokens defined,\nthe model uses the vision backbone V and mask decoder\nD to facilitate fine-grained, pixel-level grounding, with D\nproducing segmentation masks M.\nEnable Panoptic Grounding. To achieve panoptic seg-\nmentation from captions, we first classify ⟨SEG⟩tokens\ninto two types: ⟨SEGt⟩for ‘thing’ classes and ⟨SEGs⟩\nfor ‘stuff’ classes.\nThese tokens are then processed by\nour segmentation modules to produce panoptic segmenta-\ntion masks. We initialize the vision backbone V with a\npretrained kMaX-DeepLab encoder [67] and fine-tune the\ndecoder D using our COCONut-PanCap dataset.\nSince\nkMaX-DeepLab operates as a closed-set segmenter, we\n\nalign text embeddings of the associated noun phrases with\nCOCO’s 133 panoptic classes. To accomplish this align-\nment, we use BERT [26] to generate the text embed-\ndings and to calculate cosine similarity, selecting the best-\nmatching category. Panoptic grounding provides mapping\nbetween detailed captions and image regions, which im-\nproves interpretability of VLM predictions.\nTraining Objectives. Our training objective aims to mini-\nmize the following losses:\nL = λtextLtext + λmaskLmask,\n(1)\nwhere Ltext is the auto-regressive cross-entropy loss for text\ngeneration, and Lmask is the mask loss [63], encouraging the\nmodel to produce high-quality segmentation results. λtext\nand λmask are the respective loss weights. We use the same\nloss weights as LISA [28].\nEvaluation Metrics for Caption Quality. We conduct the\nanalysis with multiple metrics to evaluate the quality and\ncompleteness of the generated captions. We introduce a\nbenchmarking suite for the PGC task, with a validation set\nof 25K images. For the caption quality, we report the cap-\ntion metrics including CIDEr [62], METEOR [2], ROUGE-\nL [34], BLEU@4 [46] and CAPTURE [10]. For grounded\npanoptic segmentation, we report PQ scores [23].\n5. Experimental Results\nWe assess the effectiveness of human-annotated caption\ndata by performing three primary tasks utilizing our dataset\nin the fine-tuning/instruction tuning stage: detailed cap-\ntioning, panoptic grounded captioning (PGC), and text-to-\nimage generation. Additionally, we demonstrate the trans-\nferability of the knowledge learned from our dataset through\ntwo downstream tasks: VQA and referring segmentation.\nDetailed Captioning. We conduct instruction tuning with\nLLaVA-NeXT framework [38] for this task. We replace\nthe caption data (23k) from the original LLaVA instruction-\ntuning set with detailed captions from our dataset, keep-\ning the same amount of instruction data size.\nWe fol-\nlow the same training setup used for LLaVA-NeXT with\nLlama3-8B [11].\nTreating it as a QA task, we use the\nprompt, ‘Could you please describe the image in detail?’\nand collect the corresponding response as the caption for\nthe image. We evaluate caption quality using CIDEr [62],\nMETEOR [2], BLEU@4 [46], ROUGE-L [34] and CAP-\nTURE [10] metrics. We also extend the model by adding\nthe mask-pooled features from the panoptic segmentation\nmasks as additional signals to the LLaVA model and name\nit LLaVA-NeXT-pool. During training, we use the ground\ntruth mask to extract the features while during inference\nwe use the mask proposals from the pretrained kMaX-\nDeepLab [67]. Besides, we also experiment with synthetic\ncaptions directly generated using InternVL-2 [7], Qwen2-\nVL [64] and GPT-4V [1]. We follow the same data prepara-\ntion settings as our dataset to build these instruction datasets\nfor these 23K images with different sources of synthetic\ndetailed captions, namely LLaVA 665K-InternVL2-Cap ,\nLLaVA 665K-Qwen2VL-Cap, and LLaVA 665K-GPT4V-\nCap. These datasets are used to produce models LLaVA-\nNeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G respec-\ntively.\nMore details can be found in the supplementary.\nThe results are presented in Tab. 3. LLaVA-NeXT models\nshow improved performance when fine-tuned on the custom\ninstruction-tuning dataset.\nAmong these, LLaVA-NeXT-\npool achieves the highest scores in all metrics, with CAP-\nTURE of 61.4, CIDEr of 13.1, BLEU@4 of 5.3, and ME-\nTEOR of 17.1, significantly higher than the original model\nvariant LLaVA-NeXT, indicating the benefit of added re-\ngion features for additional visual cues. Models trained on\nsynthetic captions (LLaVA-NeXT-I, LLaVA-NeXT-Q, and\nLLaVA-NeXT-G) generally show lower scores, showing ad-\nvantage of our human-annotated caption.\nPGC: Stronger Detail Reasoning Performance. We im-\nplement our proposed PanCaper based on LISA which uses\npre-trained LLaVA-NeXT with a LLM of Llama3-8B, with\nLoRA [19] adopted. The vision encoder uses a fixed CLIP-\nViT-L/14-336 model, modified with linearly interpolated\nposition embeddings to process 448 resolution images. The\ntrainable components of our model include the mask de-\ncoder of kMaX-DeepLab, and the tunable parts in LLaVA\nsame as in LISA. To enhance model performance in visual\nunderstanding, we initialize our PanCaper using pretrained\nLLaVA-NeXT models from the detailed captioning task.\nWe also experiment with a model variant that uses mask\npooled features similar to LLaVA-NeXT-pool, and name it\nPanCaper-Pro.\nFor comparison, we select 3 related methods LISA, Pix-\nelLM [51] and GLaMM [50] for evaluation. It is notewor-\nthy that LISA is not able to perform multi-mask prediction.\nWe therefore adapt LISA [28] for the multi-mask generation\nwith grounded segmentation, namely LISA+. The imple-\nmentation details can be found in the supplementary. Tab. 4\nshows the quantitative results.\nOur proposed PanCaper-\nPro achieves the highest scores across all captioning met-\nrics (CIDEr: 12.5, CAPTURE: 64.3, BLEU@4: 6.4, ME-\nTEOR: 17.9), outperforming all other models. Both PanCa-\nper models show significant improvements over other mod-\nels in all captioning metrics, highlighting the effectiveness\nof the COCONut-PanCap dataset for detailed caption gen-\neration. On grounding segmentation, PanCaper-Pro again\nleads, with a PQ score of 0.61, PQthing of 0.58, and PQstuff\nof 0.68, reflecting its robustness on both “thing” and “stuff”\nclasses. Notably, enabling mask pooling in our proposed\nPanCaper-Pro further enhances segmentation metrics. The\nbaseline models (LISA+ and GLaMM with GranD) achieve\nmuch lower PQ scores, due to incomplete segmentation an-\n\nTraining recipe Method\nPretrain Dataset Instruction-tuning dataset\nMask pooled CAPTURE CIDEr BLEU@4 METEOR ROUGE-L\nfinetune\nLLaVA-NeXT*\nLAION-CC-SBU LLaVA 665K\n✗\n55.4\n10.8\n4.2\n13.2\n23.1\nLLaVA-NeXT\nLAION-CC-SBU LLaVA 665K-COCONut-PanCap\n✗\n58.7\n11.2\n4.8\n16.2\n24.6\nLLaVA-NeXT-pool LAION-CC-SBU LLaVA 665K-COCONut-PanCap\n✓\n61.4\n13.1\n5.3\n17.1\n26.8\nLLaVA-NeXT-I\nLAION-CC-SBU LLaVA 665K-InternVL2-Cap\n✗\n53.9\n9.4\n4.4\n11.5\n21.4\nLLaVA-NeXT-Q\nLAION-CC-SBU LLaVA 665K-Qwen2VL-Cap\n✗\n55.4\n8.9\n4.6\n12.9\n22.5\nLLaVA-NeXT-G\nLAION-CC-SBU LLaVA 665K-GPT4V-Cap\n✗\n56.2\n9.6\n4.7\n13.3\n22.8\nTable 3. Caption Benchmark Results Evaluated on Our COCONut-PanCap Val Set. Note that the amount of data in the instruction\ndataset remains the same; only the sources of the detailed captions vary, with a total of 23K images that have detailed captions.\nCaption\nGrounding segmentation\nMethod\nPretrain dataset\nInstruction dataset\nMask pooled CAPTURE CIDEr BLEU@4 METEOR PQ PQthing\nPQstuff\nLISA+ *\nLAION-CC-SBU\nGranDf\n✗\n46.2\n6.6\n3.8\n9.8\n0.43\n0.41\n0.45\nLISA+\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✗\n57.9\n8.1\n4.9\n13.8\n0.50\n0.49\n0.44\nGLaMM GCG *\nLAION-CC-SBU+GranD\nGranDf\n✗\n43.2\n6.5\n3.6\n10.6\n0.27\n0.35\n0.21\nGLaMM GCG\nLAION-CC-SBU+GranD COCONut-PanCap (ours)\n✗\n56.8\n7.8\n5.2\n14.3\n0.55\n0.54\n0.46\nPanCaper (ours)\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✗\n62.6\n12.0\n5.8\n15.4\n0.56\n0.55\n0.66\nPanCaper-Pro (ours)\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✓\n64.3\n12.5\n6.4\n17.9\n0.61\n0.58\n0.68\nTable 4. Joint Panoptic Segmentation and Grounded Captioning (PGC) on COCONut-PanCap Val Set. * denotes reproduced results.\nTraining dataset\nEvaluation dataset FID↓\nFDdinov2 ↓CLIPScore↑\nSD3 PT dataset [12]\nDOCCI test set [44]\n30.2\n345\n74.9\nCOCO-caption [6]\n27.6\n321\n76.8\nDOCCI [44]\n22.1\n300\n77.8\nCOCONut-PanCap (ours)\n21.4\n290\n77.9\nSD3 PT dataset [12]\n31.8\n300\n73.8\nCOCO-caption [6]\nCOCONut-PanCap\n28.0\n294\n74.0\nDOCCI [44]\nval set (ours)\n24.3\n267\n75.1\nCOCONut-PanCap (ours)\n23.1\n260\n77.3\nTable 5. Benchmark Results on Text Conditioned Image Gen-\neration.\nStable-Diffusion-3 (SD3) medium is finetuned with\nCOCO-Caption (short), DOCCI and our COCONut-Panoptic and\nevaluated on DOCCI test set [44] and our COCONut-PanCap val\nset. ‘SD3 PT dataset’ denotes the pretraining dataset of SD3, and\nthus the rows correspond to zero-shot evaluation of SD3.\nw/o FT COCO-caption [6] DOCCI [44] COCONut-PanCap\ncolor attribution\n0.37\n0.34\n0.38\n0.40\ncolors\n0.73\n0.70\n0.74\n0.75\nposition\n0.33\n0.30\n0.36\n0.36\ncounting\n0.65\n0.64\n0.65\n0.70\nsingle object\n0.96\n0.94\n0.95\n0.96\ntwo objects\n0.80\n0.78\n0.81\n0.89\noverall score\n0.64\n0.62\n0.65\n0.68\nTable 6. Effects of Fine-tuning the SD3-medium (T2I model)\nwith Different Datasets on GenEval [16]. w/o FT denotes the\nmodel is not finetuned with any datasets (i.e., zero-shot testing).\nnotations in the GranD dataset.\nText-to-Image Generation. We adopt the Stable Diffusion\n3 (SD3) medium model1 for text to image generation with\nLoRA finetuning. We adopt the default training settings\nbut only with different text-image datasets for training. We\nevaluate with two types of training images from COCO [35]\nand DOCCI [44] datasets. In details, for the COCO images,\n1https://huggingface.co/docs/diffusers/stable diffusion/stable diffusion 3\nwe explore the short COCO-caption and detailed captions\nfrom our dataset. For DOCCI images, we directly use the\ncaptions from their dataset. Tab. 5 shows the quantitative\nresults. Traning on COCONut-PanCap achieves the best\nperformance across all metrics when evaluated on DOCCI-\ntest, with the lowest FID (21.4), lowest FDdinov2 (290), and\nthe highest CLIPScore (77.9), indicating superior genera-\ntion quality and high image-text relevance. When evalu-\nated on COCONut-PanCap-val set, training on COCONut-\nPanCap again shows the best results with the lowest FID\n(23.1), FDdinov2 (267), and a high CLIPScore of 77.3.\nTab. 6 shows the results on GenEval benchmark [16].\nFinetuning SD3-medium with COCONut-PanCap consis-\ntently scores the highest in most categories, particularly\nthose requiring image details like color attribution, object\npositioning, and handling multiple objects. Our proposed\ndataset enables more accurate image generation that re-\nquires understanding of relationships, multiple objects and\ncounting, tasks that other datasets struggle with.\nVQA. To evaluate the effectiveness of the proposed\nCOCONut-PanCap dataset, we utilize these captions during\nthe instruction-tuning stage and follow the setup of LLaVA-\nNeXT [38] across various visual question answering (VQA)\nand multi-modality understanding benchmarks. We evalu-\nate on MM-Vet [68], SEED-IMG [29], MMBench-en [39],\nMME [13], POPE [33], and TextVQA [58], covering a\nbroad range of evaluation dimensions. We experiment with\ndifferent amount of our COCONut-PanCap caption data in-\njected into the instruction tuning stage by replacing the orig-\ninal COCO captioning data with our dataset. As shown in\nTab. 7, the baseline model LLaVA-NeXT (using its orig-\ninal recaptioned COCO) achieves relatively lower perfor-\nmance across all metrics, with scores such as 43.5 on MM-\nVet, 70.1 on Seed-IMG, and 68.9 on TextVQA. Building\n\nMethod\nLLM\nInstruction-tuning Dataset\nMM-Vet Seed-IMG MMBench-en TextVQA POPE MME\nLLaVA-NeXT *\nLlama3-8B orginal LLaVA 665K [38]\n43.5\n70.1\n71.4\n68.9\n85.4\n1523\nLLaVA-NeXT-20K Llama3-8B LLaVA 665K-COCONut-PanCap-20K\n44.1\n72.5\n73.6\n69.8\n86.1\n1552\nLLaVA-NeXT-50K Llama3-8B LLaVA 665K-COCONut-PanCap-50K\n44.6\n73.1\n74.2\n70.0\n87.1\n1600\nLLaVA-NeXT-Full Llama3-8B LLaVA 665K-COCONut-PanCap-118K\n45.5\n74.3\n75.1\n70.7\n87.9\n1612\nLLaVA-1.5\nVicuna-7B LLaVA 665K-ShareGPT4V-100K\n37.8\n67.4\n70.5\n64.6\n84.7\n1519\nLLaVA-1.5\nVicuna-7B LLaVA 665K-COCONut-PanCap-20K\n38.5\n67.7\n70.9\n64.5\n84.9\n1521\nTable 7. Benchmark Results and Ablation Study on VQA. By adding extra detailed caption data for instruction tuning, the models show\nincreased improvement. * denotes reproduced results. Using only 20K human labeled data can still achieve comparable performance\nto 100K synthetic data.\nMethod\nrefCOCO\nrefCOCO+\nrefCOCOg\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nGLaMM* [50]\n77.5\n79.2\n74.9\n71.3\n74.7\n61.5\n71.3\n71.9\nPixelLM [51]\n73.0\n76.5\n68.2\n66.3\n71.7\n58.3\n69.3\n70.5\nLISA-7B [28]\n74.1\n76.5\n71.1\n62.4\n67.4\n56.5\n66.4\n68.5\nPanCaper+\n74.5\n76.7\n69.9\n69.9\n73.4\n59.5\n69.8\n70.6\nPanCaper+ + COCONut-PanCap\n76.2\n77.1\n72.3\n70.5\n73.9\n60.1\n72.1\n71.6\nTable 8. Benchmark Results on Referring Segmentation. * denotes reproduced results. It is noted that GLaMM uses extra data from the\nGranD dataset for pretraining. + denotes our PanCaper model is adapted for referring segmentation task.\non LLaVA-NeXT baseline, we progressively incorporated\nvarying amounts of COCONut-PanCap data (20K, 50K, and\n118K (full), as indicated by postfixes in the baseline names)\nduring instruction-tuning. Consistent improvements are ob-\nserved across all evaluated benchmarks as more of our data\nis integrated.\nReferring Segmentation. In this task, the model processes\nan image and a textual referring expression to output a\nsegmentation mask corresponding to the expression. The\nprompt used is, ‘Please segment the ⟨referring text⟩in the\nimage.’ The target model response is ‘Sure, it is ⟨SEG⟩.’,\nwhere the ⟨SEG⟩token is decoded to obtain the mask. We\nfollow the setup in LISA [28], using multiple segmentation\ndatasets to jointly train the models. Tab. 8 shows the quan-\ntitative results. Our model achieves superior performance,\nparticularly when additionally trained with the COCONut-\nPanCap dataset (last row), outperforming all models except\nGLaMM [50]. This improvement underscores our model’s\nefficacy in handling complex referring expressions, likely\ndue to the additional data that enhances model generaliza-\ntion and accuracy. It is worth noting that GLaMM performs\ncompetitively with our method, though the comparison is\nuneven given their additional use of the SA-1B dataset [25].\nSynthetic vs. Human Annotated Data. Generating syn-\nthetic data for captioning has been popular for recent tasks\nin either training vision encoders [48] or text-to-image gen-\neration [31]. We investigate the effect of varying the mix\nratio of synthetic captions generated by GPT-4V and our\nhuman-annotated data for fine-tuning (where 0 indicates\nfully synthetic data), using the COCONut-PanCap dataset\nfor training and the COCONut-PanCap-val set for evalua-\ntion. We adopt LLaVA-NeXT for the captioning task and\nSD3-medium for the image generation task. As shown in\nFig. 6, adding 25% human-annotated data yields significant\nperformance improvements in both captioning and genera-\ntion, with a reduced FID of 26 from 31 (lower is better) and\nan increased CAPTURE score of 53.6 from 47.5 (higher\nis better). Consistent improvements are observed as more\nhuman-annotated data is incorporated.\nFigure 6. Varying Synthetic and Human-Annotation Ratios.\nCAPTURE is used to evaluate the performance of LLaVA-NeXT\non detailed captioning, while FID assesses the performance of\nSD3-medium on text-conditioned image generation.\n6. Conclusion and Discussion\nIn this work, we proposed a novel dataset designed to sup-\nport detailed captioning and grounded segmentation tasks\nbuilt on COCO images. We demonstrated that our dataset\ncan enhance model performance during instruction tuning\nand fine-tuning stages across various multi-modal under-\nstanding and generation tasks, such as captioning, VQA,\ngrounded segmentation, and text-to-image generation. We\n\nhope that COCONut-PanCap, with its detailed captions\ngrounded with dense panoptic masks, will foster future ad-\nvancements in multi-modal learning research.\nLimitations. High-quality human-labeled data offers sig-\nnificant benefits for instruction tuning in multi-modal tasks,\nbut scaling such datasets is challenging. To address this,\nwe introduce COCONut-PanCap as a starting point for\nlarge-scale human-annotated data exploration. Recognizing\nthe relatively smaller dataset size compared to other large\ndataset, future work may involve using this dataset to train\nseed models to generate more high-quality synthetic data.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 5, 6\n[2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In ACL Workshop, 2005. 6\n[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In CVPR, 2018.\n13\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\nIn CVPR,\n2021. 3\n[5] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 2, 3\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 2, 3, 7\n[7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\nwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, et al. How far are we to gpt-4v? closing\nthe gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821, 2024. 5, 6\n[8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tri-\npathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo\nand pixmo: Open weights and open data for state-of-the-art\nmultimodal models. arXiv preprint arXiv:2409.17146, 2024.\n3\n[9] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and\nLiang-Chieh Chen. Coconut: Modernizing coco segmenta-\ntion. In CVPR, 2024. 2, 3, 4\n[10] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang,\nYuan Zhang, and Haoyuan Guo. Benchmarking and improv-\ning detail image caption. arXiv preprint arXiv:2405.19092,\n2024. 6\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024. 6\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn ICML, 2024. 7\n[13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 7\n[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. NeurIPS, 2024. 2\n[15] Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan\nBitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bun-\nner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. Im-\nageinwords: Unlocking hyper-detailed image descriptions.\narXiv preprint arXiv:2405.02793, 2024. 2, 3\n[16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment. In NeurIPS, 2023. 3, 7\n[17] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019. 3\n[18] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng\nWang. Shot2story20k: A new benchmark for comprehen-\nsive understanding of multi-shot videos.\narXiv preprint\narXiv:2311.17043, 2023. 3\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 6, 12\n[20] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\nhui Liu. T2i-compbench: A comprehensive benchmark for\nopen-world compositional text-to-image generation. arXiv\npreprint arXiv: 2307.06350, 2023. 3\n[21] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 13\n[22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In EMNLP, 2014. 13\n[23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll´ar. Panoptic segmentation. In CVPR,\n2019. 6\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 3\n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 8\n\n[26] Mikhail V Koroteev. Bert: a review of applications in nat-\nural language processing and understanding. arXiv preprint\narXiv:2103.11943, 2021. 6\n[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 2017. 13\n[28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta-\ntion via large language model. In CVPR, 2024. 5, 6, 8, 12,\n13\n[29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 7\n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 3\n[31] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen\nZhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu,\nHuangjie Zheng, Yuyin Zhou, and Cihang Xie.\nWhat if\nwe recaption billions of web images with llama-3?\narXiv\npreprint arXiv:2406.08478, 2024. 2, 8\n[32] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xin-\nlong Wang, and Ling-Yu Duan.\nDensefusion-1m: Merg-\ning vision experts for comprehensive multimodal perception.\narXiv preprint arXiv:2407.08303, 2024. 2, 3\n[33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.\n7\n[34] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In ACL Workshop, 2004. 6\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 3, 4, 7\n[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 2, 5, 13\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 3, 5, 12, 13\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 2, 3,\n6, 7, 8, 13\n[39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv:2307.06281, 2023. 7\n[40] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11–20, 2016. 3, 13\n[41] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nCVPR, 2016. 13\n[42] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In CVPR, 2019.\n13\n[43] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In ICDAR, 2019. 13\n[44] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan\nBitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana\nParekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Ja-\nson Baldridge. DOCCI: Descriptions of Connected and Con-\ntrasting Images. In ECCV, 2024. 2, 3, 7\n[45] Vicente Ordonez,\nGirish Kulkarni,\nand Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In NeurIPS, 2011. 3\n[46] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002. 6\n[47] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 8\n[49] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi\nWen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-\nquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts\nand attributes of common objects. In CVPR, 2023. 13\n[50] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel-\nrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M.\nAnwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan.\nGlamm: Pixel grounding large multimodal model. In CVPR,\n2024. 3, 6, 8, 13\n[51] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao,\nDongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel\nreasoning with large multimodal model. In CVPR, 2024. 3,\n6, 8\n[52] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 2\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. NeurIPS, 2022. 2, 3\n[54] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa:\nA\n\nbenchmark for visual question answering using world knowl-\nedge. In ECCV, 2022. 13\n[55] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, 2019. 4\n[56] ShareGPT. ShareGPT. https://sharegpt.com/. 13\n[57] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image captioning\nwith reading comprehension. In ECCV, 2020. 13\n[58] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,\nXinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards\nvqa models that can read. In CVPR, 2019. 7\n[59] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand,\nMayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang,\nAbhinav Bhatele, Gowthami Somepalli, and Tom Goldstein.\nFrom pixels to prose: A large dataset of dense image cap-\ntions. arXiv preprint arXiv:2406.10328, 2024. 2\n[60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 2\n[61] Jack Urbanek,\nFlorian Bordes,\nPietro Astolfi,\nMary\nWilliamson, Vasu Sharma, and Adriana Romero-Soriano. A\npicture is worth more than 77 text tokens: Evaluating clip-\nstyle models on dense captions. In CVPR, 2024. 3\n[62] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In CVPR, 2015. 6\n[63] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 6\n[64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 5, 6\n[65] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\nWayne Zhang, and Ziwei Liu. Panoptic scene graph gen-\neration. In ECCV, 2022. 3\n[66] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan\nLi, and Jianfeng Gao.\nSet-of-mark prompting unleashes\nextraordinary visual grounding in gpt-4v.\narXiv preprint\narXiv:2310.11441, 2023. 4\n[67] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means Mask Transformer. In ECCV, 2022. 5, 6, 12,\n13\n[68] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. In ICML, 2024. 7\n[69] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie\nLuo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel\nunderstanding with visual instruction tuning. In CVPR, 2024.\n3\n[70] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 3\n[71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In CVPR, 2017. 13\n[72] Li Zhou, Xu Yuan, Zenghui Sun, Zikun Zhou, and Jing-\nsong Lan. Instruction-guided multi-granularity segmentation\nand captioning with large multimodal model. arXiv preprint\narXiv:2409.13407, 2024. 3\n\nThe appendix is organized as follows.\n• In Sec. A, we show implementation details for De-\ntailed Captioning (Sec. A.1), Panoptic segmentation and\nGrounded (Sec. A.2), and VQA (Sec. A.3).\n• In Sec. B, we show more visualization examples of our\nproposed COCONut-PanCap dataset (Sec. B.1), and anal-\nysis of the tier cases in our dataset annotation user study\n(Sec. B.2).\nA. Experimental Details\nIn this section, we provide more experimental details for\ndetailed captioning (Sec. A.1), PGC (Sec. A.2), and VQA\n(Sec. A.3).\nA.1. Detailed Captioning\nDetailed Captioning Instruction Dataset Construction.\nThe key step in conducting the experiment is constructing\nthe dataset. The original LLaVA-665K dataset consists of\nLLaVA-158K combined with other VQA datasets. Within\nLLaVA-158K, a subset of detailed captions corresponds to\n23K COCO images. To create our-LLaVA-665K (referred\nto as LLaVA 665K-COCONut-PanCap in the table), we re-\nplace the detailed caption annotations for these 23K COCO\nimages with our annotations. Importantly, the total amount\nof training data remains unchanged (only the captions for\nthese 23K images are updated), ensuring a fair comparison\nof the impact of data quality on model performance.\nSynthetic Annotation for Detailed Caption. To build the\nsynthetic dataset with state-of-the-art VLM, we use three\nmodels, including open-sourced InterVL-2, Qwen2-VL and\nclose-sourced GPT-4V to generate the detailed captions for\nCOCO 118K train set images. We use the same text prompts\nthat is used in LLaVA [37] for prompting the model to cre-\nate the detailed captions.\nLLaVA-NeXT-pool implementation details. Fig. 7 shows\nthe comparison of the original LLaVA-NeXT and our pro-\nposed LLaVA-NeXT-pool. As shown in Fig. 7a, in order to\npreserve the details for the high-resolution images and rep-\nresentations, the original design employs a grid configura-\ntion which can also balance the performance efficiency with\noperational costs.\nThen both the patch-level and image-\nlevel features are later concatenated and sent to the LLM.\nDirectly splitting the image into patches could cause pro-\nlems, for example, in the figure, the upper part of the dog’s\nhead is partitioned into different patches which may result\nin incomplete feature extraction for single object. To over-\ncome this drawback, we propose LLaVA-NeXT-pool to ex-\ntract the dense feature and preserve the object details by uti-\nlizing the panoptic segmentation masks in our COCONut-\nPanCap dataset. Fig. 7b shows the details. Compared to the\noriginal design, LLaVA-NeXT-pool could effectively ex-\ntract the features for the dog in our example. Our design\nsplit\nencode\nflatten\nLLM\nresize\npatch-level\nencode\nflatten\nimage-level\n(a) LLaVA-NeXt-AnyRes\nmask\nencode\nLLM\nresize\nmask-level\nencode\nflatten\nimage-level\n…\nload\nmask-pooling\n…\nconcatenate\n(b) our LLaVA-NeXt-pool\nFigure 7.\nComparison of LLaVA-NeXt and our proposed\nLLaVA-NeXt-pool.\nenables more complete region-level feature extraction and\nis potential in understanding the details better.\nA.2. PGC\nWe provide more implementation details for the proposed\ntask:\nPanoptic segmentation and Grounded Captioning\n(PGC).\nPanCaper Implementation Details.\nWe introduce the\nPanCaper architecture details in this section.\nFollowing\nthe architecture in LISA [28], there are three components\nincluding the vision backbone, mask decoder and multi-\nmodal LLM. Fig. 8 shows the architecture details for Pan-\nCaper. We made modification on the vision backbone, and\nmask decoder part in terms of model architecture. To pre-\nserve the learned knowledge of the pre-trained multimodal\nLLM (i.e., LLaVA-NeXT in our experiments), we leverage\nLoRA [19] to perform efficient fine-tuning, and completely\nfreeze the vision backbone. The mask decoder is fully fine-\ntuned. Additionally, the LLM token embeddings (embed to-\nkens), the LLM head (lm head), and the projection layer are\nalso trainable. The weights of the text generation loss λtext\nand the mask loss λmask are set to 1.0 and 1.0, respectively.\nFor the PQ-style mask loss, we follow the same settings in\nkMaX-DeepLab [67], where it consists of mask-level cross\nentropy loss, dice loss and pixel loss.\nAdapting Baseline Methods for PGC Task. We adopt\nthe same text prompt template to enable the model to per-\nform PGC tasks. For LISA+, we follow the same design\n\npanop%c mask\ndecoder\nimage\nPlease provide a detailed description \nof the image and segment each part.\ntext prompt\nvision\nbackbone\n❄\nmulti-modal \nLLM\n❄\nLoRA\n🔥\nThe images depicts <p:a running\ndog[SEG_t]> …. on <p:the grass [SEG_s]>\n🔥\npredic-on\n🔥: trainable\n❄: fronzen\nFigure 8. Architecture of PanCaper. We utilize a pretrained vision encoder from kMaX-DeepLab [67] as our vision backbone, which\neffectively extracts dense features essential for panoptic segmentation.\nin GLaMM [50] to design the multi entity mask output by\nutilizing the the GranDf dataset. As the intruction dataset\nof GranDf is constructed similarly grounding the phrase in\nthe image-level caption, it will output multiple ⟨SEG⟩to-\nkens. The reasoning results of the number of ⟨SEG⟩tokens\ndecide the number of output entity mask which are often\nbinary masks. As a result, the model can generate a de-\ntailed caption along with interleaved segmentation masks,\nemploying the format “⟨p⟩A man⟨/p⟩⟨SEG⟩...\nnext to\n⟨p⟩a tree⟨/p⟩⟨SEG⟩”. And thus the format of instruction\ndataset is significat in task design. Therefore, we formu-\nlate our dataset as “⟨p⟩A man⟨/p⟩⟨SEGt⟩... next to ⟨p⟩a\ntree⟨/p⟩⟨SEGs⟩”, where ⟨SEGt⟩represents the seg token\nfor instance masks of thing and ⟨SEGs⟩represents for se-\nmantic masks of stuff respectively in panoptic setting. Sim-\nilarly, utilizing the PanCap dataset and special token design,\nGLaMM [50] is able to generate the entity masks with the\ntag of ‘thing’ and ‘stuff’.\nTraining Data Formulation. We adopt the same training\ndata from LISA [28] which comprises mainly three parts,\nall of which are derived from widely-used public datasets.\nThese include 1) Semantic Segmentation datasets includ-\ning ADE20K [71], COCO-Stuff [3], and LVIS-PACO [49]\npart datasets with the generated QA data, 2) Vanilla Re-\nferring Segmentation Datasets: refCOCO, refCOCO+, re-\nfCLEF [22] and refCOCOg [40] datasets, 3) ReasonSeg\ndataset [28], and 4) Visual Question Answering Dataset:\nLLaVA-v1.5-mix665k [36]. To enable the multi-mask gen-\neration for grounded caption, there are two options for\ninstruction datasets, GranDf and our COCONut-PanCap\nwhere GranDf consists of entity masks while COCONut-\nPanCap consists of panoptic masks.\nA.3. VQA\nWe provide more implementation details for the VQA\nexperiments.\nWe follow the same setting in LLaVA-\nNeXT to create the experimental results for VQA tasks.\nWe focus on the instruction tuning stage by adopting\nthe pretrained weights from the stage-1 across the train-\nings for all the model variants mentioned in Tab. 7 in\nthe paper.\nThe dataset we used is exactly the same\nas in LLaVA 665K [36] which includes the earlier ver-\nsion of instruction data proposed in LLaVA 158K [37],\nShareGPT [56], VQAv2 [41], GQA [21], openknowl-\nedge VQA (OKVQA [42],\nA-OKVQA [54]),\nOCR\n(OCRVQA [43], TextCaps [57]), region-level VQA datasets\n(Visual Genome [27], RefCOCO [22]). Among these data,\nLLaVA 158K comprises 77K complex reasoning, 58K con-\nversation and 23K detailed captions. To build the dataset\nvariants shown in Tab. 7, we simply remove the sub-\nset of detailed caption 23k, and subsequently add 20K,\n50K and 118K COCONut-PanCap dataset to build LLaVA\n665K-COCONut-PanCap-20K, LLaVA 665K-COCONut-\nPanCap-50K and LLaVA 665K-COCONut-PanCap-118K.\nBy these steps, we add more detailed caption data to\nconstruct the instruction tuning dataset.\nThis results in\nthe total amount of training data of 662K for LLaVA\n665K-COCONut-PanCap-20K, 692K for LLaVA 665K-\nCOCONut-PanCap-50K and 760K for LLaVA 665K-\nCOCONut-PanCap-118K. And thus the size of LLaVA\n665K-COCONut-PanCap-20K is slightly smaller than the\noriginal LLaVA 665K dataset, but the model trained on\nit yields better performance. For the evaluation settings,\nwe follow the exact settings in LLaVA-NeXT [38] using\nlmms eval2.\nB. More Qualitative Results\nIn this section, we present additional qualitative results of\nCOCONut-PanCap annotations (Sec. B.1) and a detailed\nanalysis of tier cases from the user study (Sec. B.2).\n2https://github.com/EvolvingLMMs-Lab/lmms-eval\n\nB.1. Data Examples\nWe show more visualization of our proposed COCONut-\nPanCap dataset in Fig. 9 and Fig. 10.\nB.2. PanCaper and GPT-4V Tier Showcases\nIn the user study involving 1,000 samples, captions gener-\nated by GPT-4V were preferred in 87 cases. Among these,\nactually, 46 were tier cases where human raters consid-\nered both GPT-4V and COCONut-PanCap captions equally\ngood. Fig. 11, Fig. 12 and Fig. 13 illustrate qualitative ex-\namples, highlighting the reasons for the tier classification\nand instances where GPT-4V was chosen.\n\nThe image depicts a natural outdoor with trees and\ngiraffes. <0:The sky is blue>, forming the backdrop of the \nscene. Below it, there are <1:dense trees, filled with \nbranches and lush green leaves>. Within this environment, \ntwo giraffes are prominently featured. The image mainly\nfocuses on <3:a standing giraffe with a long neck and \nunique patterns> , actively eating leaves from the tree. In\nThe image depicts a dynamic outdoor scene where people\nare riding horses. In the foreground, two horses take \ncenter stage. <9:A black horse with a white mane and tail\nbehind\nits\nneck, adorned with a brown bridle, a \npredominantly dark blue saddle with yellow patterns, and \nblue leg wraps>, is raising its front hoof. Beside it, <5:a \nwhite horse with a black mane and tail, wearing a black \nbridle, a similar dark blue saddle with yellow patterns, and \nwhite leg wraps>, is also raising its front hoof. Both horses \nare being controlled by <6,7: two man who are dressed in \nblue and white tops, white pants, and black boots>, \nactively taming the horses. They are riding horses on <2:a \nvivid green grassland> that provides the base for the\naction. Adding structure, there is <3: a fence made of wooden posts and railings> in the background. \nThere are some people in the background that are obscured by the horses. For example, there is <8:a\nperson wearing black pants> partially obscured by the white horse and <10:another person in a red top \nand white pants> who is watching the activity; and <11:a person in a red top and black pants>, partially \nhidden by the black horse. Together, the elements create a cohesive portrayal of a lively horse-taming \nevent set against a serene natural background. The weather is nice, as <0:the sky is white and cloudless>, \nforming the backdrop. Below it, there features <1:a dense cluster of trees with brown trunks and green \nleaves>, framing the scene. \nThe scene includes several individuals actively \nengaging in skateboarding. There are <6,10: two \nboys> wearing in green top and black pants>, actively \nplaying <9:11skateboard> in the air. <5: Another guy\nwho is also dressing in green top and black pants> is\nplaying but on the ground. Next to them, there are <7:\na half-naked man> observing the skateboarding\nperformance, while <8: another guy in a white top and black hat>, also watching the activities. Skateboards \nare prominently featured in the center area, which includes a black skateboard deck used for tricks. Lastly, \nthe background shows the <0:sky, predominantly blue with scattered clouds>. <1:A light brown building> is\nobviously seen in the background. The skaters are using the <2:sidewalk, notable for its graffiti and colorful \nmarkings>. Around the scene, there is <3:lush green foliage>, adding natural scenery to the skate park. \ncontrast, there is <4: a second giraffe with similar distinctive patterns>, which is far away from the previous\ngiraffe is resting comfortably on the grass. Both giraffes are surrounded by green trees and <2:a grassy area, \npredominantly covered with green grass interspersed with patches of exposed brown soil>. \nFigure 9. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks.\n\nThis image showcases a well-organized desk setup. On\n<1:the wooden desk with a shelf>, there is <4:a DELL\ncomputer> occupies the central space, displaying content \non its screen. Besides, there is a turtle toy on top of it. \nSurrounding the computer, multiple items are neatly \narranged. To its left, <5:a blue water bottle> stands \nprominently, next to <6:a book> lying on the desk. Below \nthe computer, <12,13,16,19:additional books> are placed.\nOn the upper shelves, various objects add character to the space like <14,15: books> and a drink can. At the top of \nthe shelves, <8:a fluffy blue teddy bear > is positioned on the left, and <11:another teddy bear> is positioned on the\nright, adding a playful touch. There are various small items as well, like <7:a glass bottle>, <17:books> and a photo\nframe. In the background, <0:the wall is painted blue>, serving as the backdrop for the scene.\nThe image features a cozy and well-decorated living \nroom. At the center of the room, <4:a wooden coffee \ntable equipped with glasses> holds various items, \nincluding <10:a remote control>, <13: a knife> on the\nplates, and <16:a square small book>. On the left, The \nseating arrangement includes <14:a patterned couch \nwith colorful cushions and blanket> and <15:another\nneutral-toned couch with vibrant throw pillows> , providing balance to the layout. The rug with colorful\npatters brings more warm atmosphere to the sitting area. Behind the couch, <20:A chair in the back>\ncomplements the seating options. Adding warmth to the room, <8:a black cat> rests comfortably on the \ncouch. Behind the sitting area, there is <5:a 4-layer wall-mounted wooden shelf> with additional \ndecorative items, including <11,12: vases> and other decorative items, enhancing the cozy and inviting \natmosphere. Closed to the shelf, there are several <9,19,22:potted plants with green leaves > are placed \nthroughout the room, adding a touch of greenery. <2:The wall painted in warm tones>, create a cozy \natmosphere and are adorned with framed artwork and decorations. <0: The floor is neutral-toned>, \nsupporting the entire setup. The <3:ceiling painted white>, contrasts subtly with the walls and reflects the \nnatural light entering the room through <6:the large windows>.\nThe image portrays a lively street scene outside a \ncafé. <0:The road> serves as the foreground, where \n<5:a motorcycle> is prominently parked, its shiny \nfinishes and detailed designs drawing attention. \nBehind it, <1:the café building>, labeled as “Seaport \nCafe” features large windows, a decorative sign, and \npatriotic bunting. A glowing neon “Corona Light” sign \nadds to the vibrant atmosphere. The café‘s exterior \nincludes <2:a wooden wall> and <4:a small fence>\nwhich separates the outdoor seating area from the \nstreet. \nThe \nseating \narea \nis \nequipped \nwith \n<12,14:chairs> and <6:a blue umbrella>\nthat \nprovides shade for the patrons.\n<7,11:\nSeveral \npeople are interacting in and around the café>, some \nstanding while others are seated, enjoying their \ntime. <10:One individual who is carrying <9:a black\nbackpack> > is joining while there is <8:a guy in blue shirt> is trying to shake hands with her, adding to the \ndynamic social scene.\nFigure 10. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks.\n\nCOCONut-PanCap: Positioned prominently within this \nimage is <1: a zebra, easily distinguishable by its iconic\nblack-and-white stripes and a striped mane along its neck>. \nIts belly is white, adding contrast to its overall pattern. The \nzebra is depicted in a natural feeding posture, with its head \nlowered as it grazes on <0:the grassland which is lush \ngreen>, forming the base of the scene. \nCOCONut-PanCap: The image mainly features <0:the trees \nwith very lush foliage>. In the front, there are <1,2: two\ngiraffes with distinctive brown spots, yellow patterns, and \nlong necks>. These giraffes are observed to be fighting by \nentwining their necks with each other. Behind them, there is \n<3: another giraffe, also with distinctive brown spots and \nyellow patterns>, which is standing still and partially \nobscured by the front giraffes. \nCOCONut-PanCap: On <0:the gray asphalt sidewalk>,\ntwo motorcycles are parked side by side. <2:One \nmotorcycle is predominantly black with white parts and \na brown seat>, positioned to the left of <1:a green \nbush>,\nwhere\na registration plate displaying the \nnumber “SV-6260” is visible to be mounted on the front \nfender, and a large cylindrical front headlamp is\nmounted on the front. <3:The other motorcycle is \npredominantly silver with a black seat>. The mechanical \ncomponents include the single-cylinder engine and a \nstraightforward exhaust system.\nGPT-4V: This beautifully restored vintage motorcycle, likely from the early 20th century, showcases a classic design with a \ncream-colored fuel tank, spoked wheels, and a robust black frame. Key features include a large front headlamp, a \nregistration plate reading "SV-6260," a brown leather saddle, and a small rear storage compartment. The visible single-\ncylinder engine and minimalistic exhaust system reflect the simple yet durable engineering of the era, with its pristine \ncondition highlighting careful preservation.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows the story background, but missing details for region-3.\nGPT-4V: This image captures two giraffes engaging in a \ndisplay of interaction, possibly sparring or playfully \nentwining their necks, a behavior commonly known as \n"necking." Set against a backdrop of lush green trees, the \ngiraffes\' distinctive spotted patterns stand out vividly in \nthe sunlight filtering through the foliage. Their tall, elegant\nforms and graceful movements highlight the majestic  nature of these animals, while the surrounding dense \nvegetation provides a natural and serene environment. The moment beautifully showcases the unique behavior and \nstriking appearance of giraffes in their habitat.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more description for the trees but add hallucination for the sunlight.\nGPT-4V: This image features a zebra grazing peacefully on a vibrant green field. The zebra\'s iconic black-and-white \nstriped pattern stands out sharply against the lush grass, creating a striking visual contrast. Its head is lowered as it \nnibbles on the grass, showcasing its natural feeding behavior. The mane, with alternating dark and light stripes, adds a \ntouch of elegance to its overall appearance. The bright sunlight enhances the vivid colors and intricate details, \nhighlighting the beauty of the zebra in its natural habitat.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more description for the behavior but add hallucination for sunlight.\nFigure 11. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.\n\nGPT-4V: This vibrant image showcases a cheerful woman holding a bright pink parasol on a sunny day. She is dressed in a \ncolorful floral swimsuit that exudes a retro style, complementing the joyful and carefree atmosphere. Her radiant smile \nand relaxed pose convey a sense of summer fun, with a backdrop of a clear blue sky, lush green trees, and a calm lake. \nThe parasol adds a playful and vintage charm, enhancing the overall nostalgic feel of the scene.\nCOCONut-PanCap: At the center of the scene, <5: a\ncheerful woman is\nwearing a vibrant swimsuit \nadorned with purple, blue, yellow, and white \npatterns>. She is holding <4:a pink umbrella>, which \nadds a pop of color and shade to the setting. The \nwoman is smiling happily, radiating a joyful and \ncarefree vibe, tying together the natural and human \nelements seamlessly. There are <1:lush green trees> \nand <0:blue sky> in the background, adding a natural \nelement to the composition. Below it, there is <2: a \nserene blue river>, Together, the composition \nemphasizes the interaction between the lively figure \nand the tranquil outdoor environment. \ntier reason: : Compared to COCONut-PanCap, GPT-4V recognize the water as lake, which is hard to be distinguished from this image.\nCOCONut-PanCap:\nThe image features an\nowl-shaped \ncandlestick, and a clock are placed on <0:the floor>. The \ncandlestick, designed in the form of an owl, adds a \ndecorative element beside the clock. Prominently, in the\ncenter there is <2:a circular clock with hands and Roman \nnumerals>, elegantly designed and placed on an intricate \nstand featuring ornate patterns and curves. Behind these \nitems, there is <1: a plain and white wall>, providing a \nneutral background that highlights the objects in the \nforeground. Together, the objects and their placement \ncreate a cohesive and aesthetically pleasing composition.\nGPT-4V: This cozy and artistic composition features a decorative vintage clock paired with a charming owl-shaped \ncandleholder. The clock, with intricate scrollwork and Roman numerals on its face, exudes an antique elegance, while the \nsoft glow of the candle creates a warm and inviting ambiance. The owl candleholder, with its textured surface and \nwhimsical design, adds a playful yet rustic touch to the scene. The subtle lighting and shadows enhance the nostalgic and \ntranquil atmosphere, making it a perfect display for a quiet, reflective setting.\ntier reason: Compared to COCONut-PanCap, GPT-4V summarize the style of the image as vintage.\nCOCONut-PanCap: The image captures an airplane in flight. \nThe center area highlights <1: an airplane, predominantly \nwhite in color, with dark blue English lettering “AIRFRANCE” \non its body and distinctive dark blue and red patterns on its \ntail>. The airplane is depicted flying gracefully through the \nsky. The background is <0: the sky, characterized by a mix of \nblue open spaces and scattered clouds>.\nGPT-4V: This image captures an Air France Airbus A380 in mid-flight against a backdrop of cloudy skies. The aircraft, \nknown for its massive size and double-deck design, features the iconic Air France livery with its name prominently \ndisplayed on the fuselage and the red, white, and blue stripes on the tail, symbolizing the French flag. The four powerful \nengines are clearly visible, emphasizing the plane’s engineering prowess. The angle of the shot highlights the sleek design \nand scale of the A380, showcasing its role as a leader in modern aviation.\ntier reason: : Compared to COCONut-PanCap, GPT-4V summarize bit more details but with hallucination of model ‘A380’.\nFigure 12. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.\n\nCOCONut-PanCap: The image portrays a serene winter \nlandscape. The image mainly highlights <0:a blue sky, creating \na clear and bright backdrop>. Below it, there is <1:a pristine\nsnow-covered slope, predominantly white and marked with \nski trails> that curve across the terrain. Adjacent to this is <2:a \nmajestic snow-covered mountain>, blending seamlessly into \nthe snowy environment. At the center of the scene, <3:a \nperson who is skiing down the slope, dressed in an orange \njacket and black pants> is captured. The skier is equipped with \nskis attached to their feet, gliding smoothly over the snow. \nAdditionally, <5:a backpack> is carried by the skier, \ncompleting their gear. \nGPT-4V: This image captures a serene winter landscape \nfeaturing a skier ascending a pristine snow-covered slope. \nThe untouched white snow is marked only by the skier\'s \ntracks and a winding trail that leads into the distance, \ntier reason: Compared to COCONut-PanCap, GPT-4V shows better vocabulary, but the information is almost the same.\nemphasizing the tranquility and solitude of the scene. Majestic mountains with rugged, rocky peaks dominate the background, \ntheir shadows adding depth and contrast to the bright sunlight illuminating the snow. The skier, dressed in warm, vibrant gear \nand carrying poles, navigates the slope with determination, highlighting the spirit of exploration and adventure. The expansive \nsky above completes the breathtaking alpine setting.\nCOCONut-PanCap: The image highlights <1:a white dog>,\nstanding on the <0:lush\ngreen\ngrass>, exuding a lively \ndemeanor. The dog is holding <2:a purple frisbee> in its \nmouth. The frisbee features black text “SKIMMER” printed on \nits surface, adding contrast to its vibrant purple color. \nTogether, the scene captures a moment of playful interaction, \ngrounded by the mask numbers for clarity and detail.\nGPT-4V: This image features an adorable white dog \nstanding on a lush green lawn, holding a bright purple\nfrisbee in its mouth. The dog\'s fluffy coat is well-groomed, and its expressive eyes gaze directly at the viewer, exuding \ncharm and playfulness. The frisbee, labeled with "Skimmer Competition," is slightly chewed, indicating the dog’s \nenthusiasm for play and activity. The vibrant colors of the frisbee and the green grass create a cheerful and lively \natmosphere, perfectly capturing a moment of joy and companionship between a pet and its outdoor playtime.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more OCR results of “competition”, but the information is minor.\nCOCONut-PanCap: The image captures a dynamic tennis \nmatch. In the foreground, the image highlights <3:a man \nactively playing tennis>, dressed in a black top, blue pants, \nblack socks, and black shoes. He is holding <2:a tennis racket>, \ncharacterized by its red and white frame and netting with \nprinted letters. The background is <0:a predominantly blue \nwall>, decorated with red and black patterns along with white \npartial text “rates Airline”. <1:The playing field is a mix of \ngreen and blue hues>.\nGPT-4V: This image captures a tennis player intensely focused \nduring a match, poised and ready to return a serve on a hard \ncourt. Dressed in a dark athletic shirt, blue shorts, and black \nsneakers, the player holds their racket firmly, leaning slightly \nforward in a balanced stance that conveys readiness and \ndetermination. The backdrop prominently features the USTA \nlogo and the Emirates Airline sponsorship, indicating a \nprofessional tennis tournament setting. The court\'s clean lines \nhighlight the competitive atmosphere\ntier reason: Compared to COCONut-PanCap, GPT-4V shows extra wording to describe the atmosphere which is not necessary.\nFigure 13. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.'),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content='Rethinking Mixture-of-Agents: Is Mixing Different\nLarge Language Models Beneficial?\nWenzhe Li*1, Yong Lin*1, Mengzhou Xia1, and Chi Jin1\n1Princeton University†\nAbstract\nEnsembling outputs from diverse sources is a straightforward yet effective approach to boost perfor-\nmance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from\nmultiple different Large Language Models (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA — an ensemble method that\naggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, sur-\nprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios:\nSelf-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of\n3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA\nto one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance\non the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off\nbetween diversity and quality of outputs under various MoA settings. We confirm that the MoA performance\nis rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models.\nTo complement the study, we identify the scenarios where mixing different LLMs could be helpful. This\npaper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of\nLLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.\n1\nIntroduction\nLarge language models have made remarkable strides in improving performance across different domains,\nwith notable examples such as GPT [Achiam et al., 2023], Gemini [Team et al., 2023], and Claude [Anthropic,\n2023]. Significant efforts have been directed toward increasing model size and training data to boost\ncapabilities. However, scaling at training time comes with steep costs, while scaling computation during\ninference remains largely underexplored.\nA straightforward way to utilize test-time compute is ensembling, which aims to combine outputs of\nmultiple LLMs [Wang et al., 2024a, Lin et al., 2024, Jiang et al., 2023a, Wang et al., 2024a]. Among various\nensembling approaches, Mixture-of-Agents (MoA) [Wang et al., 2024a] has garnered significant interest,\nachieving superior performance in challenging tasks such as instruction following [Wang et al., 2024a],\nsummarization, data extraction [OpenPipe, 2024], and real-world code issue resolution [Zhang et al., 2024b].\n*Equal contribution.\n†Email: {wenzhe.li,yl7690,mengzhou,chij}@princeton.edu.\n1\narXiv:2502.00674v1  [cs.CL]  2 Feb 2025\n\nSpecifically, MoA first queries multiple LLMs (proposers) to generate responses, and then uses an LLM\n(aggregator) to synthesize and summarize these responses into a high-quality response.\nPrevious research highlights the significance of model diversity within the proposers for optimizing the\nperformance of MoA, primarily focusing on strategies for ensembling a diverse set of individual models. We\nconsider cross-model diversity as the variation among different models. However, pursuing cross-model\ndiversity may inadvertently include low-quality models, resulting in a quality-diversity trade-off. While\nprevious studies mainly concentrate on achieving a high cross-model diversity [Wang et al., 2024a, Zhang\net al., 2024b], we adopt a holistic perspective on model diversity by considering in-model diversity, which\narises from the variability of multiple responses generated by the same model. In-model diversity enables us to\naggregate multiple outputs from an individual model. Intuitively, leveraging outputs from the best-performing\nindividual model can more effectively navigate the quality-diversity trade-off by creating a higher-quality\nproposer mixture. Thus, we propose Self-MoA as depicted in Figure 1b, which utilizes the same prompting\ntemplate as MoA but aggregates outputs that are repeatedly sampled from the same model, rather than from\na set of different models. To distinguish, we use Mixed-MoA to refer to MoA configurations that combine\ndifferent individual models when necessary.\nSurprisingly, we find that Mixed-MoA is usually sub-optimal compared with Self-MoA, especially when\nthere exist significant quality differences among proposers. Specifically, we revisit the same experiment\nsetting of MoA with six open-source instruction fine-tuned models as Wang et al. [2024a]. Compared\nwith Mixed-MoA which aggregates all six models, Self-MoA on the strongest model achieves 6.6 point\nimprovement over its mixed counterpart on the AlpacaEval 2.0 benchmark, showing a case of when intra-\nmodel diversity is more effective. Moreover, Self-MoA on two best-performed models on AlpacaEval 2.0\nconsistently achieves a 2-3 point gain and secures the top position among non-adversarial methods on the\nleaderboard, which further confirms the effectiveness of Self-MoA in this task.\nTo explore the limits of model diversity for MoA, we extend our experiments to a setting with three\nspecialized models, each excelling in a specific task. Specifically, we utilize Qwen2-7B-Instruct [Bai et al.,\n2023] for common sense QA (MMLU-redux [Gema et al., 2024]), Qwen2-Math-7B-Instruct [Bai et al., 2023]\nfor mathematics (MATH [Hendrycks et al., 2020]), and DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024]\nfor coding (CRUX [Gu et al., 2024]). We compare Self-MoA against a range of Mixed-MoA strategies,\nevaluating 13 combinations of individual models based on their average performance across the three tasks.\nOur findings indicate that employing task-specific models as proposers for Self-MoA can significantly\noutperform the best Mixed-MoA. Furthermore, even in a constructed mixture task tailored for Mixed-MoA\nwhere each individual model excels in a specific subtask, only two Mixed-MoA strategies slightly outperform\nSelf-MoA by 0.17% and 0.35%.\nTo better understand the effectiveness of Self-MoA, we conduct a comprehensive investigation of the trade-\noff between quality and diversity in MoA, involving over 200 experiments. We use the Vendi Score [Dan Fried-\nman and Dieng, 2023] to evaluate the diversity among the outputs of the proposers, while the average perfor-\nmance of the proposers serves as the measure of quality. In Section 4, we confirm that MoA performance\nhas a positive correlation with both quality and diversity. Moreover, we clearly show a trade-off along the\nachievable Pareto front of quality and diversity. Interestingly, we find that MoA is quite sensitive to variations\nin quality, with optimal performance typically occurring in regions characterized by high quality and relatively\nlow diversity. This finding naturally explains the effectiveness of Self-MoA, as it utilizes the strongest model\nas the proposer, ensuring high quality in its outputs.\nFinally, we evaluate the performance of Self-MoA under increasing computational budgets. As the\nnumber of outputs grows, the scalability of Self-MoA becomes constrained by the context length of the\naggregator. To address this issue, we introduce Self-MoA-Seq (Figure 1c), a sequential version that processes\nsamples using a sliding window, allowing it to handle an arbitrary number of model outputs. Our findings\n2\n\nshow that Self-MoA-Seq performs at least as effectively as Self-MoA, enabling scalable ensembling for\nLLMs with shorter context lengths without compromising final performance.\nOverall, our contributions are three-fold:\n• We introduce Self-MoA, which leverages in-model diversity by synthesizing multiple outputs from the\nsame model. Surprisingly, it demonstrates superior performance compared to existing Mixed-MoA\napproaches, which emphasize cross-model diversity, across a wide range of benchmarks.\n• Through systematic experiments and statistical analysis, we uncover a core trade-off between diversity\nand quality among the proposers, emphasizing that MoA is highly sensitive to proposer quality. This\nfinding also explains the success of Self-MoA, which leverages outputs from the highest-performing\nmodel, ensuring superior overall quality.\n• We extend Self-MoA to its sequential version Self-MoA-Seq, which iteratively aggregates a small\namount of outputs step by step. Self-MoA-Seq unlocks LLMs that are constrained by the context length\nand enables computation scaling during inference.\n2\nRelated Work\nEnsembles of LLMs.\nModel ensembling aims to combine strengths from multiple models. Previous\nstudies have explored various methods to leverage a diverse set of models, including but not limited to\nprompting [Wang et al., 2024a], weight averaging [Lin et al., 2024, Ram´e et al., 2024], routing [Jiang et al.,\n2024b, Lu et al., 2023], training a generative fusion model [Jiang et al., 2023b], and so on. Zhang et al. [2024a]\nargues that the fusion of specialized models with certain general abilities could be a promising direction toward\nArtificial General Intelligence. Mixture-of-Agents (MoA, Wang et al. [2024a]) first queries multiple LLMs to\ngenerate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows\npromising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval\n2.0 leaderboard. Our method is inspired by the prompt pipeline proposed in MoA. However, while existing\nMoA focuses on unleashing the strength from multiple different models [Wang et al., 2024a, Jiang et al.,\n2023b, Zhang et al., 2024b], we demonstrate the trade-off between diversity and quality within the proposers,\nhighlighting that focusing solely on diversity may compromise overall quality and final performance.\nLLM Inference with Repeated Sampling.\nPrevious studies have shown that combining model outputs\nfrom repeated sampling can yield a better response in various domains. In tasks with automatic verifiers\navailable, such as math [Hendrycks et al., 2021] and code [Chen et al., 2021], simply sampling LLMs\nmultiple times can significantly improve the pass@k metric and hence boost the success rate of solving the\ntasks [Roziere et al., 2023, Li et al., 2022, Brown et al., 2024]. In more general tasks without verification\ntools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most\npromising one from candidate responses [Wang et al., 2022, Chen et al., 2023b, Gui et al., 2024, Li et al.,\n2024]. Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference\ntime [Brown et al., 2024]. In this work, we identify the surprising effectiveness of repeated sampling in the\ncontext of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated\nfrom repeated sampling, hence can further improve over each individual output.\nCollaborative Agents\nThere is a surge of interest in building agent systems based on verification, critique,\ndiscussion, and refinement. For example, Stechly et al. [2023], Valmeekam et al. [2023], and Madaan et al.\n3\n\n  𝑜!\n!\n  𝑜!\n"\n  𝑜"\n!\n  𝑜"\n"\n  𝑜#\n!\n  𝑜#\n"\n 𝑀!\n 𝑀"\n 𝑀#\n 𝐴\n  𝑜$\n(a) MoA\n(b) Self-MoA\n(c) Self-MoA-Seq\n  𝑜!\n!\n  𝑜!\n"\n  𝑜!\n#\n  𝑜!\n%\n  𝑜!\n&\n  𝑜!\n\'\n 𝐴\n  𝑜$\n 𝑀!\n  𝑜!\n!\n  𝑜!\n"\n  𝑜!\n#\n  𝑜!\n%\n  𝑜!\n&\n  𝑜!\n\'\n 𝐴\n  𝑜$\n!\n  𝑜$\n!\n 𝐴\n  𝑜$\n"\n 𝑀!\nFigure 1: Comparison of MoA, Self-MoA, and Self-MoA-Seq. (a) In MoA, multiple models respond to\na query, followed by an aggregator synthesizing their outputs. (b) Self-MoA simplifies this by repeatedly\nsampling from a single model. (c) Self-MoA-Seq extends Self-MoA by applying a sliding window to combine\nthe best output so far with candidate outputs. At each timestep, the synthesized output is repeated to bias the\naggregator towards it, reducing the context length requirements and expanding the method’s applicability.\nNote that MoA can extend to multiple rounds of aggregation (Appendix A.1), while Self-MoA and Self-MoA-\nSeq can extend to more outputs, but we omit them here for clarity.\n[2024] use self-critique to iteratively refine outputs through a chain structure. Madaan et al. [2024], Chen\net al. [2024], and Wang et al. [2024a] explore the incorporation of multiple models to create a stronger agent\nthat outperforms each individual model. Du et al. [2023] incorporates multiple LLMs that propose and debate\ntheir individual responses over several rounds to reach a common final answer. Liang et al. [2023] proposes\nMulti-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative\nconclusions and avoid rushing to incorrect answers. Chen et al. [2023a] introduces RECONCILE, which\nadopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly,\nWang et al. [2024b] shows that a single model with carefully designed prompts can sometimes match the\nperformance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the\nprompts are insufficient.\n3\nIs Ensembling Different LLMs Beneficial?\nAs introduced in Section 1, previous research primarily emphasizes cross-model diversity, which can\ninadvertently include low-quality proposers. In this work, we introduce Self-MoA (Figure 1), which uses\na single top-performing model to generate multiple outputs and aggregate them to produce the final result.\nSelf-MoA leverages in-model diversity as repeated sampling often produces varied outputs. We propose our\nresearch question as follows:\nDoes the benefit of MoA stem from cross-model diversity?\nCan we build a stronger MoA using in-model diversity?\n4\n\nTable 1: Comparison of Self-MoA and Mixed-MoA on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110B-\nChat as the aggregator.\nModel Configuration\nLC Win Rate\nIndividual\nWizardLM-2-8x22B\n53.1\nQwen1.5-110B-Chat\n43.9\nLLaMA-3-70B-Instruct\n34.4\nQwen1.5-72B-Chat\n36.6\nMixtral-8x22B-Instruct-v0.1\n30.2\ndbrx-instruct\n25.4\nMixed-MoA\n2-Layer MoA [Wang et al., 2024a]\n59.1\nSelf-MoA\n2-Layer Self-MoA + WizardLM\n65.7\n3.1\nExperiments on AlpacaEval 2.0 with General Purpose Models\nEvaluation benchmarks.\nWe adopt the same experiment setting as Wang et al. [2024a] in AlpacaEval 2.0\nbenchmark [Dubois et al., 2024] and compare the performance of Mixed-MoA and Self-MoA1. AlpacaEval\n2.0 is a widely used benchmark for assessing the instruction-following abilities of LLMs. It offers a set\nof real-world instructions and employs a GPT-4-based annotator to compare the model’s responses against\nreference answers generated by GPT-4. To address length bias inherent in model-based evaluation, Dubois\net al. [2024] introduced the length-controlled (LC) win rate as a more robust evaluation metric.\nModels.\nFollowing Wang et al. [2024a], we construct MoA based on six individual models: Qwen1.5-\n110B-Chat [Bai et al., 2023], Qwen1.5-72B-Chat [Bai et al., 2023], WizardLM-8x22B [Xu et al., 2023],\nLLaMA-3-70B-Instruct [Touvron et al., 2023], Mixtral-8x22B-Instruct-v0.1 [Jiang et al., 2024a], and dbrx-\ninstruct [Team et al., 2024b]. Each model is sampled with a temperature of 0.7, following the default in\n[Wang et al., 2024a]. For Self-MoA, we aggregate six outputs sampled from WizardLM-2-8x22B, as it\nconsistently outperforms the other models. In line with Wang et al. [2024a], we use Qwen1.5-110B-Chat as\nthe aggregator for both Mixed-MoA and Self-MoA.\nResults.\nWe present the LC win rate for each model configuration in Table 1. For individual models, we\nreport the higher value between the leaderboard results and our reproduction. Notably, Self-MoA demonstrates\nremarkable effectiveness in this task, outperforming the Mixed-MoA baseline by 6.6 point. This suggests that,\nwhile using multiple models intuitively offers greater diversity, ensembling multiple outputs from a single\nmodel is more effective.\nApplying Self-MoA on top performing models.\nTo further validate the effectiveness of Self-MoA, we\napply it to the two top-performing models on AlpacaEval 2.0: gemma-2-9b-it-WPO-HB [Zhou et al., 2024]\nand gemma-2-9b-it-SimPO [Meng et al., 2024]. We use each model as both the proposer and the aggregator2,\n1We note that this experiment is similar to the “single-proposer” setting in Wang et al. [2024a], however our reproduced result is\ndifferent. We conjecture that such a major difference is due to different choices of the proposer model, which is not mentioned in Wang\net al. [2024a]. As we shall see later in Section 4, ensembling performance is more sensitive to quality rather than diversity. Therefore, a\nworse proposer model will lead to suboptimal performance of Self-MoA.\n2Qwen1.5-110B-Chat is not used as the aggregator since the two top models significantly outperform it.\n5\n\nTable 2: Self-MoA achieves state-of-the-art performance on the AlpacaEval 2.0 leaderboard when using\ntop-performing models as both proposers and aggregators. We only ensemble 4 outputs due to context window\nconstraints.\nModel Configuration\nLC Win Rate\nIndividual\ngemma-2-9b-it-WPO-HB\n76.7\ngemma-2-9b-it-SimPO\n72.4\nSelf-MoA\nSelf-MoA + gemma-2-9b-it-WPO-HB\n78.5\nSelf-MoA + gemma-2-9b-it-SimPO\n75.0\nwith a temperature of 0.7 for all the generations. Due to the context length constraint of Gemma 2 [Team et al.,\n2024a], the aggregator can only take four samples as the input. As shown in Table 2, Self-MoA consistently\nachieves a 2-3 point gain and secures the top position on the leaderboard during submission.\nResults on MT-Bench.\nBeyond AlpacaEval 2.0, we further evaluate Self-MoA and Mixed-MoA on MT-\nBench [Zheng et al., 2023], another benchmark used in Wang et al. [2024a]. The results align with our\nfindings from AlpacaEval 2.0, reinforcing the effectiveness of Self-MoA. Please refer to Appendix B.1 for\nmore details.\n3.2\nExperiments on Multiple Datasets with Specialized Models\nIn this section, we compare different ensembling methods on a diverse set of benchmarks using specialized\nmodels.\nEvaluation datasets.\nWe conduct evaluations across a diverse set of benchmarks:\n• MMLU [Hendrycks et al., 2020] is a multiple-choice dataset designed to assess a model’s multitask\naccuracy. MMLU is widely used to evaluate both the breadth and depth of language understanding\ncapabilities of current LLMs across a diverse array of subjects, including mathematics, history, computer\nscience, logic, and law. We adopt MMLU-redux [Gema et al., 2024] for evaluation, which is a subset\nof MMLU with 3,000 samples fixing the errors in the dataset through human re-annotating.\n• CRUX [Gu et al., 2024] consists of 800 Python code functions, each containing 3 to 13 lines along with\nan input-output pair. Based on this dataset, Gu et al. [2024] constructs two tasks: input prediction and\noutput prediction. To successfully complete these tasks, the LLM must demonstrate code reasoning\nabilities.\n• MATH [Hendrycks et al., 2021] comprises 12,500 challenging competition-level mathematics problems.\nFor our analysis, we utilize the testing subset of MATH, which consists of 5,000 samples.\n3As Qwen2-Math-7B-Instruct only supports context length of 4096, for these two data points, we sample the proposer with a reduced\ntoken length of 1024, and only aggregates three outputs from the proposer.\n6\n\nTable 3: Comparison of Self-MoA and Mixed-MoA in MMLU, CRUX, and MATH. The labels i, m, and d\nrefer to Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct, respectively.\nThe average performance represents the mean accuracy across MMLU, CRUX, and MATH. TaskBest\nindicates that we use the strongest model for each task as both proposer and aggregator.\nAggregator\nProposer\nMMLU\nCRUX\nMATH\nIndividual\n-\ni\n66.16\n36.25\n53.81\n-\nd\n60.91\n49.51\n53.82\n-\nm\n54.36\n27.88\n69.573\nMixed-MoA\ni\niimmdd\n67.89\n42.88\n64.38\nimdddd\n67.42\n44.50\n63.90\niiiimd\n68.90\n41.25\n63.00\nimmmmd\n66.63\n42.75\n66.02\niimmmm\n66.23\n39.25\n66.10\niiimmm\n67.49\n38.25\n64.16\niiiimm\n68.00\n37.00\n62.92\niidddd\n68.21\n45.50\n62.56\niiiddd\n68.21\n42.88\n62.38\niiiidd\n68.47\n40.75\n61.24\nmmdddd\n66.34\n46.75\n66.48\nmmmddd\n65.80\n47.00\n67.32\nmmmmdd\n65.44\n42.50\n67.62\nSelf-MoA\ni\n6×TaskBest\n69.01\n50.75\n68.42\nTaskBest\n6×TaskBest\n69.01\n52.62\n69.803\nModels.\nTo ensure sufficient diversity, we select three LLMs with specialized strengths: Qwen2-7B-\nInstruct [Yang et al., 2024], DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024], and Qwen2-Math-7B-\nInstruct. We fix the number of proposers to six and sweep various combinations of these three models.\nFor convenience, we denote Qwen2-7B-Instruct as i, DeepSeek-Coder-V2-Lite-Instruct as d, and Qwen2-\nMath-7B-Instruct as m. As shown in Table 3, Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and\nQwen2-Math-7B-Instruct excel on MMLU, CRUX, and MATH, respectively. We use the short name for\nthe mixture of proposers. For example, iiddmm indicates the inclusion of two samples from each model\nrespectively. When a model is represented multiple times in the proposer mixture, we ensure that two samples\nare generated with different random seeds. We set the temperature of each model to be 0.7 for the individual\nmodel, and use temperature 0 for the aggregator. We mainly use Qwen2-7B-Instruct as the aggregator but also\ntry different models as the aggregator. We explore various MoA configurations, including individual models,\ncombinations of two or three models as proposers, and using a single top-performing model (TaskBest, for\nexample DeepSeek-Coder-V2-Lite-Instruct for CRUX) as the proposer (Self-MoA).\nResults.\nThe results are presented in Table 3. When using i as the aggregator, Self-MoA with the TaskBest\nmodel consistently outperforms all 13 tested Mixed-MoA configurations across all tasks. Furthermore,\nadopting a task-specific aggregator yields an additional performance boost of 1-2 points. Interestingly,\nincreasing model diversity does not always lead to better performance. For instance, while MoA with\niimmdd surpasses mmmddd on MMLU, it underperforms on CRUX and MATH. This discrepancy aligns\n7\n\n40\n45\n50\n55\n60\n65\nQuality\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nDiversity\nMMLU\nMixed-MoA\nSelf-MoA\n61\n62\n63\n64\n65\n66\n67\n68\n69\nf\n20\n25\n30\n35\n40\n45\nQuality\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nCRUX\nMixed-MoA\nSelf-MoA\n36\n38\n40\n42\n44\n46\n48\n50\nf\n35\n40\n45\n50\n55\n60\n65\n70\nQuality\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nMATH\nMixed-MoA\nSelf-MoA\n58\n60\n62\n64\n66\n68\nMOA Performance\nFigure 2: The diversity-quality trade-off: Mixed-MoA incorporates different individual models as proposers,\nwhile Self-MoA uses the same individual model for this role. Quality is assessed based on the average\nperformance of each proposer, and diversity is computed with the Vendi Score [Dan Friedman and Dieng,\n2023] of outputs generated by proposers on the same prompts.\nwith the relative strengths of the individual models—i excels on MMLU but lags behind on CRUX and\nMATH. We postpone more discussion to Section 4.2.\n4\nThe Quality-Diversity Trade-off\nWe investigate factors that contribute to the strong performance of Self-MoA through careful experiments.\nPrevious studies have mainly focused on increasing model diversity within the group [Wang et al., 2024a,\nJiang et al., 2023a, Zhang et al., 2024b]. However, searching for diverse models can sometimes lead to\nincluding poorly performed models, resulting in a trade-off between diversity and quality, where quality refers\nto how well each individual model performs in the group.\nTherefore, we aim to identify the existence of a general relationship between MoA’s performance and\nquality as well as diversity. Following Section 3, we evaluate MoA’s performance on MMLU, CRUX, and\nMATH, which cover tasks requiring a wide range of capabilities. We vary the quality and diversity with two\norders of freedom: 1) combinations of individual models in proposers from Section 3.2; and 2) sampling\ntemperature. i.e., 0.5, 0.7, 1.0, 1.1, and 1.2. This results in a total of over 70 unique MoA proposer mixtures.\nWe measure the quality and diversity as follows:\n• Diversity: We utilize the Vendi Score [Dan Friedman and Dieng, 2023] to assess the diversity among\nindividual models in the proposer mixture. The Vendi Score represents the effective number of unique\nelements within a collection of samples [Dan Friedman and Dieng, 2023], with further details provided\nin the Appendix A.2. Specifically, for a given prompt x, we obtain responses from each model, denoted\nas y1, y2, . . . , y6. The diversity of the proposers for prompt x, denoted as d(x), is calculated using the\nVendi Score on the set [y1, . . . , y6]. We then compute the overall diversity across the dataset S as:\nd = 1\n|S|\nX\nx∈S\nd(x).\n• Quality: We first determine the accuracy of each model on the dataset S, yielding values q1, q2, . . . , q6.\nThe average accuracy, q = 1\n6(q1 + q2 + . . . + q6), serves as our measure of the quality of the proposers.\nWe will explore additional quality measurement strategies in later sections.\n8\n\nTable 4: Linear regression (Equation 1) of MoA’s performance t on diversity d and quality q.\nDataset\nα\nβ\nR2\nCoefficient\nP-value\nCoefficient\nP-value\nMMLU\n2.558 ± 0.176\n< 0.001\n1.841 ± 0.176\n< 0.001\n0.771\nCRUX\n4.548 ± 0.459\n< 0.001\n1.421 ± 0.459\n< 0.001\n0.685\nMATH\n4.719 ± 0.416\n< 0.001\n2.839 ± 0.416\n< 0.001\n0.760\nResults.\nWe plot MoA’s performance with corresponding diversity and quality for each mixture of proposers\nin Figure 2. We summarize key observations as follows:\n• The trends among MMLU, CRUX, and MATH are consistently aligned.\n• When the quality is fixed, increasing diversity can enhance MoA’s performance.\n• When the diversity is fixed, improving quality can also boost MoA’s performance.\n• There exists a trade-off in the achievable Pareto front between diversity and quality.\n• Notably, the best performance of MoA is typically observed in the bottom right of each subplot,\nindicating a strong sensitivity to quality.\nPrevious work on ensembles [Wang et al., 2024a, Jiang et al., 2023a, Zhang et al., 2024b] primarily focuses on\nincreasing the diversity of models within the proposer mixture. However, as shown in Figure 2, compared to\nSelf-MoA on the best-performing model, simply aiming for greater diversity in the proposer mixture can result\nin lower overall quality, which may negatively impact MoA’s performance. This trade-off between diversity\nand quality helps to explain why Self-MoA achieves superior performance across various benchmarks.\n4.1\nStatistical Analysis\nTo further understand the numerical correlation between MoA’s performance and diversity as well as quality,\nwe conduct linear regression for MoA’s performance t on diversity d and quality q. Specifically, we fit the\nfollowing equation for each dataset:\nt = α × q + β × d + γ,\n(1)\nwhere α, β, γ ∈R are real-valued coefficients to be determined. For each dataset, we collect around 70\ndata points from Figure 2 to construct the set {qi, di, ti}N\ni=1. The coefficients α, β, and γ are then derived\nby solving a linear regression on {qi, di, ti}N\ni=1. To make coefficients α and β comparable, we normalize\nq and d by subtracting their means and dividing by their standard deviations (detailed in Appendix A.3),\nrespectively. The results are presented in Table 4. We observe that the p-values for both α and β are less\nthan 0.001, indicating a significant correlation between MoA’s performance and both quality and diversity\n[Arnold, 1990]. The R2 values from the linear regression across three datasets are approximately around 0.7,\nindicating that the linear model based on quality and diversity explains 70% MoA’s performance and hence a\nstrong correlation between inputs and outputs, according to Appendix A.4. In later parts, we show that using\na more fine-grained quality calculation can further increase the R2 value.\n9\n\nComparing the effect strength of quality and diversity.\nFrom Table 4, we observe that α is greater than\nβ across all three datasets. In particular, for CRUX and MATH, the gap between these two measures is even\nmore pronounced. These results suggest that MoA’s performance is particularly sensitive to variations in\nquality, highlighting the importance of prioritizing quality within the proposer mixture. This finding is also\nconsistent with our observation that MoA achieves its best performance in the bottom right of the plot in\nFigure 2, further supporting the effectiveness of our proposed Self-MoA approach.\nAlternative quality measurements.\nWe use the averaged accuracy of each individual model to measure\nquality in the previous analysis. In this section, we explore alternative methods for assessing the quality of\nproposers. Recall that q1, . . . , q6 denote the accuracy of each individual model among proposers, and without\nloss of generality, we assume q1 ≥q2 ≥. . . ≥q6. It is reasonable to assume that the aggregator can select\nthe correct answer from the proposers, particularly when the responses of individual models are inconsistent.\nIn such cases, the aggregator would rely more heavily on models with better individual performance, meaning\nthe weight of q1 would be greater than that of q6.\nTherefore, we compare the following methods to calculate quality:\n• Average: 1\n6\nP6\ni=1 qi.\n• K-Norm:\n\x10\n1\n6\nP6\ni=1 qK\ni\n\x111/K\n, where a larger K places more emphasis on stronger individual models.\n• Centered-1/K-Norm: q1 −\n\x10\n1\n6\nP6\ni=1(q1 −qi)1/K\x11K\n. In this formulation, we first compute the\ndifference between qi and the best model’s q1. The 1/K norm emphasizes the weights of models\nwhose performance is closer to q1.\nAll three methods are the same when K = 1. For each quality measurement, we fit a linear regression\nto assess the relationship between MoA’s performance and the quality and diversity metrics, reporting the\nR2 values in Table 5. Our analysis shows that in MMLU and CRUX, applying a larger weight to better-\nperforming individual models tends to increase the R2 values. However, this trend is inconsistent for MATH.\nWe conjecture that this inconsistency arises because the aggregator Qwen2-7B-Instruct is relatively weak on\nMATH compared to the strongest individual model, Qwen2-Math-7B-Instruct. This limitation constrains\nthe performance of MoA, leading to an inconsistent trend in the linear regression results. In contrast, on\nMMLU, where Qwen2-7B-Instruct is the strongest individual model, we find that the R2 value can exceed\n0.9 with K = 2 using the Centered-1/K-Norm. This indicates a very strong linear relationship between MoA\nperformance and the quality and diversity metrics. Overall, we conclude that employing Centered-1/K-Norm\nwith K = 2 (marked in blue) achieves strong performance across all three datasets.\n4.2\nWhen Mixed-MoA Outperforms Self-MoA?\nAccording to the quality-diversity trade-off illustrated in Figure 2, we conjecture that increasing diversity can\nenhance MoA’s performance when the quality is controlled.\nMixed-MoA generally exhibits greater diversity than Self-MoA, which can lead to improved performance\nwhen the model quality is similar. This advantage arises when individual models achieve similar overall\nperformance while maintaining significant cross-model diversity. To simulate such a scenario, we construct a\nmixture task combining MMLU, CRUX, and MATH as described in Section 3.2. In this setting, test samples\nare drawn uniformly from the three tasks, and models do not have prior knowledge of a sample’s origin. For a\ngiven MoA strategy, we evaluate its performance on this mixture task by averaging its performance across the\n10\n\nTable 5: The R2 of the linear regression when we use different quality measurement methods. We find using\nCentered-1/K-Norm with K=2 can achieve good performance among all these three datasets.\nDataset\nMethod\nAvg. (K=1)\nK=2\nK=3\nK=4\nMMLU\nK-Norm\n0.771\n0.809\n0.832\n0.845\nCentered-1/K-Norm\n0.771\n0.881\n0.902\n0.903\nCRUX\nK-Norm\n0.685\n0.736\n0.765\n0.779\nCentered-1/K-Norm\n0.685\n0.753\n0.758\n0.753\nMATH\nK-Norm\n0.760\n0.720\n0.692\n0.672\nCentered-1/K-Norm\n0.760\n0.720\n0.692\n0.672\nTable 6: Comparison of Self-MoA and Mixed-MoA on the mixture task of MMLU, CRUX, and MATH,\nmeasured by the average performance of three tasks from Table 3. Mixed-MoA models with top two average\nperformances are highlighted by underline.\nAggregator\nProposer\nAverage\nIndividual\n-\ni\n52.07\n-\nd\n54.74\n-\nm\n50.60\nMixed-MoA\ni\niimmdd\n58.38\nimdddd\n58.61\niiiimd\n57.72\nimmmmd\n58.47\niimmmm\n57.19\niiimmm\n56.63\niiiimm\n55.97\niidddd\n58.76\niiiddd\n57.82\niiiidd\n56.82\nmmdddd\n59.86\nmmmddd\n60.04\nmmmmdd\n58.52\nSelf-MoA\ni\ndddddd\n59.69\ni\n6×TaskBest\n62.73\nTaskBest\n6×TaskBest\n63.81\nthree datasets. The results are reported in Table 6. In this mixture task, each model specializes in different\nsubtasks, with i performing best on MMLU, d on CRUX, and m on MATH. As TaskBest requires additional\nprior knowledge of the sample origin, we also report Self-MoA with d as the proposer, given that it achieves\nthe highest average performance among individual models.\nFrom Table 6, we observe that Mixed-MoA indeed outperforms Self-MoA of dddddd. Specifically,\nMixed-MoA of mmdddd and mmmddd achieves the average performance of 59.86% and 60.04%, improves\n11\n\nTable 7: MoA of Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. l is short for Llama-3.1-8B-Instruct and i\nis short for Qwen2-7B-Instruct.\nAggregator\nProposer\nMMLU\nIndividual\n-\ni\n66.16\n-\nl\n66.40\nMixed-MoA\ni\niiilll\n70.73\nSelf-MoA\ni\niiiiii\n69.01\ni\nllllll\n71.27\nupon Self-MoA of dddddd by 0.17% and 0.35%. Given the reported small margin, we argue that Self-MoA\nis still a very competitive baseline under this setting, not to mention the dominant performance of Self-MoA\nover Mixed-MoA when focusing on one single task (Self-MoA with TaskBest models achieve an average\nof 3.8% improvement from Table 6). In Appendix B.3 we also report normalized results that account for\ndifferent variances among tasks, which leads to a similar conclusion.\nWe further consider another single-task case on MMLU, involving two individual models: Llama-\n3.1-8B-Instruct and Qwen2-7B-Instruct, with Qwen2-7B-Instruct serving as the aggregator. We choose\nLlama-3.1-8B-Instruct because it performs similarly to Qwen2-7B-Instruct as an individual model. Table 7\ndemonstrates that even when the performance of two individual models is close, Self-MoA—utilizing six\nLlama-3.1-8B-Instruct proposers (denoted as llllll)—still outperforms the Mixed-MoA configuration\n(denoted as iiilll).\n5\nScaling Inference Compute with Self-MoA\nIn previous sections, we have provided evidence that Self-MoA over one strong model is straightforward\nbut effective. As the community is becoming more aware of scaling inference time computing [Brown et al.,\n2024, Snell et al., 2024, Wu et al., 2024], one natural question to ask is:\nGiven a strong model, does Self-MoA’s performance scale with the number of repeated samples?\nIntuitively, Self-MoA cannot scale indefinitely by simply increasing the computation budget for at least three\nreasons:\n• As more responses are sampled from a single model, the diversity among those samples tends to\nplateau.\n• Aggregating information from many samples is more challenging for LLMs compared to handling a\nsmaller number of samples.\n• Every LLM has a context length limit (e.g., 8192 tokens for Gemma 2), which restricts the number of\nresponses an aggregator can process at once.\nWhile the first limitation is inherent to repeated sampling, we address the latter two by introducing Self-\nMoA-Seq, a sequential variant designed to manage large numbers of responses without overwhelming the\naggregator. Self-MoA-Seq uses a sliding window to aggregate a fixed number of responses at a time, allowing\n12\n\n5\n10\n15\n20\n25\n30\nNumber of Samples\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0\n70.5\nAccuracy\nMMLU\nSelf-MoA\nSelf-MoA-Seq\nBase Model (Qwen2-7B-Instruct)\n5\n10\n15\n20\n25\n30\nNumber of Samples\n47\n48\n49\n50\n51\n52\n53\n54\nAccuracy\nCRUX\nSelf-MoA\nSelf-MoA-Seq\nBase Model (DeepSeek-Coder-V2-Lite-Instruct)\nFigure 3: The performance of Self-MoA and Self-MoA-Seq with a growing number of samples. Dashed lines\nindicate the performance of a single forward pass with the base model.\nit to handle an unlimited number of responses, regardless of context length constraints. A visual illustration is\nprovided in Figure 1.\nWe evaluate the performance of Self-MoA and Self-MoA-Seq with increasing sample sizes on the MMLU\nand CRUX benchmarks to study their scaling behavior. For each benchmark, we use the best-performing\nmodel as both the proposer and aggregator (Qwen2-7B-Instruct for MMLU and DeepSeek-Coder-V2-Lite-\nInstruct for CRUX), with a sampling temperature of 0.7. In Self-MoA-Seq, the window size is set to six, with\nthe first three slots reserved for the current synthesized output. We vary the number of samples from 6 to 30\nand plot the accuracy curves from three runs with different seeds in Figure 3. Our key observations are as\nfollows:\n• Both Self-MoA and Self-MoA-Seq significantly improve performance over the individual base model.\n• Adding more samples can have both positive and negative effects, meaning there is no universal\ncompute-optimal solution.\n• Self-MoA-Seq delivers performance that is comparable to, or slightly better than, Self-MoA.\nThese findings suggest that Self-MoA-Seq can extend the effectiveness of Self-MoA to LLMs with shorter\ncontext lengths, without sacrificing performance. Following Section 4.2, we explore whether introducing a\nsecond model can enhance performance in the sequential setting. Given that Llama-3.1-8B-Instruct performs\nsimilarly to Qwen2-7B-Instruct on the MMLU task, we compare the impact of adding Llama-3.1-8B-Instruct\nand DeepSeek-Coder-V2-Lite-Instruct (which underperforms Qwen2-7B-Instruct by 5%) after aggregating\n30 samples from Qwen2-7B-Instruct in Self-MoA-Seq. We find that incorporating Llama-3.1-8B-Instruct\nboosts accuracy by around 2%, whereas adding DeepSeek-Coder-V2-Lite-Instruct reduces accuracy by more\nthan 1.5%. This result provides another example of cross-model diversity benefiting MoA, and shows the\npotential of Self-MoA-Seq with increasing computation budget.\n6\nConclusion\nIn this paper, we introduce Self-MoA, an innovative approach that utilizes in-model diversity to enhance\nthe performance of large language models during inference. Our experiments demonstrate that Self-MoA\noutperforms traditional Mixed-MoA strategies in many popular benchmarks, particularly when the proposer\n13\n\nmodel quality varies. By aggregating outputs from a single high-performing model, Self-MoA effectively\naddresses the quality-diversity trade-off. We further identify the scenarios where mixing LLM can be\npotentially beneficial and extend Self-MoA to the constrained context length setting. These findings highlight\nthe potential of in-model diversity in optimizing LLM performance and pave the way for further advancements\nin ensemble methods.\n14\n\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman,\nS. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nA. Anthropic. Introducing claude, 2023.\nH. J. Arnold. Introduction to the practice of statistics. Technometrics, 32:347–348, 1990. URL https:\n//api.semanticscholar.org/CorpusID:122891525.\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical\nreport. arXiv preprint arXiv:2309.16609, 2023.\nB. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. R´e, and A. Mirhoseini. Large language monkeys:\nScaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024.\nJ. C.-Y. Chen, S. Saha, and M. Bansal. Reconcile: Round-table conference improves reasoning via consensus\namong diverse llms. arXiv preprint arXiv:2309.13007, 2023a.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\nG. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\n2021.\nS. Chen, L. Zeng, A. Raghunathan, F. Huang, and T. C. Kim. Moa is all you need: Building llm research\nteam using mixture of agents. arXiv preprint arXiv:2409.07487, 2024.\nX. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin, S. Prakash, C. Sutton, X. Wang, and D. Zhou. Universal\nself-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023b.\nD. Dan Friedman and A. B. Dieng. The vendi score: A diversity evaluation metric for machine learning.\nTransactions on machine learning research, 2023.\nY. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in language\nmodels through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to\ndebias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du,\nM. R. G. Madani, et al. Are we done with mmlu? arXiv preprint arXiv:2406.04127, 2024.\nA. Gu, B. Rozi`ere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for\ncode reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024.\nL. Gui, C. Gˆarbacea, and V. Veitch. Bonbon alignment for large language models and the sweetness of\nbest-of-n sampling. arXiv preprint arXiv:2406.00832, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\nmathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n15\n\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B.\nHanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a.\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas,\nE. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux,\nP. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and\nW. E. Sayed. Mixtral of experts, 2024b. URL https://arxiv.org/abs/2401.04088.\nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and\ngenerative fusion. arXiv preprint arXiv:2306.02561, 2023a.\nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and\ngenerative fusion, 2023b. URL https://arxiv.org/abs/2306.02561.\nJ. Li, Q. Zhang, Y. Yu, Q. Fu, and D. Ye. More agents is all you need, 2024. URL https://arxiv.org/\nabs/2402.05120.\nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,\nA. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097,\n2022.\nT. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent\nthinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.\nY. Lin, H. Lin, W. Xiong, S. Diao, J. Liu, J. Zhang, R. Pan, H. Wang, W. Hu, H. Zhang, H. Dong, R. Pi,\nH. Zhao, N. Jiang, H. Ji, Y. Yao, and T. Zhang. Mitigating the alignment tax of rlhf, 2024. URL\nhttps://arxiv.org/abs/2309.06256.\nK. Lu, H. Yuan, R. Lin, J. Lin, Z. Yuan, C. Zhou, and J. Zhou. Routing to the expert: Efficient reward-guided\nensemble of large language models, 2023. URL https://arxiv.org/abs/2311.08692.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information\nProcessing Systems, 36, 2024.\nY. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with a reference-free reward. arXiv\npreprint arXiv:2405.14734, 2024.\nOpenPipe. Openpipe mixture of agents: Outperform gpt-4 at 1/25th the cost, 2024. URL https://\nopenpipe.ai/blog/mixture-of-agents.\nA. Ram´e, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard,\nand O. Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024. URL https:\n//arxiv.org/abs/2406.16768.\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, et al.\nCode llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nK. Sarjana, L. Hayati, and W. Wahidaturrahmi. Mathematical modelling and verbal abilities: How they\ndetermine students’ ability to solve mathematical word problems? Beta: Jurnal Tadris Matematika, 13(2):\n117–129, 2020.\n16\n\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than\nscaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314.\nK. Stechly, M. Marquez, and S. Kambhampati. Gpt-4 doesn’t know it’s wrong: An analysis of iterative\nprompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\net al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nG. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari,\nA. Ram´e, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome,\nA. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J.-B. Grill,\nB. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl,\nA. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen,\nC. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar,\nD. Rogozi´nska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin,\nG. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-Pluci´nska, H. Batra, H. Dhand,\nI. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi,\nJ. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. yeong Ji, K. Mohamed,\nK. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sjoesund, L. Usui,\nL. Sifre, L. Heuermann, L. Lago, L. McNealus, L. B. Soares, L. Kilpatrick, L. Dixon, L. Martins, M. Reid,\nM. Singh, M. Iverson, M. G¨orner, M. Velloso, M. Wirth, M. Davidow, M. Miller, M. Rahtz, M. Watson,\nM. Risdal, M. Kazemi, M. Moynihan, M. Zhang, M. Kahng, M. Park, M. Rahman, M. Khatwani, N. Dao,\nN. Bardoliwalla, N. Devanathan, N. Dumai, N. Chauhan, O. Wahltinez, P. Botarda, P. Barnes, P. Barham,\nP. Michel, P. Jin, P. Georgiev, P. Culliton, P. Kuppala, R. Comanescu, R. Merhej, R. Jana, R. A. Rokni,\nR. Agarwal, R. Mullins, S. Saadat, S. M. Carthy, S. Perrin, S. M. R. Arnold, S. Krause, S. Dai, S. Garg,\nS. Sheth, S. Ronstrom, S. Chan, T. Jordan, T. Yu, T. Eccles, T. Hennigan, T. Kocisky, T. Doshi, V. Jain,\nV. Yadav, V. Meshram, V. Dharmadhikari, W. Barkley, W. Wei, W. Ye, W. Han, W. Kwon, X. Xu,\nZ. Shen, Z. Gong, Z. Wei, V. Cotruta, P. Kirk, A. Rao, M. Giang, L. Peran, T. Warkentin, E. Collins,\nJ. Barral, Z. Ghahramani, R. Hadsell, D. Sculley, J. Banks, A. Dragan, S. Petrov, O. Vinyals, J. Dean,\nD. Hassabis, K. Kavukcuoglu, C. Farabet, E. Buchatskaya, S. Borgeaud, N. Fiedel, A. Joulin, K. Kenealy,\nR. Dadashi, and A. Andreev. Gemma 2: Improving open language models at a practical size, 2024a. URL\nhttps://arxiv.org/abs/2408.00118.\nM. R. Team et al. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https://www. databricks.\ncom/blog/introducing-dbrx-new-state-art-open-llm. Accessed on April, 26, 2024b.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nK. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by self-\ncritiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.\nJ. Wang, J. Wang, B. Athiwaratkun, C. Zhang, and J. Zou. Mixture-of-agents enhances large language model\ncapabilities. arXiv preprint arXiv:2406.04692, 2024a.\nQ. Wang, Z. Wang, Y. Su, H. Tong, and Y. Song. Rethinking the bounds of llm reasoning: Are multi-agent\ndiscussions the key? arXiv preprint arXiv:2402.18272, 2024b.\n17\n\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nY. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. An empirical analysis of compute-optimal inference for\nproblem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724.\nC. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large\nlanguage models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\nA. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin,\nJ. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang,\nK. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin,\nS. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren,\nX. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan. Qwen2\ntechnical report, 2024. URL https://arxiv.org/abs/2407.10671.\nK. Zhang, B. Qi, and B. Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion.\narXiv preprint arXiv:2407.08642, 2024a.\nK. Zhang, W. Yao, Z. Liu, Y. Feng, Z. Liu, R. Murthy, T. Lan, L. Li, R. Lou, J. Xu, et al. Diversity empowers\nintelligence: Integrating expertise of software engineering agents. arXiv preprint arXiv:2408.07060, 2024b.\nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nW. Zhou, R. Agrawal, S. Zhang, S. R. Indurthi, S. Zhao, K. Song, S. Xu, and C. Zhu. Wpo: Enhancing rlhf\nwith weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024.\nQ. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseek-coder-v2:\nBreaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024.\n18\n\nA\nSupplements\nA.1\nMulti-Layer MoA\nMoA can be extended to multiple layers. For MoA with l layers and n LLMs {Ai,j}n\nj=1 in each layer i, we\ncan formulate it as follows:\nyi =\nn\nM\nj=1\n[Ai,j(xi)] + x1,\nxi+1 = yi,\nwhere each LLM Aj\ni generates a response for the query xi, which is further concatenated with the original\nquery by the aggregator’s prompt L.\nTable 8 compares the performance of 3-Layer Mixed-MoA and 2-Layer Self-MoA as well as the total\nnumber of forward passes required for each method. Specifically, one forward pass is counted each time a\nproposer model generates an output or an aggregator synthesizes a result. Notably, Self-MoA outperforms the\n3-Layer Mixed-MoA baseline with only half the forward passes.\nTable 8: Results of 3-Layer Mixed-MoA.\nModel Configuration\nLC Win Rate\n# Forward Passes\nMixed-MoA\n3-Layer MoA [Wang et al., 2024a]\n65.4\n13\nSelf-MoA\n2-Layer Self-MoA + WizardLM-2-8x22B\n65.7\n7\nA.2\nVendi Score\nThe Vendi Score (VS) is a metric designed to evaluate diversity in machine learning. It takes as input a\ncollection of samples along with a pairwise similarity function, and it outputs a single value that represents\nthe effective number of unique elements within the sample set.\nThe score is computed using a positive semi-definite similarity matrix K ∈Rn×n as follows:\nV S(K) = exp\n\x12\n−tr\n\x12K\nn log\n\x12K\nn\n\x13\x13\x13\n= exp\n \n−\nn\nX\ni=1\nλi log(λi)\n!\nHere, λi are the eigenvalues of the normalized matrix K\nn , and 0 log 0 = 0. Essentially, the Vendi Score is\nthe exponential of the von Neumann entropy of K\nn , which reflects the Shannon entropy of its eigenvalues,\nalso referred to as the effective rank. This metric provides a quantitative measure of diversity based on the\ndistribution of similarity scores among the samples.\nA.3\nNormalization of Inputs\nGiven a sequence of inputs x1, ..., xn. Let x′ denote the normalized x. We have\nx′ = xi −¯x\nstd(x) , where ¯x = 1\nn\nn\nX\ni=1\nxi, and std(x) =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(xi −¯x)2\n19\n\nA.4\nImplication of R-squre\nThe implications of R2 are presented in Table 9, illustrating the degree of influence between the independent\nand dependent variables. [Sarjana et al., 2020].\nTable 9: The interpretation of R-square\nR-square\nLevel\n[0, 0.2)\nVery weak\n[0.2, 0.4)\nWeak\n[0.4, 0.6)\nMedian\n[0.6, 0.8)\nStrong\n[0.8, 1.0]\nVery Strong\nB\nAdditional Results\nB.1\nMT-Bench Results\nWe also compare MoA and Self-MoA on the MT-Bench [Zheng et al., 2023] benchmark under the same\nexperiment setting as Wang et al. [2024a]. We copy the numbers from Wang et al. [2024a] for 3-Layer MoA\nsettings, and report our implemented results for the other experiments to ensure that 2-Layer experiments are\nfair comparisons. Table 10 shows that Self-MoA outperforms its Mixed-MoA counterpart, and using GPT-4o\nas the aggregator can achieve the best performance even with fewer forward passes compared to 3-Layer\nMoA with GPT-4o.\nB.2\nComparison to Universal Self-Consistency\nWe conduct further experiments to compare Self-Consistency [Wang et al., 2022] with MoA and Self-MoA\non the AlpacaEval 2.0 benchmark. As this benchmark is an instruction-following task without exact answers,\nwe evaluate on Universal Self-Consistency (USC) [Chen et al., 2023b] which prompts LLMs to generate\nthe most consistent response. We report the result in Table 12, which shows that USC performs worse than\nits MoA counterpart when proposers and aggregators are controlled. This further suggests that rather than\nfinding the most consistent response, MoA and Self-MoA can encourage LLM to synthesize the references\nand produce a better response.\nB.3\nNormalizing Sub-tasks in Table 6\nThe results in Table 3 indicate that the variance of models on CRUX is generally higher than that of the other\ntwo tasks, which could bias the average performance towards CRUX. To ensure that each task contributes\nequally to the overall performance metric, we assign weights to the three tasks based on the inverse of their\nvariance.\nFor example, considering MMLU, we report 19 performance metrics (including individual models,\nMixed-MoA, and Self-MoA) in Table 3. The standard deviation of performance for MMLU across these 19\n20\n\nTable 10: Comparison of Self-MoA and Mixed-MoA on MT-Bench. We use Qwen1.5-110B-Chat and GPT-4o\nas the aggregator.\nModel Configuration\nAvg.\n1st turn\n2nd turn\n# Forward Passes\nIndividual\nWizardLM-2-8x22B\n8.99\n9.05\n8.93\n1\nQwen1.5-110B-Chat\n8.61\n8.77\n8.45\n1\nLLaMA-3-70B-Instruct\n8.84\n9.14\n8.54\n1\nQwen1.5-72B-Chat\n8.62\n8.66\n8.58\n1\nMixtral-8x22B-Instruct-v0.1\n8.49\n8.89\n8.09\n1\ndbrx-instruct\n7.82\n8.21\n7.43\n1\nMixed-MoA\n2-Layer MoA\n9.06\n9.23\n8.89\n7\n2-Layer MoA w/ GPT-4o\n9.39\n9.40\n9.37\n7\n3-Layer MoA\n9.25\n9.44\n9.07\n13\n3-Layer MoA w/ GPT-4o\n9.40\n9.49\n9.31\n13\nSelf-MoA +\nWizardLM-2-8x22B\n2-Layer Self-MoA\n9.13\n9.36\n8.89\n7\n2-Layer Self-MoA w/ GPT-4o\n9.52\n9.56\n9.47\n7\nsettings is calculated to be 3.50. In comparison, the standard deviation for CRUX and MATH are 5.70 and\n4.27, respectively. Consequently, the weight assigned to MMLU when calculating the “WeightedAvg” is\ngiven by:\nWeightMMLU =\n1/3.50\n(1/3.50) + (1/5.70) + (1/4.27).\nThe normalized results are shown in Table 11.\n21\n\nTable 11: This table compares Self-MoA and Mixed-MoA using a weighted composition of three sub-\ntasks. The weights are assigned to each sub-task to prevent a high-variance task, such as CRUX, from\ndisproportionately influencing the overall performance metrics. This approach ensures a more balanced\nevaluation, allowing for a fairer comparison between the two models.\nAggregator\nProposer\nMMLU\nCRUX\nMATH\nAverage\nWeightedAvg\nIndividual\n-\ni\n66.16\n36.25\n53.81\n52.07\n54.46\nIndividual\n-\nd\n60.91\n49.51\n53.82\n54.74\n55.65\nIndividual\n-\nm\n54.36\n27.88\n69.57\n50.60\n52.80\nMixed-MoA\ni\niimmdd\n67.89\n42.88\n64.38\n58.38\n60.40\nMixed-MoA\ni\nimdddd\n67.42\n44.50\n63.90\n58.61\n60.46\nMixed-MoA\ni\niiiimd\n68.90\n41.25\n63.00\n57.72\n59.94\nMixed-MoA\ni\nimmmmd\n66.63\n42.75\n66.02\n58.47\n60.40\nMixed-MoA\ni\niimmmm\n66.23\n39.25\n66.10\n57.19\n59.38\nMixed-MoA\ni\niiimmm\n67.49\n38.25\n64.16\n56.63\n59.00\nMixed-MoA\ni\niiiimm\n68.00\n37.00\n62.92\n55.97\n58.47\nMixed-MoA\ni\niidddd\n68.21\n45.50\n62.56\n58.76\n60.58\nMixed-MoA\ni\niiiddd\n68.21\n42.88\n62.38\n57.82\n59.86\nMixed-MoA\ni\niiiidd\n68.47\n40.75\n61.24\n56.82\n59.05\nMixed-MoA\ni\nmmdddd\n66.34\n46.75\n66.48\n59.86\n61.45\nMixed-MoA\ni\nmmmddd\n65.80\n47.00\n67.32\n60.04\n61.57\nMixed-MoA\ni\nmmmmdd\n65.44\n42.50\n67.62\n58.52\n60.39\nSelf-MoA\ni\ndddddd\n65.23\n50.75\n63.08\n59.69\n60.86\nSelf-MoA\ni\n6×TaskBest\n69.01\n50.75\n68.42\n62.73\n64.21\nSelf-MoA\nTaskBest\nTaskBest\n69.01\n52.62\n69.80\n63.81\n65.14\nTable 12: Comparison of Self-MoA, Mixed-MoA, and Universal Self-Consistency (USC) on AlpacaEval 2.0\nleaderboard. We use Qwen1.5-110B-Chat as the aggregator.\nModel Configuration\nLC Win Rate\n# Forward Passes\nMixed-MoA\nMoA\n59.1\n7\nSelf-MoA\nSelf-MoA + WizardLM-2-8x22B\n65.7\n7\nUniversal Self-Consistency\nMixed-USC\n53.8\n7\nSelf-USC + WizardLM-2-8x22B\n60.2\n7\n22'),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content="1. Introduction\nText-to-image (T2I) generative models have revolutionized\ncontent generation by producing diverse and highly photo-\nrealistic images, enabling a wide range of applications such\nas digital art creation (Mazzone & Elgammal, 2019), image\nediting (Brooks et al., 2023), and medical imaging (Kaze-\nrouni et al., 2023). These models are usually trained on\nseveral billions of web-scraped image and text pairs pre-\nsumably capturing a broad spectrum of semantic concepts.\nConsequently, these models are also prone to be exposed\nto and thus generate disturbing content containing nudity,\nviolence, child exploitation, and self-harm – raising serious\n1Department of Computer Science, Boston University 2Runway.\nCorrespondence to: Deepti Ghadiyaram <dghadiya@bu.edu>.\nRemove nudity\nChange\nPhotographic\nStyles\nChange\nObject \nattributes\nRemove violence\nRemove\nUnsafe \nconcepts\nMake the image darker\nChange the season style to winter \nChange to a full shot of a dog\nChange car color to blue\nFigure 1. Monosemantic interpretable concepts such as nudity,\nphotographic styles, and object attributes are identified using k-\nsparse autoencoders (k-SAE). We leverage them to enable precise\nmodification of a desired concept during the generation process,\nwithout impacting the overall image structure, photo-realism, vi-\nsual quality, and prompt alignment (for safe concepts). Our frame-\nwork can be used to remove unsafe concepts (top row), photo-\ngraphic styles (middle row), and object attributes (last row).\nethical concerns about their downstream applications.\nSeveral attempts have been made to enforce safe generations\nin the past: integrating safety filters as part of the generation\npipeline (Rando et al., 2022), guiding the generation process\naway from a pre-defined unsafe latent space (Schramowski\net al., 2023), or directly erasing inappropriate concepts by\nmodifying model weights (Gandikota et al., 2023; Heng\n& Soh, 2024; Li et al., 2024). While partially success-\nful, some of these methods involve model training which\nis not only computationally expensive but also alters the\noverall model generative capabilities. More recently, a few\ninference-based approaches have been proposed, which do\nnot alter model weights (Yoon et al., 2024; Jain et al., 2024).\nSAFREE (Yoon et al., 2024) alters the semantics of the input\nprompt by filtering toxic tokens, while TraSCE (Jain et al.,\n2024) modifies negative prompting with gradient compu-\ntation to guide the model towards safer outputs. Crucially,\nsometimes these models have the undesirable consequence\n1\narXiv:2501.19066v1  [cs.CV]  31 Jan 2025\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nof visual degraded output generations or being misaligned\nwith input prompts, even when the prompts are benign. Ad-\nditionally, the increased inference time (e.g., 8.84s overhead\nper image as noted in TraSCE (Jain et al., 2024)) due to\nonline filtering makes them difficult to deploy in practice.\nIn this work, we posit that the semantic information is in-\nterwoven across different layers of a generative model in\ncomplex ways that is not fully understood. Subsequently, ex-\nisting training or inference-based safe generation techniques\ncould be altering this latent landscape in undesirable ways\nleading to misaligned or irrelevant outputs. To this end, we\napproach the generation process from the ground up and ex-\nplore the following crucial question: can we systematically\nisolate monosemantic1 concepts of varied granularities (fine-\ngrained and abstract) from the generative latent space and\nsurgically manipulate only them? Having such a tool would\nbe invaluable as it would allow the user to intentionally con-\ntrol just the relevant concept of interest without disrupting\nthe overall latent landscape.\nTo this end, we leverage k-sparse autoencoders (k-\nSAE) (Makhzani & Frey, 2013) to design controllable gen-\nerative models. k-SAEs have shown promising progress in\ninterpreting language models by learning a sparse dictionary\nof monosemantic concepts (Bricken et al., 2023; Cunning-\nham et al., 2023). In our work, we first train a k-SAE on the\nembeddings extracted from a corpus of text prompts contain-\ning semantic concepts we wish to control (e.g., unsafe con-\ncepts). Once trained, each k-SAE’s hidden state corresponds\nto an isolated monosemantic concept. During the generation\nprocess, given a concept we wish to steer, we use k-SAE to\nidentify its corresponding latent direction and precisely ma-\nnipulate the presence of that concept in the outcome, without\nimpacting the overall generation capability (Fig. 1). Notably,\nour method does not require any fine-tuning as in Zhang\net al. (2024), synthetic data generation as in Esposito et al.\n(2023), training a separate LoRA adapter (Hu et al., 2021)\nfor each concept as in Gandikota et al. (2025) to manipulate\nmaking it fast, efficient, and adaptable to any pre-trained\ntext to image generative framework. We summarize our\nempirical findings and key contributions below:\n• We identify interpretable monosemantic concepts\nin text-to-image generation latent landscape using\na k-sparse autoencoder. Once trained, k-SAE serves\nas a Concept Steerer to provide precise control over\nspecific visual concepts (e.g., nudity, violence, etc.)\n• Concept Steerer achieves state-of-the-art perfor-\nmance on unsafe concept removal while being ∼5x\nfaster than the existing best method, without compro-\nmising visual quality.\n1In contrast to the one-to-many mapping of polysemantic neu-\nrons, monosemantic neurons form a one-to-one correlation with\ntheir related input features (Yan et al., 2024).\n• Concept Steerer effectively manipulates photo-\ngraphic and artistic styles, object attributes, enabling\ncontrolled yet creative image generation.\n• Concept Steerer is robust to adversarial prompt ma-\nnipulations, achieving a 20.01% improvement against\nred-teaming tools, ensuring reliable image generation\neven under challenging scenarios.\n• Concept Steerer works out-of-the-box to any text-\nto-image model, is extremely simple, requires no re-\ntraining nor LoRA adapters, and is highly efficient.\n2. Related Work\nControlling diffusion models: Wu et al. (2023); Wallace\net al. (2024) fine-tune diffusion models using human feed-\nback and Bansal et al. (2023); Singhal et al. (2025) pro-\npose inference-time diffusion steering with reward functions.\nHowever, these methods rely on strong reward functions,\nand are computationally intensive (Uehara et al., 2025).\nSome methods achieve controllability by training additional\nmodules such as low-rank adapters (LoRAs) (Gandikota\net al., 2025; Stracke et al., 2025), which requires millions of\nparameters per concept and significantly increases genera-\ntion time (Sridhar & Vasconcelos, 2024). Several inference-\ntime intervention works attempt fine-grained control at test\ntime. However, estimating noise at each step for each con-\ncept during generation (Brack et al., 2022; 2023) signifi-\ncantly slows down generation and steering model activa-\ntions based on optimal transport (Rodriguez et al., 2024)\nrequires learning activation mapping for each style. By con-\ntrast, our approach is very simple, requires no training of the\nbase model or LoRA adapters, no additional noise/gradient\ncomputation during the generation process. Moreover, once\ntrained, our approach allows us to manipulate any concept\nwe want without further tuning.\nSafe generation: Given the growing concerns of genera-\ntive models’ capability to produce inappropriate content,\nseveral valuable research has emerged in this space. Some\ntraining-based methods (Gandikota et al., 2023; Zhang et al.,\n2024) directly remove inappropriate concepts from the dif-\nfusion model through additional fine-tuning, while some\nothers like (Gandikota et al., 2024; Gong et al., 2025) up-\ndate model weights to erase concepts without retraining\nthe model. Some recently proposed inference-based ap-\nproaches (Yoon et al., 2024; Jain et al., 2024) do not require\ntraining or weight updates. While effective, these methods\noften result in degraded image quality and increased infer-\nence time. Unlike all prior works, our method surgically\nisolates interpretable concepts in the generative latent space\nand manipulating only these in the text encoder. Thus, our\napproach enjoys the benefit of precise control of inappropri-\nate concepts, does not compromise on generation quality,\nand maintains prompt-image alignment.\n2\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nDiffusion model\nText prompt \n“Greek goddess \nposing …”\nText\nEncoder\nUnsafe \nSafe \nConcept 𝑪\n“Nudity”\nk-SAE\nDecoder\nEncoder\nSafe path\nUnsafe path\nText\nEncoder\n∗λ\nFigure 2. K-sparse autoencoder (k-SAE) is trained on feature\nrepresentations from the text encoder of the diffusion model. Once\ntrained, it serves as a Concept Steerer, enabling precise, surgical\nconcept manipulation by adjusting λ.\nInterpreting diffusion models: Recent works have demon-\nstrated that sparse autoencoders (SAE) could recover inter-\npretable features in large language models (Bricken et al.,\n2023; Cunningham et al., 2023), CLIP vision features (Fry,\n2024; Daujotas, 2024) and diffusion features (Kim et al.,\n2024; Surkov et al., 2024). Kim et al. (2024) reveal monose-\nmantic interpretable features represented within rich visual\nfeatures of the diffusion model while Surkov et al. (2024)\ninvestigate how text information is integrated via cross-\nattention. By contrast, we focus on the text encoder of a dif-\nfusion model, identify interpretable directions via k-SAEs,\nand demonstrate precise steering of a variety of concepts.\n3. Approach\nWe propose a simple yet effective technique to precisely\nisolate and steer semantic concepts such as nudity or pho-\ntographic styles using k-sparse autoencoders (Makhzani &\nFrey, 2013) (k-SAE). We first present how we train such\na k-SAE (Sec. 3.2), followed by our method to combine\ndifferent monosemantic neurons to steer abstract concepts\n(Sec. 3.3). We stress that a k-SAE is trained only once and\nno training is required for any concept the user wishes to\nintroduce, eliminate, or modulate.\n3.1. Preliminaries on text to image models\nText-to-image diffusion models (Rombach et al., 2022;\nRamesh et al., 2022; Saharia et al., 2022) primarily con-\nsist of a text encoder to extract a text prompt’s interme-\ndiate embedding and a diffusion model. During training,\nthe diffusion model progressively denoises a noisy im-\nage (or its latent representation) conditioned on the text\nprompt’s intermediate embedding. Formally, given an in-\nput y0, the forward diffusion process progressively adds\nnoise to y0 over T timesteps. The intermediate noisy im-\nage at timestep t is yt =\np\n(1 −βt)y0 + √βtϵ where ϵ is\nthe Gaussian noise and βt is a timestep-dependent hyper\nparameter. In the reverse process, the diffusion model ϵθ\niteratively denoises yt at each timestep, conditioned on the\ntext prompt embedding c, to predict noise ϵ. The objective\nfunction for training the model is to minimize the error be-\ntween the introduced and the predicted noise, defined as:\nEy,t,ϵ∼N(0,1)\n\x02\n∥ϵ −ϵθ(yt, c, t)∥2\n2\n\x03\n3.2. Preliminaries on k-sparse autoencoders\nSparse autoencoders (Ng et al., 2011) are neural networks\ndesigned for learning compact and meaningful feature rep-\nresentations in an unsupervised manner. They consist of an\nencoder and a decoder, optimized jointly using a reconstruc-\ntion loss and a sparsity regularization term to encourage\nonly a few neurons to be maximally activated for a given in-\nput. However, the sparsity constraint introduces significant\nchallenges during optimization (Tibshirani, 1996; Makhzani\n& Frey, 2013). To mitigate these issues, k-sparse autoen-\ncoders (k-SAEs) (Makhzani & Frey, 2013) were introduced.\nThey explicitly control the number of active neurons to k\nduring training by applying a Top-k activation function at\neach training step. Consequently, this retains only the k\nhighest activations and zeroes out the rest.\nLet Wenc ∈Rn×d and Wdec ∈Rd×n represent the weight\nmatrices of the k-SAE’s encoder and decoder respectively\n(Fig. 2). The hidden layer dimension n is defined as an\ninteger multiple of the input feature dimension d. The ratio\nn/d, referred to as the expansion factor, controls the extent\nto which the hidden dimension is expanded relative to the\ninput dimension. Following Bricken et al. (2023), bpre ∈Rd\ndenotes the bias term added to input x before feeding to the\nencoder (aka pre-encoder bias), while benc ∈Rn denotes\nthe bias term of the encoder.\nLet x ∈RL×d denote the intermediate representation of the\ntext encoder for an input prompt in a text-to-image model,\nwhere L denotes the number of tokens. The encoded latent\nz is computed as:\nz = ENC(x) = Top-k(ReLU(Wenc(x −bpre) + benc)),\n(1)\nwhere the Top-k function retains only the top k neuron acti-\nvations and sets the remaining activations to zero (Makhzani\n& Frey, 2013). The decoder reconstructs ˆx as:\nˆx = DEC(x) = Wdecz + bpre,\n(2)\nThe training objective of a standard k-SAE is to minimize\nthe normalized mean squared error (MSE) between the orig-\ninal feature x and the reconstructed feature ˆx, denoted by\nLmse. However, both SAEs and k-SAEs suffer from the pres-\nence of “dead latents,” where a large proportion of latents\nstop activating entirely at some point in training. Presence\nof dead latents decreases the likelihood of the network dis-\ncovering separable, interpretable features while incurring\n3\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nunnecessary computational cost (Bricken et al., 2023). To\ndiscourage dead latents, we incorporate an auxiliary MSE\nloss as suggested in Gao et al. (2024). Specifically, in every\ntraining step, we identify top kaux dead latents and recon-\nstruct a latent ˆz exclusively from them, as defined below:\nˆz = Top-kaux(ReLU(Wenc(x −bpre) + benc)),\n(3)\nNow, let ˆe = Wdecˆz represent the reconstruction using the\ntop kaux dead latents. Laux is defined as a reconstruction loss\nbetween the auto encoder’s residual and the output from the\ndead neurons (ˆe). As discussed in Gao et al. (2024), the\nintuition behind Laux is to compute gradients that push the\nparameters of the dead neurons in the direction of explaining\nthe autoencoder residual (e). Thus, the total training loss is:\nL = Lmse + αLaux = ∥x −ˆx∥2\n2 + α∥e −ˆe∥2\n2,\n(4)\nThe scalar α is a weighting factor that controls the relative\ncontribution of the auxiliary loss.\n3.3. Concept Steerers\nGiven a human-interpretable concept C2 we wish to steer,\nwe first extract its text embedding xC, pass it through k-\nSAE, and finally perform an element-wise addition with the\ninput prompt embedding x. This can be expressed as:\nxsteered = x + Wdec(λ ∗ENC(xC)),\n(5)\nwhere λ denotes a scalar that controls the steering strength.\nThe steered vector xsteered is used to condition the generation\nprocess. As we show in Sec. 4, our approach requires a k-\nSAE to be trained only once, and provides model-agnostic,\nfine-grained control over concept steering without degrading\nthe overall generation quality.\n4. Experiments\nWe first share the training setup followed by numerous re-\nsults and ablations on concept steering.\nImplementation details: We train k-sparse autoencoders\non text embeddings with kaux = 256, and loss weight pa-\nrameter α = 1/32 for 10k training steps. We train for a total\ntraining tokens of 400M on a batch size of 4096 with the\nlearning rate 0.0004 using Adam (Kingma, 2014) optimizer.\nThe k-SAE is trained with k = 32 and an expansion factor\nof 4, resulting in a total hidden size dimension n = 3072\nfor Stable Diffusion (SD) 1.4 (Rombach et al., 2022) in\nthe unsafe removal task. For style manipulation, we use\nk = 64 with an expansion factor of 64, resulting in a total\nhidden size dimension n = 49152 for SD1.4 and k = 64\nwith an expansion factor of 16, resulting in a total hidden\nsize dimension n = 32768 for SDXL-Turbo (Sauer et al.,\n2Defined by any user-provided prompt, e.g., “nudity”.\n2025). These settings were found via ablation studies on\ndownstream tasks and/or chosen based on overall training\nstability and sparsity. We apply a unit normalization con-\nstraint (Sharkey et al., 2023) on the decoder weights Wdec\nof the k-SAE after each update. Although our method can\nbe applied in an out-of-the-box manner to any text-to-image\nmodel, for a fair comparison with existing methods, we con-\nduct experiments using SD 1.4 for unsafe concept removal\nand then expand our evaluation to more recent SDXL-Turbo\nand FLUX.1-dev. When steering concepts, we use a slightly\nmodified encoder of k-SAE without the TopK activation\nfunction, as the TopK function clamps some possible impor-\ntant information that is critical for both maintaining visual\nquality and effective concept removal. More details in Ap-\npendix A.\n4.1. Steering towards safety\nSetup: First, we demonstrate the effectiveness of erasing\nunsafe concepts using k-SAEs. We use the Inappropriate\nImage Prompts (I2P) dataset (Schramowski et al., 2023)\nto evaluate our method on steering nudity concepts and\nthe Ring-A-Bell benchmark (Tsai et al., 2023) to assess\nsteering performance on violent content. We train a k-SAE\nusing features extracted from the residual stream of the 10th\nlayer (out of 12 layers) of the text encoder in SD 1.4, on the\nprompts in I2P dataset, and evaluate its performance on both\nthe I2P and Ring-A-Bell datasets. As in prior works (Tsai\net al., 2023; Jain et al., 2024), we use nudity concept Cnudity\nas “naked, nude, bare, exposed, stripped, topless, male geni-\ntalia, penis, buttocks” and use a slightly modified version\nfor the violent concept Cviolence as “violence, blood”. We set\nsteering strength λ = −0.5 for I2P dataset and λ = −0.7\nfor adversarial datasets including violent concept.\nEvaluation metrics: To quantify the impact of our method\non generation quality, we use FID (Heusel et al., 2017) and\nCLIP score (Hessel et al., 2021; Radford et al., 2021) on the\nCOCO-30k dataset, evaluating 10k generated samples. We\nreport Attack Success Rate (ASR), i.e., the percentage of\ngenerated images containing nudity or violence as a measure\nof how well a model reduces unsafe content generation.\nTo this end, we use the NudeNet (Bedapudi, 2019) with a\nthreshold of 0.45 and Q16 violence detector (Schramowski\net al., 2022), following prior work (Jain et al., 2024).\nBaselines: We compare our method against inference-based\napproaches that do not require training or weight updates to\nthe generative model, including SLD (Schramowski et al.,\n2023), SD with negative prompt (SD-NP), SAFREE (Yoon\net al., 2024), and TraSCE (Jain et al., 2024). Additionally,\nwe evaluate our method against training-based approaches,\nincluding ESD (Gandikota et al., 2023), FMN (Zhang et al.,\n2024), CA (Kumari et al., 2023), MACE (Lu et al., 2024),\nand SA (Heng & Soh, 2024), as well as approaches that\n4\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 1. Performance comparison across different methods on\nI2P and COCO datasets. Lower ASR and FID indicate better\nperformance; higher is better for CLIP. Our method achieves the\nlowest ASR by effectively removing nudity while preserving visual\nquality and prompt alignment. Bold: best. Underline: second-best.\nGray : require training and weight updates, Pink : do not require\ntraining but update model weights, Blue : do not require either.\nMETHOD\nI2P\nCOCO\nASR ↓\nFID ↓\nCLIP ↑\nSDV1.4\n17.80\n16.71\n31.3\nESD (GANDIKOTA ET AL., 2023)\n2.87\n18.18\n30.2\nCA (KUMARI ET AL., 2023)\n1.04\n24.12\n30.1\nMACE (LU ET AL., 2024)\n1.51\n16.80\n28.7\nSA (HENG & SOH, 2024)\n2.81\n25.80\n29.7\nUCE (GANDIKOTA ET AL., 2024)\n0.87\n17.99\n30.2\nRECE (GONG ET AL., 2025)\n0.72\n17.74\n30.2\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n1.74\n28.75\n28.4\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n2.28\n24.40\n29.1\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n3.95\n21.17\n29.8\nSD-NP\n0.74\n18.33\n30.1\nSAFREE (YOON ET AL., 2024)\n1.45\n19.32\n30.1\nTRASCE (JAIN ET AL., 2024)\n0.45\n17.41\n29.9\nOURS (W/O NEGATIVE STEERING)\n0.57\n18.37\n30.8\nOURS\n0.36\n18.67\n30.8\nSD 1.4\nOurs\nTraSCE\nSAFREE\nRemove \nnudity\nPrompt: “ken doll, commoner, concept art, art in the style of Mesopotamia 3000 …”\nPrompt: “painting of a cyberpunk android … fully clothed in futuristic armor …”\nRemove \nnudity\nFigure 3. Qualitative comparisons of different approaches, in-\ncluding TraSCE and SAFREE, on the I2P dataset. Our method\nremoves nudity without significantly altering the generated images,\nresulting in outputs that are better aligned with the input prompt.\nrequire no training but involve weight updates, such as\nUCE (Gandikota et al., 2024) and RECE (Gong et al., 2025).\nWe also try a variant of our model, where we steer in the op-\nposite direction of the layer activation corresponding to the\nnull text used for classifier-free guidance (Ho & Salimans,\n2022), which we refer to as negative steering.\n4.1.1. STEERING NUDITY CONCEPT\nAs shown in Table 1, our approach achieves state-of-the-art\nperformance in steering unsafe concepts, yielding the lowest\nASR (0.36) on the I2P dataset and surpassing the previous\nbest method. We note that incorporating negative steering\nslightly improves performance, demonstrating that our con-\ncept vector effectively models abstract concepts and, similar\nto negative prompting, yields a slight improvement in per-\nformance. Notably, our approach even outperforms both\ntraining-based methods (Gandikota et al., 2023; Kumari\net al., 2023; Lu et al., 2024; Heng & Soh, 2024) and weight-\nSD 1.4\nOurs\nRemove \nnudity\nRemove \nnudity\nFigure 4. Qualitative examples from the I2P dataset.\nOur\nmethod allows fine-grained control over the removal of specific\nconcepts, removing only the intended concept while preserving\nthe overall structure and style of the generated images.\nPrompt: “… future bass girl unwrapped smooth body fabric unfolds statue bust … front and side view body …”\nλ = −0.3\nλ = −0.25\nλ = −0.2\nλ = −0.15\nλ = −0.1\nλ = −0.05\nλ = 0\nFigure 5. Qualitative example from the I2P dataset with FLUX.\nOur method is model-agnostic and can be applied to both U-Net-\nbased SD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.\nSD 1.4\nOurs\nRemove \nviolence\nRemove \nviolence\nFigure 6. Qualitative examples from the Ring-A-Bell dataset.\nOur method successfully removes the abstract concept of violence,\nas shown by the absence of blood in the right images. The images\nare intentionally blurred for display purposes as they are disturbing.\nupdating methods (Gandikota et al., 2024; Gong et al., 2025),\nunderscoring the effectiveness of our method. Furthermore,\nour method achieved the highest prompt-image correspon-\ndence, as indicated by the CLIP score on the COCO dataset\n(30.8), ranking just below the original SD 1.4 model (31.3).\nThis is demonstrated in Fig. 3 and Fig. 4, where previous\nmethods sometimes generate unrelated images when the\nprompt triggers unsafe content. By contrast, our method\nsuccessfully removes nudity while preserving the overall\nstructure and maintaining alignment with the input prompt.\nMoreover, as shown in Fig. 5, we demonstrate that our\nmethod can also steer the DiT-based (Peebles & Xie, 2023)\nFLUX (Labs, 2023) model in an out-of-the-box manner.\n4.1.2. STEERING VIOLENCE CONCEPT\nWe also evaluate our method’s performance in suppress-\ning violent content generation, as presented in Table 2. As\nshown in Fig. 6, our method effectively reduces the genera-\ntion of violent content compared to existing training-based\nand weight-update-based methods. Although SLD-Max\n5\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 2. Performance comparison across different methods on\nthe Ring-A-Bell-Union (Violence) dataset. Lower values indicate\nbetter performance. Our method demonstrates competitive perfor-\nmance without compromising generation quality, as indicated by\nthe FID scores in Table 1. Bold: best. Underline: second-best.\nGray : require training and weight updates, Pink : do not require\ntraining but update model weights, Blue : do not require either.\nMETHOD\nRING-A-BELL-UNION\n(VIOLENCE)↓\nSDV1.4\n99.6\nESD (GANDIKOTA ET AL., 2023)\n86.0\nFNM (ZHANG ET AL., 2024)\n98.8\nCA (KUMARI ET AL., 2023)\n100.0\nUCE (GANDIKOTA ET AL., 2024)\n89.8\nRECE (GONG ET AL., 2025)\n89.2\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n40.4\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n80.4\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n97.2\nSD-NP\n94.8\nTRASCE (JAIN ET AL., 2024)\n72.4\nOURS\n43.7\nPrompt: “geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k, ”\nC=“Minimal-\nist”\nC=“Zoom-in, \nmagnify”\nFigure 7. Photographic style manipulation of SD 1.4 for the\ngiven prompt “geodesic landscape, john chamberlain, christopher\nbalaskas, tadao ando, 4 k, ” where concept prompts are “minimalist”\n(Top) and “zoom-in, magnify” (Bottom), respectively. In the top\nrow, the image is manipulated toward a maximalist style as λ →\n−1, while it adopts a minimalist style as λ →1. Similarly, in the\nbottom row, the image appears zoomed out and becomes blurred\nas λ →−1, whereas it becomes zoomed in and clearer as λ →1.\nachieves slightly better performance than ours, it signifi-\ncantly degrades overall image quality, yielding an FID of\n28.75 compared to 18.67 for our approach (Table 1).\n4.2. Steering of photographic styles and object\nattributes\nSetup: In this section, we demonstrate the effectiveness\nof steering photographic styles and object attributes. We\ntrain a k-SAE using features extracted from the residual\nstream of the 11th (out of 12) layer of the text encoder\nin SD 1.4. To observe the effect of photographic style\nchanges, we designed a dataset dedicated to 40 photographic\nstyles, including black-and-white, HDR, minimalist, etc.\nFor each class, we generated 100 prompts, totaling around\n4000 prompts, by querying ChatGPT. We also experiment\nwith SDXL-Turbo, where we train using features from both\nof its text encoders:11th (out of 12) and 29th (out of 32)\nlayers with prompts from I2P dataset.\nAs shown in Fig. 7 and Fig. 8 we can adjust its photographic\nConcept Sliders\nOurs\n𝐶=“Winter”\n𝐶= “Low light”\nSDXL-Turbo\nPrompt: “A photo of a forest, realistic, 8k”\nPrompt: “A photo of a tree with a bench, realistic, 8k”\nFigure 8. Qualitative comparisons with weather Concept Slid-\ners on SDXL-Turbo. Note that Concept Sliders train specific\nsliders: winter weather slider and a dark weather slider, whereas\nour method trains a k-SAE only once for different concepts. Top:\n“A photo of a tree with a bench, realistic, 8k” with concept to steer\n= “winter.” Bottom: “A photo of a forest, realistic, 8k” with the\nconcept to steer = “low light.” Notice how in the top image our\nmethod also removes leaves while in the bottom image, our method\neffectively applies a low-light effect to the original image.\n= 0\n= 0.1\n= 0.3\n= 0.4\n= 0.7\n= 0.9\nFigure 9. Image composition manipulation using SDXL-Turbo\nfor the prompt “A dog” with the concept prompt “Full shot.” Notice\nhow as λ →1, the generated image transitions from a close-up of\nthe face to a full shot.\nstyle, including “zoom-in” and “minimalist.” In Fig. 8, we\ncompare our results with Concept Sliders (Gandikota et al.,\n2025) on SDXL-Turbo where Concept Sliders train sepa-\nrate models for each weather condition style. Remarkably,\nour method can effectively steer concepts like weather con-\nditions and photographic styles. We note that I2P dataset\nin addition to the semantic concepts such as nudity and\nviolence, also had descriptors about general photographic\nstyles such as “full shot” or seasons “winter”. We believe\nthat k-SAE internalized these concepts offering us a pow-\nerful tool to surgically steer them. This powerful result\nhighlights the generalizable capability of k-SAEs to learn\ndiverse monosemantic concepts. This is corroborated by\nour results in Fig. 9, where we show that our method can\nmanipulate image compositions, changing a close-up image\nof a dog into a “full shot” of a dog while preserving the\nappearance of its head part.\nFinally, in Fig. 10, we use the same k-SAE to effectively ma-\nnipulates object attributes. Here, we inject a concept for an\nobject present in the image, such as “blue [object]” or “tree\n6\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\n= 0\n= 0.1\n= 0.2\n= 0.3\n= 0.4\n= 0.5\nFigure 10. Object attribute manipulation of SDXL-Turbo for\nthe given prompts “A car” (Top) and “A photo of a tree” (Bottom),\nwhere the concept prompts are “A blue car” (Top) and “Tree with\nautumn leaves” (Bottom). By adjusting λ, our method transitions\nthe image toward the desired concept specified by the prompts.\nwith autumn leaves.” We note that the resulting generations\npreserve most of the original content while successfully in-\njecting the desired concept. These results demonstrate the\nuniversal applicability of a k-SAE without the need to train\nseparate adapters for each concept. We wish to continue\nexploring the limits of universality of k-SAEs in the future.\n4.3. Robustness to adversarial prompt manipulation\nNext, we demonstrate the robustness of our method\nagainst adversarial prompts on four datasets:\nred-\nteaming approaches like Ring-A-Bell (Tsai et al., 2023),\nP4D (Chin et al., 2023), and attack frameworks like MMA-\nDiffusion (Yang et al., 2024) and UnlearnDiffAtk (Zhang\net al., 2025). Adversarial prompts often consist of several\nnon-English phrases or nonsensical text fragments that lack\nsemantic meaning, but fool the underlying generative mod-\nels to produce unsafe content. We follow the same setup in\nSec. 4.1 and use a k-SAE trained on I2P prompts.\nAs shown in Table 3, our method achieves the best overall\nrobustness on average across all datasets, significantly out-\nperforming the most recent works TraSCE (Jain et al., 2024)\nby 1.23% and SAFREE (Yoon et al., 2024) by 20.01%.\nSpecifically, for the MMA-Diffusion and P4D datasets, our\nmethod achieves state-of-the-art results with improvements\nof 10.60% and 1.98%, respectively. This demonstrates\nthat our method performs very well and can implicitly\nidentify monosemantic interpretable directions for “nudity”\nwithin the latent space of adversarial prompts. Notably, our\nmethod outperforms RECE (Gong et al., 2025) specifically\ndesigned for tackling adversarial prompts by 4.48%. For\nother datasets, our method ranks second-best or performs\ncomparably to the best scores. We note that k-SAE is trained\non text embeddings from I2P prompts to learn unsafe con-\ncepts and is not exposed to adversarial datasets. Remarkable\nperformance in adversarial datasets demonstrates k-SAE\ngeneralizes well to unseen prompts, even without exposure\nto prompt embeddings from different distributions, similar\nobservation to Sec. 4.2. We reiterate that once a k-SAE is\ntrained on unsafe concepts, our method does not require\nλ = 0\nλ = −0.1\nλ = −0.2\nλ = −0.3\nλ = −0.4\nλ = −0.5\nFigure 11. Effect of steering strength parameter (λ) on the I2P\ndataset while we steer nudity. Notice how as λ →−0.5, the\npresence of nudity disappears completely.\nretraining.\n4.4. Efficiency of Concept Steerer\nAs shown in Table 4, our method achieves the fastest in-\nference time among all other inference-based approaches,\nwith only a 0.14 sec./sample overhead on a single L40S\nGPU compared to the original SD 1.4. We highlight that our\nmethod is approximately 5x faster than the previous state-\nof-the-art (Jain et al., 2024) in unsafe concept removal.\n4.5. Ablation Studies\nFinally, we analyze the impact of our design choices on the\noverall steering capacity and visual quality.\nEffect of Concept Steering on Visual Quality: To evaluate\nthe impact of our approach on visual quality, we conduct a\nuser study using 50 randomly selected safe images gener-\nated by the original SD 1.4 model and nudity-steered images\nproduced by applying our method on SD 1.4. We followed\nthe setup described in Sec 4.1. The study involved 22 par-\nticipants, who were shown images in a randomized order\nand were asked to select the image they preferred most\nbased purely on overall visual quality. 44.7% of users pre-\nferred images produced by concept steering, while 44.9%\npreferred images from SD 1.4, indicating that participants\nexpressed an almost equal preference for both generations.\nThis is a crucial finding because it shows that our method\ndoes not deteriorate visual quality from the base model but\noffers the additional benefit of controllability.\nEffect of Layer Selection on Steering: We examine how\nthe selection of different layers in the text encoder impacts\nthe semantic information captured in k-SAE and thereby\nconcept steering. As shown in Table 5, representations\nfrom later layers are more effective to remove nudity and\nsteering than earlier layers. We believe that earlier layers\ncapture more low-level semantic information, thus high-\nlevel concepts such as nudity are better captured in the\nlater layers, making them suitable candidates for steering.\nSimilar observations were reported in Toker et al. (2024).\n7\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 3. Attack Success Rate (ASR) of different methods on various adversarial attack datasets. Lower ASR indicates better\nperformance. Our method achieves the best overall robustness on average across all datasets by effectively removing nudity implicitly\nembedded in the model. Bold: best. Underline: second-best. Gray : require training and weight updates, Pink : do not require training\nbut update model weights, Blue : do not require either.\nMETHOD\nRING-A-BELL ↓\nMMA-DIFFUSION ↓\nP4D ↓\nUNLEARNDIFFATK ↓\nAVG ↓\nK77\nK38\nK16\nAVG\nSDV1.4\n85.26\n87.37\n93.68\n88.10\n95.70\n98.70\n69.70\n87.05\nSA (HENG & SOH, 2024)\n63.15\n56.84\n56.84\n58.94\n47.68\n12.68\n2.81\n30.53\nCA (KUMARI ET AL., 2023)\n86.32\n91.69\n94.26\n90.76\n10.60\n5.63\n1.04\n27.01\nESD (GANDIKOTA ET AL., 2023)\n20.00\n29.47\n35.79\n28.42\n9.27\n15.49\n2.87\n14.51\nMACE (LU ET AL., 2024)\n2.10\n0.00\n0.00\n0.70\n2.72\n2.82\n1.51\n1.94\nUCE (GANDIKOTA ET AL., 2024)\n10.52\n9.47\n12.61\n10.87\n29.93\n9.86\n0.87\n12.38\nRECE (GONG ET AL., 2025)\n5.26\n4.21\n5.26\n4.91\n21.77\n5.63\n0.72\n8.76\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n23.16\n32.63\n42.11\n32.63\n35.76\n9.14\n2.44\n20.24\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n56.84\n64.21\n61.05\n60.70\n68.21\n33.10\n3.10\n41.28\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n92.63\n88.42\n91.05\n90.70\n68.21\n24.00\n1.98\n46.72\nSD-NP\n17.89\n40.42\n34.74\n31.68\n24.00\n10.00\n1.46\n16.29\nSAFREE (YOON ET AL., 2024)\n35.78\n47.36\n55.78\n46.31\n40.82\n10.56\n1.45\n24.29\nTRASCE (JAIN ET AL., 2024)\n1.05\n2.10\n2.10\n1.75\n16.60\n3.97\n0.70\n5.51\nOURS\n3.16\n8.42\n9.47\n7.02\n6.00\n1.99\n2.11\n4.28\nTable 4. Model Efficiency Comparison. Experiments were con-\nducted on a single L40S GPU on P4D dataset (150 samples in\ntotal) for the task of removing nudity.\nMETHOD\nINFERENCE TIME (S/SAMPLE) ↓\nSD 1.4\n3.02\nSAFREE (YOON ET AL., 2024)\n4.24\nTRASCE (JAIN ET AL., 2024)\n15.62\nOURS\n3.16\nTable 5. Attack Success Rate (ASR) when representations from\ndifferent encoder layers are used to train k-SAE on the I2P dataset.\nThe 10th layer yields the lowest ASR, indicating that this layer\ncaptures most information about nudity concept. k-SAE expansion\nfactor = 4, hidden neurons (n) = 3072.\nLAYERS\nASR ON I2P ↓\n12\n1.02\n10\n0.36\n8\n0.45\n6\n1.72\n4\n3.85\nEffect of k-SAE capacity on steering: We investigate the\neffect of k-SAE capacity determined by different expansion\nfactors on steering results. From Table 6, we note that\nthe performance differences between capacities is relatively\nminor, using an expansion factor of 4 proves to be the most\neffective in removing nudity.\nEffect of steering strength λ: Finally, we investigate the\neffect of the steering strength, λ. Table 7 illustrates the\nimpact of λ, showing that decreasing its value enables more\neffective removal of nudity from a greater number of images.\nAs shown in the first and second rows of Fig. 11, setting\nλ = −0.1 effectively removes the nudity concept in most\nimages. However, smaller λ values lead to a more complete\nremoval, as demonstrated in the last row of Fig. 11.\nTable 6. Attack Success Rate (ASR) for different expansion\nfactors of k-SAE trained on text embeddings extracted from the\n10th layer of the I2P prompts. An expansion factor of 4 yields the\nlowest ASR, indicating its efficacy for steering.\nEXPANSION FACTOR\nCAPACITY\nASR ON I2P↓\n4\n3072\n0.36\n8\n6144\n0.51\n16\n12288\n0.47\n32\n24576\n0.49\n64\n49152\n0.53\nTable 7. Attack Success Rate (ASR) for different values of λ of\nk-SAE with an expansion factor of 4 trained on text embeddings\nof 10th layer on the I2P dataset. λ = −0.5 yields the lowest ASR.\nλ\nASR ON I2P ↓\n−0.1\n2.59\n−0.2\n1.23\n−0.3\n0.87\n−0.4\n0.60\n−0.5\n0.36\n5. Discussion and Future Work\nWe propose a novel framework leveraging k-SAEs to enable\nefficient and interpretable concept manipulation in diffu-\nsion models. Once trained, k-SAE serves as a Concept\nSteerer to precisely control specific visual concepts (e.g.,\nnudity, violence, etc.) Our extensive experiments demon-\nstrate that our approach is very simple, does not compromise\nthe generation quality, and is robust to adversarial prompt\nmanipulations. Currently, we steer concepts by extract-\ning representations from the text encoder of the generative\nmodels. In future, we wish to explore steering via visual em-\nbeddings and allow users more control by selecting regions\nin an image and locally steer.\n8\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nImpact Statement\nAs text-to-image models are increasingly integrated into\nhigh-stakes applications, discouraging unsafe generations\nis of paramount significance. This work presents an ef-\nfective approach for identifying and suppressing unsafe\nconcept directions across various generative models. By\nimproving the controllability and reliability of generative\nmodels, our method advances the development of safer\nAI systems, facilitating their responsible deployment in\nreal-world applications.\nCode is available at: https:\n//github.com/kim-dahye/steerers\nReferences\nBansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S.,\nGoldblum, M., Geiping, J., and Goldstein, T. Universal\nguidance for diffusion models. In CVPR, 2023.\nBedapudi, P. Nudenet: Neural nets for nudity classification,\ndetection and selective censoring, 2019.\nBrack, M., Schramowski, P., Friedrich, F., Hintersdorf, D.,\nand Kersting, K. The stable artist: Steering semantics in\ndiffusion latent space. arXiv preprint arXiv:2212.06013,\n2022.\nBrack, M., Friedrich, F., Hintersdorf, D., Struppek, L.,\nSchramowski, P., and Kersting, K. Sega: Instructing\ntext-to-image models using semantic guidance. NeurIPS,\n2023.\nBricken, T., Templeton, A., Batson, J., Chen, B., Jermyn,\nA., Conerly, T., Turner, N., Anil, C., Denison, C.,\nAskell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer,\nN., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin,\nA., Nguyen, K., McLean, B., Burke, J. E., Hume,\nT., Carter, S., Henighan, T., and Olah, C.\nTowards\nmonosemanticity: Decomposing language models with\ndictionary learning.\nTransformer Circuits Thread,\n2023. https://transformer-circuits.pub/\n2023/monosemantic-features/index.html.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\nLearning to follow image editing instructions. In CVPR,\n2023.\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible scaling laws for contrastive\nlanguage-image learning. In CVPR, 2023.\nChin, Z.-Y., Jiang, C.-M., Huang, C.-C., Chen, P.-Y., and\nChiu, W.-C. Prompting4debugging: Red-teaming text-to-\nimage diffusion models by finding problematic prompts.\narXiv preprint arXiv:2309.06135, 2023.\nCunningham, H., Ewart, A., Riggs, L., Huben, R., and\nSharkey, L.\nSparse autoencoders find highly inter-\npretable features in language models. arXiv preprint\narXiv:2309.08600, 2023.\nDaujotas, G. Interpreting and steering features in images.\nLessWrong, 2024. https://www.lesswrong.com/\nposts/Quqekpvx8BGMMcaem/interpreting-\nand-steering-features-in-images.\nEsposito, P., Atighehchian, P., Germanidis, A., and Ghadi-\nyaram, D. Mitigating stereotypical biases in text to image\ngenerative systems. arXiv preprint arXiv:2310.06904,\n2023.\nFry,\nH.\nTowards\nmultimodal\ninterpretabil-\nity:\nLearning\nsparse\ninterpretable\nfeatures\nin\nvision\ntransformers.\nLessWrong,\n2024.\nhttps://www.lesswrong.com/posts/\nbCtbuWraqYTDtuARg/towards-multimodal-\ninterpretability-learning-sparse.\nGandikota, R., Materzynska, J., Fiotto-Kaufman, J., and\nBau, D. Erasing concepts from diffusion models. In\nICCV, 2023.\nGandikota, R., Orgad, H., Belinkov, Y., Materzy´nska, J.,\nand Bau, D. Unified concept editing in diffusion models.\n2024.\nGandikota, R., Materzy´nska, J., Zhou, T., Torralba, A., and\nBau, D. Concept sliders: Lora adaptors for precise control\nin diffusion models. In ECCV, 2025.\nGao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R.,\nRadford, A., Sutskever, I., Leike, J., and Wu, J. Scal-\ning and evaluating sparse autoencoders. arXiv preprint\narXiv:2406.04093, 2024.\nGong, C., Chen, K., Wei, Z., Chen, J., and Jiang, Y.-G.\nReliable and efficient concept erasure of text-to-image\ndiffusion models. In ECCV, 2025.\nHeng, A. and Soh, H.\nSelective amnesia: A continual\nlearning approach to forgetting in deep generative models.\nNeurIPS, 36, 2024.\nHessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,\nY. Clipscore: A reference-free evaluation metric for im-\nage captioning. arXiv preprint arXiv:2104.08718, 2021.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. NeurIPS, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n9\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models. arXiv preprint arXiv:2106.09685,\n2021.\nJain, A., Kobayashi, Y., Shibuya, T., Takida, Y., Memon, N.,\nTogelius, J., and Mitsufuji, Y. Trasce: Trajectory steering\nfor concept erasure. arXiv preprint arXiv:2412.07658,\n2024.\nKazerouni, A., Aghdam, E. K., Heidari, M., Azad, R.,\nFayyaz, M., Hacihaliloglu, I., and Merhof, D. Diffusion\nmodels in medical imaging: A comprehensive survey.\nMedIA, 2023.\nKim, D., Thomas, X., and Ghadiyaram, D. Revelio: Inter-\npreting and leveraging semantic information in diffusion\nmodels. arXiv preprint arXiv:2411.16725, 2024.\nKingma, D. P. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\nKumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang,\nR., and Zhu, J.-Y. Ablating concepts in text-to-image\ndiffusion models. In ICCV, 2023.\nLabs, B. F.\nFlux.\nhttps://github.com/black-\nforest-labs/flux, 2023.\nLi, X., Yang, Y., Deng, J., Yan, C., Chen, Y., Ji, X.,\nand Xu, W. Safegen: Mitigating sexually explicit con-\ntent generation in text-to-image models. arXiv preprint\narXiv:2404.06666, 2024.\nLu, S., Wang, Z., Li, L., Liu, Y., and Kong, A. W.-K. Mace:\nMass concept erasure in diffusion models. In CVPR,\n2024.\nMakhzani, A. and Frey, B. K-sparse autoencoders. arXiv\npreprint arXiv:1312.5663, 2013.\nMazzone, M. and Elgammal, A. Art, creativity, and the\npotential of artificial intelligence. In Arts, 2019.\nNg, A. et al. Sparse autoencoder. CS294A Lecture notes,\n2011.\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In ICCV, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research, 2020.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022.\nRando, J., Paleka, D., Lindner, D., Heim, L., and Tram`er,\nF. Red-teaming the stable diffusion safety filter. arXiv\npreprint arXiv:2210.04610, 2022.\nRodriguez, P., Blaas, A., Klein, M., Zappella, L., Apos-\ntoloff, N., Cuturi, M., and Suau, X. Controlling language\nand diffusion models by transporting activations. arXiv\npreprint arXiv:2410.23054, 2024.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In CVPR, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\nsion models with deep language understanding. NeurIPS,\n2022.\nSauer, A., Lorenz, D., Blattmann, A., and Rombach, R.\nAdversarial diffusion distillation. In ECCV, 2025.\nSchramowski, P., Tauchmann, C., and Kersting, K. Can\nmachines help us answering question 16 in datasheets,\nand in turn reflecting on inappropriate content? In FAccT,\n2022.\nSchramowski, P., Brack, M., Deiseroth, B., and Kersting, K.\nSafe latent diffusion: Mitigating inappropriate degenera-\ntion in diffusion models. In CVPR, 2023.\nSharkey,\nL.,\nBraun,\nD.,\nand Millidge,\nB.\nTak-\ning\nfeatures\nout\nof\nsuperposition\nwith\nsparse\nautoencoders.\nAI\nAlignment\nForum,\n2023.\nhttps://www.alignmentforum.org/posts/\nz6QQJbtpkEAX3Aojj/interim-research-\nreport-taking-features-out-of-\nsuperposition.\nSinghal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McK-\neown, K., and Ranganath, R. A general framework for\ninference-time scaling and steering of diffusion models.\narXiv preprint arXiv:2501.06848, 2025.\nSridhar, D. and Vasconcelos, N. Prompt sliders for fine-\ngrained control, editing and erasing of concepts in diffu-\nsion models. arXiv preprint arXiv:2409.16535, 2024.\nStracke, N., Baumann, S. A., Susskind, J., Bautista, M. A.,\nand Ommer, B. Ctrloralter: Conditional loradapter for\nefficient 0-shot control and altering of t2i models. In\nECCV, 2025.\n10\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nSurkov, V., Wendler, C., Terekhov, M., Deschenaux, J.,\nWest, R., and Gulcehre, C. Unpacking sdxl turbo: Inter-\npreting text-to-image models with sparse autoencoders.\narXiv preprint arXiv:2410.22366, 2024.\nTibshirani, R. Regression shrinkage and selection via the\nlasso. Journal of the Royal Statistical Society Series B:\nStatistical Methodology, 1996.\nToker, M., Orgad, H., Ventura, M., Arad, D., and Belinkov,\nY. Diffusion lens: Interpreting text encoders in text-to-\nimage pipelines. arXiv preprint arXiv:2403.05846, 2024.\nTsai, Y.-L., Hsu, C.-Y., Xie, C., Lin, C.-H., Chen, J.-Y., Li,\nB., Chen, P.-Y., Yu, C.-M., and Huang, C.-Y. Ring-a-bell!\nhow reliable are concept removal methods for diffusion\nmodels? arXiv preprint arXiv:2310.10012, 2023.\nUehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine,\nS., and Biancalani, T. Reward-guided controlled gener-\nation for inference-time alignment in diffusion models:\nTutorial and review. arXiv preprint arXiv:2501.09685,\n2025.\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\nN. Diffusion model alignment using direct preference\noptimization. In CVPR, 2024.\nWu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human\npreference score: Better aligning text-to-image models\nwith human preference. In ICCV, 2023.\nYan, H., Xiang, Y., Chen, G., Wang, Y., Gui, L., and He, Y.\nEncourage or inhibit monosemanticity? revisit monose-\nmanticity from a feature decorrelation perspective. arXiv\npreprint arXiv:2406.17969, 2024.\nYang, Y., Gao, R., Wang, X., Ho, T.-Y., Xu, N., and Xu, Q.\nMma-diffusion: Multimodal attack on diffusion models.\nIn CVPR, 2024.\nYoon, J., Yu, S., Patil, V., Yao, H., and Bansal, M. Safree:\nTraining-free and adaptive guard for safe text-to-image\nand video generation. arXiv preprint arXiv:2410.12761,\n2024.\nZhang, G., Wang, K., Xu, X., Wang, Z., and Shi, H. Forget-\nme-not: Learning to forget in text-to-image diffusion\nmodels. In CVPR, 2024.\nZhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J.,\nDing, K., and Liu, S. To generate or not? safety-driven un-\nlearned diffusion models are still easy to generate unsafe\nimages... for now. In ECCV, 2025.\n11\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nAppendix\nA. Implementation details\nTraining k-SAE with FLUX: For FLUX.1-dev (Labs, 2023) visualization, we train k-SAE using features extracted from\nthe residual stream of the 23rd (out of 24) layer of the T5-XXL text encoder on prompts from the I2P dataset. The k-SAE is\ntrained with k = 64 and an expansion factor of 16, resulting in a total hidden size dimension n = 65536.\nText encoders of diffusion models: We extract text embeddings for k-SAE from CLIP ViT-L/14 (Radford et al., 2021) for\nSD 1.4, OpenCLIP-ViT/G (Cherti et al., 2023) and CLIP-ViT/L for SDXL-Turbo, and T5-XXL (Raffel et al., 2020) for\nFLUX.1-dev.\nB. More details of the benchmarks\nWe evaluate our method for unsafe concept removal tasks on five publicly available inappropriate or adversarial prompts\ndatasets following prior work (Jain et al., 2024): I2P3 (Schramowski et al., 2023), Ring-A-Bell4 (Tsai et al., 2023),\nP4D5 (Chin et al., 2023), MMA-Diffusion6 (Yang et al., 2024), and UnlearnDiffAtk7 (Zhang et al., 2025). I2P contains 4703\nreal user prompts that are likely to produce inappropriate images. Ring-A-Bell consists of two inappropriate categories:\nnudity and violence. For nudity, it contains 95 unsafe prompts for each split (K77, K38, and K16). For violence, we use the\nRing-A-Bell Union dataset, which includes 750 prompts. P4D contains 151 unsafe prompts generated by white-box attacks\non the ESD (Gandikota et al., 2023) and SLD (Schramowski et al., 2023). MMA-Diffusion contains 1000 strong adversarial\nprompts generated via a black-box attack. UnlearnDiffAtk contains 142 adversarial prompts generated using white-box\nadversarial attacks.\nC. Additional qualitative results\nIn this section, we provide additional qualitative results.\nSteering nudity concept on inappropriate dataset: Figure 12 presents additional qualitative results using FLUX on\nprompts from I2P dataset. Our method effectively removes the abstract concept of nudity in DiT-based FLUX in an\nout-of-the-box manner.\nSteering nudity concept on adversarial dataset: Figure 13 presents qualitative comparisons with different methods on\nthe P4D dataset. Since P4D contains adversarial prompts specifically designed to challenge generative models, previous\nmethods either fail by generating unsafe images or produce unrelated images as a defense mechanism when the prompt\ntriggers to generate unsafe content (middle row). In contrast, our method successfully removes nudity while preserving the\noverall structure and maintaining alignment with the input prompt, even when the prompt itself is nonsensical (first and last\nrow).\nSteering violent concept: Figure 14 presents qualitative examples on the Ring-A-Bell dataset for violent concept removal.\nOur method effectively removes the abstract concept of violence by eliminating visual cues such as blood and firearms.\nSteering photographic styles: Figure 15 presents qualitative examples of photographic style manipulations in SD 1.4,\nincluding “HDR,” “Black and White,” “Sepia Tone,” and “Astrophotography.” We note that as λ →0.5, the generated image\ngradually transitions to the desired concept.\nSteering object attributes: Figure 16 presents qualitative examples of object attributes manipulations in SDXL-Turbo.\nGiven a prompt, we inject a concept for an object present in the image, such as “an orange cake” and “a chocolate cake.” We\nnote that the resulting generations preserve most of the original content while successfully injecting the desired concept.\n3https://huggingface.co/datasets/AIML-TUDA/i2p\n4https://huggingface.co/datasets/Chia15/RingABell-Nudity\n5https://huggingface.co/datasets/joycenerd/p4d\n6https://huggingface.co/datasets/YijunYang280/MMA-Diffusion-NSFW-adv-prompts-benchmark\n7https://github.com/OPTML-Group/Diffusion-MU-Attack/blob/main/prompts/nudity.csv\n12\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nPrompt: “sensual female painting, yoga, pancakes, fire smouldering, tattoos, midnight, dark … ”\nPrompt: “excellent framing lighting texture … sunlight illuminates the body as … lovely …”\nPrompt: “… photorealistic portrait of a woman's pale fat belly by sarah moon …”\nFigure 12. Qualitative example from the I2P dataset with FLUX. Our method is model-agnostic and can be applied to both U-Net-based\nSD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.\n13\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nSD 1.4\nOurs\nTraSCE\nSAFREE\nRemove \nnudity\nPrompt: “… waterhouse motive waterhouse venus … madonna ngmale”\nPrompt: “… girl alier roman renaissance nude paintings”\nRemove \nnudity\nRemove \nnudity\nPrompt: “phyllis nude bath ccp bathing steamy … painting …”\nFigure 13. Qualitative comparisons of different methods, including TraSCE and SAFREE, on the P4D dataset. The P4D dataset\nconsists of adversarial prompts designed to challenge generative models. Our approach effectively removes the concept of nudity during\nthe generation process, producing safe and semantically meaningful outputs. In contrast, SAFREE fails to generate safe images, while\nTraSCE sometimes produces unrelated outputs despite the presence of semantically meaningful keywords in given prompts, such as “girl,”\n“roman,” “renaissance,” and “paintings” (middle row).\nSD 1.4\nOurs\nRemove \nviolence\nRemove \nviolence\nRemove \nviolence\nRemove \nviolence\nSD 1.4\nOurs\nFigure 14. Qualitative examples from the Ring-A-Bell dataset. Our method successfully removes the abstract concept of violence, as\nshown by the absence of blood in the right images. The images are intentionally blurred for display purposes as they are disturbing.\n14\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nPrompt: “geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k”\nC=“HDR”\nC=“Black \nand white”\nC=“Sepia \nTone”\nC=“Astro-\nphotography”\nFigure 15. Photographic style manipulation of SD 1.4 for the given prompt “geodesic landscape, john chamberlain, christopher balaskas,\ntadao ando, 4 k, ” where concept prompts are “HDR,” “Black and white,” “Sepia Tone,” and “Astrophotography,” respectively. As\nλ →0.5, the generated image gradually transitions to the desired concept.\nPrompt: “A photo of a cake, 4 k ”\nC=“A choco-\nlate cake”\nC=“A white \ncake”\nC=“A lemon \ncake”\nC=“An orange \ncake”\nFigure 16. Object attribute manipulation of SDXL-Turbo for the given prompts “A photo of a cake, 4k,” where the concept prompts\nare “A chocolate cake,” “A white cake,” “A lemon cake,” and “An orange cake,” respectively. By adjusting λ, our method transitions the\nimage toward the desired concept specified by the prompts.\n15"),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content='1. Introduction\nComputer-Aided Design (CAD) is indispensable for 3D\ncreation across industrial sectors (Deng et al., 2023). It rep-\nresents 3D models through a sequence of operations known\n† Work done during the internship at Microsoft Research Asia.\n1University of Toronto 2University of Science and Technology of\nChina 3Microsoft Research Asia. Correspondence to: Shizhao Sun\n<shizsu@microsoft.com>.\nPreprint. Under review.\nas a parametric sequence, which combines lines, arcs, and\ncircles to create 2D sketches and then extrude them to form\n3D models. CAD models are inherently multimodal, as they\nare constructed using parametric sequences for precise edit-\ning and manufacturing, while also being rendered as visual\nobjects for practical use, referred to as multimodal charac-\nteristic (Figure 1(b)(c)). Moreover, the process of rendering\nparametric sequences into visual objects exhibits a many-\nto-one mapping, where different parametric sequences can\nresult in identical visual objects, referred to as many-to-one\nrendering characteristic (Figure 1(d)).\nCreating CAD models demands considerable expertise\nand numerous iterations, making it complex and time-\nconsuming. Text-to-CAD (Figure 1(a)(b)), which refers to\nthe automatic generation of parametric sequences from tex-\ntual descriptions, is critical for streamlining this creation pro-\ncess. It allows designers and engineers to quickly prototype\nand iterate designs by describing their intent in natural lan-\nguage, reducing the time spent on manually creating CAD\nmodels from scratch. Additionally, it makes the creation\nprocess more accessible to individuals without extensive\ntraining, enabling wider participation.\nWhile important, Text-to-CAD has received limited atten-\ntion. Most studies do not utilize text to control CAD gener-\nation. Instead, they explore generating CAD designs from\nrandom noise (Wu et al., 2021; Xu et al., 2022), by randomly\naltering components of existing CAD designs (Xu et al.,\n2022; 2023; Zhang et al., 2024b), or from point cloud (Khan\net al., 2024a). A few studies make preliminary attempts at\nText-to-CAD (Khan et al., 2024b; Li et al., 2024b). They\ntrain Transformer-based framework with ground-truth para-\nmetric sequences as supervision, termed sequential signal.\nHowever, due to multimodal and many-to-one rendering\ncharacteristic of CAD models (Figure 1(b)(c)(d)), both the\nsequential signal and visual signal are crucial for training\na Text-to-CAD model. The sequential signal, derived from\nground-truth parametric sequences, provides critical infor-\nmation about sequence structure and parametric operations.\nWithout it, learning to generate logically coherent paramet-\nric sequences becomes challenging, as there is no direct\nsupervision for sequence structure and parametric opera-\ntions. The visual signal, obtained from rendered visual\n1\narXiv:2501.19054v2  [cs.CV]  5 Feb 2025\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nl i ne, 10, 7 <cur ve_end> l i ne, 52, 7 <cur ve_end> l i ne, 52, 55 <cur ve_end> l i ne, 10, 55 \n<cur ve_end> <l oop_end> l i ne, 12, 9<cur ve_end> l i ne, 50, 9 <cur ve_end> l i ne, 50, 53 \n<cur ve_end> l i ne, 12, 53 <cur ve_end> <l oop_end> l i ne, 11, 8 <cur ve_end> l i ne, 12, 8 \n<cur ve_end> l i ne, 12, 9 <cur ve_end> l i ne, 11, 9 <cur ve_end> <l oop_end> l i ne, 53, 8 \n<cur ve_end> l i ne, 54, 8 <cur ve_end> l i ne, 54, 9 <cur ve_end> l i ne, 53, 9 <cur ve_end> \n<l oop_end> <f ace_end> <sket ch_end> \nadd, 16, 31, 31, 31, 31, 1, 0, 0, 0, 0, 1, 0, - 1, 0, 29, 31, 48 <ext r ude_end>\nA rectangular prism with a \ntotal of five square holes. One \ncentrally located and four \nsurrounding it.\n(a) Input Prompt\n(b) CAD Design Sequence\n(c) CAD Visual Object\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nValid Design 3\nSame Design\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nValid Design 2\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end> \nValid Design 1\nGenerates \nRenders\n(d) The Many-to-one Relationship\nFigure 1. (a) and (b): Illustration of Text-to-CAD, which converts a texutal description into CAD parametric sequences. (b) and (c):\nIllustration of multimodal characteristics. CAD models are created using parametric sequences and rendered as visual objects for practical\nuse. (d): Illustration of many-to-one rendering characteristics. Different parametric sequences can produce identical visual objects.\nobjects, indicates how CAD models are perceived and evalu-\nated in practical applications. Without it, learning efficiency\nis compromised, as the goal of Text-to-CAD is for the ren-\ndered visual objects of the generated parametric sequences\nto match ground-truth visual objects. First, sequential signal\nlearning typically depends on auto-regressive generation,\nwhich emphasizes the local continuity between tokens but\nmay not fully capture the global appearance of the CAD\nmodel. Second, given the many-to-one rendering character-\nistic, multiple parametric sequences can produce the same\nvisual object. Training solely on parametric sequences may\ncause the model to give more emphasis to those present\nin the training set, overlooking other valid ones that could\nachieve the same visual outcome.\nTo this end, we propose CADFusion, a framework that\ncombines sequential and visual signals to train a Text-to-\nCAD model. It uses Large Language Models (LLMs) as its\nbackbone and alternates between two stages: the sequential\nlearning stage and the visual feedback stage. In the sequen-\ntial learning stage, LLMs are fine-tuned using ground-truth\nparametric sequences. Unlike prior works (Khan et al.,\n2024b; Li et al., 2024b) that train Transformer-based mod-\nels from scratch, we take advantage of pre-trained LLMs,\nwhich leverages their inherent natural language understand-\ning and foundational knowledge of CAD design (Makatura\net al., 2023) acquired during the extensive pre-training. In\nthe visual feedback stage, feedback derived from rendered\nvisual objects is integrated into the LLMs. This stage ad-\ndresses two critical challenges. First, the rendering process\nthat converts parametric sequences into visual objects is\nnon-differentiable, making backpropagation through this\npathway infeasible. To overcome this, we frame the problem\nas preference learning task and adopt direct preference opti-\nmization (DPO) (Rafailov et al., 2024). Specifically, pref-\nerences are assigned to the rendered visual objects, and the\nLLMs are optimized to increase the likelihood of parametric\nsequences that produce preferred visual objects while de-\ncreasing the likelihood of those that yield less preferred ones.\nThis approach enables effective training of LLMs, even with\na non-differentiable rendering pathway. Second, collecting\nreliable preference data is costly and labor-intensive. To\naddress this, we introduce an automated pipeline that uti-\nlizes large vision-language models (LVMs) to efficiently\nscore the rendered visual objects. Finally, to ensure bal-\nanced learning and retain the contributions of both signals,\nwe alternate between the sequential learning stage and the\nvisual feedback stage throughout training.\nWe summarize our main contributions as follows:\n• We propose to leverage both the sequential signal and\nvisual signal to train a Text-to-CAD model.\n• For the sequential signal, we use LLMs as the back-\nbone and fine-tune it on ground-truth parametric se-\nquences. For the visual signal, we adopt direct prefer-\nence optimization to bypass non-differentiable render-\ning and introduce a LVM-based scoring pipeline for\nefficient preference data collection. To balance both\nsignals, we alternate between the sequential learning\nand the visual feedback stage.\n• We contribute two datasets for Text-to-CAD: one with\nthe sequential signal and another with the visual signal.\n• We present qualitative and quantitative experiments to\nshowcase CADFusion’s superior ability.\n2. Related Works\nCAD Generation. CAD generation takes user requirements\nas input and generates CAD models as output.\nOn the input side, user requirements can be expressed in\ndiverse ways. Wu et al. (2021) uses random noise as input\nto generate CAD models randomly. Zhang et al. (2024b),\nXu et al. (2022) and Xu et al. (2023) modify specific parts\nof the existing CAD models to generate new ones. Khan\net al. (2024a) and Ma et al. (2024) take point cloud as input\nto produce corresponding CAD models. In contrast, our\nwork focuses on textual descriptions as input. Recent stud-\n2\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nies (Khan et al., 2024b; Li et al., 2024b) explore text-based\ninput for CAD generation. Khan et al. (2024b) proposes a\ndata annotation pipeline for synthesizing training data and a\ntransformer-based autoregressive network. Li et al. (2024b)\ndesigns an encoder-decoder framework with a cascading\ncontrastive strategy and CT-Mix to align text with paramet-\nric sequences. Unlike these studies, which rely solely on\nsequential signals, our work combines sequential and visual\nsignals for improved performance.\nOn the output side, CAD models can be represented in vari-\nous formats, including Constructive Solid Geometry (CSG),\nBoundary Representation (B-Rep) and Sketch-and-Extrude\n(SE). CSG constructs 3D models by combining basic primi-\ntives such as cubes, cylinders, and spheres, through Boolean\noperations and subtractions (Du et al., 2018; Kania et al.,\n2020; Yu et al., 2021; 2023). B-Rep represents 3D models\nusing geometric elements such as vertices, edges, and faces\n(Jayaraman et al., 2023; Wang et al., 2022; Xu et al., 2024).\nSE begins with 2D sketches composed of lines, arcs, and\ncircles, which are then extruded to form 3D models (Willis\net al., 2021; Wu et al., 2021). In this work, we adopt SE\nas it preserves the design history of CAD models, making\nthem more intuitive to edit.\nLarge Language Models (LLMs). LLMs have recently\nachieved remarkable success (Touvron et al., 2023; Brown\net al., 2020; OpenAI, 2024; Bubeck et al., 2023; Zhao et al.,\n2023). Supervised fine-tuning (SFT) is widely used to im-\nprove performance, while reinforcement learning (RL) is\noften employed to align LLM output with human prefer-\nence (Brown et al., 2020; Hong et al., 2024; Kaufmann\net al., 2024). Our work leverages SFT and RL1 but intro-\nduces two key differences. First, we utilize SFT and RL\nto learn from different signals (i.e., sequential and visual\nsignals) whereas existing work focuses on a single signal\n(i.e., sequential signals). Second, we alternate between SFT\nand RL stages to preserve contributions from both signals,\na strategy not commonly employed in prior work.\nReinforcement Learning with Human Feedback (RLHF).\nRLHF has been widely applied to align model output\nwith human preference across various domains, including\nLLMs (Brown et al., 2020; Radford et al., 2021; Meta, 2024),\ntext-to-image models (?) and text-to-video models (Wu\net al., 2024). As human annotation in RLHF is costly and\nnot easily scalable, reinforcement learning on AI feedback\n(RLAIF) (Liu et al., 2023; Zhang et al., 2024a; Lee et al.,\n2024), which leverages machine learning models to anno-\ntate data, has been proposed as a more affordable alternative\nto RLHF. Since RLHF/RLAIF pipeline are complex, di-\nrect preference optimization (DPO) (Rafailov et al., 2024),\n1We adopt DPO (Rafailov et al., 2024) in practice and refer\nto it as RL here for simplicity, as it implicitly optimizes the same\nobjective as traditional RLHF despite not being a typical RL.\nwhich directly optimize a model to adhere to human prefer-\nences, has been proposed to avoid explicit reward modeling\nor reinforcement learning. In this work, we adopt DPO to\naddress the challenge of non-differentiable rendering when\nlearning from visual signals, as it offers a simpler yet ef-\nfective solution compared to RLHF. Besides, inspired by\nRLAIF, we propose an automatic scoring pipeline for CAD\nmodels using LVMs. The generated scores are used to con-\nstruct preference data, enabling efficient learning without\nreliance on costly human annotations.\n3. Method\n3.1. Approach Overview\nLet a textual description be denoted as x, a CAD parametric\nsequence as y, and a rendered visual object as o. The render-\ning process from a parametric sequence y to a visual object\no is represented as r(·), such that o = r(y). Text-to-CAD\ninvolves learning a function f(·) that transforms the textual\ndescription x into the CAD parametric sequence y. i.e.,\ny = f(x). The goal is for the rendered visual object o of the\ngenerated parametric sequence y, i.e., o = r(y) = r(f(x)),\nto match the user’s desired visual object (Figure 1).\nCADFusion introduces a framework that combines sequen-\ntial and visual signal for training a Text-to-CAD model\n(Figure 2). It leverages Large Language Models (LLMs)\nas the backbone and alternates between two stages: the se-\nquential learning (SL) stage and the visual feedback (VF)\nstage. We denote the model after the i-th round of sequential\nlearning as f i\nSL(·) and after the i-th round of visual feed-\nback as f i\nVF(·). In the sequential learning stage, CADFusion\ntrains LLMs to learn sequence structures and parametric\noperations from ground-truth parametric sequences, guiding\nLLMs to generate logically coherent parametric sequences\n(Section 3.2). In the visual feedback stage, CADFusion\ntrains LLMs to understand how the rendered visual object\nwill be perceived and evaluated. By rewarding parametric\nsequences that render into visually preferred objects and\npenalizing those that do not, this stage encourages LLMs\nto generate parametric sequences capable of producing the\ndesired visual object (Section 3.3). These two stages are\nalternated throughout training, ensuring balanced learning\nand preserving contributions of both signals (Section 3.4).\n3.2. Sequential Learning Stage\nText-to-CAD requires a model capable of understanding tex-\ntual descriptions and generating CAD parametric sequences\nthat adhere to valid sequence formats and employ meaning-\nful parametric operations. We adopt the following strategies\nto efficiently achieve these capabilities.\n1) Model architecture. We use LLMs as the backbone, lever-\naging their strong natural language understanding and basic\n3\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n(c) The iterative training procedure\n(a) The Sequential Learning (SL)\n(b) The Visual Feedback (VF)\nPredicted CAD \nTokens\nGround truth \nSequence\nPreferred CAD\nProbability\nRejected CAD \nProbability\nPre-trained\nBackbone LLM\nInitial Sequential \nLearning\nPre-trained Backbone LLM\nText prompts\nCE Loss\nText prompts\nPreferred \nCAD object\nRejected \nCAD object\nDPO Loss\nPre-trained \nBackbone LLM\nVisual \nFeedback\nSequential\nLearning\nVisual \nFeedback\nSequential\nLearning\nN times\nOurs\nFigure 2. Overview of CADFusion. (a): The sequential learning stage trains LLMs using ground-truth CAD parametric sequences. (b):\nThe visual feedback stage rewards CAD parametric sequences that render into preferred visual objects and penalizes those that do not. (c):\nThe two stages are alternated to preserve contributions of both signals.\nCAD design knowledge (Makatura et al., 2023).\n2) CAD Parametric Sequence Format. We adopt the for-\nmat proposed by Zhang et al. (2024b) (Figure 1(b)), which\nrepresents CAD parametric sequences as text tokens rather\nthan binary representations or numerical attributes (Xu et al.,\n2022; 2023; Wu et al., 2021). This text-based format simpli-\nfies processing and interpretation by LLMs.\n3) Dataset.\nExisting CAD datasets (Wu et al., 2021)\ninclude CAD parametric sequences but lack paired tex-\ntual descriptions. To address this, we construct a dataset\nDSL = {(x, y)}M\n1 (‘SL’ for sequential learning) containing\npaired text x and CAD parametric sequences y. We first\nprompt a LVM to generate draft captions for rendered CAD\nmodel images and then refine these drafts through human\nannotation to ensure accuracy and conciseness.\n4) Training. We fine-tune the pre-trained LLMs by mini-\nmizing the discrepancy between the generated parametric\nsequence ˆy = f i\nSL(x) and the ground-truth parametric se-\nquence y using cross entropy loss, denoted as LSL:\nLSL = −E(x,y)∼DSL\n"\n1\nT\nT\nX\nt=1\nlog p(ˆy = yt|x)\n#\n,\n(1)\nwhere T is the sequence length and p(·) is the predicted\nprobability of the t-th token by the model f i\nSL(x).\nWhile existing studies (Khan et al., 2024b; Li et al., 2024b)\nalso consider sequential signals, CADFusion introduces\nthree distinctions: 1) it uses an LLM backbone to leverage\npre-trained knowledge, unlike prior work that trains Trans-\nformers from scratch; 2) it represents CAD sequences as\ntext tokens, processed with the LLM’s tokenizer, whereas\nothers use custom tokenizers; 3) its training data undergoes\nhuman annotation, while prior work relies solely on syn-\nthesized data. These enhancements enable it to outperform\nexisting approaches, even without the visual feedback stage.\n3.3. Visual Feedback Stage\nThe goal of Text-to-CAD is to ensure the rendered visual\nobject from the generated parametric sequence matches the\ndesired visual object. Relying solely on sequential signals\ncompromises training efficiency (see Section 1). To address\nthis, we incorporate visual feedback into the model already\ntrained on sequential signals (i.e., f i\nSL(x)).\nLearning Visual Feedback through DPO. A straightfor-\nward way to incorporate visual feedback is through super-\nvised learning, which minimizes the loss between the ren-\ndered visual object from the generated parametric sequence\nˆo = r(f(x)), and the ground-truth visual object o. How-\never, since the rendering process r(·) is non-differentiable,\nthis loss cannot be backpropagated to the model f(·). To\naddress this, we reformulate the task as a reward maxi-\nmization problem, where visual feedback serves as the re-\nward, enabling optimization without requiring a differen-\ntiable rendering process. Since conventional RL is computa-\ntionally expensive, we adopt direct preference optimization\n(DPO) (Rafailov et al., 2024), a simpler and more efficient\napproach that implicitly performs reward maximization.\nSpecifically, we construct a preference dataset DVF =\n{(x, ow, ol)}N\n1 where ow and ol are rendered from the para-\nmetric sequences yw and yl, representing preferred and less\npreferred visual objects, respectively. We then optimize the\nmodel to increase the likelihood of parametric sequences\nthat produce preferred visual objects (yw), while decreasing\nthe likelihood of those that yield less preferred ones (yl):\nLVF = −E(x,yw,yl)∼DVF\n(2)\n\x14\nlogσ(β log p(ˆy = yw|x)\npref(ˆy = yw|x) −β log p(ˆy = yl|x)\npref(ˆy = yl|x))\n\x15\n,\n4\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n(a) Sample sequence responses & Render Images\nBackbone after \nSequential \nLearining\n(b) Assigning visual scores to CAD objects\n(c) Form preference data \n      from eligible pairs\nInput \nCommand\n<seq><seq>\n<seq><seq>\nLVM\nMulti-Aspect \nGrading Criteria\n10/ 10\n7/ 10\n3/ 10\n9/ 10\n, ...\nCAD\nKernel\n10/10\n10/10\n7/10\n7/10\n9/10\n9/10\nFigure 3. Illustration of preference data construction. (a): Sample CAD parametric sequences and render them into visual objects. (b):\nScore the visual objects using LVMs with multi-aspect grading criteria. (c): Construct preference data based on LVM-generated scores.\nwhere p(·) is the predicted probability of a parametric se-\nquence under the current model (f i\nVF(x)), pref(·) the prob-\nability under the reference model from the last round of\nsequential learning (f i\nSL(x)) and β is scaling factor.\nConstructing Preference Data with LVM Scoring. Col-\nlecting preference data is both costly and labor-intensive.\nThe iterative use of the visual feedback stage in our frame-\nwork (Section 3.4) further highlights the need for a quick\nand efficient approach for obtaining preference data. To\naddress this, we propose leveraging the strong visual under-\nstanding capabilities of LVMs to score visual objects and\nconstruct preference data. Figure 3 outlines the pipeline.\nFirst, the textual description x is input into the finetuned\nmodel after sequential learning (f i\nSL) to generate multiple\nparametric sequences, which are then rendered into visual\nobjects (e.g., CAD images in our implementation). Next,\nthe rendered CAD images, along with an instruction detail-\ning the evaluation criteria, are input into an LVM to obtain\nscores. Finally, the CAD image with the higher score is\nregarded as the preferred one (i.e., ow), while the one with\nthe lower score is deemed as the less preferred one (i.e., ol).\nSpecifically, inspired by recent work (Liang et al., 2024)\non evaluating text-to-image generation across rich aspects,\nwe incorporate multiple evaluation criteria into the LVM\ninstruction. As shown in Figure 4, these criteria assess\nboth the appearance of CAD designs and their alignment\nwith textual descriptions: 1) shape quality evaluates the\nregularity, naturalness, and realism of the design; 2) shape\nquantity checks whether the number of components matches\nthe description; and 3) distribution ensures components are\narranged naturally, avoiding collisions or excessive spacing.\n3.4. Alternate Training\nEach stage of the training process — sequential learning and\nvisual feedback — has a specialized focus. Excessive train-\ning in one stage can lead to the degradation of skills acquired\nin the other. For example, we empirically observe that ex-\ntended training with visual feedback can impair the model’s\nability to generate well-formatted parametric sequences,\na skill developed during sequential learning. Conversely,\nprolonged training with sequential signals can weaken the\nmodel’s capacity to produce parametric sequences that ren-\ni. Shape Quality\niii. Item Distribution\nii. Shape Quantity\n... with 4 \ncircular holes\nA Trianglular \nshape that ...\n... with 4 \ncircular holes\nFigure 4. An illustrative example of the multi-aspect evaluation\ncriteria used in LVM scoring. Note that the illustrations are simpli-\nfied to conceptually represent each criterion.\nder visually natural objects, a capability enhanced during\nthe visual feedback stage. To mitigate this, we introduce an\nalternate training strategy (Figure 2(c)). The process begins\nwith the sequential learning stage, ensuring the model ac-\nquires the ability to generate logically coherent parametric\nsequences. Subsequently, the training is divided into smaller\nblocks. Within each block, the model first learns from the\nvisual signal, followed by the sequential signal, balancing\nthe two objectives effectively.\n4. Experiments\n4.1. Setups\nDatasets.\nFor the dataset used in the sequential learn-\ning stage, we use DeepCAD dataset (Wu et al., 2021)\nas the source for CAD parametric sequences (specifically\nthe version processed by Xu et al. (2022)). We construct\na dataset compromising 20k pairs of textual instructions\nand CAD parametric sequence using the techniques intro-\nduced in Section 3.2 and Appendix A.3. For the prefer-\nence data used in the visual feedback stage, we employ\nllava-onevision-qwen2-7b (Li et al., 2024a) to\nconstruct it using the method introduced in Section 3.3. For\neach iteration of the visual feedback, we generate approxi-\nmately 1,500 preference pairs, by using 1,000 text prompts\nas input, sampling 5 times per prompt, and filtering out in-\nvalid or low-quality samples. For the test set, we construct\nit by splitting the dataset used in sequential learning into\ntrain, validation, and test sets with a 90:5:5 ratio.\nImplementation Details. LLaMA-3-8b-Instruct is\nused as the LLM backbone, with a maximum token length of\n1024. For efficient fine-tuning, we adopt Low-Rank Adapta-\n5\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\ntion (LoRA) (Hu et al., 2021) with hyperparameters r = 32\nand α = 32. The initial sequential learning stage lasts for 40\nepochs with a learning rate of 1 × 10−4, using the AdamW\noptimizer. Following this, we run 5 iterations of alternat-\ning visual feedback and sequential learning stages. The\nvisual feedback stage lasts for 5 epochs on the preference\ndata, while the sequential learning stage lasts for 1 epoch\nusing the same dataset as the initial sequential learning stage.\nTraining is conducted on four NVIDIA A6000-48GB SMX\nGPUs using PyTorch Distributed Data Parallel (DDP).\nBaselines. We consider two types of baselines. The first is\na specialized model for Text-to-CAD (Khan et al., 2024b;\nLi et al., 2024b). We use Khan et al. (2024b) as our base-\nline, as Li et al. (2024b) is not open-sourced and we were\nunable to reproduce it ourselves. The second baseline is a\ngeneral model that acquires some CAD knowledge during\npre-training. We use the most powerful model, GPT-4o,\nas our baseline. Specifically, we apply few-shot learning,\nproviding 8 examples as input for GPT-4o.\nMetrics. Our evaluation focuses on assessing the alignment\nof generated CAD models with input instructions and the\noverall quality of the generated CAD models. We employ\nthe metrics at both the sequential level and visual level. First,\nto evaluate the correspondence between the ground-truth\nand generated parametric sequences, we use F1 scores fol-\nlowing Khan et al. (2024b). Specifically, we compute F1\nscore for primitives (averaged over lines, arcs, and circles\nfor brevity) and extrusions, denoted as F1-Sketch and F1-\nExtrusion. Second, to assess the quality of the generated\nCAD models, we compare the ground-truth and generated\npoint clouds. We adopt Chamfer Distance (CD) from Khan\net al. (2024b) and additional metrics from Xu et al. (2022),\nincluding Coverage (COV), which quantifies the percent-\nage of real data covered by generated samples using CD;\nMinimum Matching Distance (MMD), which evaluates the\nclosest match between generated samples and real data; and\nJensen-Shannon Divergence (JSD), which measures distri-\nbution similarity. Additionally, we compute the Invalidity\nRatio (IR), which quantifies the percentage of generated\nparametric sequences that fail to render into valid visual\nobjects. Furthermore, we introduce an LVM-based met-\nric, denoted as LVM Score, to assess the visual correspon-\ndence between model predictions and input instructions. To\nthis end, we employ GPT-4o with a dedicated evaluation\nprompt. Further details are provided in Appendix C.1. Fi-\nnally, we conduct human assessments to rank generations\nfrom different baselines, denoted as Avg. Rank. Details on\nthis evaluation can be found in Appendix C.2.\n4.2. Main Results\nQuantitative Evaluation. Table 1 summarizes the quantita-\ntive results comparing CADFusion with baseline methods\n(see Appendix C.4 for more details). Compared to GPT-4o,\nCADFusion outperforms it across all metrics. This sug-\ngests that while the general model may have acquired some\nCAD knowledge during pre-training, explicitly optimiz-\ning for Text-to-CAD, as in our approach, is crucial for im-\nproving performance, Compared to Text2CAD, CADFusion\nachieves comparable or better performance on all metrics,\nwith particular strengths in metrics reflecting the visual qual-\nity such as LVM score and Avg. Rank. This highlights the\neffectiveness of incorporating visual signals in our approach,\nas opposed to Text2CAD, which relies solely on sequential\nsignals. This outcome also aligns with Khan et al. (2024b)’s\nlimitation statement that Text2CAD is limited to generating\nonly rectangular and cylindrical shapes. When faced with\ncomplex geometries, it struggles to perform effectively.\nQualitative Evaluation.\nFigure 5 compares the re-\nsults among the ground truth, our method, GPT-4o, and\nText2CAD on the test set. GPT-4o frequently fails to pro-\nduce renderable results across most test cases, which aligns\nwith its high invalidity ratio (IR) reported in Table 1. While\nit occasionally generates valid shapes, its outputs are of-\nten misaligned with the input prompts. Text2CAD generate\nwell-formed shapes without irregular edges or corners. How-\never, it often produces oversimplified shapes and, for more\ncomplex prompts, tends to generate multiple cubes or pan-\nels instead of accurately capturing the intended structure.\nThis aligns with its low invalidity ratio (IR) but poor visual\nscores. such as LVM score and Avg. Rank, in Table 1.\nCADFusion provides the most precise response to input in-\nstructions and achieves the highest similarity to the ground\ntruth. It successfully captures complex shapes, including\nrectangles, hexagons, and nested structures, such as a hexag-\nonal hole within a cylinder. Additionally, it exhibits a strong\nunderstanding of language cues, accurately interpreting nu-\nmerical and qualitative descriptors like “long" or “T-shape".\nAdditional qualitative results, as well as our model’s abil-\nity to generate multiple varied outputs, are presented and\ndiscussed in Appendix C.6 and C.7.\n4.3. Ablation Studies\nWe conduct ablation studies on the effectiveness of the vi-\nsual feedback stage, the impact of the alternate training, and\nthe choice between human and LVM annotation for data.\nVisual Feedback. To assess the importance of visual feed-\nback, we conduct an ablation study on CADFusion using\nonly sequential learning, denoted as CADFusionSL. The first\nrow of Table 2 presents its LVM score and invalidity ratio.\nCompared to our approach, denoted as CADFusionSL-VFSL(5)\nin Table 2, while CADFusionSL improves the invalidity ratio\nby 1.36%, it results in a significant decrease in the LVM\nscore. This underscores the crucial role of the visual feed-\nback stage: by leveraging visual preference data, our frame-\n6\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nF1↑\nCD↓\nCOV ↑\nMMD ↓\nJSD ↓\nIR ↓\nLVM Score ↑\nAvg. Rank ↓\nSketch\nExtrusion\nGPT-4o\n82.96\n85.72\n85.04\n72.40\n6.60\n37.93\n74.26\n5.13\n3.22\nText2CAD\n63.94\n92.13\n30.23\n-\n-\n-\n3.37\n2.01\n2.97\nCADFusion\n85.22\n92.79\n45.67\n90.40\n3.49\n17.11\n6.20\n8.96\n1.86\nTable 1. Quantative results - Test results on F1 scores including Sketch (primitive, averaged) and Extrusion, Chamfer Distance (CD),\nCoverage (COV), Minimum Matching Distance (MMD), Jensen-Shannon Divergence (JSD), Invalidity Ratio (IR), the LVM Score and\nthe average rank from human evaluation (Avg. Rank). An upward arrow (↑) indicates that higher values are better, while a downward\narrow (↓) signifies that lower values are preferred. Since Text2CAD does not release COV, MMD, and JSD, and we were unable to\ncompute them ourselves due to differences in setup, these values are unavailable.\n(10) "The shape is a hollow cylindrical band with a vertical sector \nremoved, resembling an incomplete ring."\n(4) "The 3D shape is a square hexagonal plate."\n(1) "The 3D shape is a cylinder and a hexagonal hole inside, which \nis smaller and makes the wall very thin."\n(2) "The 3D shape is a rectangular block with a semicylindrical \ncutout located at its center, forming a U-shaped channel."\n(5) "The 3D shape is a teardrop-like piece with two circular holes. \none large near the broader end and one small near the narrower end."\n(8) "The 3D shape is a trapezoid thin prism."\n(11) "The 3D shape is a hollow triangular prism. The walls are the \nsame and have a smaller thickness."\n(12) "The image shows two identical parallel long slim pipes."\n(9) "Three identical rectangular sheets placed vertically, arranged in \nparallel and evenly spaced."\n(6) "The shape is a cylinder with a square hole centered at the top, \nextending from the top to the bottom."\n(3) "The 3D shape is a hollow, semi-cylindrical structure cut \nlengthwise, resembling a half-pipe."\n(14) "The three-dimensional shape is a flattened cylinder."\n(15) "The 3D shape is a rectangular prism(cuboid)."\n(13) "The three-dimensional shape is an inverted T-shaped prism."\n(16) "The 3D shape is a combination of a rectangular prism base and a \nvertically oriented half-cylinder on top."\n(17) "A flat rectangular plate. All four corners are rounded and there is a \ncircular hole of the same diameter at each corner."\nPrompts\nGround\nTruth\nOurs\nGPT-4o\nText2CAD\n(7) "The  shape is composed of four vertical cylinders, roughly the \nsame size, unevenly distributed at the four corners."\n(20) "The 3D shape is a rectangular cuboid with rounded edges and \ncorners."\n(19) "The 3D shape is a hexagonal prism. The hollow center forms an open \nhexagonal cross-section."\n(18) " The 3D shape consists of a small thin rectangular prism in the middle \nof the right side of a rectangular prism."\nFigure 5. Qualitative results. The input prompt is shown at the top of each subsection. Images are arranged from left to right in the\nfollowing order: ground truth, CADFusion, GPT-4o, and Text2CAD. Outputs that cannot be rendered are marked with a red cross.\nCADFusion outperforms all baselines in understanding instructions and generating CAD objects that are both sequentially and visually\nhigh quality. GPT-4o frequently produces invalid samples and pays little attention to shape details. Text2CAD generates well-formed\nbasic shapes with a regular appearance but struggles to accurately follow input instructions and represent complex geometries.\nwork effectively enhances the visual quality of the generated\nCAD models. Additionally, CADFusionSL outperforms the\nbaseline method, Text2CAD, which also relies solely on se-\nquential signals. Note that this advantage is achieved using\n20k data, while Text2CAD uses 150k data. This demon-\nstrates the effectiveness of the techniques employed in our\nsequential learning stage, including leveraging LLMs as the\nbackbone, representing CAD parametric sequences as tex-\ntual tokens, and utilizing human annotations (Section 3.2).\nAlternate Training. In Section 3.4, we propose an alter-\nnate training strategy to retain the benefits of both sequen-\n7\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nLVM Score ↑\nIR ↓\nCADFusionSL\n7.69\n4.84\nCADFusionSLw/o HA\n6.56\n6.00\nCADFusionSL-VF\n5.94\n88.87\nCADFusionSL-VFRPO\n6.21\n3.46\nCADFusionSL-VFSL(1)w/ HA\n8.28\n17.03\nCADFusionSL-VFSL(1)\n8.76\n4.42\nCADFusionSL-VFSL(3)\n8.89\n4.21\nCADFusionSL-VFSL(5)\n8.96\n6.20\nTable 2. LVM scores and invalidity ratios across different CADFu-\nsion variants. The suffix SL indicates that the model is trained with\nthe initial Sequential Learning stage, while VF denotes the Visual\nFeedback stage without additional Sequential Learning. VFSL\nrepresents Visual Feedback with alternating Sequential Learning.\nThe tag w/ HA signifies that the data is preprocessed with human\nannotation, whereas w/o HA denotes the absence of human an-\nnotation. Numbers in parentheses indicate the number of VFSL\nrounds performed. RPO refers to the model using Regularized Pref-\nerence Optimization (RPO) (Liu et al., 2024) to stabilize DPO.\ntial learning and visual feedback stage. We compare this\napproach with three variations: 1) visual feedback only\n(CADFusionSL-VF), 2) visual feedback with an additional\nNegative Log Likelihood loss (CADFusionSL-VFRPO) to regu-\nlarize and stabilize DPO (Liu et al., 2024), and 3) iterative\nvisual-sequential training (our method).\nTable 2 presents the results, with our approach denoted\nas CADFusionSL-VFSL(5).\nThe high invalidity ratio of\nCADFusionSL-VF indicates that it struggles to generate ren-\nderable sequences, suggesting that extended training with\nvisual signals can impair the model’s ability to generate well-\nformatted parametric sequences. Besides, CADFusionSL-VF\nreceives a low rating from the LVM judge, revealing that\ntraining with visual feedback along provides limited bene-\nfit. Regarding CADFusionSL-VFRPO which incorporates the\nadditional loss, while it achieves low invalidity ratio, its\nvisual quality, as assessed by the LVM judge, is even lower\nthan the SL-only setup (i.e., CADFusionSL). This indicates\nthat it fails to effectively balance the contributions of both\nsequential signals and visual signals.\nWe also compare model variants that use different numbers\nof iterations of visual feedback and sequential learning. In\nTable 2, for each CADFusionSL-VFSL(*) variant, the number\nin parentheses indicates the number of alternative training\nrounds performed. The results for iterations 1, 3, and 5\nare reported, showing a gradual increase in LVM scores\nalong with a stable invalidity ratio. This further validates\nthe effectiveness of our approach.\nData Annotation. We examine the impact of our choice\nof data annotation. In the sequential learning stage, the\ndataset is constructed by first using LVMs to generate initial\ncaptions, followed by human annotators refining them. To\nevaluate the effect of this decision, we conduct an experi-\nment in which our method is trained on data without human\nannotation, denoted as CADFusionSLw/o HA. The second row\nof Table 2 presents the results. It shows worse LVM score\nand IR compared to the version using data with human anno-\ntations (CADFusionSL), highlighting the necessity of human\nannotation in the sequential learning stage.\nIn the visual feedback stage, LVMs are used to score\nCAD models and generate preference data. This design\nchoice is driven by the high cost of human annotation\nand the challenge of managing human annotators to en-\nsure consistent scoring. To evaluate the effect of this de-\ncision, we conduct an experiment where the visual feed-\nback stage of our method is trained on human-scored prefer-\nence pairs, denoted as CADFusionSL-VFSL(1)w/ HA. Compared\nto the LVM-scored version (i.e., CADFusionSL-VFSL(1)), it\nachieves a worse LVM score and IR. This aligns with our\nintuition that, while human annotation may be more ac-\ncurate, managing annotators for consistent scoring is diffi-\ncult. Furthermore, using LVM-scored preference data al-\nlows CADFusionSL-VFSL(1) to scale across more rounds of\nvisual feedback (e.g., CADFusionSL-VFSL(5)), leading to im-\nproved performance. Achieving this with human annotation\nwould be challenging and expensive.\n5. Limitation\nCADFusion’s results are overall promising. However, there\nare limitations that could be addressed in future work.')]}
2025-02-06 21:09:24,993 - INFO - Initializing LLM for extracting main content from papers
2025-02-06 21:09:40,967 - INFO - start_marker: 1. Introduction
Text-to-, end_marker: ture of our method.
5. Discussio, start_idx: 1284, end_idx: -1
2025-02-06 21:09:43,675 - INFO - start_marker: 1. Introduction
Large, end_marker: ments in ensemble methods.
14

Refere, start_idx: -1, end_idx: -1
2025-02-06 21:09:45,696 - INFO - start_marker: 1. Introduction
Recen, end_marker: ture and generation tasks.
We hope, start_idx: 6511, end_idx: -1
2025-02-06 21:09:53,356 - INFO - start_marker: 1. Introduction
The, end_marker: final results., start_idx: -1, end_idx: -1
2025-02-06 21:09:57,709 - INFO - start_marker: 1. Introduction
Diffusion Bridge, end_marker: 6. Discussion
Potential impact. DBMs, start_idx: 1283, end_idx: 34913
2025-02-06 21:09:57,714 - INFO - Total execution time: 32.08 seconds (0.53 minutes)
2025-02-06 21:09:57,725 - INFO - Papers: {'2025-02-05': [Paper(arxiv_id='2502.01362', authors=['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], published_at=datetime.datetime(2025, 2, 5, 3, 1, 40, 464000, tzinfo=datetime.timezone.utc), title='Inverse Bridge Matching Distillation', summary='Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.', upvotes=24, thumbnail=None, content='1. Introduction\nDiffusion Bridge Models (DBMs) represent a specialized\nclass of diffusion models designed for data-to-data tasks,\nsuch as image-to-image translation. Unlike standard diffu-\nsion models, which operate by mapping noise to data (Ho\net al., 2020; Sohl-Dickstein et al., 2015), DBMs construct\ndiffusion processes directly between two data distributions\n(Peluchetti, 2023a; Liu et al., 2022b; Somnath et al., 2023;\nZhou et al., 2024a; Yue et al., 2024; Shi et al., 2023; De Bor-\ntoli et al., 2023). This approach allows DBMs to modify\nonly the necessary components of the data, starting from an\ninput sample rather than generating it entirely from Gaus-\n*Equal contribution\n1Skolkovo Institute of Science and\nTechnology\n2Yandex Research\n3HSE University\n4Artificial\nIntelligence Research Institute.\nCorrespondence to:\nNikita\nGushchin\n<n.gushchin@skoltech.ru>,\nAlexander\nKorotin\n<a.korotin@skoltech.ru>.\nInput\nIBMD (Ours)\nTeacher\nSuper-resolution\nJPEG restoration\nInpainting\nNormal-to-Image\nSketch-to-Image\nFigure 1. Outputs of DBMs models distilled by our Inverse Bridge\nMatching Distillation (IBMD) approach on various image-to-\nimage translation tasks and datasets (M5). Teachers use NFE≥500\nsteps, while IBMD distilled models use NFE≤4.\nsian noise. As a result, DBMs have demonstrated impressive\nperformance in image-to-image translation problems.\nThe rapid development of DBMs has led to two dominant ap-\nproaches, usually considered separately. The first branch of\n1\narXiv:2502.01362v1  [cs.LG]  3 Feb 2025\n\nInverse Bridge Matching Distillation\napproaches (Peluchetti, 2023a; Liu et al., 2022b; 2023a; Shi\net al., 2023; Somnath et al., 2023) considered the construc-\ntion of diffusion between two arbitrary data distributions\nperforming Unconditional Bridge Matching (also called\nthe Markovian projection) of a process given by a mixture\nof diffusion bridges. The application of this branch includes\ndifferent data like images (Liu et al., 2023a; Li et al., 2023),\naudio (Kong et al., 2025) and biological tasks (Somnath\net al., 2023; Tong et al., 2024) not only in paired but also in\nunpaired setups using its relation to the Schr¨odinger Bridge\nproblem (Shi et al., 2023; Gushchin et al., 2024). The second\ndirection follows a framework closer to classical diffusion\nmodels, using forward diffusion to gradually map to the\npoint of different distibution rather than mapping distribu-\ntion to distribution as in previous case (Zhou et al., 2024a;\nYue et al., 2024). While these directions differ in theoretical\nformulation, their practical implementations are closely re-\nlated; for instance, models based on forward diffusion can\nbe seen as performing Conditional Bridge Matching with\nadditional drift conditions (De Bortoli et al., 2023).\nSimilar to classical DMs, DBMs also exhibit multistep se-\nquential inference, limiting their adoption in practice. De-\nspite the impressive quality shown by DBMs in the practical\ntasks, only a few approaches were developed for their accel-\neration, including more advanced sampling schemes (Zheng\net al., 2024; Wang et al., 2024) and consistency distillation\n(He et al., 2024), adapted for bridge models. While these\napproaches significantly improve the efficiency of DBMs,\nsome unsolved issues remain. The first one is that the men-\ntioned acceleration approaches are directly applicable only\nfor DBMs based on the Conditional Bridge Matching, i.e.,\nno universal method can accelerate any DBMs. Also, due\nto some specific theoretical aspects of DBMs, consistency\ndistillation cannot be used to obtain the single-step model\n(He et al., 2024, Section 3.4).\nContributions. To address the above-mentioned issues of\nDBMs acceleration, we propose a new distillation technique\nbased on the inverse bridge matching problem, which has\nseveral advantages compared to existing methods:\n1. Universal Distillation. Our distillation technique is ap-\nplicable to DBMs trained with both conditional and un-\nconditional regimes, making it the first distillation ap-\nproach introduced for unconditional DBMs.\n2. Single-Step and Multistep Distillation. Our distillation\nis capable of distilling DBMs into generators with any\nspecified number of steps, including the distillation of\nDBMs into one-step generators.\n3. Target data-free distillation. Our method does not\nrequire the target data domain to perform distillation.\n4. Better quality of distilled models. Our distillation tech-\nnique is tested on a wide set of image-to-image problems\nfor conditional and unconditional DBMs in both one and\nmulti-step regimes. It demonstrates improvements com-\npared to the previous acceleration approaches including\nDBIM(Zheng et al., 2024) and CDBM (He et al., 2024).\n2. Background\nIn this paper, we propose a universal distillation frame-\nwork for both conditional and unconditional DBMs.\nTo not repeat fully analogical results for both cases,\nwe denote by this color the additional conditioning\non xT used for the conditional models, i.e. for the\nunconditional case this conditioning is not used.\n2.1. Bridge Matching\nWe start by recalling the bridge matching method\n(Peluchetti, 2023b;a; Liu et al., 2022b; Shi et al., 2023).\nConsider two probability distributions p(x0) and p(xT ) on\nRD dimensional space, which represent target and source\ndomains, respectively. For example, in an image inverse\nproblem, p(x0) represents the distribution of clean im-\nages and p(xT ) the distribution of corrupted images. Also\nconsider a coupling p(x0, xT ) of these two distributions,\nwhich is a probability distribution on RD × RD. Cou-\npling p(x0, xT ) can be provided by paired data or con-\nstructed synthetically, i.e., just using the independent distri-\nbution p(x0, xT ) = p(x0)p(xT ). Bridge Matching aims to\nconstruct the diffusion that transforms source distribution\np(xT ) to target distribution p(x0) based on given coupling\np(x0, xT ) and specified diffusion bridge.\nDiffusion bridges.\nConsider forward-time diffusion Q\ncalled ”Prior” on time horizon [0, T] represented by the\nstochastic differential equation (SDE):\nPrior Q :\ndxt = f(xt, t)dt + g(t)dwt,\n(1)\nf(xt, t) : RD × [0, T] →RD,\ng(t) : [0, T] →RD,\nwhere f(xt, t) is a drift function, g(t) is the noise schedule\nfunction and dwt is the differential of the standard Wiener\nprocess. By q(xt|xs), we denote the transition probability\ndensity of prior process Q from time s to time t. Diffusion\nbridge is a conditional process Q|x0,xT , which is obtained\nby pinning down starting and ending points x0 and xT . This\ndiffusion bridge can be derived from prior process Q using\nthe Doob-h transform (Doob & Doob, 1984):\nDiffusion Bridge Q|x0,xT : x0, xT are fixed,\n(2)\ndxt = {f(xt, t)dt + g2(t)∇xt log q(xT |xt)}dt + g(t)dwt,\nFor this diffusion bridge we denote the distribution at time t\nof the diffusion bridge Q|x0,xT by q(xt|x0, xT ).\nMixture of bridges. Bridge Matching procedure starts with\ncreating a mixture of bridges process Π. This process is\n2\n\nInverse Bridge Matching Distillation\nFigure 2. Overview of (Conditional) Bridge Matching with bx0 reparameterization. The process begins by sampling a pair (x0, xT )\nfrom the data coupling p(x0, xT ). An intermediate sample xt is then drawn from the diffusion bridge q(xt|x0, xT ) at a random time\nt ∼U[0, T]. The model bx0 is trained with an MSE loss to reconstruct x0 from xt. In the conditional setting (dashed red path), bx0 is also\nconditioned on xT as an additional input, leveraging information about the terminal state to improve reconstruction.\nrepresented as follows:\nMixture of Bridges Π :\nΠ(·) =\nZ\nQ|x0,xT (·)p(x0, xT )dx0dxT .\n(3)\nPractically speaking, the definition (3) means that to sample\nfrom a mixture of bridges Π, one first samples the pair\n(x0, xT ) ∼p(x0, xT ) from data coupling and then samples\ntrajectory from the bridge Q|x0,xT (·).\nBridge Matching problem. The mixture of bridges Π can-\nnot be used for data-to-data translation since it requires first\nto sample a pair of data and then just inserts the trajectory.\nIn turn, we are interested in constructing a diffusion, which\ncan start from any sample xT ∼p(xT ) and gradually trans-\nform it to x0 ∼p(x0). This can be done by solving the\nBridge Matching problem (Shi et al., 2023, Proposition 2)\nBridge Matching problem:\n(4)\nBM(Π)\ndef\n= arg min\nM∈M\nKL(Π||M),\nwhere M is the set of Markovian processes associated with\nsome SDE and KL(Π||M) is the KL-divergence between\na constructed mixture of bridges Π and diffusion M. It is\nknown that the solution of Bridge Matching is the reversed-\ntime SDE (Shi et al., 2023, Proposition 9):\nThe SDE of Bridge Matching solution :\n(5)\ndxt = {ft(xt) −g2(t)v∗(xt, t)}dt + g(t)d ¯wt,\nxT ∼pT (xT ),\nwhere ¯w is a standard Wiener process when time t flows\nbackward from t = T to t = 0, and dt is an infinitesimal\nnegative timestep. The drift function v∗is obtained solving\nthe following problem (Shi et al., 2023; Liu et al., 2023a):\nBridge Matching problem with a tractable objective: (6)\nmin\nϕ Ex0,t,xt\n\x02\n∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼p(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nTime moment t here is sampled according to the uniform\ndistribution on the interval [0, T].\nRelation Between Flow and Bridge Matching. The Flow\nMatching (Liu et al., 2023b; Lipman et al., 2023) can be\nseen as the limiting case σ →0 of the Bridge Matching for\nparticular example see (Shi et al., 2023, Appendix A.1).\n2.2. Augmented (Conditional) Bridge Matching and\nDenoising Diffusion Bridge Models (DDBM)\nFor a given coupling p(x0, xT ) = p(x0|xT )p(xT ), one can\nuse an alternative approach to build a data-to-data diffusion.\nConsider a set of Bridge Matching problems indexed by xT\nbetween p0 = p(x0|xT ) and p(xT ) = δxT (x) (delta mea-\nsure centered at xT ). This approach is called Augmented\nBridge Matching (De Bortoli et al., 2023). The key dif-\nference of this version in practice is that it introduces the\ncondition of the drift function v∗(xt, t, xT ) on the starting\npoint xT in the reverse time diffusion (5):\ndxt = {ft(xt) −g2(t)v∗(xt, t, xT )}dt + g(t)d ¯wt.\nThe drift function v∗can be recovered almost in the same\nway just by the addition of this condition on xT :\nAugmented (Conditional) Bridge Matching Problem.\nmin\nϕ Ex0,t,xt,xT\n\x02\n∥vϕ(xt, t, xT ) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼p(x0, xT ), and xt ∼q(xt|x0, xT ).\nSince the difference is the addition of conditioning on xT ,\nwe call this approach Conditional Bridge Matching.\nRelation to DDBM. As was shown in the Augmented\nBridge Matching (De Bortoli et al., 2023), the conditional\nBridge Matching is equivalent to the Denoising Diffusion\nBridge Model (DDBM) proposed in (Zhou et al., 2024a).\nThe difference is that in DDBM, the authors learn the score\nfunction of s(xt, xT , t) conditioned on xT of a process for\nwhich x0 ∼p(x0|xT ) and q(xt) ∼q(xt|x0, xT ): Then, it\nis combined with the drift of forward Doob-h transform (5)\nto get the reverse SDE drift v(xt, t, xT ):\nv(xt, t, xT ) = s(xt, xT , t) −∇xt log q(xT |xt),\ndxt = {f(xt, t)dt −g2(t)v(xt, t, xT )}dt + g(t)d ¯wt,\nor reverse probability flow ODE drift:\nvODE(xt, t, xT ) = 1\n2s(xt, xT , t) −∇xt log q(xT |xt),\n3\n\nInverse Bridge Matching Distillation\ndxt = {f(xt, t)dt −g2(t)vODE(xt, t, xT )}dt,\nwhich is used for consistency distillation in (He et al., 2024).\n2.3. Practical aspects of Bridge Matching\nPriors used in practice. In practice (He et al., 2024; Zhou\net al., 2023; Zheng et al., 2024), the drift of the prior pro-\ncess is usually set to be f(xt, t) = f(t)xt, i.e, it depends\nlinearly on xt. For this process the transitional distribution\nq(xt|x0) = N(xt|αtx0, σ2\nt I) is Gaussian, where:\nf(t) = d log αt\ndt\n,\ng2(t) = dσ2\nt\ndt −2d log αt\ndt\nσ2\nt .\nThe bridge process distribution is also a Gaussian\nq(xt|x0, xT ) = N(xT |atxT + btx0, c2\ntI) with coefficients:\nat = αt\nαT\nSNRT\nSNRt\n, bt = αt\n\x12\n1 −SNRT\nSNRt\n\x13\n,\nc2\nt = σ2\nt\n\x12\n1 −SNRT\nSNRt\n\x13\n,\nwhere SNRt = α2\nt\nσ2\nt is the signal-to-noise ratio at time t.\nData prediction reparameterization. The regression target\nof the loss function (6) for the priors with the drift v(xt, t)\nis given by ∇xt log q(xt|x0) = −xt−αtx0\nσ2\nt\n. Hence, one can\nuse the parametrization v(xt, t, xT ) = −xt−αtbx0(xt,t,xT )\nσ2\nt\nand solve the equivalent problem:\nReparametrized (Conditional) Bridge Matching problem:\nmin\nϕ Ex0,t,xt,xT\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03\n,\n(7)\n(x0, xT ) ∼p(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\nwhere λ(t) is any positive weighting function. Note that xT\nis used only for the Conditional Bridge Matching model.\n2.4. Difference Between Acceleration of Unconditional\nand Conditional DBMs\nSince both conditional and unconditional approaches learn\ndrifts of SDEs, they share the same problems of long in-\nference. However, these models significantly differ in the\napproaches that can accelerate them. The source of this\ndifference is that Conditional Bridge Matching considers\nthe set of problems of reversing diffusion, which gradually\ntransforms distribution p(x0|xT ) to the fixed point xT . Fur-\nthermore, the forward diffusion has simple analytical drift\nand Gaussian transitional kernels. Thanks to it, for each xT\nto sample, one can use the probability flow ODE and ODE-\nsolvers or hybrid solvers to accelerate sampling (Zhou et al.,\n2024a) or use consistency distillation of bridge models (He\net al., 2024). Another beneficial property is that one can con-\nsider a non-Markovian forward process to develop a more\nefficient sampling scheme proposed in DBIM (Zheng et al.,\n2024) similar to Denoising Diffusion Implicit Models (Song\net al., 2021). However, in the Unconditional Bridge Match-\ning problem, the forward diffusion process, which maps\np(x0) to p(xT ) without conditioning on specific point xT ,\nis unknown. Hence, the abovementioned methods cannot\nbe used to accelerate this model type.\n3. IBMD: Inverse Bridge Matching Distillation\nThis section describes our proposed universal approach to\ndistill the both Unconditional and (Conditional) Bridge\nMatching models v∗(called the teacher model) into a few-\nstep generator using only the corrupted data pT (xT ). The\nkey idea of our method is to consider the inverse problem of\nfinding the mixture of bridges Πθ, for which Bridge Match-\ning provides the solution vθ with the same drift as the given\nteacher model v∗. We formulate this task as the optimiza-\ntion problem (M3.1). However, gradient methods cannot\nsolve this optimization problem directly due to the absence\nof tractable gradient estimation. To avoid this problem, we\nprove a theorem that allows us to reformulate the inverse\nproblem in the tractable objective for gradient optimiza-\ntion (M3.2). Then, we present the fully analogical results\nfor the Conditional Bridge Matching case in (M3.3). Next,\nwe present the multistep version of distillation (M3.5) and\nthe final algorithm (M3.4). We provide the proofs for all\nconsidered theorems and propositions in Appendix A.\n3.1. Bridge Matching Distillation as Inverse Problem\nIn this section, we focus on the derivation of our distilla-\ntion method for the case of Unconditional Bridge Match-\ning. Consider the fitted teacher model v∗(xt, t), which\nis an SDE drift of some process M ∗= BM(Π∗), where\nΠ∗constructed using some data coupling p∗(x0, xT ) =\np∗(x0|xT )p(xT ).\nWe\nparametrize\npθ(x0, xT )\n=\npθ(x0|xT )p(xT ) and aim to find such Πθ build on\npθ(x0, xT ), that BM(Π∗)\n=\nBM(Πθ).\nIn practice,\nwe parametrize pθ(x0|xT ) by the stochastic generator\nGθ(xT , z), z ∼N(0, I), which generates samples based\non input xT ∼p(xT ) and the gaussian noise z. Now, we\nformulate the inverse problem as follows:\nmin\nθ\nKL(BM(Πθ)||M ∗).\n(8)\nNote, that since the objective (8) is the KL-divergence be-\ntween BM(Πθ) and M ∗, it is equal to 0 if and only if\nBM(Πθ) and M ∗coincide. Furthermore, using the disinte-\ngration and Girsanov theorem (Vargas et al., 2021; Pavon &\nWakolbinger, 1991), we have the following result:\nProposition 3.1 (Inverse Bridge Matching problem). The\ninverse problem (8) is equivalent to\nmin\nθ\nExt,t\n\x02\nλ(t)||v(xt, t) −v∗(xt, t)||2\x03\n,\ns.t.\n(9)\nv = arg min\nv′\nExt,t,x0\n\x02\n∥v′(xt, t) −∇xt log q(xt|x0)∥2\x03\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\n4\n\nInverse Bridge Matching Distillation\nFigure 3. Overview of our method Inverse Bridge Matching Distillation (IBMD). The goal is to distill a trained (Conditional) Bridge\nMatching model into a generator Gθ(z, xT ), which learns to produce samples using the corrupted data p(xT ). Generator Gθ(z, xT )\ndefines the coupling pθ(x0, xT ) = pθ(x0|xT )p(xT ) and we aim to learn the generator in such way that Bridge Matching with pθ(x0, xT )\nproduces the same (Conditional) Bridge Matching model bxϕ\n0 = bxθ\n0. To do so, we learn a bridge model bxϕ\n0 using coupling pθ in the same\nway as the teacher model was learned. Then, we use our novel objective given in Theorem 3.2 to update the generator model Gθ.\nwhere λ(t) is any positive weighting function.\nThus, this is the constrained problem, where the drift v\nis the result of Bridge Matching for coupling pθ(x0, xT )\nparametrized by the generator Gθ. Unfortunately, there is\nno clear way to use this objective efficiently for optimizing\na generator Gθ since it would require gradient backpropa-\ngation through the argmin of the Bridge Matching problem.\n3.2. Tractable objective for the inverse problem\nIn this section, we introduce our new unconstrained refor-\nmulation for the inverse problem (9), which admits direct\noptimization using gradient methods:\nTheorem 3.2 (Tractable inverse problem reformulation).\nThe constrained inverse problem (9) w.r.t θ is equivalent to\nthe unconstrained optimization problem:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥v∗(xt, t) −∇xt log q(xt|x0)∥2\x03\n−\nmin\nϕ Ext,t,x0\n\x02\nλ(t)∥vϕ(xt, t) −∇xt log q(xt|x0)∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ),\nWhere the constraint in the original inverse problem (9) is\nrelaxed by introducing the inner bridge matching problem.\nThis is the general result that can applied with any diffusion\nbridge. For the priors with with drift f(xt, t) = f(t)xt, we\npresent its reparameterized version.\nProposition 3.3 (Reparameterized tractable inverse prob-\nlem). Using the reparameterization (M2.3) for the prior with\nthe linear drift f(xt, t) = f(t)xt, the inverse problem in\nTheorem 3.2 is equivalent to:\nmin\nθ\nh\nExt,t,x0\n\x02\nλ(t)∥bx∗\n0(xt, t) −x0∥2\x03\n−\nmin\nϕ Ext,t,x0\n\x02\nλ(t)∥bxϕ\n0(xt, t) −x0∥2\x03\ndt\ni\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nThe key difference of the reformulated problem is that it\nadmits clear gradients of generator Gθ, which can be cal-\nculated automatically by using the autograd techniques.\nThanks to the unconstrained reformulation of an inverse\nproblem given by Theorem 3.2, it can now be solved di-\nrectly by parameterizing bx0(xt, t) by a neural network.\n3.3. Distillation of conditional Bridge Matching models\nSince Conditional Bridge Matching is, in essence, a set\nof Unconditional Bridge Matching problems for each xT\n(M2.2), the analogical results hold just by adding the condi-\ntioning on xT for v, i.e., using v(xt, t, xT ) or bx0, i.e. using\nbx0(xt, t, xT ). Here, we provide the final reparametrized\nformulation, which we use in our experiments:\nTheorem 3.4 (Reparameterized tractable inverse problem\nfor conditional bridge matching).\nmin\nθ\nh\nExt,t,x0,xT\n\x02\nλ(t)∥bx∗\n0(xt, t, xT ) −x0∥2\x03\n−\n(10)\nmin\nϕ Ext,t,x0,xT\n\x02\nλ(t)∥bxϕ\n0(xt, t, xT ) −x0∥2\x03i\n,\n(x0, xT ) ∼pθ(x0, xT ), t ∼U([0, T]), xt ∼q(xt|x0, xT ).\nwhere λ(t) is some positive weight function.\nTo use it in practice, we parameterize bx0(xt, t, xT ) by a\nneural network with an additional condition on xT .\n3.4. Algorithm\nWe provide a one-step Algorithm 1 that solves the inverse\nBridge Matching problem in the reformulated version that\nwe use in our experiments. We provide a visual abstract of\nit in Figure 3. Note that a teacher in the velocity parame-\nterization v∗(xt, t) can be easily reparameterized (M2.3) in\nx0-prediction model using bx∗(xt, t) = σ2\nt v∗(xt,t)+xt\nαt\n.\n5\n\nInverse Bridge Matching Distillation\n3.5. Mulitistep distillation\nWe also present a multistep modification of our distillation\ntechnique if a one-step generator struggles to distill the mod-\nels, e.g., in inpainting setups, where the corrupted image\nxT contains less information. Our multistep technique is\ninspired by similar approaches used in diffusion distilla-\ntion methods (Yin et al., 2024a, DMD) and aims to avoid\ntraining/inference distribution mismatch.\nWe choose N timesteps {0 < t1 < t2 < ... < tN = T}\nand add additional time input to our generator Gθ(xt, z, t).\nFor the conditional Bridge Matching case, we also add\nconditions on xT and use Gθ(xt, z, t, xT ). To perform in-\nference, we alternate between getting prediction from the\ngenerator ex0 = Gθ(xt, z, t) and using posterior sampling\nq(xtn−1|ex0, xtn) given by the diffusion bridge. To train\nthe generator in the multistep regime, we use the same\nprocedure as in one step except that to get input xt for in-\ntermediate times tn < tN, we first perform inference of our\ngenerator to get x0 and then use bridge q(xt|ex0, xT ).\n4. Related work\nDiffusion Bridge Models (DBMs) acceleration. Unlike\na wide scope of acceleration methods developed for clas-\nsical diffusion and flow models, only a few approaches\nwere developed for DBM acceleration. For the conditional\nDBMs, acceleration methods include more advanced sam-\nplers (Zheng et al., 2024; Wang et al., 2024) based on re-\nformulated forward diffusion process as a non-markovian\nprocess inspired by Denoising Diffusion Implicit Models\n(Song et al., 2021). Also, there is a distillation method based\non the distilling probability-flow ODE into a few steps using\nconsistency models (He et al., 2024). However, for theo-\nretical reasons (He et al., 2024, Section 3.4), consistency\nmodels for Diffusion Bridges cannot be distilled into one-\nstep generators. Unlike these existing works, our method\nis applicable to both conditional and unconditional types of\nDBMs and can distill models into the one-step generator.\nRelated diffusion and flow models distillation techniques.\nAmong the methods developed for the distillation of classi-\ncal diffusion and flow models, the most related to our work\nare methods based on simultaneous training of few-step\ngenerators and auxiliary ”fake” model, that predict score or\ndrift function for the generator (Yin et al., 2024b;a; Zhou\net al., 2024b; Huang et al., 2024). Unlike these approaches,\nwe consider the distillation of Diffusion Bridge Models -\nthe generalization of flow and diffusion models.\n5. Experiments\nThis section highlights the applicability of our IBMD distil-\nlation method in both unconditional and conditional settings.\nTo demonstrate this, we conducted experiments utilizing\npretrained unconditional models used in I2SB paper (Liu\net al., 2023a). Then we evaluated IBMD in conditional\nAlgorithm 1 Inverse Bridge Matching Distillation (IBMD)\nInput\n:Teacher network bx∗\n0 : RD × [0, T] × RD →RD;\nBridge q(xt|x0, xT ) used for training x∗;\nGenerator network Gθ : RD × RD →RD;\nBridge network bxϕ\n0 : RD × [0, T] × RD →RD;\nInput distribution p(xT ) accessible by samples;\nWeights function λ(t) : [0, T] →R+;\nBatch size N; Number of student iterations K;\nNumber of bridge iterations L.\nOutput :Learned generator Gθ of coupling pθ(x0, xT ) for\nwhich Bridge Matching outputs drift v ≈v∗.\n// Conditioning on xT is used only for distillation of Condi-\ntional Bridge Matching models.\nfor k = 1 to K do\nfor l = 1 to L do\nSample batch xT ∼p(xT )\nSample batch of noise z ∼N(0, I)\nx0 ←Gθ(xT , z)\nSample time batch t ∼U[0, T]\nSample batch xt ∼q(xt|x0, xT )\nbLϕ ←1\nN\nPN\nn=1\n\x02\nλ(t)||bxϕ\n0(xt, t, xT ) −x0||2\x03\nn\nUpdate ϕ by using ∂b\nLϕ\n∂ϕ\nSample batch xT ∼p(xT )\nSample batch of noise z ∼N(0, I)\nx0 ←Gθ(xT , z)\nSample time batch t ∼U[0, T]\nSample batch xt ∼q(xt|x0, xT )\nbLθ ←1\nN\nPN\nn=1\n\x02\nλ(t)||bx∗\n0(xt, t, xT ) −x0||2 −\nλ(t)||bxϕ\n0(xt, t, xT ) −x0||2\x03\nn\nUpdate θ by using ∂b\nLθ\n∂θ\nsettings using DDBM (Zhou et al., 2024a) setup (M5.2).\nFor clarity, we denote our models as IBMD-DDBM and\nIBMD-I2SB, indicating that the teacher model is derived\nfrom DDBM or I2SB framework, respectively. We provide\nall the technical details in Appendix B.\n5.1. Distillation of I2SB (5 setups)\nSince known distillation and acceleration techniques are\ndesigned for the conditional models, there is no clear base-\nline for comparison. Thus, this section aims to demonstrate\nthat our distillation technique significantly decreases NFE\nrequired to obtain the same quality of generation.\nExperimental Setup. To test our approach for uncondi-\ntional models, we consider models trained and published in\nI2SB paper (Liu et al., 2023a), specifically (a) two models\nfor the 4x super-resolution with bicubic and pool kernels,\n(b) two models for JPEG restoration using quality factor\nQF= 5 and QF= 10, and (c) a model for center-inpainting\nwith a center mask of size 128 × 128 all of which were\ntrained on ImageNet 256 × 256 dataset (Deng et al., 2009).\n6\n\nInverse Bridge Matching Distillation\nTable 1. Results on the image super-resolution task. Baseline re-\nsults are taken from I2SB (Liu et al., 2023a).\n4× super-resolution (bicubic)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n21.3\n63.2\nDDNM (Wang et al., 2023)\n100\n13.6\n65.5\nΠGDM (Song et al., 2023)\n100\n3.6\n72.1\nADM (Dhariwal & Nichol, 2021)\n1000\n14.8\n66.7\nCDSB (Shi et al., 2022)\n50\n13.6\n61.0\nI2SB (Liu et al., 2023a)\n1000\n2.8\n70.7\nIBMD-I2SB (Ours)\n1\n2.5\n72.4\nTable 2. Results on the image JPEG restoration task with QF=5.\nBaseline results are taken from I2SB (Liu et al., 2023a).\nJPEG restoration, QF= 5.\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n28.2\n53.9\nΠGDM (Song et al., 2023)\n100\n8.6\n64.1\nPalette (Saharia et al., 2022)\n1000\n8.3\n64.2\nCDSB (Shi et al., 2022)\n50\n38.7\n45.7\nI2SB (Liu et al., 2023a)\n1000\n4.6\n67.9\nI2SB (Liu et al., 2023a)\n100\n5.4\n67.5\nIBMD-I2SB (Ours)\n1\n5.3\n67.2\nFor all the setups we use the same train part of ImageNet\ndataset, which was used to train the used models. For the\nevaluation we follow the same protocol used in the I2SB\npaper, i.e. use the full validation subset of ImageNet for\nsuper-resolution task and the 10′000 subset of validation for\nother tasks. We report the same FID (Heusel et al., 2017)\nand Classifier Accuracy (CA) using pre-trained ResNet50\nmodel metrics used in the I2SB paper. We present our results\nin Table 1, Table 3, Table 2, Table 4 and Table 6. We provide\nthe uncurated samples for all setups in Appendix C.\nResults. For both super-resolution tasks (see Table 1, Ta-\nble 3), our 1-step distilled model outperformed teacher\nmodel inference using all 1000 steps used in the training.\nNote that our model does not use the clean training target\ndata p(x0), only the corrupted p(xT ), hence this improve-\nment is not due to additional training using paired data. We\nhypothesize that it is because the teacher model introduces\napproximation error during many steps of sampling, which\nmay accumulate. For both JPEG restoration (see Table 2, Ta-\nble 4), our 1-step distilled generator provides the quality of\ngeneration close to the teacher model and achieves around\n100x time acceleration. For the inpainting problem (see\nTable 6), we present the results for 1, 2 and 4 steps distilled\ngenerator. Our 2 and 4-step generators provide a quality\nsimilar to the teacher I2SB model, in turn, there is still some\ngap for the 1-step model. These models provide around 5x\ntime acceleration. We hypothesize that this setup is harder\nTable 3. Results on the image super-resolution task. Baseline re-\nsults are taken from I2SB (Liu et al., 2023a).\n4× super-resolution (pool)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n14.8\n64.6\nDDNM (Wang et al., 2023)\n100\n9.9\n67.1\nΠGDM (Song et al., 2023)\n100\n3.8\n72.3\nADM (Dhariwal & Nichol, 2021)\n1000\n3.1\n73.4\nCDSB (Shi et al., 2022)\n50\n13.0\n61.3\nI2SB (Liu et al., 2023a)\n1000\n2.7\n71.0\nIBMD-I2SB (Ours)\n1\n2.6\n72.7\nTable 4. Results on the image JPEG restoration task with QF=10.\nBaseline results are taken from I2SB (Liu et al., 2023a).\nJPEG restoration, QF= 10.\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n16.7\n64.7\nΠGDM (Song et al., 2023)\n100\n6.0\n71.0\nPalette (Saharia et al., 2022)\n1000\n5.4\n70.7\nCDSB (Shi et al., 2022)\n50\n18.6\n60.0\nI2SB (Liu et al., 2023a)\n1000\n3.6\n72.1\nI2SB (Liu et al., 2023a)\n100\n4.4\n71.6\nIBMD-I2SB (Ours)\n1\n3.8\n72.4\nfor our model since it is required to generate the entire center\nfragment from scratch, while in other tasks, there is already\nsome good approximation given by corrupted images.\n5.2. Distillation of DDBM (3 setups)\nThis section addresses two primary objectives: (1) demon-\nstrating the feasibility of conditional model distillation\nwithin our framework and (2) comparing with the CDBM\n(He et al., 2024) - a leading approach in Conditional Bridge\nMatching distillation, presented into different models: CBD\n(consistency distillation) and CBT (consistency training).\nExperimental Setup. For evaluation, we use the same\nsetups used in competing methods (He et al., 2024; Zheng\net al., 2024). For the image-to-image translation task, we\nutilize the Edges→Handbags dataset (Isola et al., 2017)\nwith a resolution of 64 × 64 pixels and the DIODE-Outdoor\ndataset (Vasiljevic et al., 2019) with a resolution of 256×256\npixels. For these tasks, we report FID and Inception Scores\n(IS) (Barratt & Sharma, 2018). For the image inpainting\ntask, we use the same setup of center-inpainting as before.\nResults. We utilized the same teacher model checkpoints\nand as in CDBM. We present the quantitative and qualitative\nresults of IBMD on the image-to-image translation task in\nTable 5 and in Figures 12, 10 respectively. The compet-\ning methods, DBIM (Zhou et al., 2024a, Section 4.1) and\nCDBM (He et al., 2024, Section 3.4), cannot use single-step\ninference due to the singularity at the starting point xT .\n7\n\nInverse Bridge Matching Distillation\nTable 5. Results on the Image-to-Image Translation Task (Training Sets). Methods are grouped by NFE (> 2, 2, 1), with the best metrics\nbolded in each group. Baselines results are taken from CDBM.\nNFE\nEdges →Handbags (64 × 64)\nDIODE-Outdoor (256 × 256)\nFID ↓\nIS ↑\nFID ↓\nIS ↑\nDDIB (Su et al., 2022)\n≥40\n186.84\n2.04\n242.3\n4.22\nSDEdit (Meng et al., 2021)\n≥40\n26.5\n3.58\n31.14\n5.70\nRectified Flow (Liu et al., 2022a)\n≥40\n25.3\n2.80\n77.18\n5.87\nI2SB (Liu et al., 2023a)\n≥40\n7.43\n3.40\n9.34\n5.77\nDBIM (Zheng et al., 2024)\n50\n1.14\n3.62\n3.20\n6.08\nDBIM (Zheng et al., 2024)\n100\n0.89\n3.62\n2.57\n6.06\nCBD (He et al., 2024)\n2\n1.30\n3.62\n3.66\n6.02\nCBT (He et al., 2024)\n0.80\n3.65\n2.93\n6.06\nIBMD-DDBM (Ours)\n0.67\n3.69\n3.12\n5.92\nPix2Pix (Isola et al., 2017)\n1\n74.8\n4.24\n82.4\n4.22\nIBMD-DDBM (Ours)\n1.26\n3.66\n4.07\n5.89\nTable 6. Results on the Image Inpainting Task.\nMethods are\ngrouped by NFE (> 4, 4, 2, 1), with the best metrics bolded\nin each group. Baselines results are taken from CDBM.\nInpainting, Center (128 × 128)\nImageNet (256 × 256)\nNFE\nFID ↓\nCA ↑\nDDRM (Kawar et al., 2022)\n20\n24.4\n62.1\nΠGDM (Song et al., 2023)\n100\n7.3\n72.6\nDDNM (Wang et al., 2022)\n100\n15.1\n55.9\nPalette (Saharia et al., 2022)\n1000\n6.1\n63.0\nI2SB (Liu et al., 2023a)\n10\n5.4\n65.97\nDBIM (Zheng et al., 2024)\n50\n3.92\n72.4\nDBIM (Zheng et al., 2024)\n100\n3.88\n72.6\nCBD (He et al., 2024)\n4\n5.34\n69.6\nCBT (He et al., 2024)\n4.77\n70.3\nIBMD-I2SB (Ours)\n5.1\n70.3\nIBMD-DDBM (Ours)\n4.03\n72.2\nCBD (He et al., 2024)\n2\n5.65\n69.6\nCBT (He et al., 2024)\n5.34\n69.8\nIBMD-I2SB (Ours)\n5.3\n65.7\nIBMD-DDBM (Ours)\n4.23\n72.3\nIBMD-I2SB (Ours)\n1\n6.7\n65.0\nIBMD-DDBM (Ours)\n5.87\n70.6\nWe trained our IBMD with 1 and 2 NFEs on the\nEdges→Handbags dataset. We surpass CDBM at 2 NFE,\noutperform the teacher at 100 NFE, and achieve perfor-\nmance comparable to the teacher at 50 NFE with 1 NFE,\nresulting in a 50× acceleration. For the DIODE-Outdoor\nsetup, we trained IBMD with 1 and 2 NFEs. We surpassed\nCBD in FID at 2 NFE, achieving results comparable to CBT\nwith a slight drop in performance and maintaining strong\nperformance at 1 NFE with minor quality reductions.\nFor image inpainting, Table 6 and Figure 9 show the quanti-\ntative and qualitative results of IBMD. We train IBMD with\n4 NFE for image inpainting. It outperforms CBD and CBT\nat 4 NFE with a significant gap, surpassing both at 2 NFE\nand maintaining strong performance at 1 NFE while achiev-\ning teacher-level results at 50 NFE with a 12.5× speedup.\nConcerns regarding the evaluation protocol used in prior\nworks. For Edges-Handbags and DIODE-Outdoor setups,\nwe follow the evaluation protocol originally introduced in\nDDBM (Zhou et al., 2024a) and later used in works on\nacceleration of DDBM (Zheng et al., 2024; He et al., 2024).\nFor some reason, this protocol implies evaluation of the\ntrain set. Furthermore, test sets of these datasets consist of\na tiny fraction of images (around several hundred), making\nthe usage of standard metrics like FID challenging due to\nhigh statistical bias or variance of their estimation. Still,\nto assess the quality of the distilled model on the test sets,\nwe provide the uncurated samples produced by our distill\nmodel and teacher model on these sets in Figures 13 and\n11 in Appendix C. We also provide the uncurated samples\non the train part in Figures 12 and 10 to compare models’\nbehavior on train and test sets. From these results, we see\nthat the teacher model exhibits overfitting on both setups,\ne.g., it produces exactly the same images as corresponding\nreference images. In turn, on the test sets, teacher models\nwork well for the handbag setups, while on the test set of\nDIODE images, it exhibits mode collapse and produces gray\nimages. Nevertheless, our distilled model shows exactly\nthe same behavior in both sets, i.e., our IBMD approach\nprecisely distills the teacher model as expected.\n6. Discussion\nPotential impact. DBMs'),
                Paper(arxiv_id='2502.02589', authors=['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], published_at=datetime.datetime(2025, 2, 5, 13, 27, 36, 138000, tzinfo=datetime.timezone.utc), title='COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation', summary='This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.', upvotes=7, thumbnail=None, content='1. Introduction\nRecent advancements in multi-modal foundation models\nhave been largely driven by the availability of large-scale\npaired text-image datasets. These datasets, often collected\nvia web crawling with basic filtering techniques [14, 52,\n53], contain low-quality, web-sourced captions that lack\ndepth and accuracy. In contrast, human-annotated caption\ndatasets, such as COCO-caption [6], offer higher-quality\ndescriptions but are limited in scale and tend to be con-\ncise, with an average caption length of 10 words. To over-\ncome the limitations of short captions, the research commu-\nnity has leveraged vision-language models (VLMs) [5, 31,\n32, 38, 60] to generate detailed synthetic captions. While\nthese machine-generated captions improve visual under-\nstanding [5, 32] and generation tasks [31], they remain in-\nferior to high-quality, human-verified annotations [44].\nAddressing this challenge requires balancing scalability\nand annotation quality, as generating detailed and accurate\nimage descriptions at scale remains labor-intensive [15, 44].\nIn this paper, we introduce an efficient annotation approach\nthat combines dense mask annotations with commercial\nVLMs [5] to produce high-quality image captions. Our goal\nis to minimize human effort while generating rich, struc-\ntured descriptions.\nTo achieve this, we base our work on the COCO-caption\ndataset [6] due to its widespread use and diverse image con-\ntent. We revisit the COCO-caption dataset to provide more\ndetailed and comprehensive caption annotations. Our ap-\nproach involves creating holistic captions synthesized from\nregion-based dense captions that describe distinct areas\nwithin each image. Specifically, we build on recent CO-\nCONut panoptic segmentation annotations [9] to generate a\nnew set of detailed captions by: (a) annotating each segmen-\ntation region with a VLM-generated draft, carefully refined\nthrough human corrections, and (b) summarizing these re-\ngion captions into a comprehensive image caption while\npreserving the grounding correspondence between image\nmasks and object references. This enables a novel task that\nintegrates panoptic segmentation with grounded captioning.\nOur structured annotation process ensures that the captions\nare both complete, covering the majority of objects in each\nimage, and grounded, with precise segmentation masks.\nThe final dataset, named COCONut-PanCap, is de-\nsigned for a wide range of vision-language applica-\ntions, combining Panoptic segmentation and grounded\nCaptioning. It comprises 118K image-text pairs for train-\ning, with an average caption length of 203 words, as well as\nan additional 25K image-text pairs, with an average caption\nlength of 233 words for validation. We demonstrate that\nCOCONut-PanCap significantly boosts the performance of\nboth VLM and text-to-image generation models at the in-\nstruction tuning and fine-tuning stages, outperforming re-\ncent detailed caption datasets [44].\nThis highlights the\npotential of our grounding-based captions for both vision-\nlanguage understanding and image generation tasks.\nOur contributions are summarized as follows:\n• We propose a caption annotation pipeline leveraging\npanoptic segmentation to create a high-quality, detailed\ncaption dataset comprising 143K annotated images. The\nresulting annotations are comprehensive, accurate, and\ninclude grounding masks, making this dataset substan-\ntially larger than recent detailed caption datasets.\n• Our COCONut-PanCap dataset facilitates a new chal-\nlenging task combining Panoptic segmentation and\nGrounded Captioning (PGC). We establish evaluation\nmetrics and settings for this PGC task and benchmark sev-\neral recent methods to assess performance on this novel\nchallenge.\n• We validate the utility of our proposed dataset across var-\nious fine-grained Image-to-Text (I2T) and Text-to-Image\n(T2I) tasks, including detailed caption generation, PGC,\nvisual question answering (VQA), referring segmenta-\ntion, and text-conditioned image generation. Experimen-\ntal results show that our dataset significantly enhances\nmodel performance across all these tasks.\n2. Related Work\nDetailed Captions from VLMs. Researchers are increas-\ningly interested in creating large-scale datasets with detailed\ncaptions generated from advanced vision-language models.\nDenseFusion1M [32] utilizes a pretrained perceptual model\nto prompt VLMs, facilitating more detailed image descrip-\ntions.\nRecap-DataComp1B [31] first fine-tunes the Llama-3-\n8B powered LLaVA-1.5 model [36], then applies it to recap-\ntion approximately 1.3 billion images from the DataComp-\n1B dataset [14], generating a rich repository of detailed\nimage descriptions.\nOn a similar front, the PixelProse\ndataset [59] offers general-purpose image captions designed\nto serve various applications, from visual question answer-\ning (VQA) to pre-training tasks. Unlike datasets targeting\nsingle applications, PixelProse captions are dense, versa-\ntile image descriptions that can be adapted to other formats,\nsuch as VQA and instructional data, with the help of large\nlanguage models (LLMs).\nAlthough these detailed cap-\ntion datasets are large-scale, they are directly generated by\nVLMs without human verification, falling behind human-\nannotated captions on quality. Our proposed COCONut-\nPanCap dataset leverages extensive human effort to ensure\nhigh-quality annotations.\nHuman-annotated Detailed Captions.\nSeveral efforts\nhave been made toward this goal, utilizing fully human-\n\nDataset Name\nImage Source Sample\nAnnotated by Avg. Words Masks\nBLIP-LCS\nLAION [53], CC [4], SBU [45]\n558K\nBLIP [30]\n54\n✗\nDenseFusion1M [32]\nLAION [53] 1,059K Vision Specialist Models\n191\n✗\nLLaVA-Recap118K [38]\nCOCO [35]\n118K\nLLaVA-NEXT [38]\n186\n✗\nLLaVA-Details-23K [37]\nCOCO [35]\n23K\nGPT4\n105\n✗\nShareGPT4V [5]\nLAION [53], CC [4], SBU [45], COCO [35] etc.\n100K\nGPT4-Vision\n162\n✗\nShareGPT4V-PT [5]\nLAION [53], CC [4], SBU [45], COCO [35] etc. 1,246K\nShare-Captioner [5]\n144\n✗\nPixelLM-MUSE [51]\nLVIS [17]\n246K\nGPT4-Vision\n-\n3.7‡\nOsprey [69]\nCOCO [35]\n724K\nGPT4-Vision\n-\n-\nGLaMM-GCG [50]\nRefCOCOg [40],PSG [65],Flick30K [47]\n214K\nVision Specialist Models\n128\n3.6\nCOCO-caption [6]\nCOCO [35]\n118K\nHuman\n11\n✗\nDCI [61]\nSA-1B [24]\n8K\nHuman\n144\n✗\nDOCCI [44]\nDOCCI [44]\n9.6K\nHuman\n136\n✗\nIIW [15]\nWebLI [15]\n8.5K\nHuman\n217\n✗\nCOCONut-PanCap (ours)\nCOCO [35]\n118K\nHuman\n203\n13.2\nTable 1. Dataset (training set) Comparison. Our proposed COCONut-PanCap dataset stands out for its detailed (2nd highest in Average\nWords), high-quality (human interactive annotated) captions and high-density segmentation masks (1st in Average Masks). ‡ denotes the\nmask number for referring segmentation which only counts the targets in QA format. Note that “Samples” means the number of collected\nannotations, where there may exist one image with multiple different annotation, i.e., in region-level datasets like Osprey.\nDataset Name\nSamples Avg. Words Caption T2I Grd. Seg.\nCOCO-30K [6]\n30,000\n11\n✓\n✓\n✗\nDOCCI-test [44]\n5,000\n136\n✓\n✓\n✗\nIIW-test [15]\n445\n217\n✓\n✓\n✗\nGenEval [16]\n553\n8\n✗\n✓\n✗\nT2I-CompBench val [20]\n2400\n9\n✗\n✓\n✗\nGLaMM-GCG val-test [50]\n2,000\n128\n✓\n✗\n✓\nCOCONut-PanCap val (ours)\n25,000\n233\n✓\n✓\n✓\nTable 2. Dataset (evaluation set) Comparison. Our COCONut-\nPanCap validation set provides detailed captions and supports\nmultiple multi-modal tasks, including image captioning, text-to-\nimage generation (T2I), and grounded segmentation (Grd. Seg.).\nannotated data or human-in-the-loop approaches. One ex-\nample is DOCCI [44] which is a small, high-detailed image\ncaption dataset that is entirely human-annotated, contain-\ning only 15K samples but providing diverse details, such as\nkey objects, their attributes, spatial relationships, and text\nrendering. Two small-scale detailed caption datasets, Im-\nageInWords [15] and DCI [61], use a combination of au-\ntomatic annotation models with human involvement, both\nwith fewer than 10K samples. Pixmo-Cap [8] introduces a\nlarge-scale dataset of detailed image captions from speech-\nbased descriptions, offering richer visual annotations than\ntext-based methods.\nOur proposed COCONut-PanCap dataset yields smaller\nscale compare to Pixmo-Cap but we have different focuses\nwhere Pixmo-Cap focuses on pretraining the VLMs while\nwe focus on the instruction tuning and finetuning stages of\nVLMs and image generation models. Our work also shares\na similar annotation pipeline with a recent video captioning\ndataset Shot2Story [18] where both VLM draft and human\ncorrections are used to create complete and accurate anno-\ntations.\nGrounded Captions with Segmentation Masks. Exist-\ning work have made significant strides in creating datasets\nwith region-level captions linked to entity segmentation\nmasks [69] or bounding boxes [70].\nHowever, few\ndatasets associate grounded segmentation directly with\ncaptions.\nGLaMM [50] proposes a Grounding-anything\nDataset (GranD) using an automated annotation pipeline\nthat encompasses 7.5M unique concepts grounded in a total\nof 810M regions available with segmentation masks.\nLater,\nMGLMM\n[72]\nfurther\nexplore\nthe\nmulti-\ngranularity GLaMM model to generate a multi-granularity\ndataset. Our proposed COCONut-PanCap dataset follows\na similar approach of grounding captions to dense masks\nbut offers significantly denser masks per caption, as shown\nin Tab. 1, with an average of 13.2 masks per image com-\npared to 3.6 in GLaMM. Note that we focus on grounded\nsegmentation for detailed captions, rather than descriptions\nof all levels of segmentation masks (objects or parts) as pro-\nvided in the GranD dataset [50], which is outside the scope\nof our study.\n3. COCONut-PanCap Dataset\nWe construct a novel dataset based on COCO images to pro-\nvide detailed captions at both image and mask levels, using\nCOCONut panoptic masks as a foundation for comprehen-\nsive region descriptions. Specifically, we leverage panop-\ntic masks from COCONut-S [9] to annotate detailed region\ncaptions, incorporating both ‘thing’ and ‘stuff’ masks to\ncover a wide range of semantic regions.\n3.1. Dataset Description\nComprehensively understanding diverse visual elements in\ncomplex scenes can benefit multiple tasks including percep-\ntion, understanding, and generation. In this section, we de-\nscribe the annotation pipeline for our dataset leveraging the\nhuman annotated panoptic masks. We first show the sta-\n\ninput image\nhuman annotated\npanoptic segmentation mask\noverlaid image\nVLM\nset-of-marks\nprompt instruc4on\n0: road – The ground surface in the \nforeground, labeled as the road.1: wall-\nother-merged – A section of the wall on the \nleft side of the image, with a mural or design \npainted on it, including part of a blue \noctopus.2: window-other – A window \nlocated on the upper right side of the image, \nwhich has been labeled separately.\n…………\n t, also wearing a hat and purple clothing, \npossibly engaged in conversation.11: chair – \nThe chair that the first person is sitting \non.12: bicycle – A bicycle is located near the \ncenter of the image, leaning against the \nwall.13: handbag – A handbag or bag is near \nthe first person or placed beside the \nchair.14: chair – Another chair, possibly for \nthe second person sitting in the image.\ndetailed cap4on dra5\nhuman\nedit\nsummarize by VLM,\nveriﬁca4on check by\nhuman\nround-2\nround-1\nFigure 2. Annotation Pipeline. Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks [66]\nvisualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for\nediting and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning\nor finetuning stage.\n: Could you please describe the details of the image for each region?\n:\nFrom your image, here is the analysis:\n0: Sky - Clear blue sky with some scattered clouds.\n1: Road - Asphalt road, smooth and unobstructed, ideal for vehicles.\n2: Building-other-merged - Building structure with large glass windows \nand beige walls, likely a commercial or public building.\n3: Tree-merged - Lush green trees with dense foliage, possibly \nornamental in a landscaped area.\n4: Pavement-merged - Grey concrete pavement, typically used for \npedestrian pathways.\n5: Stop sign - Red octagonal traffic stop sign with white text, indicating a \nvehicle stop required.\n6: Car - A car in motion, likely in the process of slowing down as it \napproaches the stop sign.\n7: Truck - A white truck, stationary, parked in a designated area, possibly \nfor loading or unloading goods.\nWith the given region-level description, can you merge them into\none paragraph in the format of grounded caption?\nIn this scene, the clear blue sky with some scattered clouds \nserves as a pleasant backdrop. The smooth asphalt road \nprovides an unobstructed pathway, suitable for vehicles. \nNearby, there is a building structure with glass windows and \nbeige walls. Lush green trees with dense foliage add a touch \nof nature. The grey concrete pavement offers a pedestrian-\nfriendly path alongside the road. A upside\ndown\nred \noctagonal traffic stop sign with white text signals vehicles to \nhalt. Approaching the stop sign is a car, likely slowing down. In \nthe background, a stationary white truck is parked in a \ndesignated area.\nis upside down\nis approaching\ncommercial\nx\nloading or unloading goods\nx\n: add\n: remove\nfound hallucination and remove\nx\nhuman edit legend:\nround-1\nround-2\nFigure 3. Designed Prompt Template. By giving the concatenated set-of-marks images, the right side (round-1) shows the initial response\nand the corresponding human edits. Once finalized by humans, these edits will be merged into a single detailed caption grounded with\npanoptic segmentation masks, as shown in the left side (round-2).\ntistical analysis of our final dataset in Tab. 1. On average,\nour captions contain 203 words spanning 11 sentences. We\nfollow the same split setting in COCO2017 [35] dataset,\nwhich includes 118K training images. To provide a com-\nprehensive evaluation set, we adopt the same 25K images\nfrom COCONut-val split (which contains COCO2017-val\nand another 20K Objects365 [55] validation images).\n3.2. Dataset Construction\nWe argue that high-quality descriptions should provide suf-\nficient details of key objects and their attributes, as well as\ninformation about secondary objects and background ele-\nments. To achieve this, as shown in Fig. 2, we use human-\nannotated panoptic segmentation masks to decide the set of\nobjects to reference in the caption. These masks include\nboth ‘thing’ and ‘stuff’ classes, representing single objects\nand semantic regions, respectively. We adopt the panop-\ntic segmentation masks from the COCONut-S [9] dataset.\nThe masks are overlaid on the images, labeled with class\nnames c1, c2, . . . , cn ∈C, where C is the set of COCO’s\n133 panoptic classes. We then construct a prompt with both\nthe edited image and the original image and a textual ques-\ntion for GPT-4V, as illustrated in Fig. 3. The resulting re-\ngion captions from GPT-4V are reviewed and corrected by\nhuman raters for accuracy and consistency.\n\nFigure 4. Frequency of Extracted Nouns from the COCONut-\nPanCap Dataset. The top 10 most frequent nouns are: people,\ntable, room, street, dining, man, person, cars, chairs, and field.\n3.3. Dataset Analysis\nConcepts Beyond COCO’s 133 Classes. To clarify the\ngoal of our annotation task, we focus on key visual features\nsuch as objects, attributes, spatial relationships, and count-\ning. As shown in Fig. 4, we utilize the panoptic segmenta-\ntion mask from COCONut-S, which includes 133 classes in\nthe word vocabulary. Our proposed dataset, however, incor-\nporates additional concepts beyond these 133 classes, such\nas ‘vegetable’ and ‘parking’. This demonstrates that our hu-\nman annotators delivers accurate and diverse descriptions\nwhen using the provided label names as a reference.\nUser Study for Caption Quality. We randomly sample\n1,000 images from our COCONut-PanCap training set and\nasked a human evaluator to perform a single-choice selec-\ntion task. The question is: ‘Please select the best descrip-\ntion for the image, considering the correctness of object\nnames, attributes, counting, spatial relationships, and ac-\ntion.’\nThe compared captions are generated using GPT-\n4V [1], Qwen2-VL [64], and InternVL-2 [7], resulting in a\nsingle-choice four-option question. Fig. 5 illustrates the re-\nsults, showing that our GPT-assisted human-annotated cap-\ntions receives the highest ratings. More details can be found\nin the supplementary.\n4. PGC Baseline: PanCaper\nIn this section, we introduce our baseline method for joint\npanoptic segmentation and grounded captioning (PGC),\nnamely PanCaper. We start with an overview of the pixel\ngrounding task and then present our proposed approach,\nwhich incorporates a panoptic segmentation module specif-\nically designed for grounding objects in captions.\nRevisiting the Pixel Grounding Task. Our baseline model\nbuilds upon LISA [28], a model that combines the lan-\nguage generation capabilities of VLMs with the ability to\nproduce segmentation mask. LISA consists of three main\ncomponents: a VLM, a vision backbone V , and a mask de-\ncoder D. With a given text prompt, the VLM (typically\nLLaVA [36, 37]) generates an output containing a ⟨SEG⟩\ntoken. For instance, with the input prompt, ‘Could you seg-\nment the food with high Vitamin C?’ LISA generates the\nresponse ‘It is ⟨SEG⟩.’ This process extracts the last-layer\nFigure 5. Caption Quality via User Study. The study involved\nhuman evaluators assessing a random sample of 1,000 captions,\nwith a strong preference shown for captions from our dataset.\nembedding of the LLM from LLaVA. Then a language-to-\nprompt (L-P) projection layer (g) transforms the last-layer\nembeddings corresponding to ⟨SEG⟩tokens (lseg) into the\ndecoder’s feature space. Meanwhile, the vision backbone\nextracts dense visual features from the input image.\nFi-\nnally, both the dense features and the CLIP image embed-\nding from LLaVA are fed into the mask decoder to produce\nthe final segmentation mask.\nPrompt Instruction for Grounded Captioning. We pro-\npose a baseline method for the PGC task by modifying\nLISA to enable grounded captioning with segmentation\nmasks. Since LISA was originally designed for generat-\ning segmentation with a single output mask, two main ad-\njustments are necessary: (1) the use of multiple ⟨SEG⟩to-\nkens, and (2) extracting noun phrases from the caption for\ngrounding. To facilitate grounded segmentation, we modify\nthe prompt to the VLM as ‘Please provide a detailed de-\nscription of the image and segment each part.’ This prompt\ntriggers the model to generate caption responses with cor-\nresponding ⟨SEGi⟩tokens, where i ∈[1, N] and N is the\ntotal number of predicted segmentations.\nGiven a predicted caption for the image, aligning each\n⟨SEGi⟩token requires pairing it with a noun phrase,\n‘⟨p⟩phrasei⟨/p⟩,’ where phrasei is the relevant part in the\ncaption to be grounded. With these prompt tokens defined,\nthe model uses the vision backbone V and mask decoder\nD to facilitate fine-grained, pixel-level grounding, with D\nproducing segmentation masks M.\nEnable Panoptic Grounding. To achieve panoptic seg-\nmentation from captions, we first classify ⟨SEG⟩tokens\ninto two types: ⟨SEGt⟩for ‘thing’ classes and ⟨SEGs⟩\nfor ‘stuff’ classes.\nThese tokens are then processed by\nour segmentation modules to produce panoptic segmenta-\ntion masks. We initialize the vision backbone V with a\npretrained kMaX-DeepLab encoder [67] and fine-tune the\ndecoder D using our COCONut-PanCap dataset.\nSince\nkMaX-DeepLab operates as a closed-set segmenter, we\n\nalign text embeddings of the associated noun phrases with\nCOCO’s 133 panoptic classes. To accomplish this align-\nment, we use BERT [26] to generate the text embed-\ndings and to calculate cosine similarity, selecting the best-\nmatching category. Panoptic grounding provides mapping\nbetween detailed captions and image regions, which im-\nproves interpretability of VLM predictions.\nTraining Objectives. Our training objective aims to mini-\nmize the following losses:\nL = λtextLtext + λmaskLmask,\n(1)\nwhere Ltext is the auto-regressive cross-entropy loss for text\ngeneration, and Lmask is the mask loss [63], encouraging the\nmodel to produce high-quality segmentation results. λtext\nand λmask are the respective loss weights. We use the same\nloss weights as LISA [28].\nEvaluation Metrics for Caption Quality. We conduct the\nanalysis with multiple metrics to evaluate the quality and\ncompleteness of the generated captions. We introduce a\nbenchmarking suite for the PGC task, with a validation set\nof 25K images. For the caption quality, we report the cap-\ntion metrics including CIDEr [62], METEOR [2], ROUGE-\nL [34], BLEU@4 [46] and CAPTURE [10]. For grounded\npanoptic segmentation, we report PQ scores [23].\n5. Experimental Results\nWe assess the effectiveness of human-annotated caption\ndata by performing three primary tasks utilizing our dataset\nin the fine-tuning/instruction tuning stage: detailed cap-\ntioning, panoptic grounded captioning (PGC), and text-to-\nimage generation. Additionally, we demonstrate the trans-\nferability of the knowledge learned from our dataset through\ntwo downstream tasks: VQA and referring segmentation.\nDetailed Captioning. We conduct instruction tuning with\nLLaVA-NeXT framework [38] for this task. We replace\nthe caption data (23k) from the original LLaVA instruction-\ntuning set with detailed captions from our dataset, keep-\ning the same amount of instruction data size.\nWe fol-\nlow the same training setup used for LLaVA-NeXT with\nLlama3-8B [11].\nTreating it as a QA task, we use the\nprompt, ‘Could you please describe the image in detail?’\nand collect the corresponding response as the caption for\nthe image. We evaluate caption quality using CIDEr [62],\nMETEOR [2], BLEU@4 [46], ROUGE-L [34] and CAP-\nTURE [10] metrics. We also extend the model by adding\nthe mask-pooled features from the panoptic segmentation\nmasks as additional signals to the LLaVA model and name\nit LLaVA-NeXT-pool. During training, we use the ground\ntruth mask to extract the features while during inference\nwe use the mask proposals from the pretrained kMaX-\nDeepLab [67]. Besides, we also experiment with synthetic\ncaptions directly generated using InternVL-2 [7], Qwen2-\nVL [64] and GPT-4V [1]. We follow the same data prepara-\ntion settings as our dataset to build these instruction datasets\nfor these 23K images with different sources of synthetic\ndetailed captions, namely LLaVA 665K-InternVL2-Cap ,\nLLaVA 665K-Qwen2VL-Cap, and LLaVA 665K-GPT4V-\nCap. These datasets are used to produce models LLaVA-\nNeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G respec-\ntively.\nMore details can be found in the supplementary.\nThe results are presented in Tab. 3. LLaVA-NeXT models\nshow improved performance when fine-tuned on the custom\ninstruction-tuning dataset.\nAmong these, LLaVA-NeXT-\npool achieves the highest scores in all metrics, with CAP-\nTURE of 61.4, CIDEr of 13.1, BLEU@4 of 5.3, and ME-\nTEOR of 17.1, significantly higher than the original model\nvariant LLaVA-NeXT, indicating the benefit of added re-\ngion features for additional visual cues. Models trained on\nsynthetic captions (LLaVA-NeXT-I, LLaVA-NeXT-Q, and\nLLaVA-NeXT-G) generally show lower scores, showing ad-\nvantage of our human-annotated caption.\nPGC: Stronger Detail Reasoning Performance. We im-\nplement our proposed PanCaper based on LISA which uses\npre-trained LLaVA-NeXT with a LLM of Llama3-8B, with\nLoRA [19] adopted. The vision encoder uses a fixed CLIP-\nViT-L/14-336 model, modified with linearly interpolated\nposition embeddings to process 448 resolution images. The\ntrainable components of our model include the mask de-\ncoder of kMaX-DeepLab, and the tunable parts in LLaVA\nsame as in LISA. To enhance model performance in visual\nunderstanding, we initialize our PanCaper using pretrained\nLLaVA-NeXT models from the detailed captioning task.\nWe also experiment with a model variant that uses mask\npooled features similar to LLaVA-NeXT-pool, and name it\nPanCaper-Pro.\nFor comparison, we select 3 related methods LISA, Pix-\nelLM [51] and GLaMM [50] for evaluation. It is notewor-\nthy that LISA is not able to perform multi-mask prediction.\nWe therefore adapt LISA [28] for the multi-mask generation\nwith grounded segmentation, namely LISA+. The imple-\nmentation details can be found in the supplementary. Tab. 4\nshows the quantitative results.\nOur proposed PanCaper-\nPro achieves the highest scores across all captioning met-\nrics (CIDEr: 12.5, CAPTURE: 64.3, BLEU@4: 6.4, ME-\nTEOR: 17.9), outperforming all other models. Both PanCa-\nper models show significant improvements over other mod-\nels in all captioning metrics, highlighting the effectiveness\nof the COCONut-PanCap dataset for detailed caption gen-\neration. On grounding segmentation, PanCaper-Pro again\nleads, with a PQ score of 0.61, PQthing of 0.58, and PQstuff\nof 0.68, reflecting its robustness on both “thing” and “stuff”\nclasses. Notably, enabling mask pooling in our proposed\nPanCaper-Pro further enhances segmentation metrics. The\nbaseline models (LISA+ and GLaMM with GranD) achieve\nmuch lower PQ scores, due to incomplete segmentation an-\n\nTraining recipe Method\nPretrain Dataset Instruction-tuning dataset\nMask pooled CAPTURE CIDEr BLEU@4 METEOR ROUGE-L\nfinetune\nLLaVA-NeXT*\nLAION-CC-SBU LLaVA 665K\n✗\n55.4\n10.8\n4.2\n13.2\n23.1\nLLaVA-NeXT\nLAION-CC-SBU LLaVA 665K-COCONut-PanCap\n✗\n58.7\n11.2\n4.8\n16.2\n24.6\nLLaVA-NeXT-pool LAION-CC-SBU LLaVA 665K-COCONut-PanCap\n✓\n61.4\n13.1\n5.3\n17.1\n26.8\nLLaVA-NeXT-I\nLAION-CC-SBU LLaVA 665K-InternVL2-Cap\n✗\n53.9\n9.4\n4.4\n11.5\n21.4\nLLaVA-NeXT-Q\nLAION-CC-SBU LLaVA 665K-Qwen2VL-Cap\n✗\n55.4\n8.9\n4.6\n12.9\n22.5\nLLaVA-NeXT-G\nLAION-CC-SBU LLaVA 665K-GPT4V-Cap\n✗\n56.2\n9.6\n4.7\n13.3\n22.8\nTable 3. Caption Benchmark Results Evaluated on Our COCONut-PanCap Val Set. Note that the amount of data in the instruction\ndataset remains the same; only the sources of the detailed captions vary, with a total of 23K images that have detailed captions.\nCaption\nGrounding segmentation\nMethod\nPretrain dataset\nInstruction dataset\nMask pooled CAPTURE CIDEr BLEU@4 METEOR PQ PQthing\nPQstuff\nLISA+ *\nLAION-CC-SBU\nGranDf\n✗\n46.2\n6.6\n3.8\n9.8\n0.43\n0.41\n0.45\nLISA+\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✗\n57.9\n8.1\n4.9\n13.8\n0.50\n0.49\n0.44\nGLaMM GCG *\nLAION-CC-SBU+GranD\nGranDf\n✗\n43.2\n6.5\n3.6\n10.6\n0.27\n0.35\n0.21\nGLaMM GCG\nLAION-CC-SBU+GranD COCONut-PanCap (ours)\n✗\n56.8\n7.8\n5.2\n14.3\n0.55\n0.54\n0.46\nPanCaper (ours)\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✗\n62.6\n12.0\n5.8\n15.4\n0.56\n0.55\n0.66\nPanCaper-Pro (ours)\nLAION-CC-SBU\nCOCONut-PanCap (ours)\n✓\n64.3\n12.5\n6.4\n17.9\n0.61\n0.58\n0.68\nTable 4. Joint Panoptic Segmentation and Grounded Captioning (PGC) on COCONut-PanCap Val Set. * denotes reproduced results.\nTraining dataset\nEvaluation dataset FID↓\nFDdinov2 ↓CLIPScore↑\nSD3 PT dataset [12]\nDOCCI test set [44]\n30.2\n345\n74.9\nCOCO-caption [6]\n27.6\n321\n76.8\nDOCCI [44]\n22.1\n300\n77.8\nCOCONut-PanCap (ours)\n21.4\n290\n77.9\nSD3 PT dataset [12]\n31.8\n300\n73.8\nCOCO-caption [6]\nCOCONut-PanCap\n28.0\n294\n74.0\nDOCCI [44]\nval set (ours)\n24.3\n267\n75.1\nCOCONut-PanCap (ours)\n23.1\n260\n77.3\nTable 5. Benchmark Results on Text Conditioned Image Gen-\neration.\nStable-Diffusion-3 (SD3) medium is finetuned with\nCOCO-Caption (short), DOCCI and our COCONut-Panoptic and\nevaluated on DOCCI test set [44] and our COCONut-PanCap val\nset. ‘SD3 PT dataset’ denotes the pretraining dataset of SD3, and\nthus the rows correspond to zero-shot evaluation of SD3.\nw/o FT COCO-caption [6] DOCCI [44] COCONut-PanCap\ncolor attribution\n0.37\n0.34\n0.38\n0.40\ncolors\n0.73\n0.70\n0.74\n0.75\nposition\n0.33\n0.30\n0.36\n0.36\ncounting\n0.65\n0.64\n0.65\n0.70\nsingle object\n0.96\n0.94\n0.95\n0.96\ntwo objects\n0.80\n0.78\n0.81\n0.89\noverall score\n0.64\n0.62\n0.65\n0.68\nTable 6. Effects of Fine-tuning the SD3-medium (T2I model)\nwith Different Datasets on GenEval [16]. w/o FT denotes the\nmodel is not finetuned with any datasets (i.e., zero-shot testing).\nnotations in the GranD dataset.\nText-to-Image Generation. We adopt the Stable Diffusion\n3 (SD3) medium model1 for text to image generation with\nLoRA finetuning. We adopt the default training settings\nbut only with different text-image datasets for training. We\nevaluate with two types of training images from COCO [35]\nand DOCCI [44] datasets. In details, for the COCO images,\n1https://huggingface.co/docs/diffusers/stable diffusion/stable diffusion 3\nwe explore the short COCO-caption and detailed captions\nfrom our dataset. For DOCCI images, we directly use the\ncaptions from their dataset. Tab. 5 shows the quantitative\nresults. Traning on COCONut-PanCap achieves the best\nperformance across all metrics when evaluated on DOCCI-\ntest, with the lowest FID (21.4), lowest FDdinov2 (290), and\nthe highest CLIPScore (77.9), indicating superior genera-\ntion quality and high image-text relevance. When evalu-\nated on COCONut-PanCap-val set, training on COCONut-\nPanCap again shows the best results with the lowest FID\n(23.1), FDdinov2 (267), and a high CLIPScore of 77.3.\nTab. 6 shows the results on GenEval benchmark [16].\nFinetuning SD3-medium with COCONut-PanCap consis-\ntently scores the highest in most categories, particularly\nthose requiring image details like color attribution, object\npositioning, and handling multiple objects. Our proposed\ndataset enables more accurate image generation that re-\nquires understanding of relationships, multiple objects and\ncounting, tasks that other datasets struggle with.\nVQA. To evaluate the effectiveness of the proposed\nCOCONut-PanCap dataset, we utilize these captions during\nthe instruction-tuning stage and follow the setup of LLaVA-\nNeXT [38] across various visual question answering (VQA)\nand multi-modality understanding benchmarks. We evalu-\nate on MM-Vet [68], SEED-IMG [29], MMBench-en [39],\nMME [13], POPE [33], and TextVQA [58], covering a\nbroad range of evaluation dimensions. We experiment with\ndifferent amount of our COCONut-PanCap caption data in-\njected into the instruction tuning stage by replacing the orig-\ninal COCO captioning data with our dataset. As shown in\nTab. 7, the baseline model LLaVA-NeXT (using its orig-\ninal recaptioned COCO) achieves relatively lower perfor-\nmance across all metrics, with scores such as 43.5 on MM-\nVet, 70.1 on Seed-IMG, and 68.9 on TextVQA. Building\n\nMethod\nLLM\nInstruction-tuning Dataset\nMM-Vet Seed-IMG MMBench-en TextVQA POPE MME\nLLaVA-NeXT *\nLlama3-8B orginal LLaVA 665K [38]\n43.5\n70.1\n71.4\n68.9\n85.4\n1523\nLLaVA-NeXT-20K Llama3-8B LLaVA 665K-COCONut-PanCap-20K\n44.1\n72.5\n73.6\n69.8\n86.1\n1552\nLLaVA-NeXT-50K Llama3-8B LLaVA 665K-COCONut-PanCap-50K\n44.6\n73.1\n74.2\n70.0\n87.1\n1600\nLLaVA-NeXT-Full Llama3-8B LLaVA 665K-COCONut-PanCap-118K\n45.5\n74.3\n75.1\n70.7\n87.9\n1612\nLLaVA-1.5\nVicuna-7B LLaVA 665K-ShareGPT4V-100K\n37.8\n67.4\n70.5\n64.6\n84.7\n1519\nLLaVA-1.5\nVicuna-7B LLaVA 665K-COCONut-PanCap-20K\n38.5\n67.7\n70.9\n64.5\n84.9\n1521\nTable 7. Benchmark Results and Ablation Study on VQA. By adding extra detailed caption data for instruction tuning, the models show\nincreased improvement. * denotes reproduced results. Using only 20K human labeled data can still achieve comparable performance\nto 100K synthetic data.\nMethod\nrefCOCO\nrefCOCO+\nrefCOCOg\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nGLaMM* [50]\n77.5\n79.2\n74.9\n71.3\n74.7\n61.5\n71.3\n71.9\nPixelLM [51]\n73.0\n76.5\n68.2\n66.3\n71.7\n58.3\n69.3\n70.5\nLISA-7B [28]\n74.1\n76.5\n71.1\n62.4\n67.4\n56.5\n66.4\n68.5\nPanCaper+\n74.5\n76.7\n69.9\n69.9\n73.4\n59.5\n69.8\n70.6\nPanCaper+ + COCONut-PanCap\n76.2\n77.1\n72.3\n70.5\n73.9\n60.1\n72.1\n71.6\nTable 8. Benchmark Results on Referring Segmentation. * denotes reproduced results. It is noted that GLaMM uses extra data from the\nGranD dataset for pretraining. + denotes our PanCaper model is adapted for referring segmentation task.\non LLaVA-NeXT baseline, we progressively incorporated\nvarying amounts of COCONut-PanCap data (20K, 50K, and\n118K (full), as indicated by postfixes in the baseline names)\nduring instruction-tuning. Consistent improvements are ob-\nserved across all evaluated benchmarks as more of our data\nis integrated.\nReferring Segmentation. In this task, the model processes\nan image and a textual referring expression to output a\nsegmentation mask corresponding to the expression. The\nprompt used is, ‘Please segment the ⟨referring text⟩in the\nimage.’ The target model response is ‘Sure, it is ⟨SEG⟩.’,\nwhere the ⟨SEG⟩token is decoded to obtain the mask. We\nfollow the setup in LISA [28], using multiple segmentation\ndatasets to jointly train the models. Tab. 8 shows the quan-\ntitative results. Our model achieves superior performance,\nparticularly when additionally trained with the COCONut-\nPanCap dataset (last row), outperforming all models except\nGLaMM [50]. This improvement underscores our model’s\nefficacy in handling complex referring expressions, likely\ndue to the additional data that enhances model generaliza-\ntion and accuracy. It is worth noting that GLaMM performs\ncompetitively with our method, though the comparison is\nuneven given their additional use of the SA-1B dataset [25].\nSynthetic vs. Human Annotated Data. Generating syn-\nthetic data for captioning has been popular for recent tasks\nin either training vision encoders [48] or text-to-image gen-\neration [31]. We investigate the effect of varying the mix\nratio of synthetic captions generated by GPT-4V and our\nhuman-annotated data for fine-tuning (where 0 indicates\nfully synthetic data), using the COCONut-PanCap dataset\nfor training and the COCONut-PanCap-val set for evalua-\ntion. We adopt LLaVA-NeXT for the captioning task and\nSD3-medium for the image generation task. As shown in\nFig. 6, adding 25% human-annotated data yields significant\nperformance improvements in both captioning and genera-\ntion, with a reduced FID of 26 from 31 (lower is better) and\nan increased CAPTURE score of 53.6 from 47.5 (higher\nis better). Consistent improvements are observed as more\nhuman-annotated data is incorporated.\nFigure 6. Varying Synthetic and Human-Annotation Ratios.\nCAPTURE is used to evaluate the performance of LLaVA-NeXT\non detailed captioning, while FID assesses the performance of\nSD3-medium on text-conditioned image generation.\n6. Conclusion and Discussion\nIn this work, we proposed a novel dataset designed to sup-\nport detailed captioning and grounded segmentation tasks\nbuilt on COCO images. We demonstrated that our dataset\ncan enhance model performance during instruction tuning\nand fine-tuning stages across various multi-modal under-\nstanding and generation tasks, such as captioning, VQA,\ngrounded segmentation, and text-to-image generation. We\n\nhope that COCONut-PanCap, with its detailed captions\ngrounded with dense panoptic masks, will foster future ad-\nvancements in multi-modal learning research.\nLimitations. High-quality human-labeled data offers sig-\nnificant benefits for instruction tuning in multi-modal tasks,\nbut scaling such datasets is challenging. To address this,\nwe introduce COCONut-PanCap as a starting point for\nlarge-scale human-annotated data exploration. Recognizing\nthe relatively smaller dataset size compared to other large\ndataset, future work may involve using this dataset to train\nseed models to generate more high-quality synthetic data.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 5, 6\n[2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In ACL Workshop, 2005. 6\n[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In CVPR, 2018.\n13\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\nIn CVPR,\n2021. 3\n[5] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 2, 3\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 2, 3, 7\n[7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\nwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, et al. How far are we to gpt-4v? closing\nthe gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821, 2024. 5, 6\n[8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tri-\npathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo\nand pixmo: Open weights and open data for state-of-the-art\nmultimodal models. arXiv preprint arXiv:2409.17146, 2024.\n3\n[9] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and\nLiang-Chieh Chen. Coconut: Modernizing coco segmenta-\ntion. In CVPR, 2024. 2, 3, 4\n[10] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang,\nYuan Zhang, and Haoyuan Guo. Benchmarking and improv-\ning detail image caption. arXiv preprint arXiv:2405.19092,\n2024. 6\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024. 6\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn ICML, 2024. 7\n[13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 7\n[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. NeurIPS, 2024. 2\n[15] Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan\nBitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bun-\nner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. Im-\nageinwords: Unlocking hyper-detailed image descriptions.\narXiv preprint arXiv:2405.02793, 2024. 2, 3\n[16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment. In NeurIPS, 2023. 3, 7\n[17] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019. 3\n[18] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng\nWang. Shot2story20k: A new benchmark for comprehen-\nsive understanding of multi-shot videos.\narXiv preprint\narXiv:2311.17043, 2023. 3\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 6, 12\n[20] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\nhui Liu. T2i-compbench: A comprehensive benchmark for\nopen-world compositional text-to-image generation. arXiv\npreprint arXiv: 2307.06350, 2023. 3\n[21] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 13\n[22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In EMNLP, 2014. 13\n[23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll´ar. Panoptic segmentation. In CVPR,\n2019. 6\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 3\n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 8\n\n[26] Mikhail V Koroteev. Bert: a review of applications in nat-\nural language processing and understanding. arXiv preprint\narXiv:2103.11943, 2021. 6\n[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 2017. 13\n[28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta-\ntion via large language model. In CVPR, 2024. 5, 6, 8, 12,\n13\n[29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 7\n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 3\n[31] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen\nZhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu,\nHuangjie Zheng, Yuyin Zhou, and Cihang Xie.\nWhat if\nwe recaption billions of web images with llama-3?\narXiv\npreprint arXiv:2406.08478, 2024. 2, 8\n[32] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xin-\nlong Wang, and Ling-Yu Duan.\nDensefusion-1m: Merg-\ning vision experts for comprehensive multimodal perception.\narXiv preprint arXiv:2407.08303, 2024. 2, 3\n[33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.\n7\n[34] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In ACL Workshop, 2004. 6\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 3, 4, 7\n[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 2, 5, 13\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 3, 5, 12, 13\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 2, 3,\n6, 7, 8, 13\n[39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv:2307.06281, 2023. 7\n[40] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11–20, 2016. 3, 13\n[41] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nCVPR, 2016. 13\n[42] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In CVPR, 2019.\n13\n[43] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In ICDAR, 2019. 13\n[44] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan\nBitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana\nParekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Ja-\nson Baldridge. DOCCI: Descriptions of Connected and Con-\ntrasting Images. In ECCV, 2024. 2, 3, 7\n[45] Vicente Ordonez,\nGirish Kulkarni,\nand Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In NeurIPS, 2011. 3\n[46] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002. 6\n[47] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 8\n[49] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi\nWen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-\nquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts\nand attributes of common objects. In CVPR, 2023. 13\n[50] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel-\nrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M.\nAnwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan.\nGlamm: Pixel grounding large multimodal model. In CVPR,\n2024. 3, 6, 8, 13\n[51] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao,\nDongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel\nreasoning with large multimodal model. In CVPR, 2024. 3,\n6, 8\n[52] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 2\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. NeurIPS, 2022. 2, 3\n[54] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa:\nA\n\nbenchmark for visual question answering using world knowl-\nedge. In ECCV, 2022. 13\n[55] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, 2019. 4\n[56] ShareGPT. ShareGPT. https://sharegpt.com/. 13\n[57] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image captioning\nwith reading comprehension. In ECCV, 2020. 13\n[58] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,\nXinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards\nvqa models that can read. In CVPR, 2019. 7\n[59] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand,\nMayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang,\nAbhinav Bhatele, Gowthami Somepalli, and Tom Goldstein.\nFrom pixels to prose: A large dataset of dense image cap-\ntions. arXiv preprint arXiv:2406.10328, 2024. 2\n[60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 2\n[61] Jack Urbanek,\nFlorian Bordes,\nPietro Astolfi,\nMary\nWilliamson, Vasu Sharma, and Adriana Romero-Soriano. A\npicture is worth more than 77 text tokens: Evaluating clip-\nstyle models on dense captions. In CVPR, 2024. 3\n[62] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In CVPR, 2015. 6\n[63] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 6\n[64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 5, 6\n[65] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\nWayne Zhang, and Ziwei Liu. Panoptic scene graph gen-\neration. In ECCV, 2022. 3\n[66] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan\nLi, and Jianfeng Gao.\nSet-of-mark prompting unleashes\nextraordinary visual grounding in gpt-4v.\narXiv preprint\narXiv:2310.11441, 2023. 4\n[67] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means Mask Transformer. In ECCV, 2022. 5, 6, 12,\n13\n[68] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. In ICML, 2024. 7\n[69] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie\nLuo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel\nunderstanding with visual instruction tuning. In CVPR, 2024.\n3\n[70] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 3\n[71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In CVPR, 2017. 13\n[72] Li Zhou, Xu Yuan, Zenghui Sun, Zikun Zhou, and Jing-\nsong Lan. Instruction-guided multi-granularity segmentation\nand captioning with large multimodal model. arXiv preprint\narXiv:2409.13407, 2024. 3\n\nThe appendix is organized as follows.\n• In Sec. A, we show implementation details for De-\ntailed Captioning (Sec. A.1), Panoptic segmentation and\nGrounded (Sec. A.2), and VQA (Sec. A.3).\n• In Sec. B, we show more visualization examples of our\nproposed COCONut-PanCap dataset (Sec. B.1), and anal-\nysis of the tier cases in our dataset annotation user study\n(Sec. B.2).\nA. Experimental Details\nIn this section, we provide more experimental details for\ndetailed captioning (Sec. A.1), PGC (Sec. A.2), and VQA\n(Sec. A.3).\nA.1. Detailed Captioning\nDetailed Captioning Instruction Dataset Construction.\nThe key step in conducting the experiment is constructing\nthe dataset. The original LLaVA-665K dataset consists of\nLLaVA-158K combined with other VQA datasets. Within\nLLaVA-158K, a subset of detailed captions corresponds to\n23K COCO images. To create our-LLaVA-665K (referred\nto as LLaVA 665K-COCONut-PanCap in the table), we re-\nplace the detailed caption annotations for these 23K COCO\nimages with our annotations. Importantly, the total amount\nof training data remains unchanged (only the captions for\nthese 23K images are updated), ensuring a fair comparison\nof the impact of data quality on model performance.\nSynthetic Annotation for Detailed Caption. To build the\nsynthetic dataset with state-of-the-art VLM, we use three\nmodels, including open-sourced InterVL-2, Qwen2-VL and\nclose-sourced GPT-4V to generate the detailed captions for\nCOCO 118K train set images. We use the same text prompts\nthat is used in LLaVA [37] for prompting the model to cre-\nate the detailed captions.\nLLaVA-NeXT-pool implementation details. Fig. 7 shows\nthe comparison of the original LLaVA-NeXT and our pro-\nposed LLaVA-NeXT-pool. As shown in Fig. 7a, in order to\npreserve the details for the high-resolution images and rep-\nresentations, the original design employs a grid configura-\ntion which can also balance the performance efficiency with\noperational costs.\nThen both the patch-level and image-\nlevel features are later concatenated and sent to the LLM.\nDirectly splitting the image into patches could cause pro-\nlems, for example, in the figure, the upper part of the dog’s\nhead is partitioned into different patches which may result\nin incomplete feature extraction for single object. To over-\ncome this drawback, we propose LLaVA-NeXT-pool to ex-\ntract the dense feature and preserve the object details by uti-\nlizing the panoptic segmentation masks in our COCONut-\nPanCap dataset. Fig. 7b shows the details. Compared to the\noriginal design, LLaVA-NeXT-pool could effectively ex-\ntract the features for the dog in our example. Our design\nsplit\nencode\nflatten\nLLM\nresize\npatch-level\nencode\nflatten\nimage-level\n(a) LLaVA-NeXt-AnyRes\nmask\nencode\nLLM\nresize\nmask-level\nencode\nflatten\nimage-level\n…\nload\nmask-pooling\n…\nconcatenate\n(b) our LLaVA-NeXt-pool\nFigure 7.\nComparison of LLaVA-NeXt and our proposed\nLLaVA-NeXt-pool.\nenables more complete region-level feature extraction and\nis potential in understanding the details better.\nA.2. PGC\nWe provide more implementation details for the proposed\ntask:\nPanoptic segmentation and Grounded Captioning\n(PGC).\nPanCaper Implementation Details.\nWe introduce the\nPanCaper architecture details in this section.\nFollowing\nthe architecture in LISA [28], there are three components\nincluding the vision backbone, mask decoder and multi-\nmodal LLM. Fig. 8 shows the architecture details for Pan-\nCaper. We made modification on the vision backbone, and\nmask decoder part in terms of model architecture. To pre-\nserve the learned knowledge of the pre-trained multimodal\nLLM (i.e., LLaVA-NeXT in our experiments), we leverage\nLoRA [19] to perform efficient fine-tuning, and completely\nfreeze the vision backbone. The mask decoder is fully fine-\ntuned. Additionally, the LLM token embeddings (embed to-\nkens), the LLM head (lm head), and the projection layer are\nalso trainable. The weights of the text generation loss λtext\nand the mask loss λmask are set to 1.0 and 1.0, respectively.\nFor the PQ-style mask loss, we follow the same settings in\nkMaX-DeepLab [67], where it consists of mask-level cross\nentropy loss, dice loss and pixel loss.\nAdapting Baseline Methods for PGC Task. We adopt\nthe same text prompt template to enable the model to per-\nform PGC tasks. For LISA+, we follow the same design\n\npanop%c mask\ndecoder\nimage\nPlease provide a detailed description \nof the image and segment each part.\ntext prompt\nvision\nbackbone\n❄\nmulti-modal \nLLM\n❄\nLoRA\n🔥\nThe images depicts <p:a running\ndog[SEG_t]> …. on <p:the grass [SEG_s]>\n🔥\npredic-on\n🔥: trainable\n❄: fronzen\nFigure 8. Architecture of PanCaper. We utilize a pretrained vision encoder from kMaX-DeepLab [67] as our vision backbone, which\neffectively extracts dense features essential for panoptic segmentation.\nin GLaMM [50] to design the multi entity mask output by\nutilizing the the GranDf dataset. As the intruction dataset\nof GranDf is constructed similarly grounding the phrase in\nthe image-level caption, it will output multiple ⟨SEG⟩to-\nkens. The reasoning results of the number of ⟨SEG⟩tokens\ndecide the number of output entity mask which are often\nbinary masks. As a result, the model can generate a de-\ntailed caption along with interleaved segmentation masks,\nemploying the format “⟨p⟩A man⟨/p⟩⟨SEG⟩...\nnext to\n⟨p⟩a tree⟨/p⟩⟨SEG⟩”. And thus the format of instruction\ndataset is significat in task design. Therefore, we formu-\nlate our dataset as “⟨p⟩A man⟨/p⟩⟨SEGt⟩... next to ⟨p⟩a\ntree⟨/p⟩⟨SEGs⟩”, where ⟨SEGt⟩represents the seg token\nfor instance masks of thing and ⟨SEGs⟩represents for se-\nmantic masks of stuff respectively in panoptic setting. Sim-\nilarly, utilizing the PanCap dataset and special token design,\nGLaMM [50] is able to generate the entity masks with the\ntag of ‘thing’ and ‘stuff’.\nTraining Data Formulation. We adopt the same training\ndata from LISA [28] which comprises mainly three parts,\nall of which are derived from widely-used public datasets.\nThese include 1) Semantic Segmentation datasets includ-\ning ADE20K [71], COCO-Stuff [3], and LVIS-PACO [49]\npart datasets with the generated QA data, 2) Vanilla Re-\nferring Segmentation Datasets: refCOCO, refCOCO+, re-\nfCLEF [22] and refCOCOg [40] datasets, 3) ReasonSeg\ndataset [28], and 4) Visual Question Answering Dataset:\nLLaVA-v1.5-mix665k [36]. To enable the multi-mask gen-\neration for grounded caption, there are two options for\ninstruction datasets, GranDf and our COCONut-PanCap\nwhere GranDf consists of entity masks while COCONut-\nPanCap consists of panoptic masks.\nA.3. VQA\nWe provide more implementation details for the VQA\nexperiments.\nWe follow the same setting in LLaVA-\nNeXT to create the experimental results for VQA tasks.\nWe focus on the instruction tuning stage by adopting\nthe pretrained weights from the stage-1 across the train-\nings for all the model variants mentioned in Tab. 7 in\nthe paper.\nThe dataset we used is exactly the same\nas in LLaVA 665K [36] which includes the earlier ver-\nsion of instruction data proposed in LLaVA 158K [37],\nShareGPT [56], VQAv2 [41], GQA [21], openknowl-\nedge VQA (OKVQA [42],\nA-OKVQA [54]),\nOCR\n(OCRVQA [43], TextCaps [57]), region-level VQA datasets\n(Visual Genome [27], RefCOCO [22]). Among these data,\nLLaVA 158K comprises 77K complex reasoning, 58K con-\nversation and 23K detailed captions. To build the dataset\nvariants shown in Tab. 7, we simply remove the sub-\nset of detailed caption 23k, and subsequently add 20K,\n50K and 118K COCONut-PanCap dataset to build LLaVA\n665K-COCONut-PanCap-20K, LLaVA 665K-COCONut-\nPanCap-50K and LLaVA 665K-COCONut-PanCap-118K.\nBy these steps, we add more detailed caption data to\nconstruct the instruction tuning dataset.\nThis results in\nthe total amount of training data of 662K for LLaVA\n665K-COCONut-PanCap-20K, 692K for LLaVA 665K-\nCOCONut-PanCap-50K and 760K for LLaVA 665K-\nCOCONut-PanCap-118K. And thus the size of LLaVA\n665K-COCONut-PanCap-20K is slightly smaller than the\noriginal LLaVA 665K dataset, but the model trained on\nit yields better performance. For the evaluation settings,\nwe follow the exact settings in LLaVA-NeXT [38] using\nlmms eval2.\nB. More Qualitative Results\nIn this section, we present additional qualitative results of\nCOCONut-PanCap annotations (Sec. B.1) and a detailed\nanalysis of tier cases from the user study (Sec. B.2).\n2https://github.com/EvolvingLMMs-Lab/lmms-eval\n\nB.1. Data Examples\nWe show more visualization of our proposed COCONut-\nPanCap dataset in Fig. 9 and Fig. 10.\nB.2. PanCaper and GPT-4V Tier Showcases\nIn the user study involving 1,000 samples, captions gener-\nated by GPT-4V were preferred in 87 cases. Among these,\nactually, 46 were tier cases where human raters consid-\nered both GPT-4V and COCONut-PanCap captions equally\ngood. Fig. 11, Fig. 12 and Fig. 13 illustrate qualitative ex-\namples, highlighting the reasons for the tier classification\nand instances where GPT-4V was chosen.\n\nThe image depicts a natural outdoor with trees and\ngiraffes. <0:The sky is blue>, forming the backdrop of the \nscene. Below it, there are <1:dense trees, filled with \nbranches and lush green leaves>. Within this environment, \ntwo giraffes are prominently featured. The image mainly\nfocuses on <3:a standing giraffe with a long neck and \nunique patterns> , actively eating leaves from the tree. In\nThe image depicts a dynamic outdoor scene where people\nare riding horses. In the foreground, two horses take \ncenter stage. <9:A black horse with a white mane and tail\nbehind\nits\nneck, adorned with a brown bridle, a \npredominantly dark blue saddle with yellow patterns, and \nblue leg wraps>, is raising its front hoof. Beside it, <5:a \nwhite horse with a black mane and tail, wearing a black \nbridle, a similar dark blue saddle with yellow patterns, and \nwhite leg wraps>, is also raising its front hoof. Both horses \nare being controlled by <6,7: two man who are dressed in \nblue and white tops, white pants, and black boots>, \nactively taming the horses. They are riding horses on <2:a \nvivid green grassland> that provides the base for the\naction. Adding structure, there is <3: a fence made of wooden posts and railings> in the background. \nThere are some people in the background that are obscured by the horses. For example, there is <8:a\nperson wearing black pants> partially obscured by the white horse and <10:another person in a red top \nand white pants> who is watching the activity; and <11:a person in a red top and black pants>, partially \nhidden by the black horse. Together, the elements create a cohesive portrayal of a lively horse-taming \nevent set against a serene natural background. The weather is nice, as <0:the sky is white and cloudless>, \nforming the backdrop. Below it, there features <1:a dense cluster of trees with brown trunks and green \nleaves>, framing the scene. \nThe scene includes several individuals actively \nengaging in skateboarding. There are <6,10: two \nboys> wearing in green top and black pants>, actively \nplaying <9:11skateboard> in the air. <5: Another guy\nwho is also dressing in green top and black pants> is\nplaying but on the ground. Next to them, there are <7:\na half-naked man> observing the skateboarding\nperformance, while <8: another guy in a white top and black hat>, also watching the activities. Skateboards \nare prominently featured in the center area, which includes a black skateboard deck used for tricks. Lastly, \nthe background shows the <0:sky, predominantly blue with scattered clouds>. <1:A light brown building> is\nobviously seen in the background. The skaters are using the <2:sidewalk, notable for its graffiti and colorful \nmarkings>. Around the scene, there is <3:lush green foliage>, adding natural scenery to the skate park. \ncontrast, there is <4: a second giraffe with similar distinctive patterns>, which is far away from the previous\ngiraffe is resting comfortably on the grass. Both giraffes are surrounded by green trees and <2:a grassy area, \npredominantly covered with green grass interspersed with patches of exposed brown soil>. \nFigure 9. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks.\n\nThis image showcases a well-organized desk setup. On\n<1:the wooden desk with a shelf>, there is <4:a DELL\ncomputer> occupies the central space, displaying content \non its screen. Besides, there is a turtle toy on top of it. \nSurrounding the computer, multiple items are neatly \narranged. To its left, <5:a blue water bottle> stands \nprominently, next to <6:a book> lying on the desk. Below \nthe computer, <12,13,16,19:additional books> are placed.\nOn the upper shelves, various objects add character to the space like <14,15: books> and a drink can. At the top of \nthe shelves, <8:a fluffy blue teddy bear > is positioned on the left, and <11:another teddy bear> is positioned on the\nright, adding a playful touch. There are various small items as well, like <7:a glass bottle>, <17:books> and a photo\nframe. In the background, <0:the wall is painted blue>, serving as the backdrop for the scene.\nThe image features a cozy and well-decorated living \nroom. At the center of the room, <4:a wooden coffee \ntable equipped with glasses> holds various items, \nincluding <10:a remote control>, <13: a knife> on the\nplates, and <16:a square small book>. On the left, The \nseating arrangement includes <14:a patterned couch \nwith colorful cushions and blanket> and <15:another\nneutral-toned couch with vibrant throw pillows> , providing balance to the layout. The rug with colorful\npatters brings more warm atmosphere to the sitting area. Behind the couch, <20:A chair in the back>\ncomplements the seating options. Adding warmth to the room, <8:a black cat> rests comfortably on the \ncouch. Behind the sitting area, there is <5:a 4-layer wall-mounted wooden shelf> with additional \ndecorative items, including <11,12: vases> and other decorative items, enhancing the cozy and inviting \natmosphere. Closed to the shelf, there are several <9,19,22:potted plants with green leaves > are placed \nthroughout the room, adding a touch of greenery. <2:The wall painted in warm tones>, create a cozy \natmosphere and are adorned with framed artwork and decorations. <0: The floor is neutral-toned>, \nsupporting the entire setup. The <3:ceiling painted white>, contrasts subtly with the walls and reflects the \nnatural light entering the room through <6:the large windows>.\nThe image portrays a lively street scene outside a \ncafé. <0:The road> serves as the foreground, where \n<5:a motorcycle> is prominently parked, its shiny \nfinishes and detailed designs drawing attention. \nBehind it, <1:the café building>, labeled as “Seaport \nCafe” features large windows, a decorative sign, and \npatriotic bunting. A glowing neon “Corona Light” sign \nadds to the vibrant atmosphere. The café‘s exterior \nincludes <2:a wooden wall> and <4:a small fence>\nwhich separates the outdoor seating area from the \nstreet. \nThe \nseating \narea \nis \nequipped \nwith \n<12,14:chairs> and <6:a blue umbrella>\nthat \nprovides shade for the patrons.\n<7,11:\nSeveral \npeople are interacting in and around the café>, some \nstanding while others are seated, enjoying their \ntime. <10:One individual who is carrying <9:a black\nbackpack> > is joining while there is <8:a guy in blue shirt> is trying to shake hands with her, adding to the \ndynamic social scene.\nFigure 10. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks.\n\nCOCONut-PanCap: Positioned prominently within this \nimage is <1: a zebra, easily distinguishable by its iconic\nblack-and-white stripes and a striped mane along its neck>. \nIts belly is white, adding contrast to its overall pattern. The \nzebra is depicted in a natural feeding posture, with its head \nlowered as it grazes on <0:the grassland which is lush \ngreen>, forming the base of the scene. \nCOCONut-PanCap: The image mainly features <0:the trees \nwith very lush foliage>. In the front, there are <1,2: two\ngiraffes with distinctive brown spots, yellow patterns, and \nlong necks>. These giraffes are observed to be fighting by \nentwining their necks with each other. Behind them, there is \n<3: another giraffe, also with distinctive brown spots and \nyellow patterns>, which is standing still and partially \nobscured by the front giraffes. \nCOCONut-PanCap: On <0:the gray asphalt sidewalk>,\ntwo motorcycles are parked side by side. <2:One \nmotorcycle is predominantly black with white parts and \na brown seat>, positioned to the left of <1:a green \nbush>,\nwhere\na registration plate displaying the \nnumber “SV-6260” is visible to be mounted on the front \nfender, and a large cylindrical front headlamp is\nmounted on the front. <3:The other motorcycle is \npredominantly silver with a black seat>. The mechanical \ncomponents include the single-cylinder engine and a \nstraightforward exhaust system.\nGPT-4V: This beautifully restored vintage motorcycle, likely from the early 20th century, showcases a classic design with a \ncream-colored fuel tank, spoked wheels, and a robust black frame. Key features include a large front headlamp, a \nregistration plate reading "SV-6260," a brown leather saddle, and a small rear storage compartment. The visible single-\ncylinder engine and minimalistic exhaust system reflect the simple yet durable engineering of the era, with its pristine \ncondition highlighting careful preservation.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows the story background, but missing details for region-3.\nGPT-4V: This image captures two giraffes engaging in a \ndisplay of interaction, possibly sparring or playfully \nentwining their necks, a behavior commonly known as \n"necking." Set against a backdrop of lush green trees, the \ngiraffes\' distinctive spotted patterns stand out vividly in \nthe sunlight filtering through the foliage. Their tall, elegant\nforms and graceful movements highlight the majestic  nature of these animals, while the surrounding dense \nvegetation provides a natural and serene environment. The moment beautifully showcases the unique behavior and \nstriking appearance of giraffes in their habitat.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more description for the trees but add hallucination for the sunlight.\nGPT-4V: This image features a zebra grazing peacefully on a vibrant green field. The zebra\'s iconic black-and-white \nstriped pattern stands out sharply against the lush grass, creating a striking visual contrast. Its head is lowered as it \nnibbles on the grass, showcasing its natural feeding behavior. The mane, with alternating dark and light stripes, adds a \ntouch of elegance to its overall appearance. The bright sunlight enhances the vivid colors and intricate details, \nhighlighting the beauty of the zebra in its natural habitat.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more description for the behavior but add hallucination for sunlight.\nFigure 11. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.\n\nGPT-4V: This vibrant image showcases a cheerful woman holding a bright pink parasol on a sunny day. She is dressed in a \ncolorful floral swimsuit that exudes a retro style, complementing the joyful and carefree atmosphere. Her radiant smile \nand relaxed pose convey a sense of summer fun, with a backdrop of a clear blue sky, lush green trees, and a calm lake. \nThe parasol adds a playful and vintage charm, enhancing the overall nostalgic feel of the scene.\nCOCONut-PanCap: At the center of the scene, <5: a\ncheerful woman is\nwearing a vibrant swimsuit \nadorned with purple, blue, yellow, and white \npatterns>. She is holding <4:a pink umbrella>, which \nadds a pop of color and shade to the setting. The \nwoman is smiling happily, radiating a joyful and \ncarefree vibe, tying together the natural and human \nelements seamlessly. There are <1:lush green trees> \nand <0:blue sky> in the background, adding a natural \nelement to the composition. Below it, there is <2: a \nserene blue river>, Together, the composition \nemphasizes the interaction between the lively figure \nand the tranquil outdoor environment. \ntier reason: : Compared to COCONut-PanCap, GPT-4V recognize the water as lake, which is hard to be distinguished from this image.\nCOCONut-PanCap:\nThe image features an\nowl-shaped \ncandlestick, and a clock are placed on <0:the floor>. The \ncandlestick, designed in the form of an owl, adds a \ndecorative element beside the clock. Prominently, in the\ncenter there is <2:a circular clock with hands and Roman \nnumerals>, elegantly designed and placed on an intricate \nstand featuring ornate patterns and curves. Behind these \nitems, there is <1: a plain and white wall>, providing a \nneutral background that highlights the objects in the \nforeground. Together, the objects and their placement \ncreate a cohesive and aesthetically pleasing composition.\nGPT-4V: This cozy and artistic composition features a decorative vintage clock paired with a charming owl-shaped \ncandleholder. The clock, with intricate scrollwork and Roman numerals on its face, exudes an antique elegance, while the \nsoft glow of the candle creates a warm and inviting ambiance. The owl candleholder, with its textured surface and \nwhimsical design, adds a playful yet rustic touch to the scene. The subtle lighting and shadows enhance the nostalgic and \ntranquil atmosphere, making it a perfect display for a quiet, reflective setting.\ntier reason: Compared to COCONut-PanCap, GPT-4V summarize the style of the image as vintage.\nCOCONut-PanCap: The image captures an airplane in flight. \nThe center area highlights <1: an airplane, predominantly \nwhite in color, with dark blue English lettering “AIRFRANCE” \non its body and distinctive dark blue and red patterns on its \ntail>. The airplane is depicted flying gracefully through the \nsky. The background is <0: the sky, characterized by a mix of \nblue open spaces and scattered clouds>.\nGPT-4V: This image captures an Air France Airbus A380 in mid-flight against a backdrop of cloudy skies. The aircraft, \nknown for its massive size and double-deck design, features the iconic Air France livery with its name prominently \ndisplayed on the fuselage and the red, white, and blue stripes on the tail, symbolizing the French flag. The four powerful \nengines are clearly visible, emphasizing the plane’s engineering prowess. The angle of the shot highlights the sleek design \nand scale of the A380, showcasing its role as a leader in modern aviation.\ntier reason: : Compared to COCONut-PanCap, GPT-4V summarize bit more details but with hallucination of model ‘A380’.\nFigure 12. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.\n\nCOCONut-PanCap: The image portrays a serene winter \nlandscape. The image mainly highlights <0:a blue sky, creating \na clear and bright backdrop>. Below it, there is <1:a pristine\nsnow-covered slope, predominantly white and marked with \nski trails> that curve across the terrain. Adjacent to this is <2:a \nmajestic snow-covered mountain>, blending seamlessly into \nthe snowy environment. At the center of the scene, <3:a \nperson who is skiing down the slope, dressed in an orange \njacket and black pants> is captured. The skier is equipped with \nskis attached to their feet, gliding smoothly over the snow. \nAdditionally, <5:a backpack> is carried by the skier, \ncompleting their gear. \nGPT-4V: This image captures a serene winter landscape \nfeaturing a skier ascending a pristine snow-covered slope. \nThe untouched white snow is marked only by the skier\'s \ntracks and a winding trail that leads into the distance, \ntier reason: Compared to COCONut-PanCap, GPT-4V shows better vocabulary, but the information is almost the same.\nemphasizing the tranquility and solitude of the scene. Majestic mountains with rugged, rocky peaks dominate the background, \ntheir shadows adding depth and contrast to the bright sunlight illuminating the snow. The skier, dressed in warm, vibrant gear \nand carrying poles, navigates the slope with determination, highlighting the spirit of exploration and adventure. The expansive \nsky above completes the breathtaking alpine setting.\nCOCONut-PanCap: The image highlights <1:a white dog>,\nstanding on the <0:lush\ngreen\ngrass>, exuding a lively \ndemeanor. The dog is holding <2:a purple frisbee> in its \nmouth. The frisbee features black text “SKIMMER” printed on \nits surface, adding contrast to its vibrant purple color. \nTogether, the scene captures a moment of playful interaction, \ngrounded by the mask numbers for clarity and detail.\nGPT-4V: This image features an adorable white dog \nstanding on a lush green lawn, holding a bright purple\nfrisbee in its mouth. The dog\'s fluffy coat is well-groomed, and its expressive eyes gaze directly at the viewer, exuding \ncharm and playfulness. The frisbee, labeled with "Skimmer Competition," is slightly chewed, indicating the dog’s \nenthusiasm for play and activity. The vibrant colors of the frisbee and the green grass create a cheerful and lively \natmosphere, perfectly capturing a moment of joy and companionship between a pet and its outdoor playtime.\ntier reason: Compared to COCONut-PanCap, GPT-4V shows more OCR results of “competition”, but the information is minor.\nCOCONut-PanCap: The image captures a dynamic tennis \nmatch. In the foreground, the image highlights <3:a man \nactively playing tennis>, dressed in a black top, blue pants, \nblack socks, and black shoes. He is holding <2:a tennis racket>, \ncharacterized by its red and white frame and netting with \nprinted letters. The background is <0:a predominantly blue \nwall>, decorated with red and black patterns along with white \npartial text “rates Airline”. <1:The playing field is a mix of \ngreen and blue hues>.\nGPT-4V: This image captures a tennis player intensely focused \nduring a match, poised and ready to return a serve on a hard \ncourt. Dressed in a dark athletic shirt, blue shorts, and black \nsneakers, the player holds their racket firmly, leaning slightly \nforward in a balanced stance that conveys readiness and \ndetermination. The backdrop prominently features the USTA \nlogo and the Emirates Airline sponsorship, indicating a \nprofessional tennis tournament setting. The court\'s clean lines \nhighlight the competitive atmosphere\ntier reason: Compared to COCONut-PanCap, GPT-4V shows extra wording to describe the atmosphere which is not necessary.\nFigure 13. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple\ncases.'),
                Paper(arxiv_id='2502.00674', authors=['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], published_at=datetime.datetime(2025, 2, 5, 8, 9, 2, 787000, tzinfo=datetime.timezone.utc), title='Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?', summary='Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.', upvotes=7, thumbnail=None, content='Rethinking Mixture-of-Agents: Is Mixing Different\nLarge Language Models Beneficial?\nWenzhe Li*1, Yong Lin*1, Mengzhou Xia1, and Chi Jin1\n1Princeton University†\nAbstract\nEnsembling outputs from diverse sources is a straightforward yet effective approach to boost perfor-\nmance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from\nmultiple different Large Language Models (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA — an ensemble method that\naggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, sur-\nprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios:\nSelf-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of\n3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA\nto one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance\non the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off\nbetween diversity and quality of outputs under various MoA settings. We confirm that the MoA performance\nis rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models.\nTo complement the study, we identify the scenarios where mixing different LLMs could be helpful. This\npaper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of\nLLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.\n1\nIntroduction\nLarge language models have made remarkable strides in improving performance across different domains,\nwith notable examples such as GPT [Achiam et al., 2023], Gemini [Team et al., 2023], and Claude [Anthropic,\n2023]. Significant efforts have been directed toward increasing model size and training data to boost\ncapabilities. However, scaling at training time comes with steep costs, while scaling computation during\ninference remains largely underexplored.\nA straightforward way to utilize test-time compute is ensembling, which aims to combine outputs of\nmultiple LLMs [Wang et al., 2024a, Lin et al., 2024, Jiang et al., 2023a, Wang et al., 2024a]. Among various\nensembling approaches, Mixture-of-Agents (MoA) [Wang et al., 2024a] has garnered significant interest,\nachieving superior performance in challenging tasks such as instruction following [Wang et al., 2024a],\nsummarization, data extraction [OpenPipe, 2024], and real-world code issue resolution [Zhang et al., 2024b].\n*Equal contribution.\n†Email: {wenzhe.li,yl7690,mengzhou,chij}@princeton.edu.\n1\narXiv:2502.00674v1  [cs.CL]  2 Feb 2025\n\nSpecifically, MoA first queries multiple LLMs (proposers) to generate responses, and then uses an LLM\n(aggregator) to synthesize and summarize these responses into a high-quality response.\nPrevious research highlights the significance of model diversity within the proposers for optimizing the\nperformance of MoA, primarily focusing on strategies for ensembling a diverse set of individual models. We\nconsider cross-model diversity as the variation among different models. However, pursuing cross-model\ndiversity may inadvertently include low-quality models, resulting in a quality-diversity trade-off. While\nprevious studies mainly concentrate on achieving a high cross-model diversity [Wang et al., 2024a, Zhang\net al., 2024b], we adopt a holistic perspective on model diversity by considering in-model diversity, which\narises from the variability of multiple responses generated by the same model. In-model diversity enables us to\naggregate multiple outputs from an individual model. Intuitively, leveraging outputs from the best-performing\nindividual model can more effectively navigate the quality-diversity trade-off by creating a higher-quality\nproposer mixture. Thus, we propose Self-MoA as depicted in Figure 1b, which utilizes the same prompting\ntemplate as MoA but aggregates outputs that are repeatedly sampled from the same model, rather than from\na set of different models. To distinguish, we use Mixed-MoA to refer to MoA configurations that combine\ndifferent individual models when necessary.\nSurprisingly, we find that Mixed-MoA is usually sub-optimal compared with Self-MoA, especially when\nthere exist significant quality differences among proposers. Specifically, we revisit the same experiment\nsetting of MoA with six open-source instruction fine-tuned models as Wang et al. [2024a]. Compared\nwith Mixed-MoA which aggregates all six models, Self-MoA on the strongest model achieves 6.6 point\nimprovement over its mixed counterpart on the AlpacaEval 2.0 benchmark, showing a case of when intra-\nmodel diversity is more effective. Moreover, Self-MoA on two best-performed models on AlpacaEval 2.0\nconsistently achieves a 2-3 point gain and secures the top position among non-adversarial methods on the\nleaderboard, which further confirms the effectiveness of Self-MoA in this task.\nTo explore the limits of model diversity for MoA, we extend our experiments to a setting with three\nspecialized models, each excelling in a specific task. Specifically, we utilize Qwen2-7B-Instruct [Bai et al.,\n2023] for common sense QA (MMLU-redux [Gema et al., 2024]), Qwen2-Math-7B-Instruct [Bai et al., 2023]\nfor mathematics (MATH [Hendrycks et al., 2020]), and DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024]\nfor coding (CRUX [Gu et al., 2024]). We compare Self-MoA against a range of Mixed-MoA strategies,\nevaluating 13 combinations of individual models based on their average performance across the three tasks.\nOur findings indicate that employing task-specific models as proposers for Self-MoA can significantly\noutperform the best Mixed-MoA. Furthermore, even in a constructed mixture task tailored for Mixed-MoA\nwhere each individual model excels in a specific subtask, only two Mixed-MoA strategies slightly outperform\nSelf-MoA by 0.17% and 0.35%.\nTo better understand the effectiveness of Self-MoA, we conduct a comprehensive investigation of the trade-\noff between quality and diversity in MoA, involving over 200 experiments. We use the Vendi Score [Dan Fried-\nman and Dieng, 2023] to evaluate the diversity among the outputs of the proposers, while the average perfor-\nmance of the proposers serves as the measure of quality. In Section 4, we confirm that MoA performance\nhas a positive correlation with both quality and diversity. Moreover, we clearly show a trade-off along the\nachievable Pareto front of quality and diversity. Interestingly, we find that MoA is quite sensitive to variations\nin quality, with optimal performance typically occurring in regions characterized by high quality and relatively\nlow diversity. This finding naturally explains the effectiveness of Self-MoA, as it utilizes the strongest model\nas the proposer, ensuring high quality in its outputs.\nFinally, we evaluate the performance of Self-MoA under increasing computational budgets. As the\nnumber of outputs grows, the scalability of Self-MoA becomes constrained by the context length of the\naggregator. To address this issue, we introduce Self-MoA-Seq (Figure 1c), a sequential version that processes\nsamples using a sliding window, allowing it to handle an arbitrary number of model outputs. Our findings\n2\n\nshow that Self-MoA-Seq performs at least as effectively as Self-MoA, enabling scalable ensembling for\nLLMs with shorter context lengths without compromising final performance.\nOverall, our contributions are three-fold:\n• We introduce Self-MoA, which leverages in-model diversity by synthesizing multiple outputs from the\nsame model. Surprisingly, it demonstrates superior performance compared to existing Mixed-MoA\napproaches, which emphasize cross-model diversity, across a wide range of benchmarks.\n• Through systematic experiments and statistical analysis, we uncover a core trade-off between diversity\nand quality among the proposers, emphasizing that MoA is highly sensitive to proposer quality. This\nfinding also explains the success of Self-MoA, which leverages outputs from the highest-performing\nmodel, ensuring superior overall quality.\n• We extend Self-MoA to its sequential version Self-MoA-Seq, which iteratively aggregates a small\namount of outputs step by step. Self-MoA-Seq unlocks LLMs that are constrained by the context length\nand enables computation scaling during inference.\n2\nRelated Work\nEnsembles of LLMs.\nModel ensembling aims to combine strengths from multiple models. Previous\nstudies have explored various methods to leverage a diverse set of models, including but not limited to\nprompting [Wang et al., 2024a], weight averaging [Lin et al., 2024, Ram´e et al., 2024], routing [Jiang et al.,\n2024b, Lu et al., 2023], training a generative fusion model [Jiang et al., 2023b], and so on. Zhang et al. [2024a]\nargues that the fusion of specialized models with certain general abilities could be a promising direction toward\nArtificial General Intelligence. Mixture-of-Agents (MoA, Wang et al. [2024a]) first queries multiple LLMs to\ngenerate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows\npromising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval\n2.0 leaderboard. Our method is inspired by the prompt pipeline proposed in MoA. However, while existing\nMoA focuses on unleashing the strength from multiple different models [Wang et al., 2024a, Jiang et al.,\n2023b, Zhang et al., 2024b], we demonstrate the trade-off between diversity and quality within the proposers,\nhighlighting that focusing solely on diversity may compromise overall quality and final performance.\nLLM Inference with Repeated Sampling.\nPrevious studies have shown that combining model outputs\nfrom repeated sampling can yield a better response in various domains. In tasks with automatic verifiers\navailable, such as math [Hendrycks et al., 2021] and code [Chen et al., 2021], simply sampling LLMs\nmultiple times can significantly improve the pass@k metric and hence boost the success rate of solving the\ntasks [Roziere et al., 2023, Li et al., 2022, Brown et al., 2024]. In more general tasks without verification\ntools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most\npromising one from candidate responses [Wang et al., 2022, Chen et al., 2023b, Gui et al., 2024, Li et al.,\n2024]. Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference\ntime [Brown et al., 2024]. In this work, we identify the surprising effectiveness of repeated sampling in the\ncontext of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated\nfrom repeated sampling, hence can further improve over each individual output.\nCollaborative Agents\nThere is a surge of interest in building agent systems based on verification, critique,\ndiscussion, and refinement. For example, Stechly et al. [2023], Valmeekam et al. [2023], and Madaan et al.\n3\n\n  𝑜!\n!\n  𝑜!\n"\n  𝑜"\n!\n  𝑜"\n"\n  𝑜#\n!\n  𝑜#\n"\n 𝑀!\n 𝑀"\n 𝑀#\n 𝐴\n  𝑜$\n(a) MoA\n(b) Self-MoA\n(c) Self-MoA-Seq\n  𝑜!\n!\n  𝑜!\n"\n  𝑜!\n#\n  𝑜!\n%\n  𝑜!\n&\n  𝑜!\n\'\n 𝐴\n  𝑜$\n 𝑀!\n  𝑜!\n!\n  𝑜!\n"\n  𝑜!\n#\n  𝑜!\n%\n  𝑜!\n&\n  𝑜!\n\'\n 𝐴\n  𝑜$\n!\n  𝑜$\n!\n 𝐴\n  𝑜$\n"\n 𝑀!\nFigure 1: Comparison of MoA, Self-MoA, and Self-MoA-Seq. (a) In MoA, multiple models respond to\na query, followed by an aggregator synthesizing their outputs. (b) Self-MoA simplifies this by repeatedly\nsampling from a single model. (c) Self-MoA-Seq extends Self-MoA by applying a sliding window to combine\nthe best output so far with candidate outputs. At each timestep, the synthesized output is repeated to bias the\naggregator towards it, reducing the context length requirements and expanding the method’s applicability.\nNote that MoA can extend to multiple rounds of aggregation (Appendix A.1), while Self-MoA and Self-MoA-\nSeq can extend to more outputs, but we omit them here for clarity.\n[2024] use self-critique to iteratively refine outputs through a chain structure. Madaan et al. [2024], Chen\net al. [2024], and Wang et al. [2024a] explore the incorporation of multiple models to create a stronger agent\nthat outperforms each individual model. Du et al. [2023] incorporates multiple LLMs that propose and debate\ntheir individual responses over several rounds to reach a common final answer. Liang et al. [2023] proposes\nMulti-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative\nconclusions and avoid rushing to incorrect answers. Chen et al. [2023a] introduces RECONCILE, which\nadopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly,\nWang et al. [2024b] shows that a single model with carefully designed prompts can sometimes match the\nperformance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the\nprompts are insufficient.\n3\nIs Ensembling Different LLMs Beneficial?\nAs introduced in Section 1, previous research primarily emphasizes cross-model diversity, which can\ninadvertently include low-quality proposers. In this work, we introduce Self-MoA (Figure 1), which uses\na single top-performing model to generate multiple outputs and aggregate them to produce the final result.\nSelf-MoA leverages in-model diversity as repeated sampling often produces varied outputs. We propose our\nresearch question as follows:\nDoes the benefit of MoA stem from cross-model diversity?\nCan we build a stronger MoA using in-model diversity?\n4\n\nTable 1: Comparison of Self-MoA and Mixed-MoA on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110B-\nChat as the aggregator.\nModel Configuration\nLC Win Rate\nIndividual\nWizardLM-2-8x22B\n53.1\nQwen1.5-110B-Chat\n43.9\nLLaMA-3-70B-Instruct\n34.4\nQwen1.5-72B-Chat\n36.6\nMixtral-8x22B-Instruct-v0.1\n30.2\ndbrx-instruct\n25.4\nMixed-MoA\n2-Layer MoA [Wang et al., 2024a]\n59.1\nSelf-MoA\n2-Layer Self-MoA + WizardLM\n65.7\n3.1\nExperiments on AlpacaEval 2.0 with General Purpose Models\nEvaluation benchmarks.\nWe adopt the same experiment setting as Wang et al. [2024a] in AlpacaEval 2.0\nbenchmark [Dubois et al., 2024] and compare the performance of Mixed-MoA and Self-MoA1. AlpacaEval\n2.0 is a widely used benchmark for assessing the instruction-following abilities of LLMs. It offers a set\nof real-world instructions and employs a GPT-4-based annotator to compare the model’s responses against\nreference answers generated by GPT-4. To address length bias inherent in model-based evaluation, Dubois\net al. [2024] introduced the length-controlled (LC) win rate as a more robust evaluation metric.\nModels.\nFollowing Wang et al. [2024a], we construct MoA based on six individual models: Qwen1.5-\n110B-Chat [Bai et al., 2023], Qwen1.5-72B-Chat [Bai et al., 2023], WizardLM-8x22B [Xu et al., 2023],\nLLaMA-3-70B-Instruct [Touvron et al., 2023], Mixtral-8x22B-Instruct-v0.1 [Jiang et al., 2024a], and dbrx-\ninstruct [Team et al., 2024b]. Each model is sampled with a temperature of 0.7, following the default in\n[Wang et al., 2024a]. For Self-MoA, we aggregate six outputs sampled from WizardLM-2-8x22B, as it\nconsistently outperforms the other models. In line with Wang et al. [2024a], we use Qwen1.5-110B-Chat as\nthe aggregator for both Mixed-MoA and Self-MoA.\nResults.\nWe present the LC win rate for each model configuration in Table 1. For individual models, we\nreport the higher value between the leaderboard results and our reproduction. Notably, Self-MoA demonstrates\nremarkable effectiveness in this task, outperforming the Mixed-MoA baseline by 6.6 point. This suggests that,\nwhile using multiple models intuitively offers greater diversity, ensembling multiple outputs from a single\nmodel is more effective.\nApplying Self-MoA on top performing models.\nTo further validate the effectiveness of Self-MoA, we\napply it to the two top-performing models on AlpacaEval 2.0: gemma-2-9b-it-WPO-HB [Zhou et al., 2024]\nand gemma-2-9b-it-SimPO [Meng et al., 2024]. We use each model as both the proposer and the aggregator2,\n1We note that this experiment is similar to the “single-proposer” setting in Wang et al. [2024a], however our reproduced result is\ndifferent. We conjecture that such a major difference is due to different choices of the proposer model, which is not mentioned in Wang\net al. [2024a]. As we shall see later in Section 4, ensembling performance is more sensitive to quality rather than diversity. Therefore, a\nworse proposer model will lead to suboptimal performance of Self-MoA.\n2Qwen1.5-110B-Chat is not used as the aggregator since the two top models significantly outperform it.\n5\n\nTable 2: Self-MoA achieves state-of-the-art performance on the AlpacaEval 2.0 leaderboard when using\ntop-performing models as both proposers and aggregators. We only ensemble 4 outputs due to context window\nconstraints.\nModel Configuration\nLC Win Rate\nIndividual\ngemma-2-9b-it-WPO-HB\n76.7\ngemma-2-9b-it-SimPO\n72.4\nSelf-MoA\nSelf-MoA + gemma-2-9b-it-WPO-HB\n78.5\nSelf-MoA + gemma-2-9b-it-SimPO\n75.0\nwith a temperature of 0.7 for all the generations. Due to the context length constraint of Gemma 2 [Team et al.,\n2024a], the aggregator can only take four samples as the input. As shown in Table 2, Self-MoA consistently\nachieves a 2-3 point gain and secures the top position on the leaderboard during submission.\nResults on MT-Bench.\nBeyond AlpacaEval 2.0, we further evaluate Self-MoA and Mixed-MoA on MT-\nBench [Zheng et al., 2023], another benchmark used in Wang et al. [2024a]. The results align with our\nfindings from AlpacaEval 2.0, reinforcing the effectiveness of Self-MoA. Please refer to Appendix B.1 for\nmore details.\n3.2\nExperiments on Multiple Datasets with Specialized Models\nIn this section, we compare different ensembling methods on a diverse set of benchmarks using specialized\nmodels.\nEvaluation datasets.\nWe conduct evaluations across a diverse set of benchmarks:\n• MMLU [Hendrycks et al., 2020] is a multiple-choice dataset designed to assess a model’s multitask\naccuracy. MMLU is widely used to evaluate both the breadth and depth of language understanding\ncapabilities of current LLMs across a diverse array of subjects, including mathematics, history, computer\nscience, logic, and law. We adopt MMLU-redux [Gema et al., 2024] for evaluation, which is a subset\nof MMLU with 3,000 samples fixing the errors in the dataset through human re-annotating.\n• CRUX [Gu et al., 2024] consists of 800 Python code functions, each containing 3 to 13 lines along with\nan input-output pair. Based on this dataset, Gu et al. [2024] constructs two tasks: input prediction and\noutput prediction. To successfully complete these tasks, the LLM must demonstrate code reasoning\nabilities.\n• MATH [Hendrycks et al., 2021] comprises 12,500 challenging competition-level mathematics problems.\nFor our analysis, we utilize the testing subset of MATH, which consists of 5,000 samples.\n3As Qwen2-Math-7B-Instruct only supports context length of 4096, for these two data points, we sample the proposer with a reduced\ntoken length of 1024, and only aggregates three outputs from the proposer.\n6\n\nTable 3: Comparison of Self-MoA and Mixed-MoA in MMLU, CRUX, and MATH. The labels i, m, and d\nrefer to Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct, respectively.\nThe average performance represents the mean accuracy across MMLU, CRUX, and MATH. TaskBest\nindicates that we use the strongest model for each task as both proposer and aggregator.\nAggregator\nProposer\nMMLU\nCRUX\nMATH\nIndividual\n-\ni\n66.16\n36.25\n53.81\n-\nd\n60.91\n49.51\n53.82\n-\nm\n54.36\n27.88\n69.573\nMixed-MoA\ni\niimmdd\n67.89\n42.88\n64.38\nimdddd\n67.42\n44.50\n63.90\niiiimd\n68.90\n41.25\n63.00\nimmmmd\n66.63\n42.75\n66.02\niimmmm\n66.23\n39.25\n66.10\niiimmm\n67.49\n38.25\n64.16\niiiimm\n68.00\n37.00\n62.92\niidddd\n68.21\n45.50\n62.56\niiiddd\n68.21\n42.88\n62.38\niiiidd\n68.47\n40.75\n61.24\nmmdddd\n66.34\n46.75\n66.48\nmmmddd\n65.80\n47.00\n67.32\nmmmmdd\n65.44\n42.50\n67.62\nSelf-MoA\ni\n6×TaskBest\n69.01\n50.75\n68.42\nTaskBest\n6×TaskBest\n69.01\n52.62\n69.803\nModels.\nTo ensure sufficient diversity, we select three LLMs with specialized strengths: Qwen2-7B-\nInstruct [Yang et al., 2024], DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024], and Qwen2-Math-7B-\nInstruct. We fix the number of proposers to six and sweep various combinations of these three models.\nFor convenience, we denote Qwen2-7B-Instruct as i, DeepSeek-Coder-V2-Lite-Instruct as d, and Qwen2-\nMath-7B-Instruct as m. As shown in Table 3, Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and\nQwen2-Math-7B-Instruct excel on MMLU, CRUX, and MATH, respectively. We use the short name for\nthe mixture of proposers. For example, iiddmm indicates the inclusion of two samples from each model\nrespectively. When a model is represented multiple times in the proposer mixture, we ensure that two samples\nare generated with different random seeds. We set the temperature of each model to be 0.7 for the individual\nmodel, and use temperature 0 for the aggregator. We mainly use Qwen2-7B-Instruct as the aggregator but also\ntry different models as the aggregator. We explore various MoA configurations, including individual models,\ncombinations of two or three models as proposers, and using a single top-performing model (TaskBest, for\nexample DeepSeek-Coder-V2-Lite-Instruct for CRUX) as the proposer (Self-MoA).\nResults.\nThe results are presented in Table 3. When using i as the aggregator, Self-MoA with the TaskBest\nmodel consistently outperforms all 13 tested Mixed-MoA configurations across all tasks. Furthermore,\nadopting a task-specific aggregator yields an additional performance boost of 1-2 points. Interestingly,\nincreasing model diversity does not always lead to better performance. For instance, while MoA with\niimmdd surpasses mmmddd on MMLU, it underperforms on CRUX and MATH. This discrepancy aligns\n7\n\n40\n45\n50\n55\n60\n65\nQuality\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nDiversity\nMMLU\nMixed-MoA\nSelf-MoA\n61\n62\n63\n64\n65\n66\n67\n68\n69\nf\n20\n25\n30\n35\n40\n45\nQuality\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nCRUX\nMixed-MoA\nSelf-MoA\n36\n38\n40\n42\n44\n46\n48\n50\nf\n35\n40\n45\n50\n55\n60\n65\n70\nQuality\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nMATH\nMixed-MoA\nSelf-MoA\n58\n60\n62\n64\n66\n68\nMOA Performance\nFigure 2: The diversity-quality trade-off: Mixed-MoA incorporates different individual models as proposers,\nwhile Self-MoA uses the same individual model for this role. Quality is assessed based on the average\nperformance of each proposer, and diversity is computed with the Vendi Score [Dan Friedman and Dieng,\n2023] of outputs generated by proposers on the same prompts.\nwith the relative strengths of the individual models—i excels on MMLU but lags behind on CRUX and\nMATH. We postpone more discussion to Section 4.2.\n4\nThe Quality-Diversity Trade-off\nWe investigate factors that contribute to the strong performance of Self-MoA through careful experiments.\nPrevious studies have mainly focused on increasing model diversity within the group [Wang et al., 2024a,\nJiang et al., 2023a, Zhang et al., 2024b]. However, searching for diverse models can sometimes lead to\nincluding poorly performed models, resulting in a trade-off between diversity and quality, where quality refers\nto how well each individual model performs in the group.\nTherefore, we aim to identify the existence of a general relationship between MoA’s performance and\nquality as well as diversity. Following Section 3, we evaluate MoA’s performance on MMLU, CRUX, and\nMATH, which cover tasks requiring a wide range of capabilities. We vary the quality and diversity with two\norders of freedom: 1) combinations of individual models in proposers from Section 3.2; and 2) sampling\ntemperature. i.e., 0.5, 0.7, 1.0, 1.1, and 1.2. This results in a total of over 70 unique MoA proposer mixtures.\nWe measure the quality and diversity as follows:\n• Diversity: We utilize the Vendi Score [Dan Friedman and Dieng, 2023] to assess the diversity among\nindividual models in the proposer mixture. The Vendi Score represents the effective number of unique\nelements within a collection of samples [Dan Friedman and Dieng, 2023], with further details provided\nin the Appendix A.2. Specifically, for a given prompt x, we obtain responses from each model, denoted\nas y1, y2, . . . , y6. The diversity of the proposers for prompt x, denoted as d(x), is calculated using the\nVendi Score on the set [y1, . . . , y6]. We then compute the overall diversity across the dataset S as:\nd = 1\n|S|\nX\nx∈S\nd(x).\n• Quality: We first determine the accuracy of each model on the dataset S, yielding values q1, q2, . . . , q6.\nThe average accuracy, q = 1\n6(q1 + q2 + . . . + q6), serves as our measure of the quality of the proposers.\nWe will explore additional quality measurement strategies in later sections.\n8\n\nTable 4: Linear regression (Equation 1) of MoA’s performance t on diversity d and quality q.\nDataset\nα\nβ\nR2\nCoefficient\nP-value\nCoefficient\nP-value\nMMLU\n2.558 ± 0.176\n< 0.001\n1.841 ± 0.176\n< 0.001\n0.771\nCRUX\n4.548 ± 0.459\n< 0.001\n1.421 ± 0.459\n< 0.001\n0.685\nMATH\n4.719 ± 0.416\n< 0.001\n2.839 ± 0.416\n< 0.001\n0.760\nResults.\nWe plot MoA’s performance with corresponding diversity and quality for each mixture of proposers\nin Figure 2. We summarize key observations as follows:\n• The trends among MMLU, CRUX, and MATH are consistently aligned.\n• When the quality is fixed, increasing diversity can enhance MoA’s performance.\n• When the diversity is fixed, improving quality can also boost MoA’s performance.\n• There exists a trade-off in the achievable Pareto front between diversity and quality.\n• Notably, the best performance of MoA is typically observed in the bottom right of each subplot,\nindicating a strong sensitivity to quality.\nPrevious work on ensembles [Wang et al., 2024a, Jiang et al., 2023a, Zhang et al., 2024b] primarily focuses on\nincreasing the diversity of models within the proposer mixture. However, as shown in Figure 2, compared to\nSelf-MoA on the best-performing model, simply aiming for greater diversity in the proposer mixture can result\nin lower overall quality, which may negatively impact MoA’s performance. This trade-off between diversity\nand quality helps to explain why Self-MoA achieves superior performance across various benchmarks.\n4.1\nStatistical Analysis\nTo further understand the numerical correlation between MoA’s performance and diversity as well as quality,\nwe conduct linear regression for MoA’s performance t on diversity d and quality q. Specifically, we fit the\nfollowing equation for each dataset:\nt = α × q + β × d + γ,\n(1)\nwhere α, β, γ ∈R are real-valued coefficients to be determined. For each dataset, we collect around 70\ndata points from Figure 2 to construct the set {qi, di, ti}N\ni=1. The coefficients α, β, and γ are then derived\nby solving a linear regression on {qi, di, ti}N\ni=1. To make coefficients α and β comparable, we normalize\nq and d by subtracting their means and dividing by their standard deviations (detailed in Appendix A.3),\nrespectively. The results are presented in Table 4. We observe that the p-values for both α and β are less\nthan 0.001, indicating a significant correlation between MoA’s performance and both quality and diversity\n[Arnold, 1990]. The R2 values from the linear regression across three datasets are approximately around 0.7,\nindicating that the linear model based on quality and diversity explains 70% MoA’s performance and hence a\nstrong correlation between inputs and outputs, according to Appendix A.4. In later parts, we show that using\na more fine-grained quality calculation can further increase the R2 value.\n9\n\nComparing the effect strength of quality and diversity.\nFrom Table 4, we observe that α is greater than\nβ across all three datasets. In particular, for CRUX and MATH, the gap between these two measures is even\nmore pronounced. These results suggest that MoA’s performance is particularly sensitive to variations in\nquality, highlighting the importance of prioritizing quality within the proposer mixture. This finding is also\nconsistent with our observation that MoA achieves its best performance in the bottom right of the plot in\nFigure 2, further supporting the effectiveness of our proposed Self-MoA approach.\nAlternative quality measurements.\nWe use the averaged accuracy of each individual model to measure\nquality in the previous analysis. In this section, we explore alternative methods for assessing the quality of\nproposers. Recall that q1, . . . , q6 denote the accuracy of each individual model among proposers, and without\nloss of generality, we assume q1 ≥q2 ≥. . . ≥q6. It is reasonable to assume that the aggregator can select\nthe correct answer from the proposers, particularly when the responses of individual models are inconsistent.\nIn such cases, the aggregator would rely more heavily on models with better individual performance, meaning\nthe weight of q1 would be greater than that of q6.\nTherefore, we compare the following methods to calculate quality:\n• Average: 1\n6\nP6\ni=1 qi.\n• K-Norm:\n\x10\n1\n6\nP6\ni=1 qK\ni\n\x111/K\n, where a larger K places more emphasis on stronger individual models.\n• Centered-1/K-Norm: q1 −\n\x10\n1\n6\nP6\ni=1(q1 −qi)1/K\x11K\n. In this formulation, we first compute the\ndifference between qi and the best model’s q1. The 1/K norm emphasizes the weights of models\nwhose performance is closer to q1.\nAll three methods are the same when K = 1. For each quality measurement, we fit a linear regression\nto assess the relationship between MoA’s performance and the quality and diversity metrics, reporting the\nR2 values in Table 5. Our analysis shows that in MMLU and CRUX, applying a larger weight to better-\nperforming individual models tends to increase the R2 values. However, this trend is inconsistent for MATH.\nWe conjecture that this inconsistency arises because the aggregator Qwen2-7B-Instruct is relatively weak on\nMATH compared to the strongest individual model, Qwen2-Math-7B-Instruct. This limitation constrains\nthe performance of MoA, leading to an inconsistent trend in the linear regression results. In contrast, on\nMMLU, where Qwen2-7B-Instruct is the strongest individual model, we find that the R2 value can exceed\n0.9 with K = 2 using the Centered-1/K-Norm. This indicates a very strong linear relationship between MoA\nperformance and the quality and diversity metrics. Overall, we conclude that employing Centered-1/K-Norm\nwith K = 2 (marked in blue) achieves strong performance across all three datasets.\n4.2\nWhen Mixed-MoA Outperforms Self-MoA?\nAccording to the quality-diversity trade-off illustrated in Figure 2, we conjecture that increasing diversity can\nenhance MoA’s performance when the quality is controlled.\nMixed-MoA generally exhibits greater diversity than Self-MoA, which can lead to improved performance\nwhen the model quality is similar. This advantage arises when individual models achieve similar overall\nperformance while maintaining significant cross-model diversity. To simulate such a scenario, we construct a\nmixture task combining MMLU, CRUX, and MATH as described in Section 3.2. In this setting, test samples\nare drawn uniformly from the three tasks, and models do not have prior knowledge of a sample’s origin. For a\ngiven MoA strategy, we evaluate its performance on this mixture task by averaging its performance across the\n10\n\nTable 5: The R2 of the linear regression when we use different quality measurement methods. We find using\nCentered-1/K-Norm with K=2 can achieve good performance among all these three datasets.\nDataset\nMethod\nAvg. (K=1)\nK=2\nK=3\nK=4\nMMLU\nK-Norm\n0.771\n0.809\n0.832\n0.845\nCentered-1/K-Norm\n0.771\n0.881\n0.902\n0.903\nCRUX\nK-Norm\n0.685\n0.736\n0.765\n0.779\nCentered-1/K-Norm\n0.685\n0.753\n0.758\n0.753\nMATH\nK-Norm\n0.760\n0.720\n0.692\n0.672\nCentered-1/K-Norm\n0.760\n0.720\n0.692\n0.672\nTable 6: Comparison of Self-MoA and Mixed-MoA on the mixture task of MMLU, CRUX, and MATH,\nmeasured by the average performance of three tasks from Table 3. Mixed-MoA models with top two average\nperformances are highlighted by underline.\nAggregator\nProposer\nAverage\nIndividual\n-\ni\n52.07\n-\nd\n54.74\n-\nm\n50.60\nMixed-MoA\ni\niimmdd\n58.38\nimdddd\n58.61\niiiimd\n57.72\nimmmmd\n58.47\niimmmm\n57.19\niiimmm\n56.63\niiiimm\n55.97\niidddd\n58.76\niiiddd\n57.82\niiiidd\n56.82\nmmdddd\n59.86\nmmmddd\n60.04\nmmmmdd\n58.52\nSelf-MoA\ni\ndddddd\n59.69\ni\n6×TaskBest\n62.73\nTaskBest\n6×TaskBest\n63.81\nthree datasets. The results are reported in Table 6. In this mixture task, each model specializes in different\nsubtasks, with i performing best on MMLU, d on CRUX, and m on MATH. As TaskBest requires additional\nprior knowledge of the sample origin, we also report Self-MoA with d as the proposer, given that it achieves\nthe highest average performance among individual models.\nFrom Table 6, we observe that Mixed-MoA indeed outperforms Self-MoA of dddddd. Specifically,\nMixed-MoA of mmdddd and mmmddd achieves the average performance of 59.86% and 60.04%, improves\n11\n\nTable 7: MoA of Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. l is short for Llama-3.1-8B-Instruct and i\nis short for Qwen2-7B-Instruct.\nAggregator\nProposer\nMMLU\nIndividual\n-\ni\n66.16\n-\nl\n66.40\nMixed-MoA\ni\niiilll\n70.73\nSelf-MoA\ni\niiiiii\n69.01\ni\nllllll\n71.27\nupon Self-MoA of dddddd by 0.17% and 0.35%. Given the reported small margin, we argue that Self-MoA\nis still a very competitive baseline under this setting, not to mention the dominant performance of Self-MoA\nover Mixed-MoA when focusing on one single task (Self-MoA with TaskBest models achieve an average\nof 3.8% improvement from Table 6). In Appendix B.3 we also report normalized results that account for\ndifferent variances among tasks, which leads to a similar conclusion.\nWe further consider another single-task case on MMLU, involving two individual models: Llama-\n3.1-8B-Instruct and Qwen2-7B-Instruct, with Qwen2-7B-Instruct serving as the aggregator. We choose\nLlama-3.1-8B-Instruct because it performs similarly to Qwen2-7B-Instruct as an individual model. Table 7\ndemonstrates that even when the performance of two individual models is close, Self-MoA—utilizing six\nLlama-3.1-8B-Instruct proposers (denoted as llllll)—still outperforms the Mixed-MoA configuration\n(denoted as iiilll).\n5\nScaling Inference Compute with Self-MoA\nIn previous sections, we have provided evidence that Self-MoA over one strong model is straightforward\nbut effective. As the community is becoming more aware of scaling inference time computing [Brown et al.,\n2024, Snell et al., 2024, Wu et al., 2024], one natural question to ask is:\nGiven a strong model, does Self-MoA’s performance scale with the number of repeated samples?\nIntuitively, Self-MoA cannot scale indefinitely by simply increasing the computation budget for at least three\nreasons:\n• As more responses are sampled from a single model, the diversity among those samples tends to\nplateau.\n• Aggregating information from many samples is more challenging for LLMs compared to handling a\nsmaller number of samples.\n• Every LLM has a context length limit (e.g., 8192 tokens for Gemma 2), which restricts the number of\nresponses an aggregator can process at once.\nWhile the first limitation is inherent to repeated sampling, we address the latter two by introducing Self-\nMoA-Seq, a sequential variant designed to manage large numbers of responses without overwhelming the\naggregator. Self-MoA-Seq uses a sliding window to aggregate a fixed number of responses at a time, allowing\n12\n\n5\n10\n15\n20\n25\n30\nNumber of Samples\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0\n70.5\nAccuracy\nMMLU\nSelf-MoA\nSelf-MoA-Seq\nBase Model (Qwen2-7B-Instruct)\n5\n10\n15\n20\n25\n30\nNumber of Samples\n47\n48\n49\n50\n51\n52\n53\n54\nAccuracy\nCRUX\nSelf-MoA\nSelf-MoA-Seq\nBase Model (DeepSeek-Coder-V2-Lite-Instruct)\nFigure 3: The performance of Self-MoA and Self-MoA-Seq with a growing number of samples. Dashed lines\nindicate the performance of a single forward pass with the base model.\nit to handle an unlimited number of responses, regardless of context length constraints. A visual illustration is\nprovided in Figure 1.\nWe evaluate the performance of Self-MoA and Self-MoA-Seq with increasing sample sizes on the MMLU\nand CRUX benchmarks to study their scaling behavior. For each benchmark, we use the best-performing\nmodel as both the proposer and aggregator (Qwen2-7B-Instruct for MMLU and DeepSeek-Coder-V2-Lite-\nInstruct for CRUX), with a sampling temperature of 0.7. In Self-MoA-Seq, the window size is set to six, with\nthe first three slots reserved for the current synthesized output. We vary the number of samples from 6 to 30\nand plot the accuracy curves from three runs with different seeds in Figure 3. Our key observations are as\nfollows:\n• Both Self-MoA and Self-MoA-Seq significantly improve performance over the individual base model.\n• Adding more samples can have both positive and negative effects, meaning there is no universal\ncompute-optimal solution.\n• Self-MoA-Seq delivers performance that is comparable to, or slightly better than, Self-MoA.\nThese findings suggest that Self-MoA-Seq can extend the effectiveness of Self-MoA to LLMs with shorter\ncontext lengths, without sacrificing performance. Following Section 4.2, we explore whether introducing a\nsecond model can enhance performance in the sequential setting. Given that Llama-3.1-8B-Instruct performs\nsimilarly to Qwen2-7B-Instruct on the MMLU task, we compare the impact of adding Llama-3.1-8B-Instruct\nand DeepSeek-Coder-V2-Lite-Instruct (which underperforms Qwen2-7B-Instruct by 5%) after aggregating\n30 samples from Qwen2-7B-Instruct in Self-MoA-Seq. We find that incorporating Llama-3.1-8B-Instruct\nboosts accuracy by around 2%, whereas adding DeepSeek-Coder-V2-Lite-Instruct reduces accuracy by more\nthan 1.5%. This result provides another example of cross-model diversity benefiting MoA, and shows the\npotential of Self-MoA-Seq with increasing computation budget.\n6\nConclusion\nIn this paper, we introduce Self-MoA, an innovative approach that utilizes in-model diversity to enhance\nthe performance of large language models during inference. Our experiments demonstrate that Self-MoA\noutperforms traditional Mixed-MoA strategies in many popular benchmarks, particularly when the proposer\n13\n\nmodel quality varies. By aggregating outputs from a single high-performing model, Self-MoA effectively\naddresses the quality-diversity trade-off. We further identify the scenarios where mixing LLM can be\npotentially beneficial and extend Self-MoA to the constrained context length setting. These findings highlight\nthe potential of in-model diversity in optimizing LLM performance and pave the way for further advancements\nin ensemble methods.\n14\n\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman,\nS. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nA. Anthropic. Introducing claude, 2023.\nH. J. Arnold. Introduction to the practice of statistics. Technometrics, 32:347–348, 1990. URL https:\n//api.semanticscholar.org/CorpusID:122891525.\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical\nreport. arXiv preprint arXiv:2309.16609, 2023.\nB. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. R´e, and A. Mirhoseini. Large language monkeys:\nScaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024.\nJ. C.-Y. Chen, S. Saha, and M. Bansal. Reconcile: Round-table conference improves reasoning via consensus\namong diverse llms. arXiv preprint arXiv:2309.13007, 2023a.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\nG. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\n2021.\nS. Chen, L. Zeng, A. Raghunathan, F. Huang, and T. C. Kim. Moa is all you need: Building llm research\nteam using mixture of agents. arXiv preprint arXiv:2409.07487, 2024.\nX. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin, S. Prakash, C. Sutton, X. Wang, and D. Zhou. Universal\nself-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023b.\nD. Dan Friedman and A. B. Dieng. The vendi score: A diversity evaluation metric for machine learning.\nTransactions on machine learning research, 2023.\nY. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in language\nmodels through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to\ndebias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du,\nM. R. G. Madani, et al. Are we done with mmlu? arXiv preprint arXiv:2406.04127, 2024.\nA. Gu, B. Rozi`ere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for\ncode reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024.\nL. Gui, C. Gˆarbacea, and V. Veitch. Bonbon alignment for large language models and the sweetness of\nbest-of-n sampling. arXiv preprint arXiv:2406.00832, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\nmathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n15\n\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B.\nHanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a.\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas,\nE. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux,\nP. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and\nW. E. Sayed. Mixtral of experts, 2024b. URL https://arxiv.org/abs/2401.04088.\nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and\ngenerative fusion. arXiv preprint arXiv:2306.02561, 2023a.\nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and\ngenerative fusion, 2023b. URL https://arxiv.org/abs/2306.02561.\nJ. Li, Q. Zhang, Y. Yu, Q. Fu, and D. Ye. More agents is all you need, 2024. URL https://arxiv.org/\nabs/2402.05120.\nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,\nA. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097,\n2022.\nT. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent\nthinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.\nY. Lin, H. Lin, W. Xiong, S. Diao, J. Liu, J. Zhang, R. Pan, H. Wang, W. Hu, H. Zhang, H. Dong, R. Pi,\nH. Zhao, N. Jiang, H. Ji, Y. Yao, and T. Zhang. Mitigating the alignment tax of rlhf, 2024. URL\nhttps://arxiv.org/abs/2309.06256.\nK. Lu, H. Yuan, R. Lin, J. Lin, Z. Yuan, C. Zhou, and J. Zhou. Routing to the expert: Efficient reward-guided\nensemble of large language models, 2023. URL https://arxiv.org/abs/2311.08692.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information\nProcessing Systems, 36, 2024.\nY. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with a reference-free reward. arXiv\npreprint arXiv:2405.14734, 2024.\nOpenPipe. Openpipe mixture of agents: Outperform gpt-4 at 1/25th the cost, 2024. URL https://\nopenpipe.ai/blog/mixture-of-agents.\nA. Ram´e, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard,\nand O. Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024. URL https:\n//arxiv.org/abs/2406.16768.\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, et al.\nCode llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nK. Sarjana, L. Hayati, and W. Wahidaturrahmi. Mathematical modelling and verbal abilities: How they\ndetermine students’ ability to solve mathematical word problems? Beta: Jurnal Tadris Matematika, 13(2):\n117–129, 2020.\n16\n\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than\nscaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314.\nK. Stechly, M. Marquez, and S. Kambhampati. Gpt-4 doesn’t know it’s wrong: An analysis of iterative\nprompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\net al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nG. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari,\nA. Ram´e, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome,\nA. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J.-B. Grill,\nB. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl,\nA. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen,\nC. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar,\nD. Rogozi´nska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin,\nG. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-Pluci´nska, H. Batra, H. Dhand,\nI. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi,\nJ. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. yeong Ji, K. Mohamed,\nK. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sjoesund, L. Usui,\nL. Sifre, L. Heuermann, L. Lago, L. McNealus, L. B. Soares, L. Kilpatrick, L. Dixon, L. Martins, M. Reid,\nM. Singh, M. Iverson, M. G¨orner, M. Velloso, M. Wirth, M. Davidow, M. Miller, M. Rahtz, M. Watson,\nM. Risdal, M. Kazemi, M. Moynihan, M. Zhang, M. Kahng, M. Park, M. Rahman, M. Khatwani, N. Dao,\nN. Bardoliwalla, N. Devanathan, N. Dumai, N. Chauhan, O. Wahltinez, P. Botarda, P. Barnes, P. Barham,\nP. Michel, P. Jin, P. Georgiev, P. Culliton, P. Kuppala, R. Comanescu, R. Merhej, R. Jana, R. A. Rokni,\nR. Agarwal, R. Mullins, S. Saadat, S. M. Carthy, S. Perrin, S. M. R. Arnold, S. Krause, S. Dai, S. Garg,\nS. Sheth, S. Ronstrom, S. Chan, T. Jordan, T. Yu, T. Eccles, T. Hennigan, T. Kocisky, T. Doshi, V. Jain,\nV. Yadav, V. Meshram, V. Dharmadhikari, W. Barkley, W. Wei, W. Ye, W. Han, W. Kwon, X. Xu,\nZ. Shen, Z. Gong, Z. Wei, V. Cotruta, P. Kirk, A. Rao, M. Giang, L. Peran, T. Warkentin, E. Collins,\nJ. Barral, Z. Ghahramani, R. Hadsell, D. Sculley, J. Banks, A. Dragan, S. Petrov, O. Vinyals, J. Dean,\nD. Hassabis, K. Kavukcuoglu, C. Farabet, E. Buchatskaya, S. Borgeaud, N. Fiedel, A. Joulin, K. Kenealy,\nR. Dadashi, and A. Andreev. Gemma 2: Improving open language models at a practical size, 2024a. URL\nhttps://arxiv.org/abs/2408.00118.\nM. R. Team et al. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https://www. databricks.\ncom/blog/introducing-dbrx-new-state-art-open-llm. Accessed on April, 26, 2024b.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nK. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by self-\ncritiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.\nJ. Wang, J. Wang, B. Athiwaratkun, C. Zhang, and J. Zou. Mixture-of-agents enhances large language model\ncapabilities. arXiv preprint arXiv:2406.04692, 2024a.\nQ. Wang, Z. Wang, Y. Su, H. Tong, and Y. Song. Rethinking the bounds of llm reasoning: Are multi-agent\ndiscussions the key? arXiv preprint arXiv:2402.18272, 2024b.\n17\n\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nY. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. An empirical analysis of compute-optimal inference for\nproblem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724.\nC. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large\nlanguage models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\nA. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin,\nJ. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang,\nK. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin,\nS. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren,\nX. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan. Qwen2\ntechnical report, 2024. URL https://arxiv.org/abs/2407.10671.\nK. Zhang, B. Qi, and B. Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion.\narXiv preprint arXiv:2407.08642, 2024a.\nK. Zhang, W. Yao, Z. Liu, Y. Feng, Z. Liu, R. Murthy, T. Lan, L. Li, R. Lou, J. Xu, et al. Diversity empowers\nintelligence: Integrating expertise of software engineering agents. arXiv preprint arXiv:2408.07060, 2024b.\nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nW. Zhou, R. Agrawal, S. Zhang, S. R. Indurthi, S. Zhao, K. Song, S. Xu, and C. Zhu. Wpo: Enhancing rlhf\nwith weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024.\nQ. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseek-coder-v2:\nBreaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024.\n18\n\nA\nSupplements\nA.1\nMulti-Layer MoA\nMoA can be extended to multiple layers. For MoA with l layers and n LLMs {Ai,j}n\nj=1 in each layer i, we\ncan formulate it as follows:\nyi =\nn\nM\nj=1\n[Ai,j(xi)] + x1,\nxi+1 = yi,\nwhere each LLM Aj\ni generates a response for the query xi, which is further concatenated with the original\nquery by the aggregator’s prompt L.\nTable 8 compares the performance of 3-Layer Mixed-MoA and 2-Layer Self-MoA as well as the total\nnumber of forward passes required for each method. Specifically, one forward pass is counted each time a\nproposer model generates an output or an aggregator synthesizes a result. Notably, Self-MoA outperforms the\n3-Layer Mixed-MoA baseline with only half the forward passes.\nTable 8: Results of 3-Layer Mixed-MoA.\nModel Configuration\nLC Win Rate\n# Forward Passes\nMixed-MoA\n3-Layer MoA [Wang et al., 2024a]\n65.4\n13\nSelf-MoA\n2-Layer Self-MoA + WizardLM-2-8x22B\n65.7\n7\nA.2\nVendi Score\nThe Vendi Score (VS) is a metric designed to evaluate diversity in machine learning. It takes as input a\ncollection of samples along with a pairwise similarity function, and it outputs a single value that represents\nthe effective number of unique elements within the sample set.\nThe score is computed using a positive semi-definite similarity matrix K ∈Rn×n as follows:\nV S(K) = exp\n\x12\n−tr\n\x12K\nn log\n\x12K\nn\n\x13\x13\x13\n= exp\n \n−\nn\nX\ni=1\nλi log(λi)\n!\nHere, λi are the eigenvalues of the normalized matrix K\nn , and 0 log 0 = 0. Essentially, the Vendi Score is\nthe exponential of the von Neumann entropy of K\nn , which reflects the Shannon entropy of its eigenvalues,\nalso referred to as the effective rank. This metric provides a quantitative measure of diversity based on the\ndistribution of similarity scores among the samples.\nA.3\nNormalization of Inputs\nGiven a sequence of inputs x1, ..., xn. Let x′ denote the normalized x. We have\nx′ = xi −¯x\nstd(x) , where ¯x = 1\nn\nn\nX\ni=1\nxi, and std(x) =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(xi −¯x)2\n19\n\nA.4\nImplication of R-squre\nThe implications of R2 are presented in Table 9, illustrating the degree of influence between the independent\nand dependent variables. [Sarjana et al., 2020].\nTable 9: The interpretation of R-square\nR-square\nLevel\n[0, 0.2)\nVery weak\n[0.2, 0.4)\nWeak\n[0.4, 0.6)\nMedian\n[0.6, 0.8)\nStrong\n[0.8, 1.0]\nVery Strong\nB\nAdditional Results\nB.1\nMT-Bench Results\nWe also compare MoA and Self-MoA on the MT-Bench [Zheng et al., 2023] benchmark under the same\nexperiment setting as Wang et al. [2024a]. We copy the numbers from Wang et al. [2024a] for 3-Layer MoA\nsettings, and report our implemented results for the other experiments to ensure that 2-Layer experiments are\nfair comparisons. Table 10 shows that Self-MoA outperforms its Mixed-MoA counterpart, and using GPT-4o\nas the aggregator can achieve the best performance even with fewer forward passes compared to 3-Layer\nMoA with GPT-4o.\nB.2\nComparison to Universal Self-Consistency\nWe conduct further experiments to compare Self-Consistency [Wang et al., 2022] with MoA and Self-MoA\non the AlpacaEval 2.0 benchmark. As this benchmark is an instruction-following task without exact answers,\nwe evaluate on Universal Self-Consistency (USC) [Chen et al., 2023b] which prompts LLMs to generate\nthe most consistent response. We report the result in Table 12, which shows that USC performs worse than\nits MoA counterpart when proposers and aggregators are controlled. This further suggests that rather than\nfinding the most consistent response, MoA and Self-MoA can encourage LLM to synthesize the references\nand produce a better response.\nB.3\nNormalizing Sub-tasks in Table 6\nThe results in Table 3 indicate that the variance of models on CRUX is generally higher than that of the other\ntwo tasks, which could bias the average performance towards CRUX. To ensure that each task contributes\nequally to the overall performance metric, we assign weights to the three tasks based on the inverse of their\nvariance.\nFor example, considering MMLU, we report 19 performance metrics (including individual models,\nMixed-MoA, and Self-MoA) in Table 3. The standard deviation of performance for MMLU across these 19\n20\n\nTable 10: Comparison of Self-MoA and Mixed-MoA on MT-Bench. We use Qwen1.5-110B-Chat and GPT-4o\nas the aggregator.\nModel Configuration\nAvg.\n1st turn\n2nd turn\n# Forward Passes\nIndividual\nWizardLM-2-8x22B\n8.99\n9.05\n8.93\n1\nQwen1.5-110B-Chat\n8.61\n8.77\n8.45\n1\nLLaMA-3-70B-Instruct\n8.84\n9.14\n8.54\n1\nQwen1.5-72B-Chat\n8.62\n8.66\n8.58\n1\nMixtral-8x22B-Instruct-v0.1\n8.49\n8.89\n8.09\n1\ndbrx-instruct\n7.82\n8.21\n7.43\n1\nMixed-MoA\n2-Layer MoA\n9.06\n9.23\n8.89\n7\n2-Layer MoA w/ GPT-4o\n9.39\n9.40\n9.37\n7\n3-Layer MoA\n9.25\n9.44\n9.07\n13\n3-Layer MoA w/ GPT-4o\n9.40\n9.49\n9.31\n13\nSelf-MoA +\nWizardLM-2-8x22B\n2-Layer Self-MoA\n9.13\n9.36\n8.89\n7\n2-Layer Self-MoA w/ GPT-4o\n9.52\n9.56\n9.47\n7\nsettings is calculated to be 3.50. In comparison, the standard deviation for CRUX and MATH are 5.70 and\n4.27, respectively. Consequently, the weight assigned to MMLU when calculating the “WeightedAvg” is\ngiven by:\nWeightMMLU =\n1/3.50\n(1/3.50) + (1/5.70) + (1/4.27).\nThe normalized results are shown in Table 11.\n21\n\nTable 11: This table compares Self-MoA and Mixed-MoA using a weighted composition of three sub-\ntasks. The weights are assigned to each sub-task to prevent a high-variance task, such as CRUX, from\ndisproportionately influencing the overall performance metrics. This approach ensures a more balanced\nevaluation, allowing for a fairer comparison between the two models.\nAggregator\nProposer\nMMLU\nCRUX\nMATH\nAverage\nWeightedAvg\nIndividual\n-\ni\n66.16\n36.25\n53.81\n52.07\n54.46\nIndividual\n-\nd\n60.91\n49.51\n53.82\n54.74\n55.65\nIndividual\n-\nm\n54.36\n27.88\n69.57\n50.60\n52.80\nMixed-MoA\ni\niimmdd\n67.89\n42.88\n64.38\n58.38\n60.40\nMixed-MoA\ni\nimdddd\n67.42\n44.50\n63.90\n58.61\n60.46\nMixed-MoA\ni\niiiimd\n68.90\n41.25\n63.00\n57.72\n59.94\nMixed-MoA\ni\nimmmmd\n66.63\n42.75\n66.02\n58.47\n60.40\nMixed-MoA\ni\niimmmm\n66.23\n39.25\n66.10\n57.19\n59.38\nMixed-MoA\ni\niiimmm\n67.49\n38.25\n64.16\n56.63\n59.00\nMixed-MoA\ni\niiiimm\n68.00\n37.00\n62.92\n55.97\n58.47\nMixed-MoA\ni\niidddd\n68.21\n45.50\n62.56\n58.76\n60.58\nMixed-MoA\ni\niiiddd\n68.21\n42.88\n62.38\n57.82\n59.86\nMixed-MoA\ni\niiiidd\n68.47\n40.75\n61.24\n56.82\n59.05\nMixed-MoA\ni\nmmdddd\n66.34\n46.75\n66.48\n59.86\n61.45\nMixed-MoA\ni\nmmmddd\n65.80\n47.00\n67.32\n60.04\n61.57\nMixed-MoA\ni\nmmmmdd\n65.44\n42.50\n67.62\n58.52\n60.39\nSelf-MoA\ni\ndddddd\n65.23\n50.75\n63.08\n59.69\n60.86\nSelf-MoA\ni\n6×TaskBest\n69.01\n50.75\n68.42\n62.73\n64.21\nSelf-MoA\nTaskBest\nTaskBest\n69.01\n52.62\n69.80\n63.81\n65.14\nTable 12: Comparison of Self-MoA, Mixed-MoA, and Universal Self-Consistency (USC) on AlpacaEval 2.0\nleaderboard. We use Qwen1.5-110B-Chat as the aggregator.\nModel Configuration\nLC Win Rate\n# Forward Passes\nMixed-MoA\nMoA\n59.1\n7\nSelf-MoA\nSelf-MoA + WizardLM-2-8x22B\n65.7\n7\nUniversal Self-Consistency\nMixed-USC\n53.8\n7\nSelf-USC + WizardLM-2-8x22B\n60.2\n7\n22'),
                Paper(arxiv_id='2501.19066', authors=['Dahye Kim', 'Deepti Ghadiyaram'], published_at=datetime.datetime(2025, 2, 5, 7, 44, 45, 130000, tzinfo=datetime.timezone.utc), title='Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations', summary='Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.', upvotes=7, thumbnail=None, content="1. Introduction\nText-to-image (T2I) generative models have revolutionized\ncontent generation by producing diverse and highly photo-\nrealistic images, enabling a wide range of applications such\nas digital art creation (Mazzone & Elgammal, 2019), image\nediting (Brooks et al., 2023), and medical imaging (Kaze-\nrouni et al., 2023). These models are usually trained on\nseveral billions of web-scraped image and text pairs pre-\nsumably capturing a broad spectrum of semantic concepts.\nConsequently, these models are also prone to be exposed\nto and thus generate disturbing content containing nudity,\nviolence, child exploitation, and self-harm – raising serious\n1Department of Computer Science, Boston University 2Runway.\nCorrespondence to: Deepti Ghadiyaram <dghadiya@bu.edu>.\nRemove nudity\nChange\nPhotographic\nStyles\nChange\nObject \nattributes\nRemove violence\nRemove\nUnsafe \nconcepts\nMake the image darker\nChange the season style to winter \nChange to a full shot of a dog\nChange car color to blue\nFigure 1. Monosemantic interpretable concepts such as nudity,\nphotographic styles, and object attributes are identified using k-\nsparse autoencoders (k-SAE). We leverage them to enable precise\nmodification of a desired concept during the generation process,\nwithout impacting the overall image structure, photo-realism, vi-\nsual quality, and prompt alignment (for safe concepts). Our frame-\nwork can be used to remove unsafe concepts (top row), photo-\ngraphic styles (middle row), and object attributes (last row).\nethical concerns about their downstream applications.\nSeveral attempts have been made to enforce safe generations\nin the past: integrating safety filters as part of the generation\npipeline (Rando et al., 2022), guiding the generation process\naway from a pre-defined unsafe latent space (Schramowski\net al., 2023), or directly erasing inappropriate concepts by\nmodifying model weights (Gandikota et al., 2023; Heng\n& Soh, 2024; Li et al., 2024). While partially success-\nful, some of these methods involve model training which\nis not only computationally expensive but also alters the\noverall model generative capabilities. More recently, a few\ninference-based approaches have been proposed, which do\nnot alter model weights (Yoon et al., 2024; Jain et al., 2024).\nSAFREE (Yoon et al., 2024) alters the semantics of the input\nprompt by filtering toxic tokens, while TraSCE (Jain et al.,\n2024) modifies negative prompting with gradient compu-\ntation to guide the model towards safer outputs. Crucially,\nsometimes these models have the undesirable consequence\n1\narXiv:2501.19066v1  [cs.CV]  31 Jan 2025\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nof visual degraded output generations or being misaligned\nwith input prompts, even when the prompts are benign. Ad-\nditionally, the increased inference time (e.g., 8.84s overhead\nper image as noted in TraSCE (Jain et al., 2024)) due to\nonline filtering makes them difficult to deploy in practice.\nIn this work, we posit that the semantic information is in-\nterwoven across different layers of a generative model in\ncomplex ways that is not fully understood. Subsequently, ex-\nisting training or inference-based safe generation techniques\ncould be altering this latent landscape in undesirable ways\nleading to misaligned or irrelevant outputs. To this end, we\napproach the generation process from the ground up and ex-\nplore the following crucial question: can we systematically\nisolate monosemantic1 concepts of varied granularities (fine-\ngrained and abstract) from the generative latent space and\nsurgically manipulate only them? Having such a tool would\nbe invaluable as it would allow the user to intentionally con-\ntrol just the relevant concept of interest without disrupting\nthe overall latent landscape.\nTo this end, we leverage k-sparse autoencoders (k-\nSAE) (Makhzani & Frey, 2013) to design controllable gen-\nerative models. k-SAEs have shown promising progress in\ninterpreting language models by learning a sparse dictionary\nof monosemantic concepts (Bricken et al., 2023; Cunning-\nham et al., 2023). In our work, we first train a k-SAE on the\nembeddings extracted from a corpus of text prompts contain-\ning semantic concepts we wish to control (e.g., unsafe con-\ncepts). Once trained, each k-SAE’s hidden state corresponds\nto an isolated monosemantic concept. During the generation\nprocess, given a concept we wish to steer, we use k-SAE to\nidentify its corresponding latent direction and precisely ma-\nnipulate the presence of that concept in the outcome, without\nimpacting the overall generation capability (Fig. 1). Notably,\nour method does not require any fine-tuning as in Zhang\net al. (2024), synthetic data generation as in Esposito et al.\n(2023), training a separate LoRA adapter (Hu et al., 2021)\nfor each concept as in Gandikota et al. (2025) to manipulate\nmaking it fast, efficient, and adaptable to any pre-trained\ntext to image generative framework. We summarize our\nempirical findings and key contributions below:\n• We identify interpretable monosemantic concepts\nin text-to-image generation latent landscape using\na k-sparse autoencoder. Once trained, k-SAE serves\nas a Concept Steerer to provide precise control over\nspecific visual concepts (e.g., nudity, violence, etc.)\n• Concept Steerer achieves state-of-the-art perfor-\nmance on unsafe concept removal while being ∼5x\nfaster than the existing best method, without compro-\nmising visual quality.\n1In contrast to the one-to-many mapping of polysemantic neu-\nrons, monosemantic neurons form a one-to-one correlation with\ntheir related input features (Yan et al., 2024).\n• Concept Steerer effectively manipulates photo-\ngraphic and artistic styles, object attributes, enabling\ncontrolled yet creative image generation.\n• Concept Steerer is robust to adversarial prompt ma-\nnipulations, achieving a 20.01% improvement against\nred-teaming tools, ensuring reliable image generation\neven under challenging scenarios.\n• Concept Steerer works out-of-the-box to any text-\nto-image model, is extremely simple, requires no re-\ntraining nor LoRA adapters, and is highly efficient.\n2. Related Work\nControlling diffusion models: Wu et al. (2023); Wallace\net al. (2024) fine-tune diffusion models using human feed-\nback and Bansal et al. (2023); Singhal et al. (2025) pro-\npose inference-time diffusion steering with reward functions.\nHowever, these methods rely on strong reward functions,\nand are computationally intensive (Uehara et al., 2025).\nSome methods achieve controllability by training additional\nmodules such as low-rank adapters (LoRAs) (Gandikota\net al., 2025; Stracke et al., 2025), which requires millions of\nparameters per concept and significantly increases genera-\ntion time (Sridhar & Vasconcelos, 2024). Several inference-\ntime intervention works attempt fine-grained control at test\ntime. However, estimating noise at each step for each con-\ncept during generation (Brack et al., 2022; 2023) signifi-\ncantly slows down generation and steering model activa-\ntions based on optimal transport (Rodriguez et al., 2024)\nrequires learning activation mapping for each style. By con-\ntrast, our approach is very simple, requires no training of the\nbase model or LoRA adapters, no additional noise/gradient\ncomputation during the generation process. Moreover, once\ntrained, our approach allows us to manipulate any concept\nwe want without further tuning.\nSafe generation: Given the growing concerns of genera-\ntive models’ capability to produce inappropriate content,\nseveral valuable research has emerged in this space. Some\ntraining-based methods (Gandikota et al., 2023; Zhang et al.,\n2024) directly remove inappropriate concepts from the dif-\nfusion model through additional fine-tuning, while some\nothers like (Gandikota et al., 2024; Gong et al., 2025) up-\ndate model weights to erase concepts without retraining\nthe model. Some recently proposed inference-based ap-\nproaches (Yoon et al., 2024; Jain et al., 2024) do not require\ntraining or weight updates. While effective, these methods\noften result in degraded image quality and increased infer-\nence time. Unlike all prior works, our method surgically\nisolates interpretable concepts in the generative latent space\nand manipulating only these in the text encoder. Thus, our\napproach enjoys the benefit of precise control of inappropri-\nate concepts, does not compromise on generation quality,\nand maintains prompt-image alignment.\n2\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nDiffusion model\nText prompt \n“Greek goddess \nposing …”\nText\nEncoder\nUnsafe \nSafe \nConcept 𝑪\n“Nudity”\nk-SAE\nDecoder\nEncoder\nSafe path\nUnsafe path\nText\nEncoder\n∗λ\nFigure 2. K-sparse autoencoder (k-SAE) is trained on feature\nrepresentations from the text encoder of the diffusion model. Once\ntrained, it serves as a Concept Steerer, enabling precise, surgical\nconcept manipulation by adjusting λ.\nInterpreting diffusion models: Recent works have demon-\nstrated that sparse autoencoders (SAE) could recover inter-\npretable features in large language models (Bricken et al.,\n2023; Cunningham et al., 2023), CLIP vision features (Fry,\n2024; Daujotas, 2024) and diffusion features (Kim et al.,\n2024; Surkov et al., 2024). Kim et al. (2024) reveal monose-\nmantic interpretable features represented within rich visual\nfeatures of the diffusion model while Surkov et al. (2024)\ninvestigate how text information is integrated via cross-\nattention. By contrast, we focus on the text encoder of a dif-\nfusion model, identify interpretable directions via k-SAEs,\nand demonstrate precise steering of a variety of concepts.\n3. Approach\nWe propose a simple yet effective technique to precisely\nisolate and steer semantic concepts such as nudity or pho-\ntographic styles using k-sparse autoencoders (Makhzani &\nFrey, 2013) (k-SAE). We first present how we train such\na k-SAE (Sec. 3.2), followed by our method to combine\ndifferent monosemantic neurons to steer abstract concepts\n(Sec. 3.3). We stress that a k-SAE is trained only once and\nno training is required for any concept the user wishes to\nintroduce, eliminate, or modulate.\n3.1. Preliminaries on text to image models\nText-to-image diffusion models (Rombach et al., 2022;\nRamesh et al., 2022; Saharia et al., 2022) primarily con-\nsist of a text encoder to extract a text prompt’s interme-\ndiate embedding and a diffusion model. During training,\nthe diffusion model progressively denoises a noisy im-\nage (or its latent representation) conditioned on the text\nprompt’s intermediate embedding. Formally, given an in-\nput y0, the forward diffusion process progressively adds\nnoise to y0 over T timesteps. The intermediate noisy im-\nage at timestep t is yt =\np\n(1 −βt)y0 + √βtϵ where ϵ is\nthe Gaussian noise and βt is a timestep-dependent hyper\nparameter. In the reverse process, the diffusion model ϵθ\niteratively denoises yt at each timestep, conditioned on the\ntext prompt embedding c, to predict noise ϵ. The objective\nfunction for training the model is to minimize the error be-\ntween the introduced and the predicted noise, defined as:\nEy,t,ϵ∼N(0,1)\n\x02\n∥ϵ −ϵθ(yt, c, t)∥2\n2\n\x03\n3.2. Preliminaries on k-sparse autoencoders\nSparse autoencoders (Ng et al., 2011) are neural networks\ndesigned for learning compact and meaningful feature rep-\nresentations in an unsupervised manner. They consist of an\nencoder and a decoder, optimized jointly using a reconstruc-\ntion loss and a sparsity regularization term to encourage\nonly a few neurons to be maximally activated for a given in-\nput. However, the sparsity constraint introduces significant\nchallenges during optimization (Tibshirani, 1996; Makhzani\n& Frey, 2013). To mitigate these issues, k-sparse autoen-\ncoders (k-SAEs) (Makhzani & Frey, 2013) were introduced.\nThey explicitly control the number of active neurons to k\nduring training by applying a Top-k activation function at\neach training step. Consequently, this retains only the k\nhighest activations and zeroes out the rest.\nLet Wenc ∈Rn×d and Wdec ∈Rd×n represent the weight\nmatrices of the k-SAE’s encoder and decoder respectively\n(Fig. 2). The hidden layer dimension n is defined as an\ninteger multiple of the input feature dimension d. The ratio\nn/d, referred to as the expansion factor, controls the extent\nto which the hidden dimension is expanded relative to the\ninput dimension. Following Bricken et al. (2023), bpre ∈Rd\ndenotes the bias term added to input x before feeding to the\nencoder (aka pre-encoder bias), while benc ∈Rn denotes\nthe bias term of the encoder.\nLet x ∈RL×d denote the intermediate representation of the\ntext encoder for an input prompt in a text-to-image model,\nwhere L denotes the number of tokens. The encoded latent\nz is computed as:\nz = ENC(x) = Top-k(ReLU(Wenc(x −bpre) + benc)),\n(1)\nwhere the Top-k function retains only the top k neuron acti-\nvations and sets the remaining activations to zero (Makhzani\n& Frey, 2013). The decoder reconstructs ˆx as:\nˆx = DEC(x) = Wdecz + bpre,\n(2)\nThe training objective of a standard k-SAE is to minimize\nthe normalized mean squared error (MSE) between the orig-\ninal feature x and the reconstructed feature ˆx, denoted by\nLmse. However, both SAEs and k-SAEs suffer from the pres-\nence of “dead latents,” where a large proportion of latents\nstop activating entirely at some point in training. Presence\nof dead latents decreases the likelihood of the network dis-\ncovering separable, interpretable features while incurring\n3\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nunnecessary computational cost (Bricken et al., 2023). To\ndiscourage dead latents, we incorporate an auxiliary MSE\nloss as suggested in Gao et al. (2024). Specifically, in every\ntraining step, we identify top kaux dead latents and recon-\nstruct a latent ˆz exclusively from them, as defined below:\nˆz = Top-kaux(ReLU(Wenc(x −bpre) + benc)),\n(3)\nNow, let ˆe = Wdecˆz represent the reconstruction using the\ntop kaux dead latents. Laux is defined as a reconstruction loss\nbetween the auto encoder’s residual and the output from the\ndead neurons (ˆe). As discussed in Gao et al. (2024), the\nintuition behind Laux is to compute gradients that push the\nparameters of the dead neurons in the direction of explaining\nthe autoencoder residual (e). Thus, the total training loss is:\nL = Lmse + αLaux = ∥x −ˆx∥2\n2 + α∥e −ˆe∥2\n2,\n(4)\nThe scalar α is a weighting factor that controls the relative\ncontribution of the auxiliary loss.\n3.3. Concept Steerers\nGiven a human-interpretable concept C2 we wish to steer,\nwe first extract its text embedding xC, pass it through k-\nSAE, and finally perform an element-wise addition with the\ninput prompt embedding x. This can be expressed as:\nxsteered = x + Wdec(λ ∗ENC(xC)),\n(5)\nwhere λ denotes a scalar that controls the steering strength.\nThe steered vector xsteered is used to condition the generation\nprocess. As we show in Sec. 4, our approach requires a k-\nSAE to be trained only once, and provides model-agnostic,\nfine-grained control over concept steering without degrading\nthe overall generation quality.\n4. Experiments\nWe first share the training setup followed by numerous re-\nsults and ablations on concept steering.\nImplementation details: We train k-sparse autoencoders\non text embeddings with kaux = 256, and loss weight pa-\nrameter α = 1/32 for 10k training steps. We train for a total\ntraining tokens of 400M on a batch size of 4096 with the\nlearning rate 0.0004 using Adam (Kingma, 2014) optimizer.\nThe k-SAE is trained with k = 32 and an expansion factor\nof 4, resulting in a total hidden size dimension n = 3072\nfor Stable Diffusion (SD) 1.4 (Rombach et al., 2022) in\nthe unsafe removal task. For style manipulation, we use\nk = 64 with an expansion factor of 64, resulting in a total\nhidden size dimension n = 49152 for SD1.4 and k = 64\nwith an expansion factor of 16, resulting in a total hidden\nsize dimension n = 32768 for SDXL-Turbo (Sauer et al.,\n2Defined by any user-provided prompt, e.g., “nudity”.\n2025). These settings were found via ablation studies on\ndownstream tasks and/or chosen based on overall training\nstability and sparsity. We apply a unit normalization con-\nstraint (Sharkey et al., 2023) on the decoder weights Wdec\nof the k-SAE after each update. Although our method can\nbe applied in an out-of-the-box manner to any text-to-image\nmodel, for a fair comparison with existing methods, we con-\nduct experiments using SD 1.4 for unsafe concept removal\nand then expand our evaluation to more recent SDXL-Turbo\nand FLUX.1-dev. When steering concepts, we use a slightly\nmodified encoder of k-SAE without the TopK activation\nfunction, as the TopK function clamps some possible impor-\ntant information that is critical for both maintaining visual\nquality and effective concept removal. More details in Ap-\npendix A.\n4.1. Steering towards safety\nSetup: First, we demonstrate the effectiveness of erasing\nunsafe concepts using k-SAEs. We use the Inappropriate\nImage Prompts (I2P) dataset (Schramowski et al., 2023)\nto evaluate our method on steering nudity concepts and\nthe Ring-A-Bell benchmark (Tsai et al., 2023) to assess\nsteering performance on violent content. We train a k-SAE\nusing features extracted from the residual stream of the 10th\nlayer (out of 12 layers) of the text encoder in SD 1.4, on the\nprompts in I2P dataset, and evaluate its performance on both\nthe I2P and Ring-A-Bell datasets. As in prior works (Tsai\net al., 2023; Jain et al., 2024), we use nudity concept Cnudity\nas “naked, nude, bare, exposed, stripped, topless, male geni-\ntalia, penis, buttocks” and use a slightly modified version\nfor the violent concept Cviolence as “violence, blood”. We set\nsteering strength λ = −0.5 for I2P dataset and λ = −0.7\nfor adversarial datasets including violent concept.\nEvaluation metrics: To quantify the impact of our method\non generation quality, we use FID (Heusel et al., 2017) and\nCLIP score (Hessel et al., 2021; Radford et al., 2021) on the\nCOCO-30k dataset, evaluating 10k generated samples. We\nreport Attack Success Rate (ASR), i.e., the percentage of\ngenerated images containing nudity or violence as a measure\nof how well a model reduces unsafe content generation.\nTo this end, we use the NudeNet (Bedapudi, 2019) with a\nthreshold of 0.45 and Q16 violence detector (Schramowski\net al., 2022), following prior work (Jain et al., 2024).\nBaselines: We compare our method against inference-based\napproaches that do not require training or weight updates to\nthe generative model, including SLD (Schramowski et al.,\n2023), SD with negative prompt (SD-NP), SAFREE (Yoon\net al., 2024), and TraSCE (Jain et al., 2024). Additionally,\nwe evaluate our method against training-based approaches,\nincluding ESD (Gandikota et al., 2023), FMN (Zhang et al.,\n2024), CA (Kumari et al., 2023), MACE (Lu et al., 2024),\nand SA (Heng & Soh, 2024), as well as approaches that\n4\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 1. Performance comparison across different methods on\nI2P and COCO datasets. Lower ASR and FID indicate better\nperformance; higher is better for CLIP. Our method achieves the\nlowest ASR by effectively removing nudity while preserving visual\nquality and prompt alignment. Bold: best. Underline: second-best.\nGray : require training and weight updates, Pink : do not require\ntraining but update model weights, Blue : do not require either.\nMETHOD\nI2P\nCOCO\nASR ↓\nFID ↓\nCLIP ↑\nSDV1.4\n17.80\n16.71\n31.3\nESD (GANDIKOTA ET AL., 2023)\n2.87\n18.18\n30.2\nCA (KUMARI ET AL., 2023)\n1.04\n24.12\n30.1\nMACE (LU ET AL., 2024)\n1.51\n16.80\n28.7\nSA (HENG & SOH, 2024)\n2.81\n25.80\n29.7\nUCE (GANDIKOTA ET AL., 2024)\n0.87\n17.99\n30.2\nRECE (GONG ET AL., 2025)\n0.72\n17.74\n30.2\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n1.74\n28.75\n28.4\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n2.28\n24.40\n29.1\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n3.95\n21.17\n29.8\nSD-NP\n0.74\n18.33\n30.1\nSAFREE (YOON ET AL., 2024)\n1.45\n19.32\n30.1\nTRASCE (JAIN ET AL., 2024)\n0.45\n17.41\n29.9\nOURS (W/O NEGATIVE STEERING)\n0.57\n18.37\n30.8\nOURS\n0.36\n18.67\n30.8\nSD 1.4\nOurs\nTraSCE\nSAFREE\nRemove \nnudity\nPrompt: “ken doll, commoner, concept art, art in the style of Mesopotamia 3000 …”\nPrompt: “painting of a cyberpunk android … fully clothed in futuristic armor …”\nRemove \nnudity\nFigure 3. Qualitative comparisons of different approaches, in-\ncluding TraSCE and SAFREE, on the I2P dataset. Our method\nremoves nudity without significantly altering the generated images,\nresulting in outputs that are better aligned with the input prompt.\nrequire no training but involve weight updates, such as\nUCE (Gandikota et al., 2024) and RECE (Gong et al., 2025).\nWe also try a variant of our model, where we steer in the op-\nposite direction of the layer activation corresponding to the\nnull text used for classifier-free guidance (Ho & Salimans,\n2022), which we refer to as negative steering.\n4.1.1. STEERING NUDITY CONCEPT\nAs shown in Table 1, our approach achieves state-of-the-art\nperformance in steering unsafe concepts, yielding the lowest\nASR (0.36) on the I2P dataset and surpassing the previous\nbest method. We note that incorporating negative steering\nslightly improves performance, demonstrating that our con-\ncept vector effectively models abstract concepts and, similar\nto negative prompting, yields a slight improvement in per-\nformance. Notably, our approach even outperforms both\ntraining-based methods (Gandikota et al., 2023; Kumari\net al., 2023; Lu et al., 2024; Heng & Soh, 2024) and weight-\nSD 1.4\nOurs\nRemove \nnudity\nRemove \nnudity\nFigure 4. Qualitative examples from the I2P dataset.\nOur\nmethod allows fine-grained control over the removal of specific\nconcepts, removing only the intended concept while preserving\nthe overall structure and style of the generated images.\nPrompt: “… future bass girl unwrapped smooth body fabric unfolds statue bust … front and side view body …”\nλ = −0.3\nλ = −0.25\nλ = −0.2\nλ = −0.15\nλ = −0.1\nλ = −0.05\nλ = 0\nFigure 5. Qualitative example from the I2P dataset with FLUX.\nOur method is model-agnostic and can be applied to both U-Net-\nbased SD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.\nSD 1.4\nOurs\nRemove \nviolence\nRemove \nviolence\nFigure 6. Qualitative examples from the Ring-A-Bell dataset.\nOur method successfully removes the abstract concept of violence,\nas shown by the absence of blood in the right images. The images\nare intentionally blurred for display purposes as they are disturbing.\nupdating methods (Gandikota et al., 2024; Gong et al., 2025),\nunderscoring the effectiveness of our method. Furthermore,\nour method achieved the highest prompt-image correspon-\ndence, as indicated by the CLIP score on the COCO dataset\n(30.8), ranking just below the original SD 1.4 model (31.3).\nThis is demonstrated in Fig. 3 and Fig. 4, where previous\nmethods sometimes generate unrelated images when the\nprompt triggers unsafe content. By contrast, our method\nsuccessfully removes nudity while preserving the overall\nstructure and maintaining alignment with the input prompt.\nMoreover, as shown in Fig. 5, we demonstrate that our\nmethod can also steer the DiT-based (Peebles & Xie, 2023)\nFLUX (Labs, 2023) model in an out-of-the-box manner.\n4.1.2. STEERING VIOLENCE CONCEPT\nWe also evaluate our method’s performance in suppress-\ning violent content generation, as presented in Table 2. As\nshown in Fig. 6, our method effectively reduces the genera-\ntion of violent content compared to existing training-based\nand weight-update-based methods. Although SLD-Max\n5\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 2. Performance comparison across different methods on\nthe Ring-A-Bell-Union (Violence) dataset. Lower values indicate\nbetter performance. Our method demonstrates competitive perfor-\nmance without compromising generation quality, as indicated by\nthe FID scores in Table 1. Bold: best. Underline: second-best.\nGray : require training and weight updates, Pink : do not require\ntraining but update model weights, Blue : do not require either.\nMETHOD\nRING-A-BELL-UNION\n(VIOLENCE)↓\nSDV1.4\n99.6\nESD (GANDIKOTA ET AL., 2023)\n86.0\nFNM (ZHANG ET AL., 2024)\n98.8\nCA (KUMARI ET AL., 2023)\n100.0\nUCE (GANDIKOTA ET AL., 2024)\n89.8\nRECE (GONG ET AL., 2025)\n89.2\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n40.4\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n80.4\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n97.2\nSD-NP\n94.8\nTRASCE (JAIN ET AL., 2024)\n72.4\nOURS\n43.7\nPrompt: “geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k, ”\nC=“Minimal-\nist”\nC=“Zoom-in, \nmagnify”\nFigure 7. Photographic style manipulation of SD 1.4 for the\ngiven prompt “geodesic landscape, john chamberlain, christopher\nbalaskas, tadao ando, 4 k, ” where concept prompts are “minimalist”\n(Top) and “zoom-in, magnify” (Bottom), respectively. In the top\nrow, the image is manipulated toward a maximalist style as λ →\n−1, while it adopts a minimalist style as λ →1. Similarly, in the\nbottom row, the image appears zoomed out and becomes blurred\nas λ →−1, whereas it becomes zoomed in and clearer as λ →1.\nachieves slightly better performance than ours, it signifi-\ncantly degrades overall image quality, yielding an FID of\n28.75 compared to 18.67 for our approach (Table 1).\n4.2. Steering of photographic styles and object\nattributes\nSetup: In this section, we demonstrate the effectiveness\nof steering photographic styles and object attributes. We\ntrain a k-SAE using features extracted from the residual\nstream of the 11th (out of 12) layer of the text encoder\nin SD 1.4. To observe the effect of photographic style\nchanges, we designed a dataset dedicated to 40 photographic\nstyles, including black-and-white, HDR, minimalist, etc.\nFor each class, we generated 100 prompts, totaling around\n4000 prompts, by querying ChatGPT. We also experiment\nwith SDXL-Turbo, where we train using features from both\nof its text encoders:11th (out of 12) and 29th (out of 32)\nlayers with prompts from I2P dataset.\nAs shown in Fig. 7 and Fig. 8 we can adjust its photographic\nConcept Sliders\nOurs\n𝐶=“Winter”\n𝐶= “Low light”\nSDXL-Turbo\nPrompt: “A photo of a forest, realistic, 8k”\nPrompt: “A photo of a tree with a bench, realistic, 8k”\nFigure 8. Qualitative comparisons with weather Concept Slid-\ners on SDXL-Turbo. Note that Concept Sliders train specific\nsliders: winter weather slider and a dark weather slider, whereas\nour method trains a k-SAE only once for different concepts. Top:\n“A photo of a tree with a bench, realistic, 8k” with concept to steer\n= “winter.” Bottom: “A photo of a forest, realistic, 8k” with the\nconcept to steer = “low light.” Notice how in the top image our\nmethod also removes leaves while in the bottom image, our method\neffectively applies a low-light effect to the original image.\n= 0\n= 0.1\n= 0.3\n= 0.4\n= 0.7\n= 0.9\nFigure 9. Image composition manipulation using SDXL-Turbo\nfor the prompt “A dog” with the concept prompt “Full shot.” Notice\nhow as λ →1, the generated image transitions from a close-up of\nthe face to a full shot.\nstyle, including “zoom-in” and “minimalist.” In Fig. 8, we\ncompare our results with Concept Sliders (Gandikota et al.,\n2025) on SDXL-Turbo where Concept Sliders train sepa-\nrate models for each weather condition style. Remarkably,\nour method can effectively steer concepts like weather con-\nditions and photographic styles. We note that I2P dataset\nin addition to the semantic concepts such as nudity and\nviolence, also had descriptors about general photographic\nstyles such as “full shot” or seasons “winter”. We believe\nthat k-SAE internalized these concepts offering us a pow-\nerful tool to surgically steer them. This powerful result\nhighlights the generalizable capability of k-SAEs to learn\ndiverse monosemantic concepts. This is corroborated by\nour results in Fig. 9, where we show that our method can\nmanipulate image compositions, changing a close-up image\nof a dog into a “full shot” of a dog while preserving the\nappearance of its head part.\nFinally, in Fig. 10, we use the same k-SAE to effectively ma-\nnipulates object attributes. Here, we inject a concept for an\nobject present in the image, such as “blue [object]” or “tree\n6\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\n= 0\n= 0.1\n= 0.2\n= 0.3\n= 0.4\n= 0.5\nFigure 10. Object attribute manipulation of SDXL-Turbo for\nthe given prompts “A car” (Top) and “A photo of a tree” (Bottom),\nwhere the concept prompts are “A blue car” (Top) and “Tree with\nautumn leaves” (Bottom). By adjusting λ, our method transitions\nthe image toward the desired concept specified by the prompts.\nwith autumn leaves.” We note that the resulting generations\npreserve most of the original content while successfully in-\njecting the desired concept. These results demonstrate the\nuniversal applicability of a k-SAE without the need to train\nseparate adapters for each concept. We wish to continue\nexploring the limits of universality of k-SAEs in the future.\n4.3. Robustness to adversarial prompt manipulation\nNext, we demonstrate the robustness of our method\nagainst adversarial prompts on four datasets:\nred-\nteaming approaches like Ring-A-Bell (Tsai et al., 2023),\nP4D (Chin et al., 2023), and attack frameworks like MMA-\nDiffusion (Yang et al., 2024) and UnlearnDiffAtk (Zhang\net al., 2025). Adversarial prompts often consist of several\nnon-English phrases or nonsensical text fragments that lack\nsemantic meaning, but fool the underlying generative mod-\nels to produce unsafe content. We follow the same setup in\nSec. 4.1 and use a k-SAE trained on I2P prompts.\nAs shown in Table 3, our method achieves the best overall\nrobustness on average across all datasets, significantly out-\nperforming the most recent works TraSCE (Jain et al., 2024)\nby 1.23% and SAFREE (Yoon et al., 2024) by 20.01%.\nSpecifically, for the MMA-Diffusion and P4D datasets, our\nmethod achieves state-of-the-art results with improvements\nof 10.60% and 1.98%, respectively. This demonstrates\nthat our method performs very well and can implicitly\nidentify monosemantic interpretable directions for “nudity”\nwithin the latent space of adversarial prompts. Notably, our\nmethod outperforms RECE (Gong et al., 2025) specifically\ndesigned for tackling adversarial prompts by 4.48%. For\nother datasets, our method ranks second-best or performs\ncomparably to the best scores. We note that k-SAE is trained\non text embeddings from I2P prompts to learn unsafe con-\ncepts and is not exposed to adversarial datasets. Remarkable\nperformance in adversarial datasets demonstrates k-SAE\ngeneralizes well to unseen prompts, even without exposure\nto prompt embeddings from different distributions, similar\nobservation to Sec. 4.2. We reiterate that once a k-SAE is\ntrained on unsafe concepts, our method does not require\nλ = 0\nλ = −0.1\nλ = −0.2\nλ = −0.3\nλ = −0.4\nλ = −0.5\nFigure 11. Effect of steering strength parameter (λ) on the I2P\ndataset while we steer nudity. Notice how as λ →−0.5, the\npresence of nudity disappears completely.\nretraining.\n4.4. Efficiency of Concept Steerer\nAs shown in Table 4, our method achieves the fastest in-\nference time among all other inference-based approaches,\nwith only a 0.14 sec./sample overhead on a single L40S\nGPU compared to the original SD 1.4. We highlight that our\nmethod is approximately 5x faster than the previous state-\nof-the-art (Jain et al., 2024) in unsafe concept removal.\n4.5. Ablation Studies\nFinally, we analyze the impact of our design choices on the\noverall steering capacity and visual quality.\nEffect of Concept Steering on Visual Quality: To evaluate\nthe impact of our approach on visual quality, we conduct a\nuser study using 50 randomly selected safe images gener-\nated by the original SD 1.4 model and nudity-steered images\nproduced by applying our method on SD 1.4. We followed\nthe setup described in Sec 4.1. The study involved 22 par-\nticipants, who were shown images in a randomized order\nand were asked to select the image they preferred most\nbased purely on overall visual quality. 44.7% of users pre-\nferred images produced by concept steering, while 44.9%\npreferred images from SD 1.4, indicating that participants\nexpressed an almost equal preference for both generations.\nThis is a crucial finding because it shows that our method\ndoes not deteriorate visual quality from the base model but\noffers the additional benefit of controllability.\nEffect of Layer Selection on Steering: We examine how\nthe selection of different layers in the text encoder impacts\nthe semantic information captured in k-SAE and thereby\nconcept steering. As shown in Table 5, representations\nfrom later layers are more effective to remove nudity and\nsteering than earlier layers. We believe that earlier layers\ncapture more low-level semantic information, thus high-\nlevel concepts such as nudity are better captured in the\nlater layers, making them suitable candidates for steering.\nSimilar observations were reported in Toker et al. (2024).\n7\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nTable 3. Attack Success Rate (ASR) of different methods on various adversarial attack datasets. Lower ASR indicates better\nperformance. Our method achieves the best overall robustness on average across all datasets by effectively removing nudity implicitly\nembedded in the model. Bold: best. Underline: second-best. Gray : require training and weight updates, Pink : do not require training\nbut update model weights, Blue : do not require either.\nMETHOD\nRING-A-BELL ↓\nMMA-DIFFUSION ↓\nP4D ↓\nUNLEARNDIFFATK ↓\nAVG ↓\nK77\nK38\nK16\nAVG\nSDV1.4\n85.26\n87.37\n93.68\n88.10\n95.70\n98.70\n69.70\n87.05\nSA (HENG & SOH, 2024)\n63.15\n56.84\n56.84\n58.94\n47.68\n12.68\n2.81\n30.53\nCA (KUMARI ET AL., 2023)\n86.32\n91.69\n94.26\n90.76\n10.60\n5.63\n1.04\n27.01\nESD (GANDIKOTA ET AL., 2023)\n20.00\n29.47\n35.79\n28.42\n9.27\n15.49\n2.87\n14.51\nMACE (LU ET AL., 2024)\n2.10\n0.00\n0.00\n0.70\n2.72\n2.82\n1.51\n1.94\nUCE (GANDIKOTA ET AL., 2024)\n10.52\n9.47\n12.61\n10.87\n29.93\n9.86\n0.87\n12.38\nRECE (GONG ET AL., 2025)\n5.26\n4.21\n5.26\n4.91\n21.77\n5.63\n0.72\n8.76\nSLD-MAX (SCHRAMOWSKI ET AL., 2023)\n23.16\n32.63\n42.11\n32.63\n35.76\n9.14\n2.44\n20.24\nSLD-STRONG (SCHRAMOWSKI ET AL., 2023)\n56.84\n64.21\n61.05\n60.70\n68.21\n33.10\n3.10\n41.28\nSLD-MEDIUM (SCHRAMOWSKI ET AL., 2023)\n92.63\n88.42\n91.05\n90.70\n68.21\n24.00\n1.98\n46.72\nSD-NP\n17.89\n40.42\n34.74\n31.68\n24.00\n10.00\n1.46\n16.29\nSAFREE (YOON ET AL., 2024)\n35.78\n47.36\n55.78\n46.31\n40.82\n10.56\n1.45\n24.29\nTRASCE (JAIN ET AL., 2024)\n1.05\n2.10\n2.10\n1.75\n16.60\n3.97\n0.70\n5.51\nOURS\n3.16\n8.42\n9.47\n7.02\n6.00\n1.99\n2.11\n4.28\nTable 4. Model Efficiency Comparison. Experiments were con-\nducted on a single L40S GPU on P4D dataset (150 samples in\ntotal) for the task of removing nudity.\nMETHOD\nINFERENCE TIME (S/SAMPLE) ↓\nSD 1.4\n3.02\nSAFREE (YOON ET AL., 2024)\n4.24\nTRASCE (JAIN ET AL., 2024)\n15.62\nOURS\n3.16\nTable 5. Attack Success Rate (ASR) when representations from\ndifferent encoder layers are used to train k-SAE on the I2P dataset.\nThe 10th layer yields the lowest ASR, indicating that this layer\ncaptures most information about nudity concept. k-SAE expansion\nfactor = 4, hidden neurons (n) = 3072.\nLAYERS\nASR ON I2P ↓\n12\n1.02\n10\n0.36\n8\n0.45\n6\n1.72\n4\n3.85\nEffect of k-SAE capacity on steering: We investigate the\neffect of k-SAE capacity determined by different expansion\nfactors on steering results. From Table 6, we note that\nthe performance differences between capacities is relatively\nminor, using an expansion factor of 4 proves to be the most\neffective in removing nudity.\nEffect of steering strength λ: Finally, we investigate the\neffect of the steering strength, λ. Table 7 illustrates the\nimpact of λ, showing that decreasing its value enables more\neffective removal of nudity from a greater number of images.\nAs shown in the first and second rows of Fig. 11, setting\nλ = −0.1 effectively removes the nudity concept in most\nimages. However, smaller λ values lead to a more complete\nremoval, as demonstrated in the last row of Fig. 11.\nTable 6. Attack Success Rate (ASR) for different expansion\nfactors of k-SAE trained on text embeddings extracted from the\n10th layer of the I2P prompts. An expansion factor of 4 yields the\nlowest ASR, indicating its efficacy for steering.\nEXPANSION FACTOR\nCAPACITY\nASR ON I2P↓\n4\n3072\n0.36\n8\n6144\n0.51\n16\n12288\n0.47\n32\n24576\n0.49\n64\n49152\n0.53\nTable 7. Attack Success Rate (ASR) for different values of λ of\nk-SAE with an expansion factor of 4 trained on text embeddings\nof 10th layer on the I2P dataset. λ = −0.5 yields the lowest ASR.\nλ\nASR ON I2P ↓\n−0.1\n2.59\n−0.2\n1.23\n−0.3\n0.87\n−0.4\n0.60\n−0.5\n0.36\n5. Discussion and Future Work\nWe propose a novel framework leveraging k-SAEs to enable\nefficient and interpretable concept manipulation in diffu-\nsion models. Once trained, k-SAE serves as a Concept\nSteerer to precisely control specific visual concepts (e.g.,\nnudity, violence, etc.) Our extensive experiments demon-\nstrate that our approach is very simple, does not compromise\nthe generation quality, and is robust to adversarial prompt\nmanipulations. Currently, we steer concepts by extract-\ning representations from the text encoder of the generative\nmodels. In future, we wish to explore steering via visual em-\nbeddings and allow users more control by selecting regions\nin an image and locally steer.\n8\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nImpact Statement\nAs text-to-image models are increasingly integrated into\nhigh-stakes applications, discouraging unsafe generations\nis of paramount significance. This work presents an ef-\nfective approach for identifying and suppressing unsafe\nconcept directions across various generative models. By\nimproving the controllability and reliability of generative\nmodels, our method advances the development of safer\nAI systems, facilitating their responsible deployment in\nreal-world applications.\nCode is available at: https:\n//github.com/kim-dahye/steerers\nReferences\nBansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S.,\nGoldblum, M., Geiping, J., and Goldstein, T. Universal\nguidance for diffusion models. In CVPR, 2023.\nBedapudi, P. Nudenet: Neural nets for nudity classification,\ndetection and selective censoring, 2019.\nBrack, M., Schramowski, P., Friedrich, F., Hintersdorf, D.,\nand Kersting, K. The stable artist: Steering semantics in\ndiffusion latent space. arXiv preprint arXiv:2212.06013,\n2022.\nBrack, M., Friedrich, F., Hintersdorf, D., Struppek, L.,\nSchramowski, P., and Kersting, K. Sega: Instructing\ntext-to-image models using semantic guidance. NeurIPS,\n2023.\nBricken, T., Templeton, A., Batson, J., Chen, B., Jermyn,\nA., Conerly, T., Turner, N., Anil, C., Denison, C.,\nAskell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer,\nN., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin,\nA., Nguyen, K., McLean, B., Burke, J. E., Hume,\nT., Carter, S., Henighan, T., and Olah, C.\nTowards\nmonosemanticity: Decomposing language models with\ndictionary learning.\nTransformer Circuits Thread,\n2023. https://transformer-circuits.pub/\n2023/monosemantic-features/index.html.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\nLearning to follow image editing instructions. In CVPR,\n2023.\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible scaling laws for contrastive\nlanguage-image learning. In CVPR, 2023.\nChin, Z.-Y., Jiang, C.-M., Huang, C.-C., Chen, P.-Y., and\nChiu, W.-C. Prompting4debugging: Red-teaming text-to-\nimage diffusion models by finding problematic prompts.\narXiv preprint arXiv:2309.06135, 2023.\nCunningham, H., Ewart, A., Riggs, L., Huben, R., and\nSharkey, L.\nSparse autoencoders find highly inter-\npretable features in language models. arXiv preprint\narXiv:2309.08600, 2023.\nDaujotas, G. Interpreting and steering features in images.\nLessWrong, 2024. https://www.lesswrong.com/\nposts/Quqekpvx8BGMMcaem/interpreting-\nand-steering-features-in-images.\nEsposito, P., Atighehchian, P., Germanidis, A., and Ghadi-\nyaram, D. Mitigating stereotypical biases in text to image\ngenerative systems. arXiv preprint arXiv:2310.06904,\n2023.\nFry,\nH.\nTowards\nmultimodal\ninterpretabil-\nity:\nLearning\nsparse\ninterpretable\nfeatures\nin\nvision\ntransformers.\nLessWrong,\n2024.\nhttps://www.lesswrong.com/posts/\nbCtbuWraqYTDtuARg/towards-multimodal-\ninterpretability-learning-sparse.\nGandikota, R., Materzynska, J., Fiotto-Kaufman, J., and\nBau, D. Erasing concepts from diffusion models. In\nICCV, 2023.\nGandikota, R., Orgad, H., Belinkov, Y., Materzy´nska, J.,\nand Bau, D. Unified concept editing in diffusion models.\n2024.\nGandikota, R., Materzy´nska, J., Zhou, T., Torralba, A., and\nBau, D. Concept sliders: Lora adaptors for precise control\nin diffusion models. In ECCV, 2025.\nGao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R.,\nRadford, A., Sutskever, I., Leike, J., and Wu, J. Scal-\ning and evaluating sparse autoencoders. arXiv preprint\narXiv:2406.04093, 2024.\nGong, C., Chen, K., Wei, Z., Chen, J., and Jiang, Y.-G.\nReliable and efficient concept erasure of text-to-image\ndiffusion models. In ECCV, 2025.\nHeng, A. and Soh, H.\nSelective amnesia: A continual\nlearning approach to forgetting in deep generative models.\nNeurIPS, 36, 2024.\nHessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,\nY. Clipscore: A reference-free evaluation metric for im-\nage captioning. arXiv preprint arXiv:2104.08718, 2021.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. NeurIPS, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n9\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models. arXiv preprint arXiv:2106.09685,\n2021.\nJain, A., Kobayashi, Y., Shibuya, T., Takida, Y., Memon, N.,\nTogelius, J., and Mitsufuji, Y. Trasce: Trajectory steering\nfor concept erasure. arXiv preprint arXiv:2412.07658,\n2024.\nKazerouni, A., Aghdam, E. K., Heidari, M., Azad, R.,\nFayyaz, M., Hacihaliloglu, I., and Merhof, D. Diffusion\nmodels in medical imaging: A comprehensive survey.\nMedIA, 2023.\nKim, D., Thomas, X., and Ghadiyaram, D. Revelio: Inter-\npreting and leveraging semantic information in diffusion\nmodels. arXiv preprint arXiv:2411.16725, 2024.\nKingma, D. P. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\nKumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang,\nR., and Zhu, J.-Y. Ablating concepts in text-to-image\ndiffusion models. In ICCV, 2023.\nLabs, B. F.\nFlux.\nhttps://github.com/black-\nforest-labs/flux, 2023.\nLi, X., Yang, Y., Deng, J., Yan, C., Chen, Y., Ji, X.,\nand Xu, W. Safegen: Mitigating sexually explicit con-\ntent generation in text-to-image models. arXiv preprint\narXiv:2404.06666, 2024.\nLu, S., Wang, Z., Li, L., Liu, Y., and Kong, A. W.-K. Mace:\nMass concept erasure in diffusion models. In CVPR,\n2024.\nMakhzani, A. and Frey, B. K-sparse autoencoders. arXiv\npreprint arXiv:1312.5663, 2013.\nMazzone, M. and Elgammal, A. Art, creativity, and the\npotential of artificial intelligence. In Arts, 2019.\nNg, A. et al. Sparse autoencoder. CS294A Lecture notes,\n2011.\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In ICCV, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research, 2020.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022.\nRando, J., Paleka, D., Lindner, D., Heim, L., and Tram`er,\nF. Red-teaming the stable diffusion safety filter. arXiv\npreprint arXiv:2210.04610, 2022.\nRodriguez, P., Blaas, A., Klein, M., Zappella, L., Apos-\ntoloff, N., Cuturi, M., and Suau, X. Controlling language\nand diffusion models by transporting activations. arXiv\npreprint arXiv:2410.23054, 2024.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In CVPR, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\nsion models with deep language understanding. NeurIPS,\n2022.\nSauer, A., Lorenz, D., Blattmann, A., and Rombach, R.\nAdversarial diffusion distillation. In ECCV, 2025.\nSchramowski, P., Tauchmann, C., and Kersting, K. Can\nmachines help us answering question 16 in datasheets,\nand in turn reflecting on inappropriate content? In FAccT,\n2022.\nSchramowski, P., Brack, M., Deiseroth, B., and Kersting, K.\nSafe latent diffusion: Mitigating inappropriate degenera-\ntion in diffusion models. In CVPR, 2023.\nSharkey,\nL.,\nBraun,\nD.,\nand Millidge,\nB.\nTak-\ning\nfeatures\nout\nof\nsuperposition\nwith\nsparse\nautoencoders.\nAI\nAlignment\nForum,\n2023.\nhttps://www.alignmentforum.org/posts/\nz6QQJbtpkEAX3Aojj/interim-research-\nreport-taking-features-out-of-\nsuperposition.\nSinghal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McK-\neown, K., and Ranganath, R. A general framework for\ninference-time scaling and steering of diffusion models.\narXiv preprint arXiv:2501.06848, 2025.\nSridhar, D. and Vasconcelos, N. Prompt sliders for fine-\ngrained control, editing and erasing of concepts in diffu-\nsion models. arXiv preprint arXiv:2409.16535, 2024.\nStracke, N., Baumann, S. A., Susskind, J., Bautista, M. A.,\nand Ommer, B. Ctrloralter: Conditional loradapter for\nefficient 0-shot control and altering of t2i models. In\nECCV, 2025.\n10\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nSurkov, V., Wendler, C., Terekhov, M., Deschenaux, J.,\nWest, R., and Gulcehre, C. Unpacking sdxl turbo: Inter-\npreting text-to-image models with sparse autoencoders.\narXiv preprint arXiv:2410.22366, 2024.\nTibshirani, R. Regression shrinkage and selection via the\nlasso. Journal of the Royal Statistical Society Series B:\nStatistical Methodology, 1996.\nToker, M., Orgad, H., Ventura, M., Arad, D., and Belinkov,\nY. Diffusion lens: Interpreting text encoders in text-to-\nimage pipelines. arXiv preprint arXiv:2403.05846, 2024.\nTsai, Y.-L., Hsu, C.-Y., Xie, C., Lin, C.-H., Chen, J.-Y., Li,\nB., Chen, P.-Y., Yu, C.-M., and Huang, C.-Y. Ring-a-bell!\nhow reliable are concept removal methods for diffusion\nmodels? arXiv preprint arXiv:2310.10012, 2023.\nUehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine,\nS., and Biancalani, T. Reward-guided controlled gener-\nation for inference-time alignment in diffusion models:\nTutorial and review. arXiv preprint arXiv:2501.09685,\n2025.\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\nN. Diffusion model alignment using direct preference\noptimization. In CVPR, 2024.\nWu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human\npreference score: Better aligning text-to-image models\nwith human preference. In ICCV, 2023.\nYan, H., Xiang, Y., Chen, G., Wang, Y., Gui, L., and He, Y.\nEncourage or inhibit monosemanticity? revisit monose-\nmanticity from a feature decorrelation perspective. arXiv\npreprint arXiv:2406.17969, 2024.\nYang, Y., Gao, R., Wang, X., Ho, T.-Y., Xu, N., and Xu, Q.\nMma-diffusion: Multimodal attack on diffusion models.\nIn CVPR, 2024.\nYoon, J., Yu, S., Patil, V., Yao, H., and Bansal, M. Safree:\nTraining-free and adaptive guard for safe text-to-image\nand video generation. arXiv preprint arXiv:2410.12761,\n2024.\nZhang, G., Wang, K., Xu, X., Wang, Z., and Shi, H. Forget-\nme-not: Learning to forget in text-to-image diffusion\nmodels. In CVPR, 2024.\nZhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J.,\nDing, K., and Liu, S. To generate or not? safety-driven un-\nlearned diffusion models are still easy to generate unsafe\nimages... for now. In ECCV, 2025.\n11\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nAppendix\nA. Implementation details\nTraining k-SAE with FLUX: For FLUX.1-dev (Labs, 2023) visualization, we train k-SAE using features extracted from\nthe residual stream of the 23rd (out of 24) layer of the T5-XXL text encoder on prompts from the I2P dataset. The k-SAE is\ntrained with k = 64 and an expansion factor of 16, resulting in a total hidden size dimension n = 65536.\nText encoders of diffusion models: We extract text embeddings for k-SAE from CLIP ViT-L/14 (Radford et al., 2021) for\nSD 1.4, OpenCLIP-ViT/G (Cherti et al., 2023) and CLIP-ViT/L for SDXL-Turbo, and T5-XXL (Raffel et al., 2020) for\nFLUX.1-dev.\nB. More details of the benchmarks\nWe evaluate our method for unsafe concept removal tasks on five publicly available inappropriate or adversarial prompts\ndatasets following prior work (Jain et al., 2024): I2P3 (Schramowski et al., 2023), Ring-A-Bell4 (Tsai et al., 2023),\nP4D5 (Chin et al., 2023), MMA-Diffusion6 (Yang et al., 2024), and UnlearnDiffAtk7 (Zhang et al., 2025). I2P contains 4703\nreal user prompts that are likely to produce inappropriate images. Ring-A-Bell consists of two inappropriate categories:\nnudity and violence. For nudity, it contains 95 unsafe prompts for each split (K77, K38, and K16). For violence, we use the\nRing-A-Bell Union dataset, which includes 750 prompts. P4D contains 151 unsafe prompts generated by white-box attacks\non the ESD (Gandikota et al., 2023) and SLD (Schramowski et al., 2023). MMA-Diffusion contains 1000 strong adversarial\nprompts generated via a black-box attack. UnlearnDiffAtk contains 142 adversarial prompts generated using white-box\nadversarial attacks.\nC. Additional qualitative results\nIn this section, we provide additional qualitative results.\nSteering nudity concept on inappropriate dataset: Figure 12 presents additional qualitative results using FLUX on\nprompts from I2P dataset. Our method effectively removes the abstract concept of nudity in DiT-based FLUX in an\nout-of-the-box manner.\nSteering nudity concept on adversarial dataset: Figure 13 presents qualitative comparisons with different methods on\nthe P4D dataset. Since P4D contains adversarial prompts specifically designed to challenge generative models, previous\nmethods either fail by generating unsafe images or produce unrelated images as a defense mechanism when the prompt\ntriggers to generate unsafe content (middle row). In contrast, our method successfully removes nudity while preserving the\noverall structure and maintaining alignment with the input prompt, even when the prompt itself is nonsensical (first and last\nrow).\nSteering violent concept: Figure 14 presents qualitative examples on the Ring-A-Bell dataset for violent concept removal.\nOur method effectively removes the abstract concept of violence by eliminating visual cues such as blood and firearms.\nSteering photographic styles: Figure 15 presents qualitative examples of photographic style manipulations in SD 1.4,\nincluding “HDR,” “Black and White,” “Sepia Tone,” and “Astrophotography.” We note that as λ →0.5, the generated image\ngradually transitions to the desired concept.\nSteering object attributes: Figure 16 presents qualitative examples of object attributes manipulations in SDXL-Turbo.\nGiven a prompt, we inject a concept for an object present in the image, such as “an orange cake” and “a chocolate cake.” We\nnote that the resulting generations preserve most of the original content while successfully injecting the desired concept.\n3https://huggingface.co/datasets/AIML-TUDA/i2p\n4https://huggingface.co/datasets/Chia15/RingABell-Nudity\n5https://huggingface.co/datasets/joycenerd/p4d\n6https://huggingface.co/datasets/YijunYang280/MMA-Diffusion-NSFW-adv-prompts-benchmark\n7https://github.com/OPTML-Group/Diffusion-MU-Attack/blob/main/prompts/nudity.csv\n12\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nPrompt: “sensual female painting, yoga, pancakes, fire smouldering, tattoos, midnight, dark … ”\nPrompt: “excellent framing lighting texture … sunlight illuminates the body as … lovely …”\nPrompt: “… photorealistic portrait of a woman's pale fat belly by sarah moon …”\nFigure 12. Qualitative example from the I2P dataset with FLUX. Our method is model-agnostic and can be applied to both U-Net-based\nSD 1.4 and SDXL-Turbo, as well as DiT-based FLUX.\n13\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nSD 1.4\nOurs\nTraSCE\nSAFREE\nRemove \nnudity\nPrompt: “… waterhouse motive waterhouse venus … madonna ngmale”\nPrompt: “… girl alier roman renaissance nude paintings”\nRemove \nnudity\nRemove \nnudity\nPrompt: “phyllis nude bath ccp bathing steamy … painting …”\nFigure 13. Qualitative comparisons of different methods, including TraSCE and SAFREE, on the P4D dataset. The P4D dataset\nconsists of adversarial prompts designed to challenge generative models. Our approach effectively removes the concept of nudity during\nthe generation process, producing safe and semantically meaningful outputs. In contrast, SAFREE fails to generate safe images, while\nTraSCE sometimes produces unrelated outputs despite the presence of semantically meaningful keywords in given prompts, such as “girl,”\n“roman,” “renaissance,” and “paintings” (middle row).\nSD 1.4\nOurs\nRemove \nviolence\nRemove \nviolence\nRemove \nviolence\nRemove \nviolence\nSD 1.4\nOurs\nFigure 14. Qualitative examples from the Ring-A-Bell dataset. Our method successfully removes the abstract concept of violence, as\nshown by the absence of blood in the right images. The images are intentionally blurred for display purposes as they are disturbing.\n14\n\nConcept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations\nPrompt: “geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k”\nC=“HDR”\nC=“Black \nand white”\nC=“Sepia \nTone”\nC=“Astro-\nphotography”\nFigure 15. Photographic style manipulation of SD 1.4 for the given prompt “geodesic landscape, john chamberlain, christopher balaskas,\ntadao ando, 4 k, ” where concept prompts are “HDR,” “Black and white,” “Sepia Tone,” and “Astrophotography,” respectively. As\nλ →0.5, the generated image gradually transitions to the desired concept.\nPrompt: “A photo of a cake, 4 k ”\nC=“A choco-\nlate cake”\nC=“A white \ncake”\nC=“A lemon \ncake”\nC=“An orange \ncake”\nFigure 16. Object attribute manipulation of SDXL-Turbo for the given prompts “A photo of a cake, 4k,” where the concept prompts\nare “A chocolate cake,” “A white cake,” “A lemon cake,” and “An orange cake,” respectively. By adjusting λ, our method transitions the\nimage toward the desired concept specified by the prompts.\n15"),
                Paper(arxiv_id='2501.19054', authors=['Ruiyu Wang', 'Yu Yuan', 'Shizhao Sun', 'Jiang Bian'], published_at=datetime.datetime(2025, 2, 5, 21, 8, 28, 323000, tzinfo=datetime.timezone.utc), title='Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models', summary='Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.', upvotes=5, thumbnail=None, content='Text-to-CAD Generation Through Infusing Visual Feedback in\nLarge Language Models\nRuiyu Wang 1 † Yu Yuan 2 † Shizhao Sun 3 Jiang Bian 3\nAbstract\nCreating Computer-Aided Design (CAD) models\nrequires significant expertise and effort. Text-to-\nCAD, which converts textual descriptions into\nCAD parametric sequences, is crucial in stream-\nlining this process. Recent studies have utilized\nground-truth parametric sequences, known as se-\nquential signals, as supervision to achieve this\ngoal. However, CAD models are inherently mul-\ntimodal, comprising parametric sequences and\ncorresponding rendered visual objects. Besides,\nthe rendering process from parametric sequences\nto visual objects is many-to-one. Therefore, both\nsequential and visual signals are critical for effec-\ntive training. In this work, we introduce CAD-\nFusion, a framework that uses Large Language\nModels (LLMs) as the backbone and alternates be-\ntween two training stages: the sequential learning\n(SL) stage and the visual feedback (VF) stage. In\nthe SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of\nlogically coherent parametric sequences. In the\nVF stage, we reward parametric sequences that\nrender into visually preferred objects and penalize\nthose that do not, allowing LLMs to learn how ren-\ndered visual objects are perceived and evaluated.\nThese two stages alternate throughout the training,\nensuring balanced learning and preserving bene-\nfits of both signals. Experiments demonstrate that\nCADFusion significantly improves performance,\nboth qualitatively and quantitatively.\n1. Introduction\nComputer-Aided Design (CAD) is indispensable for 3D\ncreation across industrial sectors (Deng et al., 2023). It rep-\nresents 3D models through a sequence of operations known\n† Work done during the internship at Microsoft Research Asia.\n1University of Toronto 2University of Science and Technology of\nChina 3Microsoft Research Asia. Correspondence to: Shizhao Sun\n<shizsu@microsoft.com>.\nPreprint. Under review.\nas a parametric sequence, which combines lines, arcs, and\ncircles to create 2D sketches and then extrude them to form\n3D models. CAD models are inherently multimodal, as they\nare constructed using parametric sequences for precise edit-\ning and manufacturing, while also being rendered as visual\nobjects for practical use, referred to as multimodal charac-\nteristic (Figure 1(b)(c)). Moreover, the process of rendering\nparametric sequences into visual objects exhibits a many-\nto-one mapping, where different parametric sequences can\nresult in identical visual objects, referred to as many-to-one\nrendering characteristic (Figure 1(d)).\nCreating CAD models demands considerable expertise\nand numerous iterations, making it complex and time-\nconsuming. Text-to-CAD (Figure 1(a)(b)), which refers to\nthe automatic generation of parametric sequences from tex-\ntual descriptions, is critical for streamlining this creation pro-\ncess. It allows designers and engineers to quickly prototype\nand iterate designs by describing their intent in natural lan-\nguage, reducing the time spent on manually creating CAD\nmodels from scratch. Additionally, it makes the creation\nprocess more accessible to individuals without extensive\ntraining, enabling wider participation.\nWhile important, Text-to-CAD has received limited atten-\ntion. Most studies do not utilize text to control CAD gener-\nation. Instead, they explore generating CAD designs from\nrandom noise (Wu et al., 2021; Xu et al., 2022), by randomly\naltering components of existing CAD designs (Xu et al.,\n2022; 2023; Zhang et al., 2024b), or from point cloud (Khan\net al., 2024a). A few studies make preliminary attempts at\nText-to-CAD (Khan et al., 2024b; Li et al., 2024b). They\ntrain Transformer-based framework with ground-truth para-\nmetric sequences as supervision, termed sequential signal.\nHowever, due to multimodal and many-to-one rendering\ncharacteristic of CAD models (Figure 1(b)(c)(d)), both the\nsequential signal and visual signal are crucial for training\na Text-to-CAD model. The sequential signal, derived from\nground-truth parametric sequences, provides critical infor-\nmation about sequence structure and parametric operations.\nWithout it, learning to generate logically coherent paramet-\nric sequences becomes challenging, as there is no direct\nsupervision for sequence structure and parametric opera-\ntions. The visual signal, obtained from rendered visual\n1\narXiv:2501.19054v2  [cs.CV]  5 Feb 2025\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nl i ne, 10, 7 <cur ve_end> l i ne, 52, 7 <cur ve_end> l i ne, 52, 55 <cur ve_end> l i ne, 10, 55 \n<cur ve_end> <l oop_end> l i ne, 12, 9<cur ve_end> l i ne, 50, 9 <cur ve_end> l i ne, 50, 53 \n<cur ve_end> l i ne, 12, 53 <cur ve_end> <l oop_end> l i ne, 11, 8 <cur ve_end> l i ne, 12, 8 \n<cur ve_end> l i ne, 12, 9 <cur ve_end> l i ne, 11, 9 <cur ve_end> <l oop_end> l i ne, 53, 8 \n<cur ve_end> l i ne, 54, 8 <cur ve_end> l i ne, 54, 9 <cur ve_end> l i ne, 53, 9 <cur ve_end> \n<l oop_end> <f ace_end> <sket ch_end> \nadd, 16, 31, 31, 31, 31, 1, 0, 0, 0, 0, 1, 0, - 1, 0, 29, 31, 48 <ext r ude_end>\nA rectangular prism with a \ntotal of five square holes. One \ncentrally located and four \nsurrounding it.\n(a) Input Prompt\n(b) CAD Design Sequence\n(c) CAD Visual Object\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nValid Design 3\nSame Design\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\n<sket ch_end>  cut ,  . . . ,  <ext r ude_end>\nValid Design 2\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end>\nl i ne,  . . . ,  <cur ve_end> . . . ,  <l oop_end> \n<sket ch_end>  add,  . . . ,  <ext r ude_end> \nValid Design 1\nGenerates \nRenders\n(d) The Many-to-one Relationship\nFigure 1. (a) and (b): Illustration of Text-to-CAD, which converts a texutal description into CAD parametric sequences. (b) and (c):\nIllustration of multimodal characteristics. CAD models are created using parametric sequences and rendered as visual objects for practical\nuse. (d): Illustration of many-to-one rendering characteristics. Different parametric sequences can produce identical visual objects.\nobjects, indicates how CAD models are perceived and evalu-\nated in practical applications. Without it, learning efficiency\nis compromised, as the goal of Text-to-CAD is for the ren-\ndered visual objects of the generated parametric sequences\nto match ground-truth visual objects. First, sequential signal\nlearning typically depends on auto-regressive generation,\nwhich emphasizes the local continuity between tokens but\nmay not fully capture the global appearance of the CAD\nmodel. Second, given the many-to-one rendering character-\nistic, multiple parametric sequences can produce the same\nvisual object. Training solely on parametric sequences may\ncause the model to give more emphasis to those present\nin the training set, overlooking other valid ones that could\nachieve the same visual outcome.\nTo this end, we propose CADFusion, a framework that\ncombines sequential and visual signals to train a Text-to-\nCAD model. It uses Large Language Models (LLMs) as its\nbackbone and alternates between two stages: the sequential\nlearning stage and the visual feedback stage. In the sequen-\ntial learning stage, LLMs are fine-tuned using ground-truth\nparametric sequences. Unlike prior works (Khan et al.,\n2024b; Li et al., 2024b) that train Transformer-based mod-\nels from scratch, we take advantage of pre-trained LLMs,\nwhich leverages their inherent natural language understand-\ning and foundational knowledge of CAD design (Makatura\net al., 2023) acquired during the extensive pre-training. In\nthe visual feedback stage, feedback derived from rendered\nvisual objects is integrated into the LLMs. This stage ad-\ndresses two critical challenges. First, the rendering process\nthat converts parametric sequences into visual objects is\nnon-differentiable, making backpropagation through this\npathway infeasible. To overcome this, we frame the problem\nas preference learning task and adopt direct preference opti-\nmization (DPO) (Rafailov et al., 2024). Specifically, pref-\nerences are assigned to the rendered visual objects, and the\nLLMs are optimized to increase the likelihood of parametric\nsequences that produce preferred visual objects while de-\ncreasing the likelihood of those that yield less preferred ones.\nThis approach enables effective training of LLMs, even with\na non-differentiable rendering pathway. Second, collecting\nreliable preference data is costly and labor-intensive. To\naddress this, we introduce an automated pipeline that uti-\nlizes large vision-language models (LVMs) to efficiently\nscore the rendered visual objects. Finally, to ensure bal-\nanced learning and retain the contributions of both signals,\nwe alternate between the sequential learning stage and the\nvisual feedback stage throughout training.\nWe summarize our main contributions as follows:\n• We propose to leverage both the sequential signal and\nvisual signal to train a Text-to-CAD model.\n• For the sequential signal, we use LLMs as the back-\nbone and fine-tune it on ground-truth parametric se-\nquences. For the visual signal, we adopt direct prefer-\nence optimization to bypass non-differentiable render-\ning and introduce a LVM-based scoring pipeline for\nefficient preference data collection. To balance both\nsignals, we alternate between the sequential learning\nand the visual feedback stage.\n• We contribute two datasets for Text-to-CAD: one with\nthe sequential signal and another with the visual signal.\n• We present qualitative and quantitative experiments to\nshowcase CADFusion’s superior ability.\n2. Related Works\nCAD Generation. CAD generation takes user requirements\nas input and generates CAD models as output.\nOn the input side, user requirements can be expressed in\ndiverse ways. Wu et al. (2021) uses random noise as input\nto generate CAD models randomly. Zhang et al. (2024b),\nXu et al. (2022) and Xu et al. (2023) modify specific parts\nof the existing CAD models to generate new ones. Khan\net al. (2024a) and Ma et al. (2024) take point cloud as input\nto produce corresponding CAD models. In contrast, our\nwork focuses on textual descriptions as input. Recent stud-\n2\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nies (Khan et al., 2024b; Li et al., 2024b) explore text-based\ninput for CAD generation. Khan et al. (2024b) proposes a\ndata annotation pipeline for synthesizing training data and a\ntransformer-based autoregressive network. Li et al. (2024b)\ndesigns an encoder-decoder framework with a cascading\ncontrastive strategy and CT-Mix to align text with paramet-\nric sequences. Unlike these studies, which rely solely on\nsequential signals, our work combines sequential and visual\nsignals for improved performance.\nOn the output side, CAD models can be represented in vari-\nous formats, including Constructive Solid Geometry (CSG),\nBoundary Representation (B-Rep) and Sketch-and-Extrude\n(SE). CSG constructs 3D models by combining basic primi-\ntives such as cubes, cylinders, and spheres, through Boolean\noperations and subtractions (Du et al., 2018; Kania et al.,\n2020; Yu et al., 2021; 2023). B-Rep represents 3D models\nusing geometric elements such as vertices, edges, and faces\n(Jayaraman et al., 2023; Wang et al., 2022; Xu et al., 2024).\nSE begins with 2D sketches composed of lines, arcs, and\ncircles, which are then extruded to form 3D models (Willis\net al., 2021; Wu et al., 2021). In this work, we adopt SE\nas it preserves the design history of CAD models, making\nthem more intuitive to edit.\nLarge Language Models (LLMs). LLMs have recently\nachieved remarkable success (Touvron et al., 2023; Brown\net al., 2020; OpenAI, 2024; Bubeck et al., 2023; Zhao et al.,\n2023). Supervised fine-tuning (SFT) is widely used to im-\nprove performance, while reinforcement learning (RL) is\noften employed to align LLM output with human prefer-\nence (Brown et al., 2020; Hong et al., 2024; Kaufmann\net al., 2024). Our work leverages SFT and RL1 but intro-\nduces two key differences. First, we utilize SFT and RL\nto learn from different signals (i.e., sequential and visual\nsignals) whereas existing work focuses on a single signal\n(i.e., sequential signals). Second, we alternate between SFT\nand RL stages to preserve contributions from both signals,\na strategy not commonly employed in prior work.\nReinforcement Learning with Human Feedback (RLHF).\nRLHF has been widely applied to align model output\nwith human preference across various domains, including\nLLMs (Brown et al., 2020; Radford et al., 2021; Meta, 2024),\ntext-to-image models (?) and text-to-video models (Wu\net al., 2024). As human annotation in RLHF is costly and\nnot easily scalable, reinforcement learning on AI feedback\n(RLAIF) (Liu et al., 2023; Zhang et al., 2024a; Lee et al.,\n2024), which leverages machine learning models to anno-\ntate data, has been proposed as a more affordable alternative\nto RLHF. Since RLHF/RLAIF pipeline are complex, di-\nrect preference optimization (DPO) (Rafailov et al., 2024),\n1We adopt DPO (Rafailov et al., 2024) in practice and refer\nto it as RL here for simplicity, as it implicitly optimizes the same\nobjective as traditional RLHF despite not being a typical RL.\nwhich directly optimize a model to adhere to human prefer-\nences, has been proposed to avoid explicit reward modeling\nor reinforcement learning. In this work, we adopt DPO to\naddress the challenge of non-differentiable rendering when\nlearning from visual signals, as it offers a simpler yet ef-\nfective solution compared to RLHF. Besides, inspired by\nRLAIF, we propose an automatic scoring pipeline for CAD\nmodels using LVMs. The generated scores are used to con-\nstruct preference data, enabling efficient learning without\nreliance on costly human annotations.\n3. Method\n3.1. Approach Overview\nLet a textual description be denoted as x, a CAD parametric\nsequence as y, and a rendered visual object as o. The render-\ning process from a parametric sequence y to a visual object\no is represented as r(·), such that o = r(y). Text-to-CAD\ninvolves learning a function f(·) that transforms the textual\ndescription x into the CAD parametric sequence y. i.e.,\ny = f(x). The goal is for the rendered visual object o of the\ngenerated parametric sequence y, i.e., o = r(y) = r(f(x)),\nto match the user’s desired visual object (Figure 1).\nCADFusion introduces a framework that combines sequen-\ntial and visual signal for training a Text-to-CAD model\n(Figure 2). It leverages Large Language Models (LLMs)\nas the backbone and alternates between two stages: the se-\nquential learning (SL) stage and the visual feedback (VF)\nstage. We denote the model after the i-th round of sequential\nlearning as f i\nSL(·) and after the i-th round of visual feed-\nback as f i\nVF(·). In the sequential learning stage, CADFusion\ntrains LLMs to learn sequence structures and parametric\noperations from ground-truth parametric sequences, guiding\nLLMs to generate logically coherent parametric sequences\n(Section 3.2). In the visual feedback stage, CADFusion\ntrains LLMs to understand how the rendered visual object\nwill be perceived and evaluated. By rewarding parametric\nsequences that render into visually preferred objects and\npenalizing those that do not, this stage encourages LLMs\nto generate parametric sequences capable of producing the\ndesired visual object (Section 3.3). These two stages are\nalternated throughout training, ensuring balanced learning\nand preserving contributions of both signals (Section 3.4).\n3.2. Sequential Learning Stage\nText-to-CAD requires a model capable of understanding tex-\ntual descriptions and generating CAD parametric sequences\nthat adhere to valid sequence formats and employ meaning-\nful parametric operations. We adopt the following strategies\nto efficiently achieve these capabilities.\n1) Model architecture. We use LLMs as the backbone, lever-\naging their strong natural language understanding and basic\n3\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n(c) The iterative training procedure\n(a) The Sequential Learning (SL)\n(b) The Visual Feedback (VF)\nPredicted CAD \nTokens\nGround truth \nSequence\nPreferred CAD\nProbability\nRejected CAD \nProbability\nPre-trained\nBackbone LLM\nInitial Sequential \nLearning\nPre-trained Backbone LLM\nText prompts\nCE Loss\nText prompts\nPreferred \nCAD object\nRejected \nCAD object\nDPO Loss\nPre-trained \nBackbone LLM\nVisual \nFeedback\nSequential\nLearning\nVisual \nFeedback\nSequential\nLearning\nN times\nOurs\nFigure 2. Overview of CADFusion. (a): The sequential learning stage trains LLMs using ground-truth CAD parametric sequences. (b):\nThe visual feedback stage rewards CAD parametric sequences that render into preferred visual objects and penalizes those that do not. (c):\nThe two stages are alternated to preserve contributions of both signals.\nCAD design knowledge (Makatura et al., 2023).\n2) CAD Parametric Sequence Format. We adopt the for-\nmat proposed by Zhang et al. (2024b) (Figure 1(b)), which\nrepresents CAD parametric sequences as text tokens rather\nthan binary representations or numerical attributes (Xu et al.,\n2022; 2023; Wu et al., 2021). This text-based format simpli-\nfies processing and interpretation by LLMs.\n3) Dataset.\nExisting CAD datasets (Wu et al., 2021)\ninclude CAD parametric sequences but lack paired tex-\ntual descriptions. To address this, we construct a dataset\nDSL = {(x, y)}M\n1 (‘SL’ for sequential learning) containing\npaired text x and CAD parametric sequences y. We first\nprompt a LVM to generate draft captions for rendered CAD\nmodel images and then refine these drafts through human\nannotation to ensure accuracy and conciseness.\n4) Training. We fine-tune the pre-trained LLMs by mini-\nmizing the discrepancy between the generated parametric\nsequence ˆy = f i\nSL(x) and the ground-truth parametric se-\nquence y using cross entropy loss, denoted as LSL:\nLSL = −E(x,y)∼DSL\n"\n1\nT\nT\nX\nt=1\nlog p(ˆy = yt|x)\n#\n,\n(1)\nwhere T is the sequence length and p(·) is the predicted\nprobability of the t-th token by the model f i\nSL(x).\nWhile existing studies (Khan et al., 2024b; Li et al., 2024b)\nalso consider sequential signals, CADFusion introduces\nthree distinctions: 1) it uses an LLM backbone to leverage\npre-trained knowledge, unlike prior work that trains Trans-\nformers from scratch; 2) it represents CAD sequences as\ntext tokens, processed with the LLM’s tokenizer, whereas\nothers use custom tokenizers; 3) its training data undergoes\nhuman annotation, while prior work relies solely on syn-\nthesized data. These enhancements enable it to outperform\nexisting approaches, even without the visual feedback stage.\n3.3. Visual Feedback Stage\nThe goal of Text-to-CAD is to ensure the rendered visual\nobject from the generated parametric sequence matches the\ndesired visual object. Relying solely on sequential signals\ncompromises training efficiency (see Section 1). To address\nthis, we incorporate visual feedback into the model already\ntrained on sequential signals (i.e., f i\nSL(x)).\nLearning Visual Feedback through DPO. A straightfor-\nward way to incorporate visual feedback is through super-\nvised learning, which minimizes the loss between the ren-\ndered visual object from the generated parametric sequence\nˆo = r(f(x)), and the ground-truth visual object o. How-\never, since the rendering process r(·) is non-differentiable,\nthis loss cannot be backpropagated to the model f(·). To\naddress this, we reformulate the task as a reward maxi-\nmization problem, where visual feedback serves as the re-\nward, enabling optimization without requiring a differen-\ntiable rendering process. Since conventional RL is computa-\ntionally expensive, we adopt direct preference optimization\n(DPO) (Rafailov et al., 2024), a simpler and more efficient\napproach that implicitly performs reward maximization.\nSpecifically, we construct a preference dataset DVF =\n{(x, ow, ol)}N\n1 where ow and ol are rendered from the para-\nmetric sequences yw and yl, representing preferred and less\npreferred visual objects, respectively. We then optimize the\nmodel to increase the likelihood of parametric sequences\nthat produce preferred visual objects (yw), while decreasing\nthe likelihood of those that yield less preferred ones (yl):\nLVF = −E(x,yw,yl)∼DVF\n(2)\n\x14\nlogσ(β log p(ˆy = yw|x)\npref(ˆy = yw|x) −β log p(ˆy = yl|x)\npref(ˆy = yl|x))\n\x15\n,\n4\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n(a) Sample sequence responses & Render Images\nBackbone after \nSequential \nLearining\n(b) Assigning visual scores to CAD objects\n(c) Form preference data \n      from eligible pairs\nInput \nCommand\n<seq><seq>\n<seq><seq>\nLVM\nMulti-Aspect \nGrading Criteria\n10/ 10\n7/ 10\n3/ 10\n9/ 10\n, ...\nCAD\nKernel\n10/10\n10/10\n7/10\n7/10\n9/10\n9/10\nFigure 3. Illustration of preference data construction. (a): Sample CAD parametric sequences and render them into visual objects. (b):\nScore the visual objects using LVMs with multi-aspect grading criteria. (c): Construct preference data based on LVM-generated scores.\nwhere p(·) is the predicted probability of a parametric se-\nquence under the current model (f i\nVF(x)), pref(·) the prob-\nability under the reference model from the last round of\nsequential learning (f i\nSL(x)) and β is scaling factor.\nConstructing Preference Data with LVM Scoring. Col-\nlecting preference data is both costly and labor-intensive.\nThe iterative use of the visual feedback stage in our frame-\nwork (Section 3.4) further highlights the need for a quick\nand efficient approach for obtaining preference data. To\naddress this, we propose leveraging the strong visual under-\nstanding capabilities of LVMs to score visual objects and\nconstruct preference data. Figure 3 outlines the pipeline.\nFirst, the textual description x is input into the finetuned\nmodel after sequential learning (f i\nSL) to generate multiple\nparametric sequences, which are then rendered into visual\nobjects (e.g., CAD images in our implementation). Next,\nthe rendered CAD images, along with an instruction detail-\ning the evaluation criteria, are input into an LVM to obtain\nscores. Finally, the CAD image with the higher score is\nregarded as the preferred one (i.e., ow), while the one with\nthe lower score is deemed as the less preferred one (i.e., ol).\nSpecifically, inspired by recent work (Liang et al., 2024)\non evaluating text-to-image generation across rich aspects,\nwe incorporate multiple evaluation criteria into the LVM\ninstruction. As shown in Figure 4, these criteria assess\nboth the appearance of CAD designs and their alignment\nwith textual descriptions: 1) shape quality evaluates the\nregularity, naturalness, and realism of the design; 2) shape\nquantity checks whether the number of components matches\nthe description; and 3) distribution ensures components are\narranged naturally, avoiding collisions or excessive spacing.\n3.4. Alternate Training\nEach stage of the training process — sequential learning and\nvisual feedback — has a specialized focus. Excessive train-\ning in one stage can lead to the degradation of skills acquired\nin the other. For example, we empirically observe that ex-\ntended training with visual feedback can impair the model’s\nability to generate well-formatted parametric sequences,\na skill developed during sequential learning. Conversely,\nprolonged training with sequential signals can weaken the\nmodel’s capacity to produce parametric sequences that ren-\ni. Shape Quality\niii. Item Distribution\nii. Shape Quantity\n... with 4 \ncircular holes\nA Trianglular \nshape that ...\n... with 4 \ncircular holes\nFigure 4. An illustrative example of the multi-aspect evaluation\ncriteria used in LVM scoring. Note that the illustrations are simpli-\nfied to conceptually represent each criterion.\nder visually natural objects, a capability enhanced during\nthe visual feedback stage. To mitigate this, we introduce an\nalternate training strategy (Figure 2(c)). The process begins\nwith the sequential learning stage, ensuring the model ac-\nquires the ability to generate logically coherent parametric\nsequences. Subsequently, the training is divided into smaller\nblocks. Within each block, the model first learns from the\nvisual signal, followed by the sequential signal, balancing\nthe two objectives effectively.\n4. Experiments\n4.1. Setups\nDatasets.\nFor the dataset used in the sequential learn-\ning stage, we use DeepCAD dataset (Wu et al., 2021)\nas the source for CAD parametric sequences (specifically\nthe version processed by Xu et al. (2022)). We construct\na dataset compromising 20k pairs of textual instructions\nand CAD parametric sequence using the techniques intro-\nduced in Section 3.2 and Appendix A.3. For the prefer-\nence data used in the visual feedback stage, we employ\nllava-onevision-qwen2-7b (Li et al., 2024a) to\nconstruct it using the method introduced in Section 3.3. For\neach iteration of the visual feedback, we generate approxi-\nmately 1,500 preference pairs, by using 1,000 text prompts\nas input, sampling 5 times per prompt, and filtering out in-\nvalid or low-quality samples. For the test set, we construct\nit by splitting the dataset used in sequential learning into\ntrain, validation, and test sets with a 90:5:5 ratio.\nImplementation Details. LLaMA-3-8b-Instruct is\nused as the LLM backbone, with a maximum token length of\n1024. For efficient fine-tuning, we adopt Low-Rank Adapta-\n5\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\ntion (LoRA) (Hu et al., 2021) with hyperparameters r = 32\nand α = 32. The initial sequential learning stage lasts for 40\nepochs with a learning rate of 1 × 10−4, using the AdamW\noptimizer. Following this, we run 5 iterations of alternat-\ning visual feedback and sequential learning stages. The\nvisual feedback stage lasts for 5 epochs on the preference\ndata, while the sequential learning stage lasts for 1 epoch\nusing the same dataset as the initial sequential learning stage.\nTraining is conducted on four NVIDIA A6000-48GB SMX\nGPUs using PyTorch Distributed Data Parallel (DDP).\nBaselines. We consider two types of baselines. The first is\na specialized model for Text-to-CAD (Khan et al., 2024b;\nLi et al., 2024b). We use Khan et al. (2024b) as our base-\nline, as Li et al. (2024b) is not open-sourced and we were\nunable to reproduce it ourselves. The second baseline is a\ngeneral model that acquires some CAD knowledge during\npre-training. We use the most powerful model, GPT-4o,\nas our baseline. Specifically, we apply few-shot learning,\nproviding 8 examples as input for GPT-4o.\nMetrics. Our evaluation focuses on assessing the alignment\nof generated CAD models with input instructions and the\noverall quality of the generated CAD models. We employ\nthe metrics at both the sequential level and visual level. First,\nto evaluate the correspondence between the ground-truth\nand generated parametric sequences, we use F1 scores fol-\nlowing Khan et al. (2024b). Specifically, we compute F1\nscore for primitives (averaged over lines, arcs, and circles\nfor brevity) and extrusions, denoted as F1-Sketch and F1-\nExtrusion. Second, to assess the quality of the generated\nCAD models, we compare the ground-truth and generated\npoint clouds. We adopt Chamfer Distance (CD) from Khan\net al. (2024b) and additional metrics from Xu et al. (2022),\nincluding Coverage (COV), which quantifies the percent-\nage of real data covered by generated samples using CD;\nMinimum Matching Distance (MMD), which evaluates the\nclosest match between generated samples and real data; and\nJensen-Shannon Divergence (JSD), which measures distri-\nbution similarity. Additionally, we compute the Invalidity\nRatio (IR), which quantifies the percentage of generated\nparametric sequences that fail to render into valid visual\nobjects. Furthermore, we introduce an LVM-based met-\nric, denoted as LVM Score, to assess the visual correspon-\ndence between model predictions and input instructions. To\nthis end, we employ GPT-4o with a dedicated evaluation\nprompt. Further details are provided in Appendix C.1. Fi-\nnally, we conduct human assessments to rank generations\nfrom different baselines, denoted as Avg. Rank. Details on\nthis evaluation can be found in Appendix C.2.\n4.2. Main Results\nQuantitative Evaluation. Table 1 summarizes the quantita-\ntive results comparing CADFusion with baseline methods\n(see Appendix C.4 for more details). Compared to GPT-4o,\nCADFusion outperforms it across all metrics. This sug-\ngests that while the general model may have acquired some\nCAD knowledge during pre-training, explicitly optimiz-\ning for Text-to-CAD, as in our approach, is crucial for im-\nproving performance, Compared to Text2CAD, CADFusion\nachieves comparable or better performance on all metrics,\nwith particular strengths in metrics reflecting the visual qual-\nity such as LVM score and Avg. Rank. This highlights the\neffectiveness of incorporating visual signals in our approach,\nas opposed to Text2CAD, which relies solely on sequential\nsignals. This outcome also aligns with Khan et al. (2024b)’s\nlimitation statement that Text2CAD is limited to generating\nonly rectangular and cylindrical shapes. When faced with\ncomplex geometries, it struggles to perform effectively.\nQualitative Evaluation.\nFigure 5 compares the re-\nsults among the ground truth, our method, GPT-4o, and\nText2CAD on the test set. GPT-4o frequently fails to pro-\nduce renderable results across most test cases, which aligns\nwith its high invalidity ratio (IR) reported in Table 1. While\nit occasionally generates valid shapes, its outputs are of-\nten misaligned with the input prompts. Text2CAD generate\nwell-formed shapes without irregular edges or corners. How-\never, it often produces oversimplified shapes and, for more\ncomplex prompts, tends to generate multiple cubes or pan-\nels instead of accurately capturing the intended structure.\nThis aligns with its low invalidity ratio (IR) but poor visual\nscores. such as LVM score and Avg. Rank, in Table 1.\nCADFusion provides the most precise response to input in-\nstructions and achieves the highest similarity to the ground\ntruth. It successfully captures complex shapes, including\nrectangles, hexagons, and nested structures, such as a hexag-\nonal hole within a cylinder. Additionally, it exhibits a strong\nunderstanding of language cues, accurately interpreting nu-\nmerical and qualitative descriptors like “long" or “T-shape".\nAdditional qualitative results, as well as our model’s abil-\nity to generate multiple varied outputs, are presented and\ndiscussed in Appendix C.6 and C.7.\n4.3. Ablation Studies\nWe conduct ablation studies on the effectiveness of the vi-\nsual feedback stage, the impact of the alternate training, and\nthe choice between human and LVM annotation for data.\nVisual Feedback. To assess the importance of visual feed-\nback, we conduct an ablation study on CADFusion using\nonly sequential learning, denoted as CADFusionSL. The first\nrow of Table 2 presents its LVM score and invalidity ratio.\nCompared to our approach, denoted as CADFusionSL-VFSL(5)\nin Table 2, while CADFusionSL improves the invalidity ratio\nby 1.36%, it results in a significant decrease in the LVM\nscore. This underscores the crucial role of the visual feed-\nback stage: by leveraging visual preference data, our frame-\n6\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nF1↑\nCD↓\nCOV ↑\nMMD ↓\nJSD ↓\nIR ↓\nLVM Score ↑\nAvg. Rank ↓\nSketch\nExtrusion\nGPT-4o\n82.96\n85.72\n85.04\n72.40\n6.60\n37.93\n74.26\n5.13\n3.22\nText2CAD\n63.94\n92.13\n30.23\n-\n-\n-\n3.37\n2.01\n2.97\nCADFusion\n85.22\n92.79\n45.67\n90.40\n3.49\n17.11\n6.20\n8.96\n1.86\nTable 1. Quantative results - Test results on F1 scores including Sketch (primitive, averaged) and Extrusion, Chamfer Distance (CD),\nCoverage (COV), Minimum Matching Distance (MMD), Jensen-Shannon Divergence (JSD), Invalidity Ratio (IR), the LVM Score and\nthe average rank from human evaluation (Avg. Rank). An upward arrow (↑) indicates that higher values are better, while a downward\narrow (↓) signifies that lower values are preferred. Since Text2CAD does not release COV, MMD, and JSD, and we were unable to\ncompute them ourselves due to differences in setup, these values are unavailable.\n(10) "The shape is a hollow cylindrical band with a vertical sector \nremoved, resembling an incomplete ring."\n(4) "The 3D shape is a square hexagonal plate."\n(1) "The 3D shape is a cylinder and a hexagonal hole inside, which \nis smaller and makes the wall very thin."\n(2) "The 3D shape is a rectangular block with a semicylindrical \ncutout located at its center, forming a U-shaped channel."\n(5) "The 3D shape is a teardrop-like piece with two circular holes. \none large near the broader end and one small near the narrower end."\n(8) "The 3D shape is a trapezoid thin prism."\n(11) "The 3D shape is a hollow triangular prism. The walls are the \nsame and have a smaller thickness."\n(12) "The image shows two identical parallel long slim pipes."\n(9) "Three identical rectangular sheets placed vertically, arranged in \nparallel and evenly spaced."\n(6) "The shape is a cylinder with a square hole centered at the top, \nextending from the top to the bottom."\n(3) "The 3D shape is a hollow, semi-cylindrical structure cut \nlengthwise, resembling a half-pipe."\n(14) "The three-dimensional shape is a flattened cylinder."\n(15) "The 3D shape is a rectangular prism(cuboid)."\n(13) "The three-dimensional shape is an inverted T-shaped prism."\n(16) "The 3D shape is a combination of a rectangular prism base and a \nvertically oriented half-cylinder on top."\n(17) "A flat rectangular plate. All four corners are rounded and there is a \ncircular hole of the same diameter at each corner."\nPrompts\nGround\nTruth\nOurs\nGPT-4o\nText2CAD\n(7) "The  shape is composed of four vertical cylinders, roughly the \nsame size, unevenly distributed at the four corners."\n(20) "The 3D shape is a rectangular cuboid with rounded edges and \ncorners."\n(19) "The 3D shape is a hexagonal prism. The hollow center forms an open \nhexagonal cross-section."\n(18) " The 3D shape consists of a small thin rectangular prism in the middle \nof the right side of a rectangular prism."\nFigure 5. Qualitative results. The input prompt is shown at the top of each subsection. Images are arranged from left to right in the\nfollowing order: ground truth, CADFusion, GPT-4o, and Text2CAD. Outputs that cannot be rendered are marked with a red cross.\nCADFusion outperforms all baselines in understanding instructions and generating CAD objects that are both sequentially and visually\nhigh quality. GPT-4o frequently produces invalid samples and pays little attention to shape details. Text2CAD generates well-formed\nbasic shapes with a regular appearance but struggles to accurately follow input instructions and represent complex geometries.\nwork effectively enhances the visual quality of the generated\nCAD models. Additionally, CADFusionSL outperforms the\nbaseline method, Text2CAD, which also relies solely on se-\nquential signals. Note that this advantage is achieved using\n20k data, while Text2CAD uses 150k data. This demon-\nstrates the effectiveness of the techniques employed in our\nsequential learning stage, including leveraging LLMs as the\nbackbone, representing CAD parametric sequences as tex-\ntual tokens, and utilizing human annotations (Section 3.2).\nAlternate Training. In Section 3.4, we propose an alter-\nnate training strategy to retain the benefits of both sequen-\n7\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nLVM Score ↑\nIR ↓\nCADFusionSL\n7.69\n4.84\nCADFusionSLw/o HA\n6.56\n6.00\nCADFusionSL-VF\n5.94\n88.87\nCADFusionSL-VFRPO\n6.21\n3.46\nCADFusionSL-VFSL(1)w/ HA\n8.28\n17.03\nCADFusionSL-VFSL(1)\n8.76\n4.42\nCADFusionSL-VFSL(3)\n8.89\n4.21\nCADFusionSL-VFSL(5)\n8.96\n6.20\nTable 2. LVM scores and invalidity ratios across different CADFu-\nsion variants. The suffix SL indicates that the model is trained with\nthe initial Sequential Learning stage, while VF denotes the Visual\nFeedback stage without additional Sequential Learning. VFSL\nrepresents Visual Feedback with alternating Sequential Learning.\nThe tag w/ HA signifies that the data is preprocessed with human\nannotation, whereas w/o HA denotes the absence of human an-\nnotation. Numbers in parentheses indicate the number of VFSL\nrounds performed. RPO refers to the model using Regularized Pref-\nerence Optimization (RPO) (Liu et al., 2024) to stabilize DPO.\ntial learning and visual feedback stage. We compare this\napproach with three variations: 1) visual feedback only\n(CADFusionSL-VF), 2) visual feedback with an additional\nNegative Log Likelihood loss (CADFusionSL-VFRPO) to regu-\nlarize and stabilize DPO (Liu et al., 2024), and 3) iterative\nvisual-sequential training (our method).\nTable 2 presents the results, with our approach denoted\nas CADFusionSL-VFSL(5).\nThe high invalidity ratio of\nCADFusionSL-VF indicates that it struggles to generate ren-\nderable sequences, suggesting that extended training with\nvisual signals can impair the model’s ability to generate well-\nformatted parametric sequences. Besides, CADFusionSL-VF\nreceives a low rating from the LVM judge, revealing that\ntraining with visual feedback along provides limited bene-\nfit. Regarding CADFusionSL-VFRPO which incorporates the\nadditional loss, while it achieves low invalidity ratio, its\nvisual quality, as assessed by the LVM judge, is even lower\nthan the SL-only setup (i.e., CADFusionSL). This indicates\nthat it fails to effectively balance the contributions of both\nsequential signals and visual signals.\nWe also compare model variants that use different numbers\nof iterations of visual feedback and sequential learning. In\nTable 2, for each CADFusionSL-VFSL(*) variant, the number\nin parentheses indicates the number of alternative training\nrounds performed. The results for iterations 1, 3, and 5\nare reported, showing a gradual increase in LVM scores\nalong with a stable invalidity ratio. This further validates\nthe effectiveness of our approach.\nData Annotation. We examine the impact of our choice\nof data annotation. In the sequential learning stage, the\ndataset is constructed by first using LVMs to generate initial\ncaptions, followed by human annotators refining them. To\nevaluate the effect of this decision, we conduct an experi-\nment in which our method is trained on data without human\nannotation, denoted as CADFusionSLw/o HA. The second row\nof Table 2 presents the results. It shows worse LVM score\nand IR compared to the version using data with human anno-\ntations (CADFusionSL), highlighting the necessity of human\nannotation in the sequential learning stage.\nIn the visual feedback stage, LVMs are used to score\nCAD models and generate preference data. This design\nchoice is driven by the high cost of human annotation\nand the challenge of managing human annotators to en-\nsure consistent scoring. To evaluate the effect of this de-\ncision, we conduct an experiment where the visual feed-\nback stage of our method is trained on human-scored prefer-\nence pairs, denoted as CADFusionSL-VFSL(1)w/ HA. Compared\nto the LVM-scored version (i.e., CADFusionSL-VFSL(1)), it\nachieves a worse LVM score and IR. This aligns with our\nintuition that, while human annotation may be more ac-\ncurate, managing annotators for consistent scoring is diffi-\ncult. Furthermore, using LVM-scored preference data al-\nlows CADFusionSL-VFSL(1) to scale across more rounds of\nvisual feedback (e.g., CADFusionSL-VFSL(5)), leading to im-\nproved performance. Achieving this with human annotation\nwould be challenging and expensive.\n5. Limitation\nCADFusion’s results are overall promising. However, there\nare limitations that could be addressed in future work. First,\nmodern LVMs show a significant performance drop when\nhandling multiple images as input. Currently, we can only\nprovide LVM with a single-view image to ensure both ac-\ncurate image understanding and prompt following. This\nlimitation prevents us from achieving a more effective Vi-\nsual Feedback pipeline and evaluator. Second, CADFusion\nstruggles to generate very complex shapes that require spa-\ntial and commonsense reasoning, such as the shapes of\nletters and words (see Appendix C.8).\n6. Conclusion\nWe propose CADFusion for Text-to-CAD, the first approach\nto incorporate visual feedback from rendered CAD objects\ninto the training pipeline. CADFusion uses LLMs as back-\nbone and alternates between the sequential learning stage\nand the visual feedback stage. We conduct extensive exper-\niments to demonstrate the superiority of CADFusion and\nvalidate the effectiveness of the design choices. In the future,\nwe plan to further improve the preference data construction\npipeline to enhance performance, and collect more CAD\ndata with more complex geometric shapes to investigate\nCADFusion’s performance on more challenging cases.\n8\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nImpact Statement\nThis paper presents work aimed at improving Text-to-CAD\ngeneration through the use of LLM-based frameworks and\nthe incorporation of visual feedback. Our work has the\npotential to enhance the CAD design process, offering the\nbenefits of automation and efficiency while reducing re-\nliance on intensive training and specialized expertise. This\ncould make CAD design more accessible, particularly in\nindustries where skilled designers are in short supply or\nwhere rapid prototyping is essential.\nEthics Statement\nIn this work, we have invited crowd workers to give textual\ndescriptions to CAD models. We conducted this work in ac-\ncordance with ethical guidelines to ensure that participants\nwere treated fairly, respectfully, and safely throughout the\nprocess. We took steps to protect the privacy of crowd work-\ners by not collecting personally identifiable information.\nThe data annotated by the crowd workers was used only\nfor research purpose related to improving CAD generating\ntechniques.\nReferences\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., and\nAmodei, D. Language Models are Few-Shot Learners,\nJuly 2020. URL http://arxiv.org/abs/2005.\n14165. arXiv:2005.14165 [cs].\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lund-\nberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang,\nY. Sparks of Artificial General Intelligence: Early experi-\nments with GPT-4, April 2023. URL http://arxiv.\norg/abs/2303.12712. arXiv:2303.12712 [cs].\nDeng, Y., Chen, J., and Olechowski, A. What Sets Proficient\nand Expert Users Apart? Results of a Computer-Aided\nDesign Experiment. Journal of Mechanical Design, 146\n(1):011401, 10 2023. ISSN 1050-0472. doi: 10.1115/\n1.4063360. URL https://doi.org/10.1115/1.\n4063360.\nDu, T., Inala, J. P., Pu, Y., Spielberg, A., Schulz, A., Rus,\nD., Solar-Lezama, A., and Matusik, W. Inversecsg: au-\ntomatic conversion of 3d models to csg trees.\nACM\nTrans. Graph., 37(6), December 2018.\nISSN 0730-\n0301. doi: 10.1145/3272127.3275006. URL https:\n//doi.org/10.1145/3272127.3275006.\nHong, Z., Yuan, Z., Chen, H., Zhang, Q., Huang, F., and\nHuang, X. Knowledge-to-SQL: Enhancing SQL Genera-\ntion with Data Expert LLM, June 2024. URL http://\narxiv.org/abs/2402.11517. arXiv:2402.11517.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models, 2021. URL https://arxiv.\norg/abs/2106.09685.\nJayaraman, P. K., Lambourne, J. G., Desai, N., Willis, K.\nD. D., Sanghi, A., and Morris, N. J. W. Solidgen: An\nautoregressive model for direct b-rep synthesis, 2023.\nURL https://arxiv.org/abs/2203.13944.\nKania, K., Zi˛eba, M., and Kajdanowicz, T. Ucsg-net –\nunsupervised discovering of constructive solid geometry\ntree, 2020. URL https://arxiv.org/abs/2006.\n09102.\nKaufmann, T., Weng, P., Bengs, V., and Hüllermeier,\nE.\nA survey of reinforcement learning from human\nfeedback, 2024. URL https://arxiv.org/abs/\n2312.14925.\nKhan, M. S., Dupont, E., Ali, S. A., Cherenkova, K., Kacem,\nA., and Aouada, D. CAD-SIGNet: CAD Language Infer-\nence from Point Clouds using Layer-wise Sketch Instance\nGuided Attention, February 2024a.\nURL http://\narxiv.org/abs/2402.17678. arXiv:2402.17678\n[cs].\nKhan, M. S., Sinha, S., Sheikh, T. U., Stricker, D., Ali, S. A.,\nand Afzal, M. Z. Text2CAD: Generating Sequential CAD\nModels from Beginner-to-Expert Level Text Prompts,\nSeptember 2024b. URL http://arxiv.org/abs/\n2409.17106. arXiv:2409.17106 [cs].\nLee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J.,\nLu, K., Bishop, C., Hall, E., Carbune, V., Rastogi, A.,\nand Prakash, S. Rlaif vs. rlhf: Scaling reinforcement\nlearning from human feedback with ai feedback, 2024.\nURL https://arxiv.org/abs/2309.00267.\nLi, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H.,\nZhang, K., Zhang, P., Li, Y., Liu, Z., et al.\nLlava-\nonevision: Easy visual task transfer.\narXiv preprint\narXiv:2408.03326, 2024a.\nLi, X., Song, Y., Lou, Y., and Zhou, X. CAD translator: An\neffective drive for text to 3d parametric computer-aided\ndesign generative modeling. In ACM Multimedia 2024,\n2024b. URL https://openreview.net/forum?\nid=DN3722rnLd.\n9\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nLiang, Y., He, J., Li, G., Li, P., Klimovskiy, A., Car-\nolan, N., Sun, J., Pont-Tuset, J., Young, S., Yang, F.,\nKe, J., Dvijotham, K. D., Collins, K., Luo, Y., Li, Y.,\nKohlhoff, K. J., Ramachandran, D., and Navalpakkam,\nV. Rich Human Feedback for Text-to-Image Generation,\nApril 2024. URL http://arxiv.org/abs/2312.\n10240. arXiv:2312.10240.\nLiu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C.\nG-eval: Nlg evaluation using gpt-4 with better human\nalignment, 2023. URL https://arxiv.org/abs/\n2303.16634.\nLiu, Z., Lu, M., Zhang, S., Liu, B., Guo, H., Yang, Y.,\nBlanchet, J., and Wang, Z. Provably mitigating overopti-\nmization in rlhf: Your sft loss is implicitly an adversarial\nregularizer, 2024. URL https://arxiv.org/abs/\n2405.16436.\nMa, W., Chen, S., Lou, Y., Li, X., and Zhou, X. Draw\nstep by step: Reconstructing cad construction sequences\nfrom point clouds via multimodal diffusion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 27154–27163, June\n2024.\nMakatura, L., Foshey, M., Wang, B., HähnLein, F., Ma, P.,\nDeng, B., Tjandrasuwita, M., Spielberg, A., Owens, C. E.,\nChen, P. Y., et al. How can large language models help\nhumans in design and manufacturing? arXiv preprint\narXiv:2307.14377, 2023.\nMeta. The llama 3 herd of models, 2024. URL https:\n//arxiv.org/abs/2407.21783.\nOpenAI. Gpt-4 technical report, 2024. URL https://\narxiv.org/abs/2303.08774.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision, 2021.\nURL https://arxiv.org/abs/2103.00020.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model, 2024. URL\nhttps://arxiv.org/abs/2305.18290.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro,\nE., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and\nLample, G. Llama: Open and efficient foundation lan-\nguage models, 2023. URL https://arxiv.org/\nabs/2302.13971.\nWang, K., Zheng, J., and Zhou, Z. Neural face identification\nin a 2d wireframe projection of a manifold object, 2022.\nURL https://arxiv.org/abs/2203.04229.\nWillis, K. D. D., Jayaraman, P. K., Lambourne, J. G.,\nChu, H., and Pu, Y. Engineering sketch generation for\ncomputer-aided design, 2021. URL https://arxiv.\norg/abs/2104.09621.\nWu, R., Xiao, C., and Zheng, C. DeepCAD: A Deep Gener-\native Network for Computer-Aided Design Models, Au-\ngust 2021. URL http://arxiv.org/abs/2105.\n09492. arXiv:2105.09492 [cs].\nWu, X., Huang, S., Wang, G., Xiong, J., and Wei, F. Boost-\ning text-to-video generative model with MLLMs feed-\nback. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024. URL https:\n//openreview.net/forum?id=3ivnixHy16.\nXu, X., Willis, K. D. D., Lambourne, J. G., Cheng, C.-Y., Ja-\nyaraman, P. K., and Furukawa, Y. SkexGen: Autoregres-\nsive Generation of CAD Construction Sequences with\nDisentangled Codebooks, July 2022. URL http://\narxiv.org/abs/2207.04632. arXiv:2207.04632\n[cs].\nXu, X., Jayaraman, P. K., Lambourne, J. G., Willis,\nK. D. D., and Furukawa, Y.\nHierarchical Neu-\nral Coding for Controllable CAD Model Generation,\nJune 2023. URL http://arxiv.org/abs/2307.\n00149. arXiv:2307.00149 [cs].\nXu, X., Lambourne, J. G., Jayaraman, P. K., Wang, Z.,\nWillis, K. D. D., and Furukawa, Y. Brepgen: A b-rep\ngenerative diffusion model with structured latent geom-\netry, 2024. URL https://arxiv.org/abs/2401.\n15563.\nYu, F., Chen, Z., Li, M., Sanghi, A., Shayani, H., Mahdavi-\nAmiri, A., and Zhang, H. Capri-net: Learning compact\ncad shapes with adaptive primitive assembly, 2021. URL\nhttps://arxiv.org/abs/2104.05652.\nYu, F., Chen, Q., Tanveer, M., Amiri, A. M., and Zhang, H.\nD2csg: Unsupervised learning of compact csg trees with\ndual complements and dropouts, 2023. URL https:\n//arxiv.org/abs/2301.11497.\nZhang, J., Hou, Z., Lv, X., Cao, S., Hou, Z., Niu, Y.,\nHou, L., Dong, Y., Feng, L., and Li, J. Longreward:\nImproving long-context large language models with ai\nfeedback, 2024a. URL https://arxiv.org/abs/\n2410.21252.\nZhang, Z., Sun, S., Wang, W., Cai, D., and Bian, J. Flexcad:\nUnified and versatile controllable cad generation with\nfine-tuned large language models, 2024b. URL https:\n//arxiv.org/abs/2411.05823.\n10\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang,\nC., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang,\nX., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A Survey\nof Large Language Models, May 2023. URL http://\narxiv.org/abs/2303.18223. arXiv:2303.18223\n[cs].\n11\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nData Preprocessing\nSkexGen\nSynthesized \nText-to-CAD data\nl i ne, 1, 3 <cur ve_end> \nl i ne, 2, 5 <cur ve_end> \n<l oop_end> <f ace_end> \n<sket ch_end> \nadd, 32, 32, 32, 13, . . .  \n<ext r usi on_end>\nThe 3D shape is a right cylinder characterized by \ntwo parallel circular bases connected by a curved \nsurface. It has three surfaces in total: one curved \nlateral surface and two flat circular bases. The \nheight is the distance between the bases, and the \nradius is the distance from the center to the edge \nof the base.\nThe 3D shape is a right \ncylinder. The height is the \ndistance between the bases, \nand the radius is the distance \nfrom the center to the edge of \nthe base.\nVLM caption\nHuman \nrevision\nFlatten\nto string \n{\n\' name\' :  . . . ,\n\' l en_xy\' :  65,  \n\' l en_ext \' :  38,  \n\' l en_pi x\' :  65,  \n\' l en_cmd\' :  35,  \n\' num_se\' :  1,  \n\' se_xy\' :  [\n[\n[ 1,  3] ,\n[ 2,  5]\n]\n] ,  \n\' se_cmd\' :  [\n[ 1,  3,  2,  4,  . . . ]\n] ,  \n\' se_pi x\' :  [\n[ 1,  600,  380,  . . . ]\n] ,   \n\' se_ext \' :  [\n[ 32,  32,  32,  13,  . . . ] ,\n}\nFigure 6. An overview of the data preprocessing steps. The original dataset is transformed into captions that serve as textual inputs, while\nthe corresponding stringified CAD representations are used as ground truth references.\nA. Additional Dataset Construction Detail\nA.1. Converting Raw Data into Strings\nCADFusion’s String Format\nOur representation adopts the Sketch-and-Extrude Modeling (SEM) format, wherein a CAD\ninstance is composed of sketches and extrusions. Each sketch is structured into multiple faces, and each face comprises\nmultiple loops. Within each loop, geometric primitives such as lines, arcs, and circles are parameterized as follows:\n• Line: Represented by a line identifier and one coordinate.\n• Arc: Defined by an arc identifier and two coordinates.2\n• Circle: Represented by a circle identifier and four coordinates.\nEach extrusion is represented as a sequence formatted as BVVTTTRRRRRRRRRSOO, where the components are defined as\nfollows:\n• B: The boolean operation, selected from add, cut, intersect.\n• V: The displacements of the top and bottom planes from the reference plane.\n• T: The 3D translation vector.\n• R: The 3D rotation, represented as a quaternion or equivalent.\n• S: The scaling factor.\n• O: The center of scaling.\nConverting Source Data to CADFusion’s Format\nThe original representation is derived from the SkexGen dataset\n(Xu et al., 2022). Each CAD instance includes several components: sketch commands, sketch coordinates, and extrusion\ncommands, which are stored in the se_cmd, se_xy, and se_ext entries, respectively. The lengths of these entries\ncorrespond to the number of sketch-extrusion pairs within the complete CAD shape. To convert these entries into strings, we\niteratively describe the sketches and extrusions in our format, ensuring that the resulting sequence reflects the chronological\ndesign order of the CAD process.\n2While lines and arcs generally require 2 and 3 coordinates for representation, respectively, this work leverages a simplified\nrepresentation where the endpoints of lines and arcs are determined by the first point of the subsequent curve. If the loop is closed at the\ncurrent curve, its endpoint is determined by the first curve in the loop.\n12\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nCAD Representation in SEM format\n\' name\' :  \' 0000/ 00006625\' ,  \n\' l en_xy\' :  41,  \n\' l en_ext \' :  19,  \n\' l en_pi x\' :  41,  \n\' l en_cmd\' :  17,  \n\' num_se\' :  1,  \n\' se_xy\' :  [ ar r ay( [\n[ 13,  13] ,  [ 3,  3] ,  [ 57,  13] ,  [  3,   3] ,  [ 57,  57] ,  [  3,   3] ,  \n[ 13,  57] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 18,  22] ,  [ 18,  15] ,  [ 22,  18] ,  [ 15,  18] ,  [ 3,  3] ,  [  2,   2] ,\n[ 18,  55] ,  [ 18,  48] ,  [ 22,  52] ,  [ 15,  52] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 35,  48] ,  [ 35,  22] ,  [ 48,  35] ,  [ 22,  35] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 52,  22] ,  [ 52,  15] ,  [ 55,  18] ,  [ 48,  18] ,  [ 3,  3] ,  [  2,   2] ,  \n[ 52,  55] ,  [ 52,  48] ,  [ 55,  52] ,  [ 48,  52] ,  [ 3,  3] ,  [  2,   2] ,  \n[  1,   1] ,  [  0,   0] ]\n) ] ,  \n\' se_cmd\' :  [ ar r ay(\n[ 3,  3,  3,  3,  2,  5,  2,  5,  2,  5,  2,  5,  2,  5,  2,  1,  0]\n) ] ,  \n\' se_pi x\' :  [ ar r ay(\n[ 589,  3,  633,  3,  3449,  3,  3405,  3,  2,  1170,  722,\n   \n 918,   911,  3,  2,  3282,  2834,  3094,  3087,  3,  2,  2851,\n       1187,  2032,  2006,  3,  2,  1204,  756,  951,  944,  3,  2,\n       3316,  2868,  3127,  3120,  3,  2,  1,  0]\n) ] ,  \n\' se_ext \' :  [ ar r ay(\n[ 22,  32,  32,  32,  32,  3,  2,  2,  2,  2,  3,  \n2,  1,  2,  1,  56,  28,  28,  0]\n) ] ,  \n\' ui d\' :  470\nFlattened CAD Sequence\nCOORD_PAD = 4\nEXT_PAD = 1\nR_PAD = 2\nline,9,9 <curve_end> line,53,9 <curve_end> line,53,53 \n<curve_end> line,9,53 <curve_end> <loop_end> \ncircle,14,18,14,11,18,14,11,14 <curve_end> <loop_end> \ncircle,14,51,14,44,18,48,11,48 <curve_end> <loop_end> \ncircle,31,44,31,18,44,31,18,31 <curve_end> <loop_end> \ncircle,48,18,48,11,51,14,44,14 <curve_end> <loop_end> \ncircle,48,51,48,44,51,48,44,48 <curve_end> <loop_end> \n<face_end> <sketch_end> \nadd,21,31,31,31,31,1,0,0,0,0,1,0,-1,0,55,27,27 \n<extrude_end>\nTextual Description Retrieved via LVM Captioning\nThe 3D shape is a square plate with a large central cylindrical hole \nand four smaller cylindrical holes near each corner. It has 6 faces, 12 \nedges, and 8 vertices. The plate is relatively thin compared to its side \nlength and is likely used for mounting or structural reinforcement, \nwith the central hole for a shaft or pipe and the corner holes for bolts \nor screws.\nTextual Description After Human \nAnnotation\nThe 3D shape is a square plate with a large \ncentral cylindrical through-hole and four \nsmaller cylindrical holes near each corner. \nThe plate is relatively thin compared to its \nside length and is likely used for mounting \nor structural reinforcement, with the central \nhole for a shaft or pipe and the corner holes \nfor bolts or screws.\nFigure 7. An overview of multiple CAD representations and their corresponding captions. Left: A CAD representation in the raw SEM\nformat alongside its stringified sequence, with values highlighted in different colors based on the padding used for decoding. Right:\nCaptions generated by the LVM and refined by human annotation. Phrases removed during human fine-tuning are marked in red, while\nthose added by humans are marked in green. All representations and captions correspond to the same CAD figure, which is displayed in\nthe bottom-right corner.\nIn iteration i, we select the i-th item from the se_xy, se_cmd, and se_ext entries. For each digit in the se_cmd array,\nwe perform operations based on the command value as follows:\n• Command value = 5: Create a circle; use the first 4 items in se_xy as XY coordinates and append the <curve_end>\ntoken. Skip 5 positions in the se_xy array.\n• Command value = 4: Create an arc; use the first 2 items in se_xy as XY coordinates and append the <curve_end>\ntoken. Skip 3 positions in the se_xy array.\n• Command value = 3: Create a line; use the first item in se_xy as an XY coordinate and append the <curve_end>\ntoken. Skip 2 positions in the se_xy array.\n• Command value = 2: Mark the end of the loop by appending the <loop_end> token. Skip 1 position in the se_xy\narray.\n• Command value = 1: Mark the end of the face by appending the <face_end> token. Skip 1 position in the se_xy\narray.\n• Command value = 0: Mark the end of the sketch by appending the <sketch_end> token. Skip 1 position in the\nse_xy array.\nExtrusions are represented by the 1D array se_ext. The operation identifier is translated into a word, and the remaining\nvalues are flattened. To distinguish coordinates from special tokens, all coordinates are initially padded; they are subsequently\nunpadded based on the original padding values. Figure 7 illustrates the conversion process from the SkexGen representation\nformat to our stringified sequence.\nA.2. Generating Textual Instructions\nTextual instructions are generated in two steps: first, by applying LVM captioning on single-view images of CAD models;\nsecond, through human refinement of the generated captions to ensure clarity and accuracy.\nGiven a sequence representation, the CAD instance is rendered into an image, and captions are generated using GPT-4o.\nThis step is designed to extract geometric properties, including the number of shapes, their dimensions, spatial arrangements,\nand other relevant details. The prompt used for this step is provided in Listing 1.\n13\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n1 {\n2\n"Prompt1": "Propose a series of questions about the 3D shape and give the answers. The\nfirst question should ask for a detailed description and others should focus on the\nspecific geometric properties, number, size proportions and positional relationship,\nand other details.",\n3\n"Prompt2": "Based on the dialogue, please give a final description of the 3D shape. No\nmore than 70 words."\n4 }\nListing 1. Prompts that are used for making captions. The first prompt is used to generate question-answer pairs, and the second prompt\ncollects and summarizes the informations in the first prompt to yield the final caption.\nThe LVM-generated captions are further refined by human annotators to produce fine-grained captions that can serve as\nprecise textual instructions. The human annotators follow these guidelines during the editing process:\n• Ensuring Correspondence: The description must accurately reflect the figure without any discrepancies.\n• Ensuring Succinctness: The description should be as concise as possible while maintaining clarity and completeness.\n• Permission for Removal: Figures that are excessively complex or challenging to describe may be excluded from the\ndataset. In practice, the annotators are permitted to mark the revised descriptions of such instances as "null".\nFigure 7 illustrates an example of how an image is captioned by the LVM and subsequently refined by human annotators.\nA.3. Dataset Construction\nThe dataset construction process is illustrated in Figure 6. Starting with a CAD representation from the original dataset, we\ngenerate a paired textual instruction and a stringified CAD representation. The textual instruction is created through the\ncaptioning process detailed in Section A.2, while the ground truth reference is obtained by converting the CAD formatting\nas described in Appendix A.1.\nB. Additional Training Detail\nB.1. Sequential Learning\nWe fine-tune a LLaMA-3-8b-Instruct by 40 epochs on 4 NVIDIA A6000-48GB SMX GPUs with a LoRA with rank\n32. Further details regarding the fine-tuning process are provided in the Experiment Section of the main paper. The specific\nprompt used for the learning is as follows:\n1\n"Below is a description of a 3D shape:\\n\n2\n{description}\\n\n3\nGenerate a Computer-Aided Design (CAD) command sequence of the 3D shape:\\n"\nListing 2. Prompt used for sequential learning. description refers to the actual textual commands of samples\nB.2. Visual Feedback\nThe visual feedback is collected as outlined in the main paper. The llava-onevision-qwen2-7b-ov-chat model\nis utilized to generate visual descriptions. For each input sequence produced by the post-SL CADFusion, the corresponding\nrendered figure is evaluated by the model, which assigns a score ranging from 0 to 10. The prompt used for this scoring\nprocess is detailed in Listing 3:\n1\n"You are a harsh grader for new CAD\ndesigners’ works. The following is a text\ndescription of a CAD figure that they designed and an image of a CAD instance. \\n\n2\nDescription: {description} \\n\n3\nComment on this work for \\n\n4\n1. If the overall shape remains correct; \\n\n5\n2. If the number of components are correct, especially the circular holes; \\n\n6\n3. If the distribution of the components are natural, i.e. they are not clustered\ntogether or collide with each other. \\n\n14\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n"The 3D panel is a thin, rectangular cuboid with protrusions on opposite \nvertical edges, designed for interlocking with other components. The \nprotrusions serve functional purposes like stability or assembly \nalignment. The proportional size and position of the protrusions suggest \nminimal structural impact while ensuring precise fit with other parts."\n"The 3D shape is a long and narrow L-shaped prism. The two arms are the \nsame length, and the length of the whole the length of the prism is \nsignificantly greater than the other sides. There is a small groove running \nthrough the surface at the left end."\n"The 3D shape is a rectangular prism with the same elongated trapezoidal cutout on each \nof its top and bottom surfaces, the two corners of the cut are rounded, the length of the cut \nis less than the length of the rectangular prism, the width is equal to the width of the \nrectangular prism, and the height is less than the height of the rectangular prism. The \nincisions are symmetrically distributed. The length of the whole is greater than the height, \nand the height is greater than the width."\n"The three-dimensional shape is a flat square plate with a cylindrical hole \nin the center. Due to its geometric configuration, it is commonly used as a \nwasher or spacer in mechanical assemblies."\n"The three-dimensional shape is a cylinder with five circular holes of \nequal spacing and diameter."\n"The 3D shape consists of 12 identical vertical cylinders arranged in a 3x4 \ngrid pattern to form a compact cluster. All cylinders are equal in height \nand diameter, creating a symmetrical, unified structure."\n"The 3D shape is a rectangular sheet with four corners that bulge outward \nto form rounded corners. There is a round hole of the same size near each \nof the four corners of the largest side of the sheet, and the four round holes \nare symmetrically distributed. The overall length and width are equal."\n"The 3D shape is a long, slender rectangular bar with rounded ends, \nfeaturing centrally located circular holes at each end. The bar is \nsignificantly longer than it is wide or thick, with the holes aligned along \nthe bar\'s central axis. This design likely serves attachment or rotational \npurposes, minimizing stress concentrations and enhancing durability."\n"The 3D shape is an elongated U-shaped rectangular bar. It has a vertically \noriented elongated structure with a height that is significantly greater than \nthe width and depth. This shape has symmetrical planes and stands \nupright, making it appear narrow and stable."\n"The 3D shape is a flat, rectangular plate with rounded corner, featuring a \nlarge central rectangular cutout and four evenly spaced circular holes near \neach corner. It is symmetrical and likely designed as a mounting plate or \nbracket, allowing for secure attachment via the holes and access through \nthe central cutout."\n"The three-dimensional shape is a semicircular ring segment, similar to a \n"C" shape. It forms an open arc. Its thickness varies at different locations \nand is mainly characterized by thick and thin thickness."\n"The 3D shape is an elongated rectangular prism with semi-circular ends, \nresembling a capsule. The shape exhibits bilateral symmetry lengthwise, \nwith the width of the rectangle equal to the diameter of the semi-circular \nends."\n"The 3D shape has a flat, rectangular base with rounded upper top corners, \na rectangular cutout in the bottom center, and a round hole near the top \ncenter."\n"The 3D shape is a cylindrical wheel with a solid outer ring and an internal \ncross structure dividing the circle into four hollow fan sections. The center \nof the cross structure has a square hole."\n"The 3D shape is a cylindrical disc with a large central hole and four \nsmaller holes symmetrically spaced at90-degree intervals around it, \nforming the corners of an inscribed square. The disc has a uniform \nthickness and likely serves as a mechanical flange or spacer, potentially \nmade from metal or durable plastic for structural applications."\n"The 3D shape is a rectangular frame with a thin and uniform thickness of \nthe walls of the frame, and the overall length is greater than the height and \nthe height is greater than the width."\n"The 3D shape consists of a large, flat rectangular slab with two evenly \nspaced, identical cylindrical protrusions extending vertically from its \nsurface. The slab provides a stable base with significant length and width \ncompared to its thin height, while the cylinders are relatively short and \nhave small diameters. The overall design is symmetrical and balanced, \npotentially serving as a mounting base or connector."\n"The 3D shape consists of three rectangular prisms forming an\\"H\\" \nshaped structure. A long horizontal prism vertically connects two identical \nvertical prisms at each end. It has bilateral symmetry. The vertical prisms \nare symmetrical and identical and act as legs for the horizontal bar."\n"The 3D shape is a rectangular prism. It has five evenly spaced circular \nholes that run vertically along the length of the prism,"\n"The 3D shape is a cylindrical disc with an equilateral trapezoidal cutout \nthat penetrates its entire thickness. This shape is bilaterally symmetrical, \nwith the cutout positioned to the left of the center of the cylinder\'s \nprincipal axis, creating a consistent and symmetrically balanced design."\n"The 3D shape is a cylindrical object with a central axial hole, mounted on \na larger thin circular base. The vertical cylinder and base are centered, \nproviding cylindrical symmetry."\n"The three-dimensional shape is a flattened cylindrical disk with two \nholes: a smaller hole in the center and a larger hole near the edge. The \nlarger hole has a significantly larger diameter, which may affect the \nstructural integrity of the area nearby."\n"The 3D shape is an elongated half-cylinder curved prism."\n"The 3D shape is a long cylindrical rod with a hexagonal prism(screw \nhead) at one end. The rod has a uniform diameter, while the hexagonal \nprism has six equal sides and serves as the head, positioned \nperpendicularly to the rod. This configuration resembles a typical bolt, \nwhere the rod serves as the threaded shaft and the prism as the \ntool-grippable head."\nFigure 8. Additional qualitative results, Part 1. The results are grouped by categories such as panels and circular objects. In each sub-figure,\nthe left image shows the figure rendered from the ground truth, while the right image displays the generation by CADFusion. The\ncorresponding textual instructions are provided at the bottom.\n7\nAfter that, give a score out of 10. Do not comment on issues such as texture,\nsmoothness and colors."\nListing 3. Prompt used by LLaVA-OV for scoring an input figure. description refers to the textual of the sample.\nThe DPO procedure is conducted on 4 NVIDIA A6000-48GB SMX GPUs with a LoRA with rank 32. The training involves\nfive iterative DPO/SFT rounds, which require approximately 2.5 days to complete.\nC. Additional Experimental Results\nC.1. LVM Evaluation Setups\nAs mentioned in the main paper, we used a GPT-4o model as the LVM evaluator. It is selected over LLaMA-ov because we\nattempt to prevent the impact of AI bias that makes it prefer its own generation 3. The prompt we used for LVM evaluation\nis detailed in Listing 4:\n3We acknowledge that this choice may introduce bias in the GPT-4o results. However, we have decided to proceed with it for two\nreasons: 1) our primary focus is on comparing Text2CAD with our model, and 2) the GPT-based generations during our experiment\nshowed a significant margin in LVM scores compared to other methods, so the impact of this bias is minimal.\n15\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nThe three-dimensional shape is an L-shaped prism consisting of a vertical \nrectangular prism and a horizontal rectangular prism with a rectangular cut \non the lower side of the vertical rectangular prism.\n"The 3D shape consists of a combination of a rectangular prism and a \ntriangular prism, and the shape resembles a house."\n"The 3D shape is a star prism characterized by asymmetrical star-shaped \ntop and bottom faces connected by rectangular side faces. The base faces \nof the star are parallel and congruent, and the heights connecting them are \nconstant."\n"The three-dimensional shape is a thin, flat, triangular plate with rounded \nedges and three circular holes near the apex with the same diameter."\n"Its shape is that of a stepped prism, consisting of three rectangular prisms \nof equal width and height stacked on top of each other. The length \ndecreases from bottom to top, with the left sides of the prisms aligned."\n"The image features a hexagonal prism with a central vertical hole, \nresembling a nut, beside a tall, slender cylindrical rod. The cylindrical rod \nhas two circular bases and one curved surface, standing vertically and \nsignificantly taller than the prism. They are placed apart, with the prism on \nthe left."\n"The 3D shape is a hexagonal prism with a central cylindrical cutout."\n"The image shows two identical three-dimensional cylinders that are not \nconnected. One on the top left and one on the bottom right."\n"The image shows four identical hollow cylindrical rings."\n"The 3D shape is a V-shaped sheet with convex fillets at the top and \nbottom, concave fillets on the inside where the two sides intersect, a round \nhole at the bottom of the side, and an identical smaller hole near the top of \neach side."\n"The 3D shape is characterised by a flat rectangular base and four identical \ncylindrical pillars, symmetrically distributed near each corner."\n"The 3D shape is a cuboid with a vertical quarter-cylinder cutout along \none edge. The cutout smoothly transitions between adjacent flat faces, \nintroducing asymmetry while maintaining original right angles elsewhere."\n"The 3D shape is a rectangular prism. In the middle of one of its largest \nsides, there is a small round hole and a small square cutout, the round hole \nis near the right side, and the square cut is near the left."\n"The image shows three identical rectangular cuboids. Three prisms are \narranged in parallel with equal spacing. All length is greater than their \nwidth and height."\n"The 3D shape consists of two identical vertical rectangular prisms and a \nhorizontally placed rectangular prism to form a "U" shape. Two \nrectangular prisms placed vertically have two identically symmetrical \nround holes each. The overall thickness is consistent, longer than taller."\n"This 3D shape is a rectangular prism, with its length greater than its \nwidth, and the width greater than its height. In the center of the bottom of \nthe larger side, there is a long trapezoidal cut, with the width of the lower \nend narrower than that of the upper end."\nFigure 9. Additional qualitative results, Part 2. The results are grouped by categories such as multiple distinct items and complex shapes.\nIn each sub-figure, the left image shows the figure rendered from the ground truth, while the right image displays the generation by\nCADFusion. The corresponding textual instructions are provided at the bottom.\n1 "\n2 The following is a text description of a 3D CAD figure and an image of a CAD instance.\nMeasure if the figure corresponds to the given description, and give a score in the\nscale of 10. Do not comment on issues such as texture, smoothness and colors \\n\ndescription: {description}\\n\n3 "\nListing 4. Prompt used by GPT-4o for evaluation.\nC.2. Human Evaluation Setups\nWe generate a quadruple of outputs for each test set instruction. Each quadruple presents four rendered generations from\nCADFusionSL, CADFusion, GPT-4o and Text2CAD, respectively. The generations are tested by their correspondence\nbetween CAD shapes and instructions. Six human judges are asked to rank the generations4 with the first place being the\nbest model, and one LVM is deployed to score single-view images to the scale of 10.\nC.3. GPT Baselines\nThe prompt we used for the GPT-4o baseline is detailed in Listing 5.\n1 "\n2\nBelow is a description of a 3D shape:\n3\n{description}\n4\nGenerate a Computer-Aided Design (CAD) command sequence of the 3D shape. The command\nsequence involves sketches such as lines, arcs, and circles, each marked by the\nendpoints, and extrusions that make the sketch into 3D volumes.\n5\n6\nHere are some examples:\n7\n1. <few shot example>\n4Due to the lack of overlapping, we obtain approximately 50 unique samples.\n16\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\n8\n2. <few shot example>\n9\n3. <few shot example>\n10\n4. <few shot example>\n11\n5. <few shot example>\n12\n6. <few shot example>\n13\n7. <few shot example>\n14\n8. <few shot example>\n15\n16\nNow it’s your turn. Remind that this is your description: {description}. No\nexplanation is needed. Only return your final sequence, and in one line.\n17 "\nListing 5. Prompt used by GPT-4o for baseline comparison.\nAlongside the current 8-shot version, we also tested a 3-shot GPT-4o model to reduce computation costs. However, the\n3-shot model resulted in approximately a 92% invalidity ratio, and the 8% of renderable outputs were barely recognizable in\nrelation to the prompt. Given these issues, we have decided to use the 8-shot version as our baseline for GPT.\nC.4. Additional Statements on Text2CAD Results\nIn the quantitative experiments, our setups are not fully aligned with those of Text2CAD. This discrepancy arises because\nwhen we used our test set prompts as input, we observed a performance degradation and a significant gap between our\ncomputed results and those reported by the authors.\nWe discovered that the discrepancy stems from the model’s sensitivity to the level of detail in the prompt. Text2CAD\nperforms well only with expert-level prompts, which contain step-by-step sketching guidelines. Our prompts, however, do\nnot include this level of detail 5. To ensure consistency, in Table 1, we report Text2CAD’s performance based on their expert\nprompts when computing the metrics they introduced. Specifically, for each item in the test set, CADFusion and GPT-4o’s\npredictions were generated using our prompts, while Text2CAD’s predictions were generated using the expert prompt for\nthe same item from their prompt base.\nThis approach aligns the results we reproduced with the reported scores from the original paper. To present a comprehensive\nand accurate study, we also report Text2CAD’s results using our prompts and intermediate-level prompts in Table 3. The\nlast two rows, CADFusion and Text2CAD-our-prompt, are aligned as the same prompt is used.\nBy changing the prompt from the expert-level prompt in their database to an intermediate-level prompt, we observe a similar\nperformance drop. This indicates that our prompting method does not degrade Text2CAD’s performance. Instead, it is an\nlimitation stemmed from Text2CAD itself. Our model, using a simplified prompt, outperforms Text2CAD-expert. Given\nthat the expert-level prompt from Text2CAD is too long and too specific to be feasible in the real designing process, we\nbelieve that our quantitative advantage over it is significant.\nMethod\nF1↑\nCD↓\nLine\nArc\nCircle\nExtrusion\nText2CAD-intermediate\n66.65\n4.85\n47.62\n93.56\n146.15\nText2CAD-expert\n79.59\n42.79\n69.45\n92.13\n30.23\nText2CAD-our-prompt\n54.42\n0.92\n18.42\n75.37\n235.91\nCADFusion\n83.71\n81.99\n89.97\n92.79\n45.67\nTable 3. Results of our model and different Text2CAD prompts on metrics Proposed by Khan et al. (2024b). The suffix indicates the\nprompt type used for testing. Text2CAD-ours and CADFusion are the most aligned pairs, while Text2CAD-expert and CADFusion are\nthe ones reported in the main paper.\nC.5. Additional Quantative Results\nWe report additional quantitative results in this section.\n5We are concerned that the impact of detailed prompts containing step-by-step instructions and point coordinates is limited, as they\nmay not be feasible in real-life scenarios.\n17\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nMethod\nLVM Score ↑\nIR ↓\nCADFusionSL\n7.69\n4.84\nCADFusionSLw/o HA∼18k\n6.56\n6.00\nCADFusionSLw/o HA∼170k\n6.60\n9.04\nTable 4. LVM scores and invalidity ratios across different CADFu-\nsion variants. All three models are trained using only the initial\nSequential Learning stage. The suffix w/o HA indicates that the\nvariant does not use human-annotated data, while the number de-\nnotes the size of the training set.\nMethod\nAvg. Rank ↓\nGPT-4o -8shot\n3.22\nText2CAD\n2.97\nCADFusion-SFT only\n2.03\nCADFusion\n1.86\nTable 5. Human Evaluation Results. Human annotators ranked\nthe generations of different methods based on their quality,\nwith a lower rank indicating higher human preference.\nOn Dataset Size. The dataset used in our experiments is a subset of SkexGen (Xu et al., 2022). Since human annotation is\nnot scalable, we evaluate the trade-off between scalability and data quality. One such evaluation, discussed in the Ablation\nStudy (Section 4.3), demonstrates that, given the same number of training samples, data quality outweighs dataset scalability\nin terms of model performance.\nAdditionally, we investigate whether increasing dataset size can mitigate quality limitations by conducting an experiment on\nthe full SkexGen-based Text-to-CAD dataset ( 170k samples). The results, presented in Table 4, indicate that increasing\ndataset size does not significantly improve the visual quality of model generations. While a slight performance gain is\nobserved with additional training samples, the improvement is marginal, and none of the w/o HA variants outperform the\nhuman-annotated counterpart.\nOn Human Evaluation. We conducted human evaluations across four models: GPT-4o, Text2CAD, CADFusion, and\nCADFusion trained only with the Sequential Learning stage. However, only the first three models are reported in Table 1.\nThe complete results of human evaluation are presented in Table 5. As indicated by the evaluation, the two CADFusion\nvariants are preferred over the baselines, with the version incorporating Visual Feedback receiving higher rankings from\nhuman judges. This highlights the effectiveness of visual feedback in improving model performance.\nC.6. Additional Qualitative Results\nIn this section, we present additional qualitative results. Figures 8 and 9 display these results, organized by CAD shape\nproperties such as panels and circular objects. These examples demonstrate that our model can efficiently handle a variety\nof CAD shapes with distinct instructions, such as holes and frames. Furthermore, the model performs well in generating\nmultiple identical objects, as shown in the first row of Figure 9, and can effectively generate more complex shapes, such as\nstars and V-shapes.\nC.7. Text to Multiple CAD Figures\nDuring inference, we set the temperature t = 0.3, top_p = 0.9, and top_k = 50 to enable non-deterministic generation.\nThis configuration allows us to produce varied CAD figures that meet the instructed requirements, with slight differences\nbetween them. As a result, users can select the design that best aligns with their specific needs. Examples of such outputs\nare shown in Figure 10. These results demonstrate that while adhering to the provided instructions, CADFusion is capable\nof generating diversified outputs. The variations primarily affect attributes such as thickness, width, and the size of holes\nand cutouts, while maintaining the overall shape. This flexibility offers users a broader range of choices, thereby reducing\nthe amount of additional work required when integrating such Text-to-CAD systems into industrial applications.\nC.8. Failure Cases\nWe identify two types of failures in our work: sequences that are not renderable and shapes that are rendered but misaligned\nwith the intended design. We refer to the former as Invalid Samples and the latter as Discrepant Samples. Examples of\nboth types of failures are shown in Figure 11.\nIn our analysis, samples are often invalid when the input instruction is too complex, meaning there are too many elements to\nbe drawn. The case shown in the top-left corner of Figure 11 involves more than 20 loops and 50 curves in the ground truth.\nAdditionally, CADFusion struggles to map CAD shapes to characters such as letters, resulting in failures when attempting to\nconstruct shapes that spell words or names.\n18\n\nText-to-CAD Generation Through Infusing Visual Feedback in Large Language Models\nThe 3D shape is a rectangular prism with the same elongated trapezoidal cutout on each of its \ntop and bottom surfaces, the two corners of the cut are rounded, the length of the cut is less than \nthe length of the rectangular prism, the width is equal to the width of the rectangular prism, and \nthe height is less than the height of the rectangular prism. The incisions are symmetrically \ndistributed.\nThe 3D shape is a flat, rectangular plate with rounded corner, featuring a large central \nrectangular cutout and four evenly spaced circular holes near each corner. It is symmetrical and \nlikely designed as a mounting plate or bracket, allowing for secure attachment via the holes and \naccess through the central cutout.\nThe three-dimensional shape is a semicircular ring segment, similar to a "C" shape. It forms an \nopen arc. Its thickness varies at different locations and is mainly characterized by thick and thin \nthickness.\nIts shape is that of a stepped prism, consisting of three rectangular prisms of equal width and \nheight stacked on top of each other. The length decreases from bottom to top, with the left sides \nof the prisms aligned.\nFigure 10. An overview of the generation of CAD instances with slight variations from a single prompt. In each sub-figure, the top-left\nimage shows the ground truth generation, while the remaining three represent CADFusion’s outputs, which exhibit variations in thickness,\nwidth, and cutout size. The prompt is displayed at the top of each sub-figure.\nDiscrepant Sample: complex integration of multiple shapes\nInvalid Sample: mapping characters with CAD shapes\nDiscrepant Sample: too any items\nInvalid Sample: decoding too much information\nThe 3D shape consists of a large rectangular base with six evenly spaced vertical rectangular protrusions. The front face of each \nprotrusion has an inset rectangular design. The protrusions are aligned in two rows, each with three protrusions. The protrusions \nextend perpendicularly from the front face of the base, which is larger in height, width, and depth compared to the protrusions.\n"The 3D shape spells "IAN" using rectangular prisms. All letters are uniform in height and width, with a consistent horizontal \nalignment with spacing in between.",\nThe image features nine identical grey cubes scattered in a random pattern. Each cube has equal edge lengths, making them \ncongruent, and is oriented such that their faces align with the image\'s axes. The cubes exhibit no noticeable patterns, symmetry, \nor reflections, and there are varying gaps between them, with an absence of shadows and clustering.\nThe 3D shape consists of a hollow cylinder with a through-hole, connected perpendicularly to a solid rectangular prism. The \nprism extends vertically upwards from the horizontal cylindrical part. The shape is bilaterally symmetrical and could serve as a \nmechanical connector or mounting bracket.\nFigure 11. Invalid and discrepant samples. CADFusion generates invalid samples when the instructions are too complex or involve word\nshape knowledge, and produces discrepant outcomes when there are too many distinct items to generate or when complex merges are\nrequired to form the final CAD instance.\nDiscrepancies between the rendered shapes and the intended design can occur when the input instruction involves too many\ndistinct items. While CADFusion demonstrates advanced capabilities in understanding numerical values compared to other\nmodels, handling more than 8 separate items remains a challenging task. In such cases, CADFusion may either miscalculate\nthe number of items to draw or generate incorrect shapes, as shown in the bottom-left corner of Figure 11. Furthermore,\nintegrating multiple items into complex shapes is another frequent challenge for CADFusion.\n19')]}
