2025-02-04 21:49:29,737 - ERROR - Unexpected error: 5 validation errors for Paper
authors.0
  Input should be a valid string [type=string_type, input_value={'_id': '679d5057ca02e327... Yang', 'hidden': False}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
authors.1
  Input should be a valid string [type=string_type, input_value={'_id': '679d5057ca02e327... Zhou', 'hidden': False}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
authors.2
  Input should be a valid string [type=string_type, input_value={'_id': '679d5057ca02e327... Zhao', 'hidden': False}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
authors.3
  Input should be a valid string [type=string_type, input_value={'_id': '679d5057ca02e327...i Tao', 'hidden': False}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
authors.4
  Input should be a valid string [type=string_type, input_value={'_id': '679d5057ca02e327...e Loy', 'hidden': False}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
2025-02-04 21:49:29,739 - INFO - Papers: {}
2025-02-04 21:50:43,119 - INFO - Papers: {'2025-02-03': [Paper(id='2501.14677', authors=['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], published_at=datetime.datetime(2025, 2, 3, 13, 15, 59, 743000, tzinfo=datetime.timezone.utc), title='MatAnyone: Stable Video Matting with Consistent Memory Propagation', summary='Auxiliary-free human video matting methods, which rely solely on input\nframes, often struggle with complex or ambiguous backgrounds. To address this,\nwe propose MatAnyone, a robust framework tailored for target-assigned video\nmatting. Specifically, building on a memory-based paradigm, we introduce a\nconsistent memory propagation module via region-adaptive memory fusion, which\nadaptively integrates memory from the previous frame. This ensures semantic\nstability in core regions while preserving fine-grained details along object\nboundaries. For robust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a novel training\nstrategy that efficiently leverages large-scale segmentation data, boosting\nmatting stability. With this new network design, dataset, and training\nstrategy, MatAnyone delivers robust and accurate video matting results in\ndiverse real-world scenarios, outperforming existing methods.', upvotes=13, thumbnail=None), Paper(id='2501.19339', authors=['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], published_at=datetime.datetime(2025, 2, 3, 10, 59, 18, 508000, tzinfo=datetime.timezone.utc), title='PixelWorld: Towards Perceiving Everything as Pixels', summary='Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models\' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels\' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.', upvotes=12, thumbnail=None), Paper(id='2501.18119', authors=['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], published_at=datetime.datetime(2025, 2, 3, 6, 6, 33, 957000, tzinfo=datetime.timezone.utc), title='Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models', summary='Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.', upvotes=12, thumbnail=None), Paper(id='2501.19399', authors=['Ken M. Nakanishi'], published_at=datetime.datetime(2025, 2, 3, 13, 1, 15, 923000, tzinfo=datetime.timezone.utc), title='Scalable-Softmax Is Superior for Attention', summary="The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.", upvotes=11, thumbnail=None)], '2025-02-02': [], '2025-02-01': [], '2025-01-31': [Paper(id='2501.18585', authors=['Yue Wang', 'Qiuzhi Liu', 'Jiahao Xu', 'Tian Liang', 'Xingyu Chen', 'Zhiwei He', 'Linfeng Song', 'Dian Yu', 'Juntao Li', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], published_at=datetime.datetime(2025, 1, 31, 0, 16, 36, 453000, tzinfo=datetime.timezone.utc), title='Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs', summary="Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.", upvotes=43, thumbnail=None), Paper(id='2501.18512', authors=['Arthur Douillard', 'Yanislav Donchev', 'Keith Rush', 'Satyen Kale', 'Zachary Charles', 'Zachary Garrett', 'Gabriel Teston', 'Dave Lacey', 'Ross McIlroy', 'Jiajun Shen', 'Alexandre Ram√©', 'Arthur Szlam', "Marc'Aurelio Ranzato", 'Paul Barham'], published_at=datetime.datetime(2025, 1, 31, 5, 7, 14, 120000, tzinfo=datetime.timezone.utc), title='Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch', summary="Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.", upvotes=23, thumbnail=None), Paper(id='2501.18009', authors=['Lan Pan', 'Hanbo Xie', 'Robert C. Wilson'], published_at=datetime.datetime(2025, 1, 31, 0, 9, 40, 77000, tzinfo=datetime.timezone.utc), title='Large Language Models Think Too Fast To Explore Effectively', summary='Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability.', upvotes=21, thumbnail=None), Paper(id='2501.18362', authors=['Yuxin Zuo', 'Shang Qu', 'Yifei Li', 'Zhangren Chen', 'Xuekai Zhu', 'Ermo Hua', 'Kaiyan Zhang', 'Ning Ding', 'Bowen Zhou'], published_at=datetime.datetime(2025, 1, 31, 4, 14, 53, 856000, tzinfo=datetime.timezone.utc), title='MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding', summary='We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.', upvotes=19, thumbnail=None), Paper(id='2501.18438', authors=['Aitor Arrieta', 'Miriam Ugarte', 'Pablo Valle', 'Jos√© Antonio Parejo', 'Sergio Segura'], published_at=datetime.datetime(2025, 1, 31, 2, 35, 40, 107000, tzinfo=datetime.timezone.utc), title='o3-mini vs DeepSeek-R1: Which One is Safer?', summary="The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.", upvotes=19, thumbnail=None)], '2025-01-30': [Paper(id='2501.14334', authors=['Cl√©ment Desroches', 'Martin Chauvin', 'Louis Ladan', 'Caroline Vateau', 'Simon Gosset', 'Philippe Cordier'], published_at=datetime.datetime(2025, 1, 30, 3, 5, 8, 789000, tzinfo=datetime.timezone.utc), title="Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts", summary='The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies\' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company\'s AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n"Return on Environment" metric to align AI development with net-zero goals.', upvotes=15, thumbnail=None), Paper(id='2501.15891', authors=['Hailong Guo', 'Bohan Zeng', 'Yiren Song', 'Wentao Zhang', 'Chuang Zhang', 'Jiaming Liu'], published_at=datetime.datetime(2025, 1, 30, 20, 14, 3, 298000, tzinfo=datetime.timezone.utc), title='Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile\n  Virtual Clothing Tasks', summary="Image-based virtual try-on (VTON) aims to generate a virtual try-on result by\ntransferring an input garment onto a target person's image. However, the\nscarcity of paired garment-model data makes it challenging for existing methods\nto achieve high generalization and quality in VTON. Also, it limits the ability\nto generate mask-free try-ons. To tackle the data scarcity problem, approaches\nsuch as Stable Garment and MMTryon use a synthetic data strategy, effectively\nincreasing the amount of paired data on the model side. However, existing\nmethods are typically limited to performing specific try-on tasks and lack\nuser-friendliness. To enhance the generalization and controllability of VTON\ngeneration, we propose Any2AnyTryon, which can generate try-on results based on\ndifferent textual instructions and model garment images to meet various needs,\neliminating the reliance on masks, poses, or other conditions. Specifically, we\nfirst construct the virtual try-on dataset LAION-Garment, the largest known\nopen-source garment try-on dataset. Then, we introduce adaptive position\nembedding, which enables the model to generate satisfactory outfitted model\nimages or garment images based on input images of different sizes and\ncategories, significantly enhancing the generalization and controllability of\nVTON generation. In our experiments, we demonstrate the effectiveness of our\nAny2AnyTryon and compare it with existing methods. The results show that\nAny2AnyTryon enables flexible, controllable, and high-quality image-based\nvirtual try-on generation.https://logn-2024.github.io/Any2anyTryonProjectPage/", upvotes=10, thumbnail=None)], '2025-01-29': [Paper(id='2501.16764', authors=['Chenguo Lin', 'Panwang Pan', 'Bangbang Yang', 'Zeming Li', 'Yadong Mu'], published_at=datetime.datetime(2025, 1, 29, 1, 12, 2, 839000, tzinfo=datetime.timezone.utc), title='DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian\n  Splat Generation', summary='Recent advancements in 3D content generation from text or a single image\nstruggle with limited high-quality 3D datasets and inconsistency from 2D\nmulti-view generation. We introduce DiffSplat, a novel 3D generative framework\nthat natively generates 3D Gaussian splats by taming large-scale text-to-image\ndiffusion models. It differs from previous 3D generative models by effectively\nutilizing web-scale 2D priors while maintaining 3D consistency in a unified\nmodel. To bootstrap the training, a lightweight reconstruction model is\nproposed to instantly produce multi-view Gaussian splat grids for scalable\ndataset curation. In conjunction with the regular diffusion loss on these\ngrids, a 3D rendering loss is introduced to facilitate 3D coherence across\narbitrary views. The compatibility with image diffusion models enables seamless\nadaptions of numerous techniques for image generation to the 3D realm.\nExtensive experiments reveal the superiority of DiffSplat in text- and\nimage-conditioned generation tasks and downstream applications. Thorough\nablation studies validate the efficacy of each critical design choice and\nprovide insights into the underlying mechanism.', upvotes=20, thumbnail=None)], '2025-01-28': [Paper(id='2501.15383', authors=['An Yang', 'Bowen Yu', 'Chengyuan Li', 'Dayiheng Liu', 'Fei Huang', 'Haoyan Huang', 'Jiandong Jiang', 'Jianhong Tu', 'Jianwei Zhang', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Kexin Yang', 'Le Yu', 'Mei Li', 'Minmin Sun', 'Qin Zhu', 'Rui Men', 'Tao He', 'Weijia Xu', 'Wenbiao Yin', 'Wenyuan Yu', 'Xiafei Qiu', 'Xingzhang Ren', 'Xinlong Yang', 'Yong Li', 'Zhiying Xu', 'Zipeng Zhang'], published_at=datetime.datetime(2025, 1, 28, 0, 35, 46, 871000, tzinfo=datetime.timezone.utc), title='Qwen2.5-1M Technical Report', summary='We introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such as long data synthesis,\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes a\nlength extrapolation method that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement a sparse attention method along with chunked\nprefill optimization for deployment scenarios and a sparsity refinement method\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, including kernel optimization, pipeline parallelism, and\nscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\ntokens of context. This framework provides an efficient and powerful solution\nfor developing applications that require long-context processing using\nopen-source models.\n  The Qwen2.5-1M series currently includes the open-source models\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\nimproved in long-context tasks without compromising performance in\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\ncontexts eight times longer.', upvotes=50, thumbnail=None), Paper(id='2501.15368', authors=['Yadong Li', 'Jun Liu', 'Tao Zhang', 'Tao Zhang', 'Song Chen', 'Tianpeng Li', 'Zehuan Li', 'Lijun Liu', 'Lingfeng Ming', 'Guosheng Dong', 'Da Pan', 'Chong Li', 'Yuanbo Fang', 'Dongdong Kuang', 'Mingrui Wang', 'Chenglin Zhu', 'Youwei Zhang', 'Hongyu Guo', 'Fengyu Zhang', 'Yuran Wang', 'Bowen Ding', 'Wei Song', 'Xu Li', 'Yuqi Huo', 'Zheng Liang', 'Shusen Zhang', 'Xin Wu', 'Shuai Zhao', 'Linchu Xiong', 'Yozhen Wu', 'Jiahui Ye', 'Wenhao Lu', 'Bowen Li', 'Yan Zhang', 'Yaqi Zhou', 'Xin Chen', 'Lei Su', 'Hongda Zhang', 'Fuzhong Chen', 'Xuezhen Dong', 'Na Nie', 'Zhiying Wu', 'Bin Xiao', 'Ting Li', 'Shunya Dang', 'Ping Zhang', 'Yijia Sun', 'Jincheng Wu', 'Jinjie Yang', 'Xionghai Lin', 'Zhi Ma', 'Kegeng Wu', 'Jia li', 'Aiyuan Yang', 'Hui Liu', 'Jianqiang Zhang', 'Xiaoxi Chen', 'Guangwei Ai', 'Wentao Zhang', 'Yicong Chen', 'Xiaoqin Huang', 'Kun Li', 'Wenjing Luo', 'Yifei Duan', 'Lingling Zhu', 'Ran Xiao', 'Zhe Su', 'Jiani Pu', 'Dian Wang', 'Xu Jia', 'Tianyu Zhang', 'Mengyu Ai', 'Mang Wang', 'Yujing Qiao', 'Lei Zhang', 'Yanjun Shen', 'Fan Yang', 'Miao Zhen', 'Yijie Zhou', 'Mingyang Chen', 'Fei Li', 'Chenzheng Zhu', 'Keer Lu', 'Yaqi Zhao', 'Hao Liang', 'Youquan Li', 'Yanzhao Qin', 'Linzhuang Sun', 'Jianhua Xu', 'Haoze Sun', 'Mingan Lin', 'Zenan Zhou', 'Weipeng Chen'], published_at=datetime.datetime(2025, 1, 28, 0, 34, 49, 721000, tzinfo=datetime.timezone.utc), title='Baichuan-Omni-1.5 Technical Report', summary='We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\nomni-modal understanding capabilities but also provides end-to-end audio\ngeneration capabilities. To achieve fluent and high-quality interaction across\nmodalities without compromising the capabilities of any modality, we\nprioritized optimizing three key aspects. First, we establish a comprehensive\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\nacoustic information from audio, enabling seamless integration and enhanced\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\nthat progressively integrates multimodal alignment and multitask fine-tuning,\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\nto leading models such as Qwen2-VL-72B across various multimodal medical\nbenchmarks.', upvotes=48, thumbnail=None), Paper(id='2501.16142', authors=['Scott Fujimoto', "Pierluca D'Oro", 'Amy Zhang', 'Yuandong Tian', 'Michael Rabbat'], published_at=datetime.datetime(2025, 1, 28, 0, 36, 9, 186000, tzinfo=datetime.timezone.utc), title='Towards General-Purpose Model-Free Reinforcement Learning', summary='Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.', upvotes=23, thumbnail=None), Paper(id='2501.15570', authors=['Lin Yueyu', 'Li Zhiyuan', 'Peter Yue', 'Liu Xiao'], published_at=datetime.datetime(2025, 1, 28, 3, 2, 56, 62000, tzinfo=datetime.timezone.utc), title='ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer', summary="As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\nhttps://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside},\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.", upvotes=22, thumbnail=None), Paper(id='2501.15907', authors=['Haorui He', 'Zengqiang Shang', 'Chaoren Wang', 'Xuyuan Li', 'Yicheng Gu', 'Hua Hua', 'Liwei Liu', 'Chen Yang', 'Jiaqi Li', 'Peiyang Shi', 'Yuancheng Wang', 'Kai Chen', 'Pengyuan Zhang', 'Zhizheng Wu'], published_at=datetime.datetime(2025, 1, 28, 5, 40, 25, 750000, tzinfo=datetime.timezone.utc), title='Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation', summary='Recent advancements in speech generation have been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexplored\nin-the-wild data that capture spontaneous human speech in real-world contexts.\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate that Emilia\nsignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiverse speaker timbre and speaking styles of real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvance speech generation research and validates the effectiveness of Emilia\nfor both multilingual and crosslingual speech generation.', upvotes=15, thumbnail=None)], '2025-01-27': [Paper(id='2501.13953', authors=['Zicheng Zhang', 'Xiangyu Zhao', 'Xinyu Fang', 'Chunyi Li', 'Xiaohong Liu', 'Xiongkuo Min', 'Haodong Duan', 'Kai Chen', 'Guangtao Zhai'], published_at=datetime.datetime(2025, 1, 27, 3, 5, 15, 907000, tzinfo=datetime.timezone.utc), title='Redundancy Principles for MLLMs Benchmarks', summary="With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively.", upvotes=25, thumbnail=None), Paper(id='2501.14176', authors=['Micah Rentschler', 'Jesse Roberts'], published_at=datetime.datetime(2025, 1, 27, 9, 59, 51, 940000, tzinfo=datetime.timezone.utc), title='RL + Transformer = A General-Purpose Problem Solver', summary='What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.', upvotes=19, thumbnail=None)]}
2025-02-04 21:51:10,516 - INFO - Papers: {'2025-01-27': [Paper(id='2501.13953', authors=['Zicheng Zhang', 'Xiangyu Zhao', 'Xinyu Fang', 'Chunyi Li', 'Xiaohong Liu', 'Xiongkuo Min', 'Haodong Duan', 'Kai Chen', 'Guangtao Zhai'], published_at=datetime.datetime(2025, 1, 27, 3, 5, 15, 907000, tzinfo=datetime.timezone.utc), title='Redundancy Principles for MLLMs Benchmarks', summary="With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively.", upvotes=25, thumbnail=None),
                Paper(id='2501.14176', authors=['Micah Rentschler', 'Jesse Roberts'], published_at=datetime.datetime(2025, 1, 27, 9, 59, 51, 940000, tzinfo=datetime.timezone.utc), title='RL + Transformer = A General-Purpose Problem Solver', summary='What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.', upvotes=19, thumbnail=None)],
 '2025-01-28': [Paper(id='2501.15383', authors=['An Yang', 'Bowen Yu', 'Chengyuan Li', 'Dayiheng Liu', 'Fei Huang', 'Haoyan Huang', 'Jiandong Jiang', 'Jianhong Tu', 'Jianwei Zhang', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Kexin Yang', 'Le Yu', 'Mei Li', 'Minmin Sun', 'Qin Zhu', 'Rui Men', 'Tao He', 'Weijia Xu', 'Wenbiao Yin', 'Wenyuan Yu', 'Xiafei Qiu', 'Xingzhang Ren', 'Xinlong Yang', 'Yong Li', 'Zhiying Xu', 'Zipeng Zhang'], published_at=datetime.datetime(2025, 1, 28, 0, 35, 46, 871000, tzinfo=datetime.timezone.utc), title='Qwen2.5-1M Technical Report', summary='We introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such as long data synthesis,\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes a\nlength extrapolation method that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement a sparse attention method along with chunked\nprefill optimization for deployment scenarios and a sparsity refinement method\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, including kernel optimization, pipeline parallelism, and\nscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\ntokens of context. This framework provides an efficient and powerful solution\nfor developing applications that require long-context processing using\nopen-source models.\n  The Qwen2.5-1M series currently includes the open-source models\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\nimproved in long-context tasks without compromising performance in\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\ncontexts eight times longer.', upvotes=50, thumbnail=None),
                Paper(id='2501.15368', authors=['Yadong Li', 'Jun Liu', 'Tao Zhang', 'Tao Zhang', 'Song Chen', 'Tianpeng Li', 'Zehuan Li', 'Lijun Liu', 'Lingfeng Ming', 'Guosheng Dong', 'Da Pan', 'Chong Li', 'Yuanbo Fang', 'Dongdong Kuang', 'Mingrui Wang', 'Chenglin Zhu', 'Youwei Zhang', 'Hongyu Guo', 'Fengyu Zhang', 'Yuran Wang', 'Bowen Ding', 'Wei Song', 'Xu Li', 'Yuqi Huo', 'Zheng Liang', 'Shusen Zhang', 'Xin Wu', 'Shuai Zhao', 'Linchu Xiong', 'Yozhen Wu', 'Jiahui Ye', 'Wenhao Lu', 'Bowen Li', 'Yan Zhang', 'Yaqi Zhou', 'Xin Chen', 'Lei Su', 'Hongda Zhang', 'Fuzhong Chen', 'Xuezhen Dong', 'Na Nie', 'Zhiying Wu', 'Bin Xiao', 'Ting Li', 'Shunya Dang', 'Ping Zhang', 'Yijia Sun', 'Jincheng Wu', 'Jinjie Yang', 'Xionghai Lin', 'Zhi Ma', 'Kegeng Wu', 'Jia li', 'Aiyuan Yang', 'Hui Liu', 'Jianqiang Zhang', 'Xiaoxi Chen', 'Guangwei Ai', 'Wentao Zhang', 'Yicong Chen', 'Xiaoqin Huang', 'Kun Li', 'Wenjing Luo', 'Yifei Duan', 'Lingling Zhu', 'Ran Xiao', 'Zhe Su', 'Jiani Pu', 'Dian Wang', 'Xu Jia', 'Tianyu Zhang', 'Mengyu Ai', 'Mang Wang', 'Yujing Qiao', 'Lei Zhang', 'Yanjun Shen', 'Fan Yang', 'Miao Zhen', 'Yijie Zhou', 'Mingyang Chen', 'Fei Li', 'Chenzheng Zhu', 'Keer Lu', 'Yaqi Zhao', 'Hao Liang', 'Youquan Li', 'Yanzhao Qin', 'Linzhuang Sun', 'Jianhua Xu', 'Haoze Sun', 'Mingan Lin', 'Zenan Zhou', 'Weipeng Chen'], published_at=datetime.datetime(2025, 1, 28, 0, 34, 49, 721000, tzinfo=datetime.timezone.utc), title='Baichuan-Omni-1.5 Technical Report', summary='We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\nomni-modal understanding capabilities but also provides end-to-end audio\ngeneration capabilities. To achieve fluent and high-quality interaction across\nmodalities without compromising the capabilities of any modality, we\nprioritized optimizing three key aspects. First, we establish a comprehensive\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\nacoustic information from audio, enabling seamless integration and enhanced\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\nthat progressively integrates multimodal alignment and multitask fine-tuning,\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\nto leading models such as Qwen2-VL-72B across various multimodal medical\nbenchmarks.', upvotes=48, thumbnail=None),
                Paper(id='2501.16142', authors=['Scott Fujimoto', "Pierluca D'Oro", 'Amy Zhang', 'Yuandong Tian', 'Michael Rabbat'], published_at=datetime.datetime(2025, 1, 28, 0, 36, 9, 186000, tzinfo=datetime.timezone.utc), title='Towards General-Purpose Model-Free Reinforcement Learning', summary='Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.', upvotes=23, thumbnail=None),
                Paper(id='2501.15570', authors=['Lin Yueyu', 'Li Zhiyuan', 'Peter Yue', 'Liu Xiao'], published_at=datetime.datetime(2025, 1, 28, 3, 2, 56, 62000, tzinfo=datetime.timezone.utc), title='ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer', summary="As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\nhttps://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside},\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.", upvotes=22, thumbnail=None),
                Paper(id='2501.15907', authors=['Haorui He', 'Zengqiang Shang', 'Chaoren Wang', 'Xuyuan Li', 'Yicheng Gu', 'Hua Hua', 'Liwei Liu', 'Chen Yang', 'Jiaqi Li', 'Peiyang Shi', 'Yuancheng Wang', 'Kai Chen', 'Pengyuan Zhang', 'Zhizheng Wu'], published_at=datetime.datetime(2025, 1, 28, 5, 40, 25, 750000, tzinfo=datetime.timezone.utc), title='Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation', summary='Recent advancements in speech generation have been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexplored\nin-the-wild data that capture spontaneous human speech in real-world contexts.\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate that Emilia\nsignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiverse speaker timbre and speaking styles of real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvance speech generation research and validates the effectiveness of Emilia\nfor both multilingual and crosslingual speech generation.', upvotes=15, thumbnail=None)],
 '2025-01-29': [Paper(id='2501.16764', authors=['Chenguo Lin', 'Panwang Pan', 'Bangbang Yang', 'Zeming Li', 'Yadong Mu'], published_at=datetime.datetime(2025, 1, 29, 1, 12, 2, 839000, tzinfo=datetime.timezone.utc), title='DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian\n  Splat Generation', summary='Recent advancements in 3D content generation from text or a single image\nstruggle with limited high-quality 3D datasets and inconsistency from 2D\nmulti-view generation. We introduce DiffSplat, a novel 3D generative framework\nthat natively generates 3D Gaussian splats by taming large-scale text-to-image\ndiffusion models. It differs from previous 3D generative models by effectively\nutilizing web-scale 2D priors while maintaining 3D consistency in a unified\nmodel. To bootstrap the training, a lightweight reconstruction model is\nproposed to instantly produce multi-view Gaussian splat grids for scalable\ndataset curation. In conjunction with the regular diffusion loss on these\ngrids, a 3D rendering loss is introduced to facilitate 3D coherence across\narbitrary views. The compatibility with image diffusion models enables seamless\nadaptions of numerous techniques for image generation to the 3D realm.\nExtensive experiments reveal the superiority of DiffSplat in text- and\nimage-conditioned generation tasks and downstream applications. Thorough\nablation studies validate the efficacy of each critical design choice and\nprovide insights into the underlying mechanism.', upvotes=20, thumbnail=None)],
 '2025-01-30': [Paper(id='2501.14334', authors=['Cl√©ment Desroches', 'Martin Chauvin', 'Louis Ladan', 'Caroline Vateau', 'Simon Gosset', 'Philippe Cordier'], published_at=datetime.datetime(2025, 1, 30, 3, 5, 8, 789000, tzinfo=datetime.timezone.utc), title="Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts", summary='The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies\' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company\'s AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n"Return on Environment" metric to align AI development with net-zero goals.', upvotes=15, thumbnail=None),
                Paper(id='2501.15891', authors=['Hailong Guo', 'Bohan Zeng', 'Yiren Song', 'Wentao Zhang', 'Chuang Zhang', 'Jiaming Liu'], published_at=datetime.datetime(2025, 1, 30, 20, 14, 3, 298000, tzinfo=datetime.timezone.utc), title='Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile\n  Virtual Clothing Tasks', summary="Image-based virtual try-on (VTON) aims to generate a virtual try-on result by\ntransferring an input garment onto a target person's image. However, the\nscarcity of paired garment-model data makes it challenging for existing methods\nto achieve high generalization and quality in VTON. Also, it limits the ability\nto generate mask-free try-ons. To tackle the data scarcity problem, approaches\nsuch as Stable Garment and MMTryon use a synthetic data strategy, effectively\nincreasing the amount of paired data on the model side. However, existing\nmethods are typically limited to performing specific try-on tasks and lack\nuser-friendliness. To enhance the generalization and controllability of VTON\ngeneration, we propose Any2AnyTryon, which can generate try-on results based on\ndifferent textual instructions and model garment images to meet various needs,\neliminating the reliance on masks, poses, or other conditions. Specifically, we\nfirst construct the virtual try-on dataset LAION-Garment, the largest known\nopen-source garment try-on dataset. Then, we introduce adaptive position\nembedding, which enables the model to generate satisfactory outfitted model\nimages or garment images based on input images of different sizes and\ncategories, significantly enhancing the generalization and controllability of\nVTON generation. In our experiments, we demonstrate the effectiveness of our\nAny2AnyTryon and compare it with existing methods. The results show that\nAny2AnyTryon enables flexible, controllable, and high-quality image-based\nvirtual try-on generation.https://logn-2024.github.io/Any2anyTryonProjectPage/", upvotes=10, thumbnail=None)],
 '2025-01-31': [Paper(id='2501.18585', authors=['Yue Wang', 'Qiuzhi Liu', 'Jiahao Xu', 'Tian Liang', 'Xingyu Chen', 'Zhiwei He', 'Linfeng Song', 'Dian Yu', 'Juntao Li', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], published_at=datetime.datetime(2025, 1, 31, 0, 16, 36, 453000, tzinfo=datetime.timezone.utc), title='Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs', summary="Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.", upvotes=43, thumbnail=None),
                Paper(id='2501.18512', authors=['Arthur Douillard', 'Yanislav Donchev', 'Keith Rush', 'Satyen Kale', 'Zachary Charles', 'Zachary Garrett', 'Gabriel Teston', 'Dave Lacey', 'Ross McIlroy', 'Jiajun Shen', 'Alexandre Ram√©', 'Arthur Szlam', "Marc'Aurelio Ranzato", 'Paul Barham'], published_at=datetime.datetime(2025, 1, 31, 5, 7, 14, 120000, tzinfo=datetime.timezone.utc), title='Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch', summary="Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.", upvotes=23, thumbnail=None),
                Paper(id='2501.18009', authors=['Lan Pan', 'Hanbo Xie', 'Robert C. Wilson'], published_at=datetime.datetime(2025, 1, 31, 0, 9, 40, 77000, tzinfo=datetime.timezone.utc), title='Large Language Models Think Too Fast To Explore Effectively', summary='Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability.', upvotes=21, thumbnail=None),
                Paper(id='2501.18362', authors=['Yuxin Zuo', 'Shang Qu', 'Yifei Li', 'Zhangren Chen', 'Xuekai Zhu', 'Ermo Hua', 'Kaiyan Zhang', 'Ning Ding', 'Bowen Zhou'], published_at=datetime.datetime(2025, 1, 31, 4, 14, 53, 856000, tzinfo=datetime.timezone.utc), title='MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding', summary='We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.', upvotes=19, thumbnail=None),
                Paper(id='2501.18438', authors=['Aitor Arrieta', 'Miriam Ugarte', 'Pablo Valle', 'Jos√© Antonio Parejo', 'Sergio Segura'], published_at=datetime.datetime(2025, 1, 31, 2, 35, 40, 107000, tzinfo=datetime.timezone.utc), title='o3-mini vs DeepSeek-R1: Which One is Safer?', summary="The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.", upvotes=19, thumbnail=None)],
 '2025-02-01': [],
 '2025-02-02': [],
 '2025-02-03': [Paper(id='2501.14677', authors=['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], published_at=datetime.datetime(2025, 2, 3, 13, 15, 59, 743000, tzinfo=datetime.timezone.utc), title='MatAnyone: Stable Video Matting with Consistent Memory Propagation', summary='Auxiliary-free human video matting methods, which rely solely on input\nframes, often struggle with complex or ambiguous backgrounds. To address this,\nwe propose MatAnyone, a robust framework tailored for target-assigned video\nmatting. Specifically, building on a memory-based paradigm, we introduce a\nconsistent memory propagation module via region-adaptive memory fusion, which\nadaptively integrates memory from the previous frame. This ensures semantic\nstability in core regions while preserving fine-grained details along object\nboundaries. For robust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a novel training\nstrategy that efficiently leverages large-scale segmentation data, boosting\nmatting stability. With this new network design, dataset, and training\nstrategy, MatAnyone delivers robust and accurate video matting results in\ndiverse real-world scenarios, outperforming existing methods.', upvotes=13, thumbnail=None),
                Paper(id='2501.19339', authors=['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], published_at=datetime.datetime(2025, 2, 3, 10, 59, 18, 508000, tzinfo=datetime.timezone.utc), title='PixelWorld: Towards Perceiving Everything as Pixels', summary='Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models\' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels\' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.', upvotes=12, thumbnail=None),
                Paper(id='2501.18119', authors=['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], published_at=datetime.datetime(2025, 2, 3, 6, 6, 33, 957000, tzinfo=datetime.timezone.utc), title='Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models', summary='Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.', upvotes=12, thumbnail=None),
                Paper(id='2501.19399', authors=['Ken M. Nakanishi'], published_at=datetime.datetime(2025, 2, 3, 13, 1, 15, 923000, tzinfo=datetime.timezone.utc), title='Scalable-Softmax Is Superior for Attention', summary="The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.", upvotes=11, thumbnail=None)]}
2025-02-04 21:53:12,042 - INFO - Papers: {'2025-01-27': [Paper(id='2501.13953', authors=['Zicheng Zhang', 'Xiangyu Zhao', 'Xinyu Fang', 'Chunyi Li', 'Xiaohong Liu', 'Xiongkuo Min', 'Haodong Duan', 'Kai Chen', 'Guangtao Zhai'], published_at=datetime.datetime(2025, 1, 27, 3, 5, 15, 907000, tzinfo=datetime.timezone.utc), title='Redundancy Principles for MLLMs Benchmarks', summary="With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively.", upvotes=25, thumbnail=None),
                Paper(id='2501.14176', authors=['Micah Rentschler', 'Jesse Roberts'], published_at=datetime.datetime(2025, 1, 27, 9, 59, 51, 940000, tzinfo=datetime.timezone.utc), title='RL + Transformer = A General-Purpose Problem Solver', summary='What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.', upvotes=19, thumbnail=None),
                Paper(id='2501.13687', authors=['Sara Kothari', 'Ayush Gupta'], published_at=datetime.datetime(2025, 1, 27, 11, 6, 4, 998000, tzinfo=datetime.timezone.utc), title='Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs', summary='Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop', upvotes=7, thumbnail=None),
                Paper(id='2411.19458', authors=['Yang You', 'Yixin Li', 'Congyue Deng', 'Yue Wang', 'Leonidas Guibas'], published_at=datetime.datetime(2025, 1, 27, 8, 43, 43, 143000, tzinfo=datetime.timezone.utc), title='Multiview Equivariance Improves 3D Correspondence Understanding with\n  Minimal Feature Finetuning', summary='Vision foundation models, particularly the ViT family, have revolutionized\nimage understanding by providing rich semantic features. However, despite their\nsuccess in 2D comprehension, their abilities on grasping 3D spatial\nrelationships are still unclear. In this work, we evaluate and enhance the 3D\nawareness of ViT-based models. We begin by systematically assessing their\nability to learn 3D equivariant features, specifically examining the\nconsistency of semantic embeddings across different viewpoints. Our findings\nindicate that improved 3D equivariance leads to better performance on various\ndownstream tasks, including pose estimation, tracking, and semantic transfer.\nBuilding on this insight, we propose a simple yet effective finetuning strategy\nbased on 3D correspondences, which significantly enhances the 3D correspondence\nunderstanding of existing vision models. Remarkably, even finetuning on a\nsingle object for just one iteration results in substantial performance gains.\nAll code and resources will be made publicly available to support further\nadvancements in 3D-aware vision models. Our code is available at\nhttps://github.com/qq456cvb/3DCorrEnhance.', upvotes=5, thumbnail=None),
                Paper(id='2501.13925', authors=['Akashah Shabbir', 'Mohammed Zumri', 'Mohammed Bennamoun', 'Fahad S. Khan', 'Salman Khan'], published_at=datetime.datetime(2025, 1, 27, 8, 27, 33, 4000, tzinfo=datetime.timezone.utc), title='GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing', summary='Recent advances in large multimodal models (LMMs) have recognized\nfine-grained grounding as an imperative factor of visual understanding and\ndialogue. However, the benefits of such representation in LMMs are limited to\nthe natural image domain, and these models perform poorly for remote sensing\n(RS). The distinct overhead viewpoint, scale variation, and presence of small\nobjects in high-resolution RS imagery present a unique challenge in\nregion-level comprehension. Moreover, the development of the grounding\nconversation capability of LMMs within RS is hindered by the lack of granular,\nRS domain-specific grounded data. Addressing these limitations, we propose\nGeoPixel - the first end-to-end high resolution RS-LMM that supports\npixel-level grounding. This capability allows fine-grained visual perception by\ngenerating interleaved masks in conversation. GeoPixel supports up to 4K HD\nresolution in any aspect ratio, ideal for high-precision RS image analysis. To\nsupport the grounded conversation generation (GCG) in RS imagery, we curate a\nvisually grounded dataset GeoPixelD through a semi-automated pipeline that\nutilizes set-of-marks prompting and spatial priors tailored for RS data to\nmethodically control the data generation process. GeoPixel demonstrates\nsuperior performance in pixel-level comprehension, surpassing existing LMMs in\nboth single-target and multi-target segmentation tasks. Our methodological\nablation studies validate the effectiveness of each component in the overall\narchitecture. Our code and data will be publicly released.', upvotes=5, thumbnail=None)],
 '2025-01-28': [Paper(id='2501.15383', authors=['An Yang', 'Bowen Yu', 'Chengyuan Li', 'Dayiheng Liu', 'Fei Huang', 'Haoyan Huang', 'Jiandong Jiang', 'Jianhong Tu', 'Jianwei Zhang', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Kexin Yang', 'Le Yu', 'Mei Li', 'Minmin Sun', 'Qin Zhu', 'Rui Men', 'Tao He', 'Weijia Xu', 'Wenbiao Yin', 'Wenyuan Yu', 'Xiafei Qiu', 'Xingzhang Ren', 'Xinlong Yang', 'Yong Li', 'Zhiying Xu', 'Zipeng Zhang'], published_at=datetime.datetime(2025, 1, 28, 0, 35, 46, 871000, tzinfo=datetime.timezone.utc), title='Qwen2.5-1M Technical Report', summary='We introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such as long data synthesis,\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes a\nlength extrapolation method that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement a sparse attention method along with chunked\nprefill optimization for deployment scenarios and a sparsity refinement method\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, including kernel optimization, pipeline parallelism, and\nscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\ntokens of context. This framework provides an efficient and powerful solution\nfor developing applications that require long-context processing using\nopen-source models.\n  The Qwen2.5-1M series currently includes the open-source models\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\nimproved in long-context tasks without compromising performance in\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\ncontexts eight times longer.', upvotes=50, thumbnail=None),
                Paper(id='2501.15368', authors=['Yadong Li', 'Jun Liu', 'Tao Zhang', 'Tao Zhang', 'Song Chen', 'Tianpeng Li', 'Zehuan Li', 'Lijun Liu', 'Lingfeng Ming', 'Guosheng Dong', 'Da Pan', 'Chong Li', 'Yuanbo Fang', 'Dongdong Kuang', 'Mingrui Wang', 'Chenglin Zhu', 'Youwei Zhang', 'Hongyu Guo', 'Fengyu Zhang', 'Yuran Wang', 'Bowen Ding', 'Wei Song', 'Xu Li', 'Yuqi Huo', 'Zheng Liang', 'Shusen Zhang', 'Xin Wu', 'Shuai Zhao', 'Linchu Xiong', 'Yozhen Wu', 'Jiahui Ye', 'Wenhao Lu', 'Bowen Li', 'Yan Zhang', 'Yaqi Zhou', 'Xin Chen', 'Lei Su', 'Hongda Zhang', 'Fuzhong Chen', 'Xuezhen Dong', 'Na Nie', 'Zhiying Wu', 'Bin Xiao', 'Ting Li', 'Shunya Dang', 'Ping Zhang', 'Yijia Sun', 'Jincheng Wu', 'Jinjie Yang', 'Xionghai Lin', 'Zhi Ma', 'Kegeng Wu', 'Jia li', 'Aiyuan Yang', 'Hui Liu', 'Jianqiang Zhang', 'Xiaoxi Chen', 'Guangwei Ai', 'Wentao Zhang', 'Yicong Chen', 'Xiaoqin Huang', 'Kun Li', 'Wenjing Luo', 'Yifei Duan', 'Lingling Zhu', 'Ran Xiao', 'Zhe Su', 'Jiani Pu', 'Dian Wang', 'Xu Jia', 'Tianyu Zhang', 'Mengyu Ai', 'Mang Wang', 'Yujing Qiao', 'Lei Zhang', 'Yanjun Shen', 'Fan Yang', 'Miao Zhen', 'Yijie Zhou', 'Mingyang Chen', 'Fei Li', 'Chenzheng Zhu', 'Keer Lu', 'Yaqi Zhao', 'Hao Liang', 'Youquan Li', 'Yanzhao Qin', 'Linzhuang Sun', 'Jianhua Xu', 'Haoze Sun', 'Mingan Lin', 'Zenan Zhou', 'Weipeng Chen'], published_at=datetime.datetime(2025, 1, 28, 0, 34, 49, 721000, tzinfo=datetime.timezone.utc), title='Baichuan-Omni-1.5 Technical Report', summary='We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\nomni-modal understanding capabilities but also provides end-to-end audio\ngeneration capabilities. To achieve fluent and high-quality interaction across\nmodalities without compromising the capabilities of any modality, we\nprioritized optimizing three key aspects. First, we establish a comprehensive\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\nacoustic information from audio, enabling seamless integration and enhanced\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\nthat progressively integrates multimodal alignment and multitask fine-tuning,\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\nto leading models such as Qwen2-VL-72B across various multimodal medical\nbenchmarks.', upvotes=48, thumbnail=None),
                Paper(id='2501.16142', authors=['Scott Fujimoto', "Pierluca D'Oro", 'Amy Zhang', 'Yuandong Tian', 'Michael Rabbat'], published_at=datetime.datetime(2025, 1, 28, 0, 36, 9, 186000, tzinfo=datetime.timezone.utc), title='Towards General-Purpose Model-Free Reinforcement Learning', summary='Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.', upvotes=23, thumbnail=None),
                Paper(id='2501.15570', authors=['Lin Yueyu', 'Li Zhiyuan', 'Peter Yue', 'Liu Xiao'], published_at=datetime.datetime(2025, 1, 28, 3, 2, 56, 62000, tzinfo=datetime.timezone.utc), title='ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer', summary="As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\nhttps://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside},\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.", upvotes=22, thumbnail=None),
                Paper(id='2501.15907', authors=['Haorui He', 'Zengqiang Shang', 'Chaoren Wang', 'Xuyuan Li', 'Yicheng Gu', 'Hua Hua', 'Liwei Liu', 'Chen Yang', 'Jiaqi Li', 'Peiyang Shi', 'Yuancheng Wang', 'Kai Chen', 'Pengyuan Zhang', 'Zhizheng Wu'], published_at=datetime.datetime(2025, 1, 28, 5, 40, 25, 750000, tzinfo=datetime.timezone.utc), title='Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation', summary='Recent advancements in speech generation have been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexplored\nin-the-wild data that capture spontaneous human speech in real-world contexts.\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate that Emilia\nsignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiverse speaker timbre and speaking styles of real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvance speech generation research and validates the effectiveness of Emilia\nfor both multilingual and crosslingual speech generation.', upvotes=15, thumbnail=None)],
 '2025-01-29': [Paper(id='2501.16764', authors=['Chenguo Lin', 'Panwang Pan', 'Bangbang Yang', 'Zeming Li', 'Yadong Mu'], published_at=datetime.datetime(2025, 1, 29, 1, 12, 2, 839000, tzinfo=datetime.timezone.utc), title='DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian\n  Splat Generation', summary='Recent advancements in 3D content generation from text or a single image\nstruggle with limited high-quality 3D datasets and inconsistency from 2D\nmulti-view generation. We introduce DiffSplat, a novel 3D generative framework\nthat natively generates 3D Gaussian splats by taming large-scale text-to-image\ndiffusion models. It differs from previous 3D generative models by effectively\nutilizing web-scale 2D priors while maintaining 3D consistency in a unified\nmodel. To bootstrap the training, a lightweight reconstruction model is\nproposed to instantly produce multi-view Gaussian splat grids for scalable\ndataset curation. In conjunction with the regular diffusion loss on these\ngrids, a 3D rendering loss is introduced to facilitate 3D coherence across\narbitrary views. The compatibility with image diffusion models enables seamless\nadaptions of numerous techniques for image generation to the 3D realm.\nExtensive experiments reveal the superiority of DiffSplat in text- and\nimage-conditioned generation tasks and downstream applications. Thorough\nablation studies validate the efficacy of each critical design choice and\nprovide insights into the underlying mechanism.', upvotes=20, thumbnail=None)],
 '2025-01-30': [Paper(id='2501.14334', authors=['Cl√©ment Desroches', 'Martin Chauvin', 'Louis Ladan', 'Caroline Vateau', 'Simon Gosset', 'Philippe Cordier'], published_at=datetime.datetime(2025, 1, 30, 3, 5, 8, 789000, tzinfo=datetime.timezone.utc), title="Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts", summary='The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies\' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company\'s AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n"Return on Environment" metric to align AI development with net-zero goals.', upvotes=15, thumbnail=None),
                Paper(id='2501.15891', authors=['Hailong Guo', 'Bohan Zeng', 'Yiren Song', 'Wentao Zhang', 'Chuang Zhang', 'Jiaming Liu'], published_at=datetime.datetime(2025, 1, 30, 20, 14, 3, 298000, tzinfo=datetime.timezone.utc), title='Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile\n  Virtual Clothing Tasks', summary="Image-based virtual try-on (VTON) aims to generate a virtual try-on result by\ntransferring an input garment onto a target person's image. However, the\nscarcity of paired garment-model data makes it challenging for existing methods\nto achieve high generalization and quality in VTON. Also, it limits the ability\nto generate mask-free try-ons. To tackle the data scarcity problem, approaches\nsuch as Stable Garment and MMTryon use a synthetic data strategy, effectively\nincreasing the amount of paired data on the model side. However, existing\nmethods are typically limited to performing specific try-on tasks and lack\nuser-friendliness. To enhance the generalization and controllability of VTON\ngeneration, we propose Any2AnyTryon, which can generate try-on results based on\ndifferent textual instructions and model garment images to meet various needs,\neliminating the reliance on masks, poses, or other conditions. Specifically, we\nfirst construct the virtual try-on dataset LAION-Garment, the largest known\nopen-source garment try-on dataset. Then, we introduce adaptive position\nembedding, which enables the model to generate satisfactory outfitted model\nimages or garment images based on input images of different sizes and\ncategories, significantly enhancing the generalization and controllability of\nVTON generation. In our experiments, we demonstrate the effectiveness of our\nAny2AnyTryon and compare it with existing methods. The results show that\nAny2AnyTryon enables flexible, controllable, and high-quality image-based\nvirtual try-on generation.https://logn-2024.github.io/Any2anyTryonProjectPage/", upvotes=10, thumbnail=None),
                Paper(id='2501.15654', authors=['Jenna Russell', 'Marzena Karpinska', 'Mohit Iyyer'], published_at=datetime.datetime(2025, 1, 30, 9, 31, 27, 980000, tzinfo=datetime.timezone.utc), title='People who frequently use ChatGPT for writing tasks are accurate and\n  robust detectors of AI-generated text', summary='In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such "expert" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts\' free-form explanations shows that while they rely heavily on specific\nlexical clues (\'AI vocabulary\'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.', upvotes=9, thumbnail=None),
                Paper(id='2501.17433', authors=['Tiansheng Huang', 'Sihao Hu', 'Fatih Ilhan', 'Selim Furkan Tekin', 'Ling Liu'], published_at=datetime.datetime(2025, 1, 30, 1, 30, 18, 13000, tzinfo=datetime.timezone.utc), title='Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation', summary='Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus', upvotes=7, thumbnail=None)],
 '2025-01-31': [Paper(id='2501.18585', authors=['Yue Wang', 'Qiuzhi Liu', 'Jiahao Xu', 'Tian Liang', 'Xingyu Chen', 'Zhiwei He', 'Linfeng Song', 'Dian Yu', 'Juntao Li', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], published_at=datetime.datetime(2025, 1, 31, 0, 16, 36, 453000, tzinfo=datetime.timezone.utc), title='Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs', summary="Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.", upvotes=43, thumbnail=None),
                Paper(id='2501.18512', authors=['Arthur Douillard', 'Yanislav Donchev', 'Keith Rush', 'Satyen Kale', 'Zachary Charles', 'Zachary Garrett', 'Gabriel Teston', 'Dave Lacey', 'Ross McIlroy', 'Jiajun Shen', 'Alexandre Ram√©', 'Arthur Szlam', "Marc'Aurelio Ranzato", 'Paul Barham'], published_at=datetime.datetime(2025, 1, 31, 5, 7, 14, 120000, tzinfo=datetime.timezone.utc), title='Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch', summary="Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.", upvotes=23, thumbnail=None),
                Paper(id='2501.18009', authors=['Lan Pan', 'Hanbo Xie', 'Robert C. Wilson'], published_at=datetime.datetime(2025, 1, 31, 0, 9, 40, 77000, tzinfo=datetime.timezone.utc), title='Large Language Models Think Too Fast To Explore Effectively', summary='Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability.', upvotes=21, thumbnail=None),
                Paper(id='2501.18362', authors=['Yuxin Zuo', 'Shang Qu', 'Yifei Li', 'Zhangren Chen', 'Xuekai Zhu', 'Ermo Hua', 'Kaiyan Zhang', 'Ning Ding', 'Bowen Zhou'], published_at=datetime.datetime(2025, 1, 31, 4, 14, 53, 856000, tzinfo=datetime.timezone.utc), title='MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding', summary='We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.', upvotes=19, thumbnail=None),
                Paper(id='2501.18438', authors=['Aitor Arrieta', 'Miriam Ugarte', 'Pablo Valle', 'Jos√© Antonio Parejo', 'Sergio Segura'], published_at=datetime.datetime(2025, 1, 31, 2, 35, 40, 107000, tzinfo=datetime.timezone.utc), title='o3-mini vs DeepSeek-R1: Which One is Safer?', summary="The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.", upvotes=19, thumbnail=None)],
 '2025-02-01': [],
 '2025-02-02': [],
 '2025-02-03': [Paper(id='2501.14677', authors=['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], published_at=datetime.datetime(2025, 2, 3, 13, 15, 59, 743000, tzinfo=datetime.timezone.utc), title='MatAnyone: Stable Video Matting with Consistent Memory Propagation', summary='Auxiliary-free human video matting methods, which rely solely on input\nframes, often struggle with complex or ambiguous backgrounds. To address this,\nwe propose MatAnyone, a robust framework tailored for target-assigned video\nmatting. Specifically, building on a memory-based paradigm, we introduce a\nconsistent memory propagation module via region-adaptive memory fusion, which\nadaptively integrates memory from the previous frame. This ensures semantic\nstability in core regions while preserving fine-grained details along object\nboundaries. For robust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a novel training\nstrategy that efficiently leverages large-scale segmentation data, boosting\nmatting stability. With this new network design, dataset, and training\nstrategy, MatAnyone delivers robust and accurate video matting results in\ndiverse real-world scenarios, outperforming existing methods.', upvotes=13, thumbnail=None),
                Paper(id='2501.19339', authors=['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], published_at=datetime.datetime(2025, 2, 3, 10, 59, 18, 508000, tzinfo=datetime.timezone.utc), title='PixelWorld: Towards Perceiving Everything as Pixels', summary='Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models\' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels\' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.', upvotes=12, thumbnail=None),
                Paper(id='2501.18119', authors=['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], published_at=datetime.datetime(2025, 2, 3, 6, 6, 33, 957000, tzinfo=datetime.timezone.utc), title='Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models', summary='Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.', upvotes=12, thumbnail=None),
                Paper(id='2501.19399', authors=['Ken M. Nakanishi'], published_at=datetime.datetime(2025, 2, 3, 13, 1, 15, 923000, tzinfo=datetime.timezone.utc), title='Scalable-Softmax Is Superior for Attention', summary="The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.", upvotes=11, thumbnail=None),
                Paper(id='2411.04983', authors=['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], published_at=datetime.datetime(2025, 2, 3, 3, 10, 8, 761000, tzinfo=datetime.timezone.utc), title='DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot\n  Planning', summary='The ability to predict future outcomes given control actions is fundamental\nfor physical reasoning. However, such predictive models, often called world\nmodels, have proven challenging to learn and are typically developed for\ntask-specific solutions with online policy learning. We argue that the true\npotential of world models lies in their ability to reason and plan across\ndiverse problems using only passive data. Concretely, we require world models\nto have the following three properties: 1) be trainable on offline,\npre-collected trajectories, 2) support test-time behavior optimization, and 3)\nfacilitate task-agnostic reasoning. To realize this, we present DINO World\nModel (DINO-WM), a new method to model visual dynamics without reconstructing\nthe visual world. DINO-WM leverages spatial patch features pre-trained with\nDINOv2, enabling it to learn from offline behavioral trajectories by predicting\nfuture patch features. This design allows DINO-WM to achieve observational\ngoals through action sequence optimization, facilitating task-agnostic behavior\nplanning by treating desired goal patch features as prediction targets. We\nevaluate DINO-WM across various domains, including maze navigation, tabletop\npushing, and particle manipulation. Our experiments demonstrate that DINO-WM\ncan generate zero-shot behavioral solutions at test time without relying on\nexpert demonstrations, reward modeling, or pre-learned inverse models. Notably,\nDINO-WM exhibits strong generalization capabilities compared to prior\nstate-of-the-art work, adapting to diverse task families such as arbitrarily\nconfigured mazes, push manipulation with varied object shapes, and\nmulti-particle scenarios.', upvotes=9, thumbnail=None)]}
2025-02-04 21:55:35,665 - INFO - Papers: {'2025-01-27': [],
 '2025-01-28': [],
 '2025-01-29': [],
 '2025-01-30': [],
 '2025-01-31': [],
 '2025-02-01': [],
 '2025-02-02': [],
 '2025-02-03': []}
2025-02-04 21:56:23,930 - INFO - Papers: {'2025-01-27': [Paper(id='2501.13953', authors=['Zicheng Zhang', 'Xiangyu Zhao', 'Xinyu Fang', 'Chunyi Li', 'Xiaohong Liu', 'Xiongkuo Min', 'Haodong Duan', 'Kai Chen', 'Guangtao Zhai'], published_at=datetime.datetime(2025, 1, 27, 3, 5, 15, 907000, tzinfo=datetime.timezone.utc), title='Redundancy Principles for MLLMs Benchmarks', summary="With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively.", upvotes=25, thumbnail=None),
                Paper(id='2501.14176', authors=['Micah Rentschler', 'Jesse Roberts'], published_at=datetime.datetime(2025, 1, 27, 9, 59, 51, 940000, tzinfo=datetime.timezone.utc), title='RL + Transformer = A General-Purpose Problem Solver', summary='What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.', upvotes=19, thumbnail=None),
                Paper(id='2501.13687', authors=['Sara Kothari', 'Ayush Gupta'], published_at=datetime.datetime(2025, 1, 27, 11, 6, 4, 998000, tzinfo=datetime.timezone.utc), title='Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs', summary='Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop', upvotes=7, thumbnail=None),
                Paper(id='2411.19458', authors=['Yang You', 'Yixin Li', 'Congyue Deng', 'Yue Wang', 'Leonidas Guibas'], published_at=datetime.datetime(2025, 1, 27, 8, 43, 43, 143000, tzinfo=datetime.timezone.utc), title='Multiview Equivariance Improves 3D Correspondence Understanding with\n  Minimal Feature Finetuning', summary='Vision foundation models, particularly the ViT family, have revolutionized\nimage understanding by providing rich semantic features. However, despite their\nsuccess in 2D comprehension, their abilities on grasping 3D spatial\nrelationships are still unclear. In this work, we evaluate and enhance the 3D\nawareness of ViT-based models. We begin by systematically assessing their\nability to learn 3D equivariant features, specifically examining the\nconsistency of semantic embeddings across different viewpoints. Our findings\nindicate that improved 3D equivariance leads to better performance on various\ndownstream tasks, including pose estimation, tracking, and semantic transfer.\nBuilding on this insight, we propose a simple yet effective finetuning strategy\nbased on 3D correspondences, which significantly enhances the 3D correspondence\nunderstanding of existing vision models. Remarkably, even finetuning on a\nsingle object for just one iteration results in substantial performance gains.\nAll code and resources will be made publicly available to support further\nadvancements in 3D-aware vision models. Our code is available at\nhttps://github.com/qq456cvb/3DCorrEnhance.', upvotes=5, thumbnail=None),
                Paper(id='2501.13925', authors=['Akashah Shabbir', 'Mohammed Zumri', 'Mohammed Bennamoun', 'Fahad S. Khan', 'Salman Khan'], published_at=datetime.datetime(2025, 1, 27, 8, 27, 33, 4000, tzinfo=datetime.timezone.utc), title='GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing', summary='Recent advances in large multimodal models (LMMs) have recognized\nfine-grained grounding as an imperative factor of visual understanding and\ndialogue. However, the benefits of such representation in LMMs are limited to\nthe natural image domain, and these models perform poorly for remote sensing\n(RS). The distinct overhead viewpoint, scale variation, and presence of small\nobjects in high-resolution RS imagery present a unique challenge in\nregion-level comprehension. Moreover, the development of the grounding\nconversation capability of LMMs within RS is hindered by the lack of granular,\nRS domain-specific grounded data. Addressing these limitations, we propose\nGeoPixel - the first end-to-end high resolution RS-LMM that supports\npixel-level grounding. This capability allows fine-grained visual perception by\ngenerating interleaved masks in conversation. GeoPixel supports up to 4K HD\nresolution in any aspect ratio, ideal for high-precision RS image analysis. To\nsupport the grounded conversation generation (GCG) in RS imagery, we curate a\nvisually grounded dataset GeoPixelD through a semi-automated pipeline that\nutilizes set-of-marks prompting and spatial priors tailored for RS data to\nmethodically control the data generation process. GeoPixel demonstrates\nsuperior performance in pixel-level comprehension, surpassing existing LMMs in\nboth single-target and multi-target segmentation tasks. Our methodological\nablation studies validate the effectiveness of each component in the overall\narchitecture. Our code and data will be publicly released.', upvotes=5, thumbnail=None)],
 '2025-01-28': [Paper(id='2501.15383', authors=['An Yang', 'Bowen Yu', 'Chengyuan Li', 'Dayiheng Liu', 'Fei Huang', 'Haoyan Huang', 'Jiandong Jiang', 'Jianhong Tu', 'Jianwei Zhang', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Kexin Yang', 'Le Yu', 'Mei Li', 'Minmin Sun', 'Qin Zhu', 'Rui Men', 'Tao He', 'Weijia Xu', 'Wenbiao Yin', 'Wenyuan Yu', 'Xiafei Qiu', 'Xingzhang Ren', 'Xinlong Yang', 'Yong Li', 'Zhiying Xu', 'Zipeng Zhang'], published_at=datetime.datetime(2025, 1, 28, 0, 35, 46, 871000, tzinfo=datetime.timezone.utc), title='Qwen2.5-1M Technical Report', summary='We introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such as long data synthesis,\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes a\nlength extrapolation method that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement a sparse attention method along with chunked\nprefill optimization for deployment scenarios and a sparsity refinement method\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, including kernel optimization, pipeline parallelism, and\nscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\ntokens of context. This framework provides an efficient and powerful solution\nfor developing applications that require long-context processing using\nopen-source models.\n  The Qwen2.5-1M series currently includes the open-source models\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\nimproved in long-context tasks without compromising performance in\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\ncontexts eight times longer.', upvotes=50, thumbnail=None),
                Paper(id='2501.15368', authors=['Yadong Li', 'Jun Liu', 'Tao Zhang', 'Tao Zhang', 'Song Chen', 'Tianpeng Li', 'Zehuan Li', 'Lijun Liu', 'Lingfeng Ming', 'Guosheng Dong', 'Da Pan', 'Chong Li', 'Yuanbo Fang', 'Dongdong Kuang', 'Mingrui Wang', 'Chenglin Zhu', 'Youwei Zhang', 'Hongyu Guo', 'Fengyu Zhang', 'Yuran Wang', 'Bowen Ding', 'Wei Song', 'Xu Li', 'Yuqi Huo', 'Zheng Liang', 'Shusen Zhang', 'Xin Wu', 'Shuai Zhao', 'Linchu Xiong', 'Yozhen Wu', 'Jiahui Ye', 'Wenhao Lu', 'Bowen Li', 'Yan Zhang', 'Yaqi Zhou', 'Xin Chen', 'Lei Su', 'Hongda Zhang', 'Fuzhong Chen', 'Xuezhen Dong', 'Na Nie', 'Zhiying Wu', 'Bin Xiao', 'Ting Li', 'Shunya Dang', 'Ping Zhang', 'Yijia Sun', 'Jincheng Wu', 'Jinjie Yang', 'Xionghai Lin', 'Zhi Ma', 'Kegeng Wu', 'Jia li', 'Aiyuan Yang', 'Hui Liu', 'Jianqiang Zhang', 'Xiaoxi Chen', 'Guangwei Ai', 'Wentao Zhang', 'Yicong Chen', 'Xiaoqin Huang', 'Kun Li', 'Wenjing Luo', 'Yifei Duan', 'Lingling Zhu', 'Ran Xiao', 'Zhe Su', 'Jiani Pu', 'Dian Wang', 'Xu Jia', 'Tianyu Zhang', 'Mengyu Ai', 'Mang Wang', 'Yujing Qiao', 'Lei Zhang', 'Yanjun Shen', 'Fan Yang', 'Miao Zhen', 'Yijie Zhou', 'Mingyang Chen', 'Fei Li', 'Chenzheng Zhu', 'Keer Lu', 'Yaqi Zhao', 'Hao Liang', 'Youquan Li', 'Yanzhao Qin', 'Linzhuang Sun', 'Jianhua Xu', 'Haoze Sun', 'Mingan Lin', 'Zenan Zhou', 'Weipeng Chen'], published_at=datetime.datetime(2025, 1, 28, 0, 34, 49, 721000, tzinfo=datetime.timezone.utc), title='Baichuan-Omni-1.5 Technical Report', summary='We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\nomni-modal understanding capabilities but also provides end-to-end audio\ngeneration capabilities. To achieve fluent and high-quality interaction across\nmodalities without compromising the capabilities of any modality, we\nprioritized optimizing three key aspects. First, we establish a comprehensive\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\nacoustic information from audio, enabling seamless integration and enhanced\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\nthat progressively integrates multimodal alignment and multitask fine-tuning,\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\nto leading models such as Qwen2-VL-72B across various multimodal medical\nbenchmarks.', upvotes=48, thumbnail=None),
                Paper(id='2501.16142', authors=['Scott Fujimoto', "Pierluca D'Oro", 'Amy Zhang', 'Yuandong Tian', 'Michael Rabbat'], published_at=datetime.datetime(2025, 1, 28, 0, 36, 9, 186000, tzinfo=datetime.timezone.utc), title='Towards General-Purpose Model-Free Reinforcement Learning', summary='Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.', upvotes=23, thumbnail=None),
                Paper(id='2501.15570', authors=['Lin Yueyu', 'Li Zhiyuan', 'Peter Yue', 'Liu Xiao'], published_at=datetime.datetime(2025, 1, 28, 3, 2, 56, 62000, tzinfo=datetime.timezone.utc), title='ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer', summary="As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\nhttps://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside},\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.", upvotes=22, thumbnail=None),
                Paper(id='2501.15907', authors=['Haorui He', 'Zengqiang Shang', 'Chaoren Wang', 'Xuyuan Li', 'Yicheng Gu', 'Hua Hua', 'Liwei Liu', 'Chen Yang', 'Jiaqi Li', 'Peiyang Shi', 'Yuancheng Wang', 'Kai Chen', 'Pengyuan Zhang', 'Zhizheng Wu'], published_at=datetime.datetime(2025, 1, 28, 5, 40, 25, 750000, tzinfo=datetime.timezone.utc), title='Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation', summary='Recent advancements in speech generation have been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexplored\nin-the-wild data that capture spontaneous human speech in real-world contexts.\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate that Emilia\nsignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiverse speaker timbre and speaking styles of real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvance speech generation research and validates the effectiveness of Emilia\nfor both multilingual and crosslingual speech generation.', upvotes=15, thumbnail=None)],
 '2025-01-29': [Paper(id='2501.16764', authors=['Chenguo Lin', 'Panwang Pan', 'Bangbang Yang', 'Zeming Li', 'Yadong Mu'], published_at=datetime.datetime(2025, 1, 29, 1, 12, 2, 839000, tzinfo=datetime.timezone.utc), title='DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian\n  Splat Generation', summary='Recent advancements in 3D content generation from text or a single image\nstruggle with limited high-quality 3D datasets and inconsistency from 2D\nmulti-view generation. We introduce DiffSplat, a novel 3D generative framework\nthat natively generates 3D Gaussian splats by taming large-scale text-to-image\ndiffusion models. It differs from previous 3D generative models by effectively\nutilizing web-scale 2D priors while maintaining 3D consistency in a unified\nmodel. To bootstrap the training, a lightweight reconstruction model is\nproposed to instantly produce multi-view Gaussian splat grids for scalable\ndataset curation. In conjunction with the regular diffusion loss on these\ngrids, a 3D rendering loss is introduced to facilitate 3D coherence across\narbitrary views. The compatibility with image diffusion models enables seamless\nadaptions of numerous techniques for image generation to the 3D realm.\nExtensive experiments reveal the superiority of DiffSplat in text- and\nimage-conditioned generation tasks and downstream applications. Thorough\nablation studies validate the efficacy of each critical design choice and\nprovide insights into the underlying mechanism.', upvotes=20, thumbnail=None),
                Paper(id='2501.16937', authors=['Makoto Shing', 'Kou Misaki', 'Han Bao', 'Sho Yokoi', 'Takuya Akiba'], published_at=datetime.datetime(2025, 1, 29, 20, 48, 38, 815000, tzinfo=datetime.timezone.utc), title='TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models', summary="Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\nTemporally Adaptive Interpolated Distillation (TAID), a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\nTAID-LLM-1.5B for language tasks and TAID-VLM-2B for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.", upvotes=4, thumbnail=None),
                Paper(id='2501.17117', authors=['Thibaud Leteno', 'Irina Proskurina', 'Antoine Gourru', 'Julien Velcin', 'Charlotte Laclau', 'Guillaume Metzler', 'Christophe Gravier'], published_at=datetime.datetime(2025, 1, 29, 3, 32, 9, 927000, tzinfo=datetime.timezone.utc), title='Histoires Morales: A French Dataset for Assessing Moral Alignment', summary='Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.', upvotes=3, thumbnail=None),
                Paper(id='2501.14417', authors=['Junhao Hu', 'Jiang Xu', 'Zhixia Liu', 'Yulong He', 'Yuetao Chen', 'Hao Xu', 'Jiang Liu', 'Baoquan Zhang', 'Shining Wan', 'Gengyuan Dan', 'Zhiyu Dong', 'Zhihao Ren', 'Jie Meng', 'Chao He', 'Changhong Liu', 'Tao Xie', 'Dayun Lin', 'Qin Zhang', 'Yue Yu', 'Hao Feng', 'Xusheng Chen', 'Yizhou Shan'], published_at=datetime.datetime(2025, 1, 29, 21, 18, 54, 916000, tzinfo=datetime.timezone.utc), title='DeepFlow: Serverless Large Language Model Serving at Scale', summary='This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers.', upvotes=2, thumbnail=None)],
 '2025-01-30': [Paper(id='2501.14334', authors=['Cl√©ment Desroches', 'Martin Chauvin', 'Louis Ladan', 'Caroline Vateau', 'Simon Gosset', 'Philippe Cordier'], published_at=datetime.datetime(2025, 1, 30, 3, 5, 8, 789000, tzinfo=datetime.timezone.utc), title="Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts", summary='The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies\' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company\'s AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n"Return on Environment" metric to align AI development with net-zero goals.', upvotes=15, thumbnail=None),
                Paper(id='2501.15891', authors=['Hailong Guo', 'Bohan Zeng', 'Yiren Song', 'Wentao Zhang', 'Chuang Zhang', 'Jiaming Liu'], published_at=datetime.datetime(2025, 1, 30, 20, 14, 3, 298000, tzinfo=datetime.timezone.utc), title='Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile\n  Virtual Clothing Tasks', summary="Image-based virtual try-on (VTON) aims to generate a virtual try-on result by\ntransferring an input garment onto a target person's image. However, the\nscarcity of paired garment-model data makes it challenging for existing methods\nto achieve high generalization and quality in VTON. Also, it limits the ability\nto generate mask-free try-ons. To tackle the data scarcity problem, approaches\nsuch as Stable Garment and MMTryon use a synthetic data strategy, effectively\nincreasing the amount of paired data on the model side. However, existing\nmethods are typically limited to performing specific try-on tasks and lack\nuser-friendliness. To enhance the generalization and controllability of VTON\ngeneration, we propose Any2AnyTryon, which can generate try-on results based on\ndifferent textual instructions and model garment images to meet various needs,\neliminating the reliance on masks, poses, or other conditions. Specifically, we\nfirst construct the virtual try-on dataset LAION-Garment, the largest known\nopen-source garment try-on dataset. Then, we introduce adaptive position\nembedding, which enables the model to generate satisfactory outfitted model\nimages or garment images based on input images of different sizes and\ncategories, significantly enhancing the generalization and controllability of\nVTON generation. In our experiments, we demonstrate the effectiveness of our\nAny2AnyTryon and compare it with existing methods. The results show that\nAny2AnyTryon enables flexible, controllable, and high-quality image-based\nvirtual try-on generation.https://logn-2024.github.io/Any2anyTryonProjectPage/", upvotes=10, thumbnail=None),
                Paper(id='2501.15654', authors=['Jenna Russell', 'Marzena Karpinska', 'Mohit Iyyer'], published_at=datetime.datetime(2025, 1, 30, 9, 31, 27, 980000, tzinfo=datetime.timezone.utc), title='People who frequently use ChatGPT for writing tasks are accurate and\n  robust detectors of AI-generated text', summary='In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such "expert" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts\' free-form explanations shows that while they rely heavily on specific\nlexical clues (\'AI vocabulary\'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.', upvotes=9, thumbnail=None),
                Paper(id='2501.17433', authors=['Tiansheng Huang', 'Sihao Hu', 'Fatih Ilhan', 'Selim Furkan Tekin', 'Ling Liu'], published_at=datetime.datetime(2025, 1, 30, 1, 30, 18, 13000, tzinfo=datetime.timezone.utc), title='Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation', summary='Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus', upvotes=7, thumbnail=None)],
 '2025-01-31': [Paper(id='2501.18585', authors=['Yue Wang', 'Qiuzhi Liu', 'Jiahao Xu', 'Tian Liang', 'Xingyu Chen', 'Zhiwei He', 'Linfeng Song', 'Dian Yu', 'Juntao Li', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], published_at=datetime.datetime(2025, 1, 31, 0, 16, 36, 453000, tzinfo=datetime.timezone.utc), title='Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs', summary="Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.", upvotes=43, thumbnail=None),
                Paper(id='2501.18512', authors=['Arthur Douillard', 'Yanislav Donchev', 'Keith Rush', 'Satyen Kale', 'Zachary Charles', 'Zachary Garrett', 'Gabriel Teston', 'Dave Lacey', 'Ross McIlroy', 'Jiajun Shen', 'Alexandre Ram√©', 'Arthur Szlam', "Marc'Aurelio Ranzato", 'Paul Barham'], published_at=datetime.datetime(2025, 1, 31, 5, 7, 14, 120000, tzinfo=datetime.timezone.utc), title='Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch', summary="Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.", upvotes=23, thumbnail=None),
                Paper(id='2501.18009', authors=['Lan Pan', 'Hanbo Xie', 'Robert C. Wilson'], published_at=datetime.datetime(2025, 1, 31, 0, 9, 40, 77000, tzinfo=datetime.timezone.utc), title='Large Language Models Think Too Fast To Explore Effectively', summary='Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability.', upvotes=21, thumbnail=None),
                Paper(id='2501.18362', authors=['Yuxin Zuo', 'Shang Qu', 'Yifei Li', 'Zhangren Chen', 'Xuekai Zhu', 'Ermo Hua', 'Kaiyan Zhang', 'Ning Ding', 'Bowen Zhou'], published_at=datetime.datetime(2025, 1, 31, 4, 14, 53, 856000, tzinfo=datetime.timezone.utc), title='MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding', summary='We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.', upvotes=19, thumbnail=None),
                Paper(id='2501.18438', authors=['Aitor Arrieta', 'Miriam Ugarte', 'Pablo Valle', 'Jos√© Antonio Parejo', 'Sergio Segura'], published_at=datetime.datetime(2025, 1, 31, 2, 35, 40, 107000, tzinfo=datetime.timezone.utc), title='o3-mini vs DeepSeek-R1: Which One is Safer?', summary="The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.", upvotes=19, thumbnail=None)],
 '2025-02-01': [],
 '2025-02-02': [],
 '2025-02-03': [Paper(id='2501.14677', authors=['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], published_at=datetime.datetime(2025, 2, 3, 13, 15, 59, 743000, tzinfo=datetime.timezone.utc), title='MatAnyone: Stable Video Matting with Consistent Memory Propagation', summary='Auxiliary-free human video matting methods, which rely solely on input\nframes, often struggle with complex or ambiguous backgrounds. To address this,\nwe propose MatAnyone, a robust framework tailored for target-assigned video\nmatting. Specifically, building on a memory-based paradigm, we introduce a\nconsistent memory propagation module via region-adaptive memory fusion, which\nadaptively integrates memory from the previous frame. This ensures semantic\nstability in core regions while preserving fine-grained details along object\nboundaries. For robust training, we present a larger, high-quality, and diverse\ndataset for video matting. Additionally, we incorporate a novel training\nstrategy that efficiently leverages large-scale segmentation data, boosting\nmatting stability. With this new network design, dataset, and training\nstrategy, MatAnyone delivers robust and accurate video matting results in\ndiverse real-world scenarios, outperforming existing methods.', upvotes=13, thumbnail=None),
                Paper(id='2501.19339', authors=['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], published_at=datetime.datetime(2025, 2, 3, 10, 59, 18, 508000, tzinfo=datetime.timezone.utc), title='PixelWorld: Towards Perceiving Everything as Pixels', summary='Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models\' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels\' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.', upvotes=12, thumbnail=None),
                Paper(id='2501.18119', authors=['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], published_at=datetime.datetime(2025, 2, 3, 6, 6, 33, 957000, tzinfo=datetime.timezone.utc), title='Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models', summary='Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.', upvotes=12, thumbnail=None),
                Paper(id='2501.19399', authors=['Ken M. Nakanishi'], published_at=datetime.datetime(2025, 2, 3, 13, 1, 15, 923000, tzinfo=datetime.timezone.utc), title='Scalable-Softmax Is Superior for Attention', summary="The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.", upvotes=11, thumbnail=None),
                Paper(id='2411.04983', authors=['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], published_at=datetime.datetime(2025, 2, 3, 3, 10, 8, 761000, tzinfo=datetime.timezone.utc), title='DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot\n  Planning', summary='The ability to predict future outcomes given control actions is fundamental\nfor physical reasoning. However, such predictive models, often called world\nmodels, have proven challenging to learn and are typically developed for\ntask-specific solutions with online policy learning. We argue that the true\npotential of world models lies in their ability to reason and plan across\ndiverse problems using only passive data. Concretely, we require world models\nto have the following three properties: 1) be trainable on offline,\npre-collected trajectories, 2) support test-time behavior optimization, and 3)\nfacilitate task-agnostic reasoning. To realize this, we present DINO World\nModel (DINO-WM), a new method to model visual dynamics without reconstructing\nthe visual world. DINO-WM leverages spatial patch features pre-trained with\nDINOv2, enabling it to learn from offline behavioral trajectories by predicting\nfuture patch features. This design allows DINO-WM to achieve observational\ngoals through action sequence optimization, facilitating task-agnostic behavior\nplanning by treating desired goal patch features as prediction targets. We\nevaluate DINO-WM across various domains, including maze navigation, tabletop\npushing, and particle manipulation. Our experiments demonstrate that DINO-WM\ncan generate zero-shot behavioral solutions at test time without relying on\nexpert demonstrations, reward modeling, or pre-learned inverse models. Notably,\nDINO-WM exhibits strong generalization capabilities compared to prior\nstate-of-the-art work, adapting to diverse task families such as arbitrarily\nconfigured mazes, push manipulation with varied object shapes, and\nmulti-particle scenarios.', upvotes=9, thumbnail=None)]}
