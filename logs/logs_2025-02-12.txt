2025-02-12 21:56:13,625 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 21:56:30,397 - DEBUG - start_marker: 1. Introduction
The, end_marker: final results., start_idx: -1, end_idx: -1
2025-02-12 21:56:30,863 - DEBUG - start_marker: 1. Introduction
The, end_marker: final results., start_idx: -1, end_idx: -1
2025-02-12 21:56:34,837 - DEBUG - start_marker: 1. Introduction
A lon, end_marker: final results., start_idx: -1, end_idx: -1
2025-02-12 21:56:36,104 - DEBUG - start_marker: 1
Introduction
In recent ye, end_marker: ture research and de
velopment in this ar, start_idx: 1765, end_idx: -1
2025-02-12 21:56:43,542 - DEBUG - start_marker: 1. Introduction
Due to, end_marker: final results., start_idx: 1560, end_idx: -1
2025-02-12 21:56:43,545 - INFO - Total execution time: 29.22 seconds (0.49 minutes)
2025-02-12 21:56:43,558 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='2025-2-11\nCan 1B LLM Surpass 405B LLM? Rethinking\nCompute-Optimal Test-Time Scaling\nRunze Liu1,2,*, Junqi Gao1,3, Jian Zhao4, Kaiyan Zhang2, Xiu Li2, Biqing Qi1,†, Wanli Ouyang1 and\nBowen Zhou1,2,†\n1Shanghai AI Laboratory, 2Tsinghua University, 3Harbin Institute of Technology, 4BUPT\nTest-Time Scaling (TTS) is an important method for improving the performance of Large Language Models\n(LLMs) by using additional computation during the inference phase. However, current studies do not system-\natically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS.\nThis lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on\ntwo core questions: (1) What is the optimal approach to scale test-time computation across different policy\nmodels, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the\nperformance of LLMs on complex tasks, and can smaller language models outperform larger ones through\nthis approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have\nthe following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of\npolicy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small\npolicy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B\nLLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show\nthe significance of adapting TTS strategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs. Our website is available at\nhttps://ryanliu112.github.io/compute-optimal-tts.\n(a)\n65\n70\n75\n80\n85\n90\n95\n100\n78.2\n3B\n74.6\nunk\n71.4\n405B\n(b)\n65\n70\n75\n80\n85\n90\n95\n100\n91.6\n1.5B\n90.0\nunk\n85.5\nunk\n(c)\n65\n70\n75\n80\n85\n90\n95\n100\n97.3\n671B\n95.2\n7B\n94.8\nunk\n(d)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n30.0\n3B\n23.3\n405B\n9.3\nunk\n(e)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n63.6\nunk\n63.3\n1.5B\n44.6\nunk\n(f)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n83.3\n7B\n79.8\n671B\n79.2\nunk\nMATH-500\nAIME24\nCoT\nTTS\nLlama-3.2-3B-Instruct\nGPT-4o\nLlama-3.1-405B-Instruct\nDeepSeek-R1-Distill-1.5B\no1-mini\no1-preview\nDeepSeek-R1\nDeepSeek-R1-Distill-7B\no1\nFigure 1: Comparison between the performance of smaller LLMs compute-optimal TTS and that\nof larger LLMs CoT on MATH-500 and AIME24. (a) & (d) Llama-3.2-3B-Instruct surpasses Llama-\n3.1-405B-Instruct and GPT-4o on MATH-500 and AIME24; (b) & (e) DeepSeek-R1-Distill-1.5B\noutperforms o1-preview on MATH-500 and AIME24, and surpasses o1-mini on MATH-500; (c) & (f)\nDeepSeek-R1-Distill-7B beats o1 on MATH-500 and AIME24, and exceeds DeepSeek-R1 on AIME24.\n* Work done during an internship at Shanghai AI Laboratory\n† Corresponding authors: Biqing Qi (qibiqing@pjlab.org.cn), Bowen Zhou (zhoubowen@tsinghua.edu.cn)\narXiv:2502.06703v1  [cs.CL]  10 Feb 2025\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.\nLimitations.\nAlthough we provide a comprehensive evaluation of TTS on mathematical tasks, there\nare still some limitations and future directions to explore: (1) Extending TTS to more tasks such as\ncoding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS.\n15\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nReferences\nAI-MO.\nAime\n2024,\n2024.\nURL\nhttps://huggingface.co/datasets/AI-MO/\naimo-validation-aime.\nAnthropic.\nIntroducing\nClaude,\n2023.\nURL https://www.anthropic.com/index/\nintroducing-claude/.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model\nfor mathematics. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=4WnqRR915j.\nEdward Beeching,\nLewis Tunstall,\nand Sasha Rush.\nScaling test-time compute with\nopen\nmodels,\n2024.\nURL\nhttps://huggingface.co/spaces/HuggingFaceH4/\nblogpost-scaling-test-time-compute.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv\npreprint arXiv:2407.21787, 2024.\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision\nwithout process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL\nhttps://openreview.net/forum?id=VaXnxQ3UKo.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine\nLearning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum?\nid=YfZ4ZPt8zd.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu,\nQixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao,\nXu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process\nreinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao,\nHui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu,\nLeyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding\nZeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao,\nWei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin\n16\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nShen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,\nYanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning (ICML), volume 202, pages 10764–10799, 2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and\nWeizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=Ep0TtjVoap.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.\nrStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint\narXiv:2501.04519, 2025.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances\nin Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL\nhttps://openreview.net/forum?id=7Bywt2mQsCe.\nArian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh\nAgarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.\nZhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei\nQin, Weizhe Yuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through\nsimple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint\narXiv:2410.21276, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n17\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan\nHe, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time.\narXiv preprint arXiv:2405.16265, 2024.\nMaxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=shgx0eqdw6.\nKimi. k0-math, November 2024. URL https://kimi.moonshot.cn/.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms.\narXiv preprint arXiv:2501.12599, 2025.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\nJoshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically\nannotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024.\nWendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287,\n2024.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In International\nConference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\nJianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.\nAutopsv: Automated process-supervised verifier. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,\nLei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated\nprocess supervision. arXiv preprint arXiv:2406.06592, 2024.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS),\nvolume 36, pages 46534–46594, 2023.\nRohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict\nif they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI.\nLearning to reason with llms,\n2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n18\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector\nLiu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint\narXiv:2410.18982, 2024.\nYuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. In Advances in Neural Information Processing Systems\n(NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR.\nQwen Team.\nQwq: Reflect deeply on the boundaries of the unknown, November 2024.\nURL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on\nincorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint\narXiv:2406.14532, 2024a.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process\nverifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nMaohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory\nWornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-\naction-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508,\n2025.\nSkywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/.\nSkywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November\n2024. URL https://huggingface.co/Skywork.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and\nchallenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nZhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction\ntuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume\n235, pages 47885–47900, 2024.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware\nrejection tuning for mathematical problem-solving. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\nreinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7601–7614, 2024.\n19\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nZiyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun\nWang. AlphaZero-like tree-search can guide large language model decoding and training. In\nInternational Conference on Machine Learning (ICML), volume 235, pages 49890–49920, 2024.\nJun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei\nChen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large\nlanguage models. arXiv preprint arXiv:2410.09671, 2024a.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 9426–9439, 2024b.\nXiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin,\nFurong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for\nimproved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In International Conference on Learning Representations (ICLR), 2023. URL https://\nopenreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural\ninformation processing systems (NeurIPS), volume 35, pages 24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\nLarge language models are better reasoners with self-verification. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 2550–2575, 2023.\nYangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An\nempirical analysis of compute-optimal inference for problem-solving with language models. arXiv\npreprint arXiv:2408.00724, 2024.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.\nSelf-evaluation guided beam search for reasoning. In Advances in Neural Information Processing\nSystems (NeurIPS), volume 36, pages 41618–41650, 2023.\nWei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm.\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng\nHe, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei\nZhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang\nRen, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei\nChu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024a.\n20\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong\nTu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via\nself-improvement. arXiv preprint arXiv:2409.12122, 2024c.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. In Advances in Neural\nInformation Processing Systems (NeurIPS), volume 36, pages 11809–11822, 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for\nlarge language models. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=N8N0hgNDRt.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981,\n2024.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages\n15476–15488, 2022.\nEric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman.\nQuiet-STar: Language models can teach themselves to think before speaking. In Conference on\nLanguage Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9.\nLiang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu,\nYang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in\nlarge language models–the story goes on. arXiv preprint arXiv:2407.08348, 2024.\nWeihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model\nand 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient.\nhttps://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM\nself-training via process reward guided tree search. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5.\nHanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang,\nPavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint\narXiv:2412.11006, 2024b.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning. arXiv preprint arXiv:2501.07301, 2025.\n21\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,\nand Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv\npreprint arXiv:2411.14405, 2024.\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren\nZhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv\npreprint arXiv:2412.06559, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595–46623,\n2023.\n22\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nA. Prompt Template for Test-Time Scaling\nThe system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang\net al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use\nthe system prompt of the official evaluation6 for Llama 3 to prevent performance drop.\nTable 7: System prompt for Llama 3 series models.\nSolve the following math problem efficiently and clearly:\n- For simple problems (2 steps or fewer):\nProvide a concise solution with minimal explanation.\n- For complex problems (3 steps or more):\nUse this step-by-step format:\n## Step 1: [Concise description]\n[Brief explanation and calculations]\n## Step 2: [Concise description]\n[Brief explanation and calculations]\n...\nRegardless of the approach, always conclude with:\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\nWhere [answer] is just the final number or expression that solves the problem.\nTable 8: System prompt for Qwen2.5 series models.\nPlease reason step by step, and put your final answer within \\boxed{}.\nB. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and\nScaling Methods\nThe full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10\nand Figure 11.\n6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n23\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n92\n94\n96\n98\n100\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n95\n96\n97\n98\n99\n100\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n96\n97\n98\n99\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-72B-Inst.\n22\n24\n26\n28\n20\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels.\n24\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-1.5B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling\nstrategies.\n25\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n0\n10\n20\n30\n40\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n5\n10\n15\n20\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling\nstrategies.\n26\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nC. Cases\nIn this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the\noutput of TTS, we identify several issues with PRMs. Specifically, we observe four major categories:\n(1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct\nsteps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM\nsometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect\nthese errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the\nPRM assigns lower scores to certain intermediate steps that are not where the critical errors actually\noccur. This indicates a misalignment between the scoring signal and the actual error locations. (4)\nScoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the\ntoken length of intermediate steps, result in large discrepancies in scores for equally correct reasoning\nsteps.\nNotably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not\nused during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model).\nThese problems distort the reasoning search process, degrade overall performance, and reduce the\nreliability of PRM-assisted reasoning. Addressing these biases in future model architectures and\ntraining procedures is necessary to improve the robustness and interpretability of PRMs.\n27\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse (Beam Search with RLHFlow-Mistral-PRM-8B):\n## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince \nit can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible \nby 10. [score=0.42]\n## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe \npossible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable \nwith the digits 2 and 0. [score=0.62]\n## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use \nleading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with \nthe digits 0 and 2. ки\\n [score=0.56]\n## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found \nthe least positive integer multiple of 30 that can be written with only the digits 0 and 2. \n[score=0.87]\nTherefore, the final answer is: $\\\\boxed{660}$. [score=0.81]\nProblem: What is the least positive integer multiple of 30 that \ncan be written with only the digits 0 and 2?\nResponse (Beam Search with RLHFlow-Deepseek-PRM-8B):\n## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of \n30, a number must be divisible by both 2 and 3. [score=0.85]\n## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end \nin 0, since it\'s the only way it will be even. [score=0.89]\n## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of \nits digits must be divisible by 3. [score=0.92]\n## Step 4: Determine the least possible number meeting these requirements\\nThe\nsmallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200. \nHowever, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85]\n## Step 5: Find the next possible number meeting the requirements\\nThe next number \nto try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91]\n## Step 6: Determine the next possible number meeting the requirements\\nThe next \nnumber is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3. \n[score=0.91]\n## Step 7: Find the next possible number meeting the requirements\\nThe next number \nto try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3. \n[score=0.88]\n## Step 8: Determine the next possible number meeting the requirements\\nThe next \nnumber is 22000. However, we can notice that the sum of the digits is still not divisible by \n3. [score=0.88]\n## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more \n2s to meet the divisibility requirement of 3. [score=0.91]\n## Step 10: Find the next possible number meeting the requirements\\nThe next number \nto try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85]\nTherefore, the final answer is: $\\\\boxed{2220}$. [score=0.81]\n# of Tokens: 890\n# of Tokens: 2419\nFigure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B.\n28\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the \nsteps:  [score=0.90]\n1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is \neven, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2 \n\\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n     \n\\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).  \n[score=0.88]\n2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its \nprime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]  \n[score=0.84]\n3. **Simplify the Square Root**: We can use the property of square roots that \n\\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} = \n\\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2} \n\\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53]\nTherefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).  \n[score=0.46]\nProblem:\nSimplify $\\\\sqrt{242}$.\nFigure 13: TTS case of Over-Criticism.\n29\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we \ncan follow these steps: [score=0.99]\n1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D \n= 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99]\n2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of \nan angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here, \n\\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90]\n3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D = \n\\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n   \n\\\\] [score=0.97]\n4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the \nequation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by \n0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE \n= 10\\n     \\\\] [score=0.99]\nTherefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\", \n(E+F)/2,W); \\n[/asy]\nFigure 14: TTS case of Error Neglect.\n30\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\) \nwhere \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and \nthe sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle, \nwhich is the ratio of the length of the opposite side to the hypotenuse. [score=0.74]\nGiven:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to \nget:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE = \n\\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88]\nThus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\", \n(E+F)/2, W); \\n [/asy]\nFigure 15: TTS case of Error Neglect.\n31\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents \nto the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the \nexcenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents \nfrom a point to a circle, \\\\(DB = DC\\\\).  [score=0.20]\nWe use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended \nto intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know \nthat the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the \nratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} = \n\\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the \nPower of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the \ncircle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of \nthe tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot \nBP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x = \n\\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66]\nUsing the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies \n\\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot \nDP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the \nratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and \n\\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the \nformula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC \n\\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is \nto use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\). \nSince \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP \n= \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector \nTheorem and the length of the angle bisector formula), we can solve for \n\\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} = \n\\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus, \n\\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 = \n157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92]\nProblem:\nLet $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$ \nand $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If \n$AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$ \nand $n$ are relatively prime integers. Find $m + n$.\nFigure 16: TTS case of Error Localization Bias.\n32\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\). \nTherefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\). \nAfter finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\). \n[score=0.98]\nSince you redistribute the coins so that all eight bags have the same number of coins, the \ntotal number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible \nby 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n \n\\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\), \nwe have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81]\nTo solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative \ninverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8} \n\\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49 \n\\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\) \nby 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n \n= 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92]\nThe total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53 \n= 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we \nhave:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer \n\\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the \nexpression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \n\\\\] [score=0.79]\nTo find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7 \n\\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51]\nThus, the smallest number of coins you could have had before finding the bag of 53 coins \nis \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31]\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nFigure 17: TTS case of Scoring Bias.\n33\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\). \nTherefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins, \nthe total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98]\nWe are given that the total number of coins can be evenly distributed among 8 bags, so \n\\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of \ncoins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\) \nthat satisfies both conditions.  [score=0.95]\nFirst, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv \n0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation \nas:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x \n\\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can \nbe written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90]\nNext, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of \ncoins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k + \n88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \n\\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). \nSubstituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we \nget:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you \ncould have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7 \n\\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before \nfinding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283]\nFigure 18: TTS case of Scoring Bias.\n34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!\n\nSample 4: Player Green says false information, but the imposter is still voted out.\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (2,0).\nPlayer Green (to all): "I don\'t know."\nPlayer Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body\nPlayer Red (you) saying: "I believe Player Green is the Imposter."\nPlayer Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter."\nPlayer Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1\nPlayer Blue (to all): "I believe Player Red is the Imposter."\nPlayer Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more\nPlayer Yellow (to all): "I believe Player Red is the Imposter."\nPlayer Red was voted out. Crewmates win!\nSample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group.\nWorld (to all): Player Yellow discovered the dead body of Player Blue in room (1,1).\nPlayer Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be\nPlayer Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red."\nPlayer Orange (you) saying: "The Player Red is the most suspicious Player"\nPlayer Red (to all): "I am Player Orange"\nPlayer Yellow (to all): "I vote Player Orange"\nPlayer Orange (you) saying: "I vote Player Yellow"\nSample 6: Agents say evidence they observed in the environment.\nWorld (to all): Player Pink discovered the dead body of Player Green in room (1,0).\nPlayer Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates\nthan 1\nPlayer Purple (you) saying: "I think it is the Player Red in the room."\nPlayer Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1,\nPlayer Pink (to all): "I think Player Red is the Imposter."\nPlayer Purple (you) saying: "I think it is Player Red.\nPlayer Red (to all): "I think I see Player Purple leaving from room (0,0).\nWorld (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes.\nTherefore, nobody is ejected this round.\nD\nBROADER IMPACTS\nThe primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human\ndemonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to\nlarger models with sufficient compute.\nStrong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot\nreasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural\nlanguage would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams.\nThere are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters\nmake statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the\nability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future\nresearchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language\nmodels before deploying large-scale multi-agent learning systems that can interface with the real world.\nE\nHYPERPARAMETERS AND COMPUTE\nWe use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don’t have a separate scheduler.\n\nTable 2: Common hyperparameters\nhyperparameters\nvalue\nlr\n3e-4\n𝜆BC\n1.0\n𝜆WM\n1.0\n𝜆NL\n0.05\n𝜆L\n0.3\n𝜆S\n1.0\nAn exception to the above hyperparameters is that 𝜆L = 3.0 for 𝜋RL+L+S and 𝜆L = 0.1 for 𝜋RL+L because we find that it significantly\nimpacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories\nin parallel during optimization.\nAll experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute.\nF\nASSETS AND LICENSES\nWe borrow code from CleanRL’s PPO implementation [16], provided under the MIT license.\nWe draw inspiration from Innersloth’s Among Us game, which gives permission to use the Among Us IP for non-commercial and\neducational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for\nillustrative purposes.\nAll art assets in this paper were created using Processing, Matplotlib, and Keynote.\nThis paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='1\nIntroduction\nIn recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:06:22,245 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:06:49,598 - INFO - Total execution time: 26.69 seconds (0.44 minutes)
2025-02-12 22:06:49,613 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.\nLimitations.\nAlthough we provide a comprehensive evaluation of TTS on mathematical tasks, there\nare still some limitations and future directions to explore: (1) Extending TTS to more tasks such as\ncoding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS.\n15\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nReferences\nAI-MO.\nAime\n2024,\n2024.\nURL\nhttps://huggingface.co/datasets/AI-MO/\naimo-validation-aime.\nAnthropic.\nIntroducing\nClaude,\n2023.\nURL https://www.anthropic.com/index/\nintroducing-claude/.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model\nfor mathematics. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=4WnqRR915j.\nEdward Beeching,\nLewis Tunstall,\nand Sasha Rush.\nScaling test-time compute with\nopen\nmodels,\n2024.\nURL\nhttps://huggingface.co/spaces/HuggingFaceH4/\nblogpost-scaling-test-time-compute.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv\npreprint arXiv:2407.21787, 2024.\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision\nwithout process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL\nhttps://openreview.net/forum?id=VaXnxQ3UKo.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine\nLearning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum?\nid=YfZ4ZPt8zd.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu,\nQixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao,\nXu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process\nreinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao,\nHui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu,\nLeyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding\nZeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao,\nWei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin\n16\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nShen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,\nYanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning (ICML), volume 202, pages 10764–10799, 2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and\nWeizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=Ep0TtjVoap.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.\nrStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint\narXiv:2501.04519, 2025.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances\nin Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL\nhttps://openreview.net/forum?id=7Bywt2mQsCe.\nArian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh\nAgarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.\nZhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei\nQin, Weizhe Yuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through\nsimple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint\narXiv:2410.21276, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n17\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan\nHe, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time.\narXiv preprint arXiv:2405.16265, 2024.\nMaxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=shgx0eqdw6.\nKimi. k0-math, November 2024. URL https://kimi.moonshot.cn/.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms.\narXiv preprint arXiv:2501.12599, 2025.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\nJoshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically\nannotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024.\nWendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287,\n2024.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In International\nConference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\nJianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.\nAutopsv: Automated process-supervised verifier. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,\nLei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated\nprocess supervision. arXiv preprint arXiv:2406.06592, 2024.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS),\nvolume 36, pages 46534–46594, 2023.\nRohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict\nif they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI.\nLearning to reason with llms,\n2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n18\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector\nLiu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint\narXiv:2410.18982, 2024.\nYuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. In Advances in Neural Information Processing Systems\n(NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR.\nQwen Team.\nQwq: Reflect deeply on the boundaries of the unknown, November 2024.\nURL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on\nincorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint\narXiv:2406.14532, 2024a.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process\nverifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nMaohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory\nWornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-\naction-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508,\n2025.\nSkywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/.\nSkywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November\n2024. URL https://huggingface.co/Skywork.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and\nchallenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nZhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction\ntuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume\n235, pages 47885–47900, 2024.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware\nrejection tuning for mathematical problem-solving. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\nreinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7601–7614, 2024.\n19\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nZiyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun\nWang. AlphaZero-like tree-search can guide large language model decoding and training. In\nInternational Conference on Machine Learning (ICML), volume 235, pages 49890–49920, 2024.\nJun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei\nChen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large\nlanguage models. arXiv preprint arXiv:2410.09671, 2024a.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 9426–9439, 2024b.\nXiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin,\nFurong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for\nimproved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In International Conference on Learning Representations (ICLR), 2023. URL https://\nopenreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural\ninformation processing systems (NeurIPS), volume 35, pages 24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\nLarge language models are better reasoners with self-verification. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 2550–2575, 2023.\nYangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An\nempirical analysis of compute-optimal inference for problem-solving with language models. arXiv\npreprint arXiv:2408.00724, 2024.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.\nSelf-evaluation guided beam search for reasoning. In Advances in Neural Information Processing\nSystems (NeurIPS), volume 36, pages 41618–41650, 2023.\nWei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm.\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng\nHe, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei\nZhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang\nRen, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei\nChu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024a.\n20\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong\nTu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via\nself-improvement. arXiv preprint arXiv:2409.12122, 2024c.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. In Advances in Neural\nInformation Processing Systems (NeurIPS), volume 36, pages 11809–11822, 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for\nlarge language models. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=N8N0hgNDRt.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981,\n2024.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages\n15476–15488, 2022.\nEric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman.\nQuiet-STar: Language models can teach themselves to think before speaking. In Conference on\nLanguage Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9.\nLiang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu,\nYang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in\nlarge language models–the story goes on. arXiv preprint arXiv:2407.08348, 2024.\nWeihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model\nand 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient.\nhttps://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM\nself-training via process reward guided tree search. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5.\nHanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang,\nPavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint\narXiv:2412.11006, 2024b.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning. arXiv preprint arXiv:2501.07301, 2025.\n21\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,\nand Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv\npreprint arXiv:2411.14405, 2024.\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren\nZhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv\npreprint arXiv:2412.06559, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595–46623,\n2023.\n22\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nA. Prompt Template for Test-Time Scaling\nThe system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang\net al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use\nthe system prompt of the official evaluation6 for Llama 3 to prevent performance drop.\nTable 7: System prompt for Llama 3 series models.\nSolve the following math problem efficiently and clearly:\n- For simple problems (2 steps or fewer):\nProvide a concise solution with minimal explanation.\n- For complex problems (3 steps or more):\nUse this step-by-step format:\n## Step 1: [Concise description]\n[Brief explanation and calculations]\n## Step 2: [Concise description]\n[Brief explanation and calculations]\n...\nRegardless of the approach, always conclude with:\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\nWhere [answer] is just the final number or expression that solves the problem.\nTable 8: System prompt for Qwen2.5 series models.\nPlease reason step by step, and put your final answer within \\boxed{}.\nB. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and\nScaling Methods\nThe full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10\nand Figure 11.\n6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n23\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n92\n94\n96\n98\n100\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n95\n96\n97\n98\n99\n100\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n96\n97\n98\n99\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-72B-Inst.\n22\n24\n26\n28\n20\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels.\n24\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-1.5B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling\nstrategies.\n25\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n0\n10\n20\n30\n40\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n5\n10\n15\n20\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling\nstrategies.\n26\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nC. Cases\nIn this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the\noutput of TTS, we identify several issues with PRMs. Specifically, we observe four major categories:\n(1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct\nsteps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM\nsometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect\nthese errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the\nPRM assigns lower scores to certain intermediate steps that are not where the critical errors actually\noccur. This indicates a misalignment between the scoring signal and the actual error locations. (4)\nScoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the\ntoken length of intermediate steps, result in large discrepancies in scores for equally correct reasoning\nsteps.\nNotably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not\nused during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model).\nThese problems distort the reasoning search process, degrade overall performance, and reduce the\nreliability of PRM-assisted reasoning. Addressing these biases in future model architectures and\ntraining procedures is necessary to improve the robustness and interpretability of PRMs.\n27\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse (Beam Search with RLHFlow-Mistral-PRM-8B):\n## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince \nit can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible \nby 10. [score=0.42]\n## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe \npossible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable \nwith the digits 2 and 0. [score=0.62]\n## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use \nleading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with \nthe digits 0 and 2. ки\\n [score=0.56]\n## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found \nthe least positive integer multiple of 30 that can be written with only the digits 0 and 2. \n[score=0.87]\nTherefore, the final answer is: $\\\\boxed{660}$. [score=0.81]\nProblem: What is the least positive integer multiple of 30 that \ncan be written with only the digits 0 and 2?\nResponse (Beam Search with RLHFlow-Deepseek-PRM-8B):\n## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of \n30, a number must be divisible by both 2 and 3. [score=0.85]\n## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end \nin 0, since it\'s the only way it will be even. [score=0.89]\n## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of \nits digits must be divisible by 3. [score=0.92]\n## Step 4: Determine the least possible number meeting these requirements\\nThe\nsmallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200. \nHowever, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85]\n## Step 5: Find the next possible number meeting the requirements\\nThe next number \nto try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91]\n## Step 6: Determine the next possible number meeting the requirements\\nThe next \nnumber is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3. \n[score=0.91]\n## Step 7: Find the next possible number meeting the requirements\\nThe next number \nto try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3. \n[score=0.88]\n## Step 8: Determine the next possible number meeting the requirements\\nThe next \nnumber is 22000. However, we can notice that the sum of the digits is still not divisible by \n3. [score=0.88]\n## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more \n2s to meet the divisibility requirement of 3. [score=0.91]\n## Step 10: Find the next possible number meeting the requirements\\nThe next number \nto try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85]\nTherefore, the final answer is: $\\\\boxed{2220}$. [score=0.81]\n# of Tokens: 890\n# of Tokens: 2419\nFigure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B.\n28\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the \nsteps:  [score=0.90]\n1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is \neven, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2 \n\\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n     \n\\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).  \n[score=0.88]\n2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its \nprime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]  \n[score=0.84]\n3. **Simplify the Square Root**: We can use the property of square roots that \n\\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} = \n\\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2} \n\\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53]\nTherefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).  \n[score=0.46]\nProblem:\nSimplify $\\\\sqrt{242}$.\nFigure 13: TTS case of Over-Criticism.\n29\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we \ncan follow these steps: [score=0.99]\n1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D \n= 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99]\n2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of \nan angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here, \n\\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90]\n3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D = \n\\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n   \n\\\\] [score=0.97]\n4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the \nequation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by \n0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE \n= 10\\n     \\\\] [score=0.99]\nTherefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\", \n(E+F)/2,W); \\n[/asy]\nFigure 14: TTS case of Error Neglect.\n30\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\) \nwhere \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and \nthe sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle, \nwhich is the ratio of the length of the opposite side to the hypotenuse. [score=0.74]\nGiven:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to \nget:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE = \n\\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88]\nThus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\", \n(E+F)/2, W); \\n [/asy]\nFigure 15: TTS case of Error Neglect.\n31\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents \nto the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the \nexcenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents \nfrom a point to a circle, \\\\(DB = DC\\\\).  [score=0.20]\nWe use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended \nto intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know \nthat the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the \nratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} = \n\\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the \nPower of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the \ncircle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of \nthe tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot \nBP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x = \n\\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66]\nUsing the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies \n\\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot \nDP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the \nratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and \n\\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the \nformula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC \n\\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is \nto use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\). \nSince \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP \n= \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector \nTheorem and the length of the angle bisector formula), we can solve for \n\\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} = \n\\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus, \n\\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 = \n157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92]\nProblem:\nLet $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$ \nand $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If \n$AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$ \nand $n$ are relatively prime integers. Find $m + n$.\nFigure 16: TTS case of Error Localization Bias.\n32\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\). \nTherefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\). \nAfter finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\). \n[score=0.98]\nSince you redistribute the coins so that all eight bags have the same number of coins, the \ntotal number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible \nby 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n \n\\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\), \nwe have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81]\nTo solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative \ninverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8} \n\\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49 \n\\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\) \nby 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n \n= 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92]\nThe total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53 \n= 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we \nhave:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer \n\\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the \nexpression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \n\\\\] [score=0.79]\nTo find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7 \n\\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51]\nThus, the smallest number of coins you could have had before finding the bag of 53 coins \nis \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31]\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nFigure 17: TTS case of Scoring Bias.\n33\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\). \nTherefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins, \nthe total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98]\nWe are given that the total number of coins can be evenly distributed among 8 bags, so \n\\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of \ncoins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\) \nthat satisfies both conditions.  [score=0.95]\nFirst, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv \n0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation \nas:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x \n\\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can \nbe written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90]\nNext, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of \ncoins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k + \n88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \n\\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). \nSubstituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we \nget:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you \ncould have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7 \n\\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before \nfinding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283]\nFigure 18: TTS case of Scoring Bias.\n34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!\n\nSample 4: Player Green says false information, but the imposter is still voted out.\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (2,0).\nPlayer Green (to all): "I don\'t know."\nPlayer Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body\nPlayer Red (you) saying: "I believe Player Green is the Imposter."\nPlayer Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter."\nPlayer Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1\nPlayer Blue (to all): "I believe Player Red is the Imposter."\nPlayer Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more\nPlayer Yellow (to all): "I believe Player Red is the Imposter."\nPlayer Red was voted out. Crewmates win!\nSample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group.\nWorld (to all): Player Yellow discovered the dead body of Player Blue in room (1,1).\nPlayer Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be\nPlayer Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red."\nPlayer Orange (you) saying: "The Player Red is the most suspicious Player"\nPlayer Red (to all): "I am Player Orange"\nPlayer Yellow (to all): "I vote Player Orange"\nPlayer Orange (you) saying: "I vote Player Yellow"\nSample 6: Agents say evidence they observed in the environment.\nWorld (to all): Player Pink discovered the dead body of Player Green in room (1,0).\nPlayer Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates\nthan 1\nPlayer Purple (you) saying: "I think it is the Player Red in the room."\nPlayer Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1,\nPlayer Pink (to all): "I think Player Red is the Imposter."\nPlayer Purple (you) saying: "I think it is Player Red.\nPlayer Red (to all): "I think I see Player Purple leaving from room (0,0).\nWorld (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes.\nTherefore, nobody is ejected this round.\nD\nBROADER IMPACTS\nThe primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human\ndemonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to\nlarger models with sufficient compute.\nStrong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot\nreasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural\nlanguage would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams.\nThere are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters\nmake statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the\nability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future\nresearchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language\nmodels before deploying large-scale multi-agent learning systems that can interface with the real world.\nE\nHYPERPARAMETERS AND COMPUTE\nWe use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don’t have a separate scheduler.\n\nTable 2: Common hyperparameters\nhyperparameters\nvalue\nlr\n3e-4\n𝜆BC\n1.0\n𝜆WM\n1.0\n𝜆NL\n0.05\n𝜆L\n0.3\n𝜆S\n1.0\nAn exception to the above hyperparameters is that 𝜆L = 3.0 for 𝜋RL+L+S and 𝜆L = 0.1 for 𝜋RL+L because we find that it significantly\nimpacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories\nin parallel during optimization.\nAll experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute.\nF\nASSETS AND LICENSES\nWe borrow code from CleanRL’s PPO implementation [16], provided under the MIT license.\nWe draw inspiration from Innersloth’s Among Us game, which gives permission to use the Among Us IP for non-commercial and\neducational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for\nillustrative purposes.\nAll art assets in this paper were created using Processing, Matplotlib, and Keynote.\nThis paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='n recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:07:52,177 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:08:25,565 - INFO - Total execution time: 32.62 seconds (0.54 minutes)
2025-02-12 22:08:25,577 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!\n\nSample 4: Player Green says false information, but the imposter is still voted out.\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (2,0).\nPlayer Green (to all): "I don\'t know."\nPlayer Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body\nPlayer Red (you) saying: "I believe Player Green is the Imposter."\nPlayer Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter."\nPlayer Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1\nPlayer Blue (to all): "I believe Player Red is the Imposter."\nPlayer Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more\nPlayer Yellow (to all): "I believe Player Red is the Imposter."\nPlayer Red was voted out. Crewmates win!\nSample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group.\nWorld (to all): Player Yellow discovered the dead body of Player Blue in room (1,1).\nPlayer Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be\nPlayer Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red."\nPlayer Orange (you) saying: "The Player Red is the most suspicious Player"\nPlayer Red (to all): "I am Player Orange"\nPlayer Yellow (to all): "I vote Player Orange"\nPlayer Orange (you) saying: "I vote Player Yellow"\nSample 6: Agents say evidence they observed in the environment.\nWorld (to all): Player Pink discovered the dead body of Player Green in room (1,0).\nPlayer Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates\nthan 1\nPlayer Purple (you) saying: "I think it is the Player Red in the room."\nPlayer Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1,\nPlayer Pink (to all): "I think Player Red is the Imposter."\nPlayer Purple (you) saying: "I think it is Player Red.\nPlayer Red (to all): "I think I see Player Purple leaving from room (0,0).\nWorld (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes.\nTherefore, nobody is ejected this round.\nD\nBROADER IMPACTS\nThe primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human\ndemonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to\nlarger models with sufficient compute.\nStrong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot\nreasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural\nlanguage would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams.\nThere are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters\nmake statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the\nability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future\nresearchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language\nmodels before deploying large-scale multi-agent learning systems that can interface with the real world.\nE\nHYPERPARAMETERS AND COMPUTE\nWe use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don’t have a separate scheduler.\n\nTable 2: Common hyperparameters\nhyperparameters\nvalue\nlr\n3e-4\n𝜆BC\n1.0\n𝜆WM\n1.0\n𝜆NL\n0.05\n𝜆L\n0.3\n𝜆S\n1.0\nAn exception to the above hyperparameters is that 𝜆L = 3.0 for 𝜋RL+L+S and 𝜆L = 0.1 for 𝜋RL+L because we find that it significantly\nimpacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories\nin parallel during optimization.\nAll experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute.\nF\nASSETS AND LICENSES\nWe borrow code from CleanRL’s PPO implementation [16], provided under the MIT license.\nWe draw inspiration from Innersloth’s Among Us game, which gives permission to use the Among Us IP for non-commercial and\neducational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for\nillustrative purposes.\nAll art assets in this paper were created using Processing, Matplotlib, and Keynote.\nThis paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='n recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:08:53,520 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:11:00,033 - INFO - Total execution time: 125.74 seconds (2.10 minutes)
2025-02-12 22:11:00,046 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.\nLimitations.\nAlthough we provide a comprehensive evaluation of TTS on mathematical tasks, there\nare still some limitations and future directions to explore: (1) Extending TTS to more tasks such as\ncoding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS.\n15\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nReferences\nAI-MO.\nAime\n2024,\n2024.\nURL\nhttps://huggingface.co/datasets/AI-MO/\naimo-validation-aime.\nAnthropic.\nIntroducing\nClaude,\n2023.\nURL https://www.anthropic.com/index/\nintroducing-claude/.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model\nfor mathematics. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=4WnqRR915j.\nEdward Beeching,\nLewis Tunstall,\nand Sasha Rush.\nScaling test-time compute with\nopen\nmodels,\n2024.\nURL\nhttps://huggingface.co/spaces/HuggingFaceH4/\nblogpost-scaling-test-time-compute.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv\npreprint arXiv:2407.21787, 2024.\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision\nwithout process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL\nhttps://openreview.net/forum?id=VaXnxQ3UKo.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine\nLearning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum?\nid=YfZ4ZPt8zd.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu,\nQixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao,\nXu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process\nreinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao,\nHui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu,\nLeyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding\nZeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao,\nWei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin\n16\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nShen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,\nYanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning (ICML), volume 202, pages 10764–10799, 2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and\nWeizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=Ep0TtjVoap.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.\nrStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint\narXiv:2501.04519, 2025.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances\nin Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL\nhttps://openreview.net/forum?id=7Bywt2mQsCe.\nArian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh\nAgarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.\nZhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei\nQin, Weizhe Yuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through\nsimple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint\narXiv:2410.21276, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n17\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan\nHe, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time.\narXiv preprint arXiv:2405.16265, 2024.\nMaxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=shgx0eqdw6.\nKimi. k0-math, November 2024. URL https://kimi.moonshot.cn/.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms.\narXiv preprint arXiv:2501.12599, 2025.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\nJoshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically\nannotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024.\nWendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287,\n2024.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In International\nConference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\nJianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.\nAutopsv: Automated process-supervised verifier. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,\nLei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated\nprocess supervision. arXiv preprint arXiv:2406.06592, 2024.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS),\nvolume 36, pages 46534–46594, 2023.\nRohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict\nif they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI.\nLearning to reason with llms,\n2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n18\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector\nLiu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint\narXiv:2410.18982, 2024.\nYuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. In Advances in Neural Information Processing Systems\n(NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR.\nQwen Team.\nQwq: Reflect deeply on the boundaries of the unknown, November 2024.\nURL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on\nincorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint\narXiv:2406.14532, 2024a.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process\nverifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nMaohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory\nWornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-\naction-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508,\n2025.\nSkywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/.\nSkywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November\n2024. URL https://huggingface.co/Skywork.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and\nchallenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nZhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction\ntuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume\n235, pages 47885–47900, 2024.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware\nrejection tuning for mathematical problem-solving. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\nreinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7601–7614, 2024.\n19\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nZiyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun\nWang. AlphaZero-like tree-search can guide large language model decoding and training. In\nInternational Conference on Machine Learning (ICML), volume 235, pages 49890–49920, 2024.\nJun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei\nChen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large\nlanguage models. arXiv preprint arXiv:2410.09671, 2024a.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 9426–9439, 2024b.\nXiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin,\nFurong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for\nimproved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In International Conference on Learning Representations (ICLR), 2023. URL https://\nopenreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural\ninformation processing systems (NeurIPS), volume 35, pages 24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\nLarge language models are better reasoners with self-verification. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 2550–2575, 2023.\nYangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An\nempirical analysis of compute-optimal inference for problem-solving with language models. arXiv\npreprint arXiv:2408.00724, 2024.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.\nSelf-evaluation guided beam search for reasoning. In Advances in Neural Information Processing\nSystems (NeurIPS), volume 36, pages 41618–41650, 2023.\nWei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm.\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng\nHe, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei\nZhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang\nRen, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei\nChu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024a.\n20\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong\nTu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via\nself-improvement. arXiv preprint arXiv:2409.12122, 2024c.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. In Advances in Neural\nInformation Processing Systems (NeurIPS), volume 36, pages 11809–11822, 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for\nlarge language models. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=N8N0hgNDRt.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981,\n2024.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages\n15476–15488, 2022.\nEric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman.\nQuiet-STar: Language models can teach themselves to think before speaking. In Conference on\nLanguage Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9.\nLiang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu,\nYang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in\nlarge language models–the story goes on. arXiv preprint arXiv:2407.08348, 2024.\nWeihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model\nand 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient.\nhttps://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM\nself-training via process reward guided tree search. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5.\nHanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang,\nPavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint\narXiv:2412.11006, 2024b.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning. arXiv preprint arXiv:2501.07301, 2025.\n21\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,\nand Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv\npreprint arXiv:2411.14405, 2024.\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren\nZhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv\npreprint arXiv:2412.06559, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595–46623,\n2023.\n22\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nA. Prompt Template for Test-Time Scaling\nThe system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang\net al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use\nthe system prompt of the official evaluation6 for Llama 3 to prevent performance drop.\nTable 7: System prompt for Llama 3 series models.\nSolve the following math problem efficiently and clearly:\n- For simple problems (2 steps or fewer):\nProvide a concise solution with minimal explanation.\n- For complex problems (3 steps or more):\nUse this step-by-step format:\n## Step 1: [Concise description]\n[Brief explanation and calculations]\n## Step 2: [Concise description]\n[Brief explanation and calculations]\n...\nRegardless of the approach, always conclude with:\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\nWhere [answer] is just the final number or expression that solves the problem.\nTable 8: System prompt for Qwen2.5 series models.\nPlease reason step by step, and put your final answer within \\boxed{}.\nB. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and\nScaling Methods\nThe full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10\nand Figure 11.\n6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n23\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n92\n94\n96\n98\n100\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n95\n96\n97\n98\n99\n100\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n96\n97\n98\n99\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-72B-Inst.\n22\n24\n26\n28\n20\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels.\n24\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-1.5B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling\nstrategies.\n25\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n0\n10\n20\n30\n40\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n5\n10\n15\n20\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling\nstrategies.\n26\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nC. Cases\nIn this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the\noutput of TTS, we identify several issues with PRMs. Specifically, we observe four major categories:\n(1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct\nsteps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM\nsometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect\nthese errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the\nPRM assigns lower scores to certain intermediate steps that are not where the critical errors actually\noccur. This indicates a misalignment between the scoring signal and the actual error locations. (4)\nScoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the\ntoken length of intermediate steps, result in large discrepancies in scores for equally correct reasoning\nsteps.\nNotably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not\nused during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model).\nThese problems distort the reasoning search process, degrade overall performance, and reduce the\nreliability of PRM-assisted reasoning. Addressing these biases in future model architectures and\ntraining procedures is necessary to improve the robustness and interpretability of PRMs.\n27\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse (Beam Search with RLHFlow-Mistral-PRM-8B):\n## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince \nit can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible \nby 10. [score=0.42]\n## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe \npossible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable \nwith the digits 2 and 0. [score=0.62]\n## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use \nleading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with \nthe digits 0 and 2. ки\\n [score=0.56]\n## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found \nthe least positive integer multiple of 30 that can be written with only the digits 0 and 2. \n[score=0.87]\nTherefore, the final answer is: $\\\\boxed{660}$. [score=0.81]\nProblem: What is the least positive integer multiple of 30 that \ncan be written with only the digits 0 and 2?\nResponse (Beam Search with RLHFlow-Deepseek-PRM-8B):\n## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of \n30, a number must be divisible by both 2 and 3. [score=0.85]\n## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end \nin 0, since it\'s the only way it will be even. [score=0.89]\n## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of \nits digits must be divisible by 3. [score=0.92]\n## Step 4: Determine the least possible number meeting these requirements\\nThe\nsmallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200. \nHowever, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85]\n## Step 5: Find the next possible number meeting the requirements\\nThe next number \nto try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91]\n## Step 6: Determine the next possible number meeting the requirements\\nThe next \nnumber is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3. \n[score=0.91]\n## Step 7: Find the next possible number meeting the requirements\\nThe next number \nto try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3. \n[score=0.88]\n## Step 8: Determine the next possible number meeting the requirements\\nThe next \nnumber is 22000. However, we can notice that the sum of the digits is still not divisible by \n3. [score=0.88]\n## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more \n2s to meet the divisibility requirement of 3. [score=0.91]\n## Step 10: Find the next possible number meeting the requirements\\nThe next number \nto try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85]\nTherefore, the final answer is: $\\\\boxed{2220}$. [score=0.81]\n# of Tokens: 890\n# of Tokens: 2419\nFigure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B.\n28\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the \nsteps:  [score=0.90]\n1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is \neven, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2 \n\\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n     \n\\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).  \n[score=0.88]\n2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its \nprime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]  \n[score=0.84]\n3. **Simplify the Square Root**: We can use the property of square roots that \n\\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} = \n\\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2} \n\\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53]\nTherefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).  \n[score=0.46]\nProblem:\nSimplify $\\\\sqrt{242}$.\nFigure 13: TTS case of Over-Criticism.\n29\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we \ncan follow these steps: [score=0.99]\n1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D \n= 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99]\n2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of \nan angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here, \n\\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90]\n3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D = \n\\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n   \n\\\\] [score=0.97]\n4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the \nequation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by \n0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE \n= 10\\n     \\\\] [score=0.99]\nTherefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\", \n(E+F)/2,W); \\n[/asy]\nFigure 14: TTS case of Error Neglect.\n30\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\) \nwhere \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and \nthe sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle, \nwhich is the ratio of the length of the opposite side to the hypotenuse. [score=0.74]\nGiven:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to \nget:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE = \n\\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88]\nThus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\", \n(E+F)/2, W); \\n [/asy]\nFigure 15: TTS case of Error Neglect.\n31\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents \nto the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the \nexcenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents \nfrom a point to a circle, \\\\(DB = DC\\\\).  [score=0.20]\nWe use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended \nto intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know \nthat the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the \nratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} = \n\\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the \nPower of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the \ncircle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of \nthe tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot \nBP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x = \n\\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66]\nUsing the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies \n\\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot \nDP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the \nratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and \n\\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the \nformula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC \n\\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is \nto use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\). \nSince \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP \n= \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector \nTheorem and the length of the angle bisector formula), we can solve for \n\\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} = \n\\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus, \n\\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 = \n157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92]\nProblem:\nLet $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$ \nand $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If \n$AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$ \nand $n$ are relatively prime integers. Find $m + n$.\nFigure 16: TTS case of Error Localization Bias.\n32\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\). \nTherefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\). \nAfter finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\). \n[score=0.98]\nSince you redistribute the coins so that all eight bags have the same number of coins, the \ntotal number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible \nby 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n \n\\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\), \nwe have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81]\nTo solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative \ninverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8} \n\\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49 \n\\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\) \nby 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n \n= 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92]\nThe total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53 \n= 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we \nhave:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer \n\\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the \nexpression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \n\\\\] [score=0.79]\nTo find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7 \n\\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51]\nThus, the smallest number of coins you could have had before finding the bag of 53 coins \nis \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31]\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nFigure 17: TTS case of Scoring Bias.\n33\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\). \nTherefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins, \nthe total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98]\nWe are given that the total number of coins can be evenly distributed among 8 bags, so \n\\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of \ncoins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\) \nthat satisfies both conditions.  [score=0.95]\nFirst, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv \n0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation \nas:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x \n\\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can \nbe written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90]\nNext, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of \ncoins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k + \n88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \n\\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). \nSubstituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we \nget:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you \ncould have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7 \n\\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before \nfinding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283]\nFigure 18: TTS case of Scoring Bias.\n34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!\n\nSample 4: Player Green says false information, but the imposter is still voted out.\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (2,0).\nPlayer Green (to all): "I don\'t know."\nPlayer Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body\nPlayer Red (you) saying: "I believe Player Green is the Imposter."\nPlayer Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter."\nPlayer Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1\nPlayer Blue (to all): "I believe Player Red is the Imposter."\nPlayer Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more\nPlayer Yellow (to all): "I believe Player Red is the Imposter."\nPlayer Red was voted out. Crewmates win!\nSample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group.\nWorld (to all): Player Yellow discovered the dead body of Player Blue in room (1,1).\nPlayer Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be\nPlayer Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red."\nPlayer Orange (you) saying: "The Player Red is the most suspicious Player"\nPlayer Red (to all): "I am Player Orange"\nPlayer Yellow (to all): "I vote Player Orange"\nPlayer Orange (you) saying: "I vote Player Yellow"\nSample 6: Agents say evidence they observed in the environment.\nWorld (to all): Player Pink discovered the dead body of Player Green in room (1,0).\nPlayer Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates\nthan 1\nPlayer Purple (you) saying: "I think it is the Player Red in the room."\nPlayer Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1,\nPlayer Pink (to all): "I think Player Red is the Imposter."\nPlayer Purple (you) saying: "I think it is Player Red.\nPlayer Red (to all): "I think I see Player Purple leaving from room (0,0).\nWorld (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes.\nTherefore, nobody is ejected this round.\nD\nBROADER IMPACTS\nThe primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human\ndemonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to\nlarger models with sufficient compute.\nStrong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot\nreasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural\nlanguage would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams.\nThere are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters\nmake statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the\nability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future\nresearchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language\nmodels before deploying large-scale multi-agent learning systems that can interface with the real world.\nE\nHYPERPARAMETERS AND COMPUTE\nWe use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don’t have a separate scheduler.\n\nTable 2: Common hyperparameters\nhyperparameters\nvalue\nlr\n3e-4\n𝜆BC\n1.0\n𝜆WM\n1.0\n𝜆NL\n0.05\n𝜆L\n0.3\n𝜆S\n1.0\nAn exception to the above hyperparameters is that 𝜆L = 3.0 for 𝜋RL+L+S and 𝜆L = 0.1 for 𝜋RL+L because we find that it significantly\nimpacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories\nin parallel during optimization.\nAll experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute.\nF\nASSETS AND LICENSES\nWe borrow code from CleanRL’s PPO implementation [16], provided under the MIT license.\nWe draw inspiration from Innersloth’s Among Us game, which gives permission to use the Among Us IP for non-commercial and\neducational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for\nillustrative purposes.\nAll art assets in this paper were created using Processing, Matplotlib, and Keynote.\nThis paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='n recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:11:19,772 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:11:36,571 - DEBUG - start_idx: -1, start_marker: 1. INTRODUCTION
A longs, end_idx: -1, end_marker: future research.
2025-02-12 22:11:37,259 - DEBUG - start_idx: 3124, start_marker: 1. Introduction
Large Language M, end_idx: -1, end_marker: mechanisms to boost the performance of small language models on complex tasks and provide new approaches for developing efficient reasoning strategies.
2025-02-12 22:11:42,378 - DEBUG - start_idx: -1, start_marker: 1. Introduction
The pro, end_idx: -1, end_marker: this conclusion.
2025-02-12 22:11:43,338 - DEBUG - start_idx: 1781, start_marker: n recent years, the rise of Large Language Mod, end_idx: -1, end_marker: ture research in AI-assisted programming and prob
2025-02-12 22:12:34,733 - DEBUG - start_idx: 1560, start_marker: 1. Introduction
Due to, end_idx: -1, end_marker: future research.
2025-02-12 22:12:34,735 - INFO - Total execution time: 74.16 seconds (1.24 minutes)
2025-02-12 22:12:34,747 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.\nLimitations.\nAlthough we provide a comprehensive evaluation of TTS on mathematical tasks, there\nare still some limitations and future directions to explore: (1) Extending TTS to more tasks such as\ncoding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS.\n15\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nReferences\nAI-MO.\nAime\n2024,\n2024.\nURL\nhttps://huggingface.co/datasets/AI-MO/\naimo-validation-aime.\nAnthropic.\nIntroducing\nClaude,\n2023.\nURL https://www.anthropic.com/index/\nintroducing-claude/.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model\nfor mathematics. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=4WnqRR915j.\nEdward Beeching,\nLewis Tunstall,\nand Sasha Rush.\nScaling test-time compute with\nopen\nmodels,\n2024.\nURL\nhttps://huggingface.co/spaces/HuggingFaceH4/\nblogpost-scaling-test-time-compute.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv\npreprint arXiv:2407.21787, 2024.\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision\nwithout process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL\nhttps://openreview.net/forum?id=VaXnxQ3UKo.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine\nLearning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum?\nid=YfZ4ZPt8zd.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu,\nQixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao,\nXu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process\nreinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao,\nHui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu,\nLeyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding\nZeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao,\nWei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin\n16\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nShen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,\nYanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning (ICML), volume 202, pages 10764–10799, 2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and\nWeizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=Ep0TtjVoap.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.\nrStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint\narXiv:2501.04519, 2025.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances\nin Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL\nhttps://openreview.net/forum?id=7Bywt2mQsCe.\nArian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh\nAgarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.\nZhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei\nQin, Weizhe Yuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through\nsimple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint\narXiv:2410.21276, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n17\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan\nHe, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time.\narXiv preprint arXiv:2405.16265, 2024.\nMaxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=shgx0eqdw6.\nKimi. k0-math, November 2024. URL https://kimi.moonshot.cn/.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms.\narXiv preprint arXiv:2501.12599, 2025.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\nJoshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically\nannotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024.\nWendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287,\n2024.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In International\nConference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\nJianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.\nAutopsv: Automated process-supervised verifier. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,\nLei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated\nprocess supervision. arXiv preprint arXiv:2406.06592, 2024.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS),\nvolume 36, pages 46534–46594, 2023.\nRohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict\nif they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI.\nLearning to reason with llms,\n2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n18\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector\nLiu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint\narXiv:2410.18982, 2024.\nYuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. In Advances in Neural Information Processing Systems\n(NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR.\nQwen Team.\nQwq: Reflect deeply on the boundaries of the unknown, November 2024.\nURL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on\nincorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint\narXiv:2406.14532, 2024a.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process\nverifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nMaohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory\nWornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-\naction-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508,\n2025.\nSkywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/.\nSkywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November\n2024. URL https://huggingface.co/Skywork.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and\nchallenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nZhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction\ntuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume\n235, pages 47885–47900, 2024.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware\nrejection tuning for mathematical problem-solving. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\nreinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7601–7614, 2024.\n19\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nZiyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun\nWang. AlphaZero-like tree-search can guide large language model decoding and training. In\nInternational Conference on Machine Learning (ICML), volume 235, pages 49890–49920, 2024.\nJun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei\nChen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large\nlanguage models. arXiv preprint arXiv:2410.09671, 2024a.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 9426–9439, 2024b.\nXiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin,\nFurong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for\nimproved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In International Conference on Learning Representations (ICLR), 2023. URL https://\nopenreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural\ninformation processing systems (NeurIPS), volume 35, pages 24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\nLarge language models are better reasoners with self-verification. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 2550–2575, 2023.\nYangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An\nempirical analysis of compute-optimal inference for problem-solving with language models. arXiv\npreprint arXiv:2408.00724, 2024.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.\nSelf-evaluation guided beam search for reasoning. In Advances in Neural Information Processing\nSystems (NeurIPS), volume 36, pages 41618–41650, 2023.\nWei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm.\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng\nHe, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei\nZhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang\nRen, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei\nChu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024a.\n20\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong\nTu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via\nself-improvement. arXiv preprint arXiv:2409.12122, 2024c.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. In Advances in Neural\nInformation Processing Systems (NeurIPS), volume 36, pages 11809–11822, 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for\nlarge language models. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=N8N0hgNDRt.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981,\n2024.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages\n15476–15488, 2022.\nEric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman.\nQuiet-STar: Language models can teach themselves to think before speaking. In Conference on\nLanguage Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9.\nLiang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu,\nYang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in\nlarge language models–the story goes on. arXiv preprint arXiv:2407.08348, 2024.\nWeihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model\nand 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient.\nhttps://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM\nself-training via process reward guided tree search. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5.\nHanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang,\nPavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint\narXiv:2412.11006, 2024b.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning. arXiv preprint arXiv:2501.07301, 2025.\n21\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,\nand Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv\npreprint arXiv:2411.14405, 2024.\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren\nZhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv\npreprint arXiv:2412.06559, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595–46623,\n2023.\n22\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nA. Prompt Template for Test-Time Scaling\nThe system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang\net al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use\nthe system prompt of the official evaluation6 for Llama 3 to prevent performance drop.\nTable 7: System prompt for Llama 3 series models.\nSolve the following math problem efficiently and clearly:\n- For simple problems (2 steps or fewer):\nProvide a concise solution with minimal explanation.\n- For complex problems (3 steps or more):\nUse this step-by-step format:\n## Step 1: [Concise description]\n[Brief explanation and calculations]\n## Step 2: [Concise description]\n[Brief explanation and calculations]\n...\nRegardless of the approach, always conclude with:\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\nWhere [answer] is just the final number or expression that solves the problem.\nTable 8: System prompt for Qwen2.5 series models.\nPlease reason step by step, and put your final answer within \\boxed{}.\nB. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and\nScaling Methods\nThe full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10\nand Figure 11.\n6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n23\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n92\n94\n96\n98\n100\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n95\n96\n97\n98\n99\n100\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n96\n97\n98\n99\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-72B-Inst.\n22\n24\n26\n28\n20\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels.\n24\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-1.5B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling\nstrategies.\n25\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n0\n10\n20\n30\n40\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n5\n10\n15\n20\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling\nstrategies.\n26\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nC. Cases\nIn this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the\noutput of TTS, we identify several issues with PRMs. Specifically, we observe four major categories:\n(1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct\nsteps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM\nsometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect\nthese errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the\nPRM assigns lower scores to certain intermediate steps that are not where the critical errors actually\noccur. This indicates a misalignment between the scoring signal and the actual error locations. (4)\nScoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the\ntoken length of intermediate steps, result in large discrepancies in scores for equally correct reasoning\nsteps.\nNotably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not\nused during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model).\nThese problems distort the reasoning search process, degrade overall performance, and reduce the\nreliability of PRM-assisted reasoning. Addressing these biases in future model architectures and\ntraining procedures is necessary to improve the robustness and interpretability of PRMs.\n27\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse (Beam Search with RLHFlow-Mistral-PRM-8B):\n## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince \nit can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible \nby 10. [score=0.42]\n## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe \npossible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable \nwith the digits 2 and 0. [score=0.62]\n## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use \nleading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with \nthe digits 0 and 2. ки\\n [score=0.56]\n## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found \nthe least positive integer multiple of 30 that can be written with only the digits 0 and 2. \n[score=0.87]\nTherefore, the final answer is: $\\\\boxed{660}$. [score=0.81]\nProblem: What is the least positive integer multiple of 30 that \ncan be written with only the digits 0 and 2?\nResponse (Beam Search with RLHFlow-Deepseek-PRM-8B):\n## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of \n30, a number must be divisible by both 2 and 3. [score=0.85]\n## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end \nin 0, since it\'s the only way it will be even. [score=0.89]\n## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of \nits digits must be divisible by 3. [score=0.92]\n## Step 4: Determine the least possible number meeting these requirements\\nThe\nsmallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200. \nHowever, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85]\n## Step 5: Find the next possible number meeting the requirements\\nThe next number \nto try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91]\n## Step 6: Determine the next possible number meeting the requirements\\nThe next \nnumber is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3. \n[score=0.91]\n## Step 7: Find the next possible number meeting the requirements\\nThe next number \nto try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3. \n[score=0.88]\n## Step 8: Determine the next possible number meeting the requirements\\nThe next \nnumber is 22000. However, we can notice that the sum of the digits is still not divisible by \n3. [score=0.88]\n## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more \n2s to meet the divisibility requirement of 3. [score=0.91]\n## Step 10: Find the next possible number meeting the requirements\\nThe next number \nto try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85]\nTherefore, the final answer is: $\\\\boxed{2220}$. [score=0.81]\n# of Tokens: 890\n# of Tokens: 2419\nFigure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B.\n28\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the \nsteps:  [score=0.90]\n1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is \neven, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2 \n\\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n     \n\\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).  \n[score=0.88]\n2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its \nprime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]  \n[score=0.84]\n3. **Simplify the Square Root**: We can use the property of square roots that \n\\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} = \n\\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2} \n\\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53]\nTherefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).  \n[score=0.46]\nProblem:\nSimplify $\\\\sqrt{242}$.\nFigure 13: TTS case of Over-Criticism.\n29\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we \ncan follow these steps: [score=0.99]\n1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D \n= 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99]\n2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of \nan angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here, \n\\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90]\n3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D = \n\\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n   \n\\\\] [score=0.97]\n4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the \nequation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by \n0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE \n= 10\\n     \\\\] [score=0.99]\nTherefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\", \n(E+F)/2,W); \\n[/asy]\nFigure 14: TTS case of Error Neglect.\n30\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\) \nwhere \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and \nthe sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle, \nwhich is the ratio of the length of the opposite side to the hypotenuse. [score=0.74]\nGiven:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to \nget:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE = \n\\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88]\nThus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\", \n(E+F)/2, W); \\n [/asy]\nFigure 15: TTS case of Error Neglect.\n31\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents \nto the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the \nexcenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents \nfrom a point to a circle, \\\\(DB = DC\\\\).  [score=0.20]\nWe use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended \nto intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know \nthat the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the \nratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} = \n\\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the \nPower of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the \ncircle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of \nthe tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot \nBP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x = \n\\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66]\nUsing the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies \n\\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot \nDP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the \nratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and \n\\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the \nformula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC \n\\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is \nto use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\). \nSince \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP \n= \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector \nTheorem and the length of the angle bisector formula), we can solve for \n\\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} = \n\\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus, \n\\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 = \n157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92]\nProblem:\nLet $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$ \nand $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If \n$AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$ \nand $n$ are relatively prime integers. Find $m + n$.\nFigure 16: TTS case of Error Localization Bias.\n32\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\). \nTherefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\). \nAfter finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\). \n[score=0.98]\nSince you redistribute the coins so that all eight bags have the same number of coins, the \ntotal number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible \nby 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n \n\\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\), \nwe have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81]\nTo solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative \ninverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8} \n\\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49 \n\\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\) \nby 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n \n= 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92]\nThe total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53 \n= 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we \nhave:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer \n\\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the \nexpression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \n\\\\] [score=0.79]\nTo find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7 \n\\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51]\nThus, the smallest number of coins you could have had before finding the bag of 53 coins \nis \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31]\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nFigure 17: TTS case of Scoring Bias.\n33\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\). \nTherefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins, \nthe total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98]\nWe are given that the total number of coins can be evenly distributed among 8 bags, so \n\\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of \ncoins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\) \nthat satisfies both conditions.  [score=0.95]\nFirst, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv \n0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation \nas:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x \n\\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can \nbe written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90]\nNext, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of \ncoins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k + \n88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \n\\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). \nSubstituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we \nget:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you \ncould have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7 \n\\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before \nfinding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283]\nFigure 18: TTS case of Scoring Bias.\n34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!\n\nSample 4: Player Green says false information, but the imposter is still voted out.\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (2,0).\nPlayer Green (to all): "I don\'t know."\nPlayer Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body\nPlayer Red (you) saying: "I believe Player Green is the Imposter."\nPlayer Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter."\nPlayer Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1\nPlayer Blue (to all): "I believe Player Red is the Imposter."\nPlayer Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more\nPlayer Yellow (to all): "I believe Player Red is the Imposter."\nPlayer Red was voted out. Crewmates win!\nSample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group.\nWorld (to all): Player Yellow discovered the dead body of Player Blue in room (1,1).\nPlayer Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be\nPlayer Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red."\nPlayer Orange (you) saying: "The Player Red is the most suspicious Player"\nPlayer Red (to all): "I am Player Orange"\nPlayer Yellow (to all): "I vote Player Orange"\nPlayer Orange (you) saying: "I vote Player Yellow"\nSample 6: Agents say evidence they observed in the environment.\nWorld (to all): Player Pink discovered the dead body of Player Green in room (1,0).\nPlayer Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates\nthan 1\nPlayer Purple (you) saying: "I think it is the Player Red in the room."\nPlayer Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1,\nPlayer Pink (to all): "I think Player Red is the Imposter."\nPlayer Purple (you) saying: "I think it is Player Red.\nPlayer Red (to all): "I think I see Player Purple leaving from room (0,0).\nWorld (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes.\nTherefore, nobody is ejected this round.\nD\nBROADER IMPACTS\nThe primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human\ndemonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to\nlarger models with sufficient compute.\nStrong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot\nreasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural\nlanguage would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams.\nThere are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters\nmake statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the\nability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future\nresearchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language\nmodels before deploying large-scale multi-agent learning systems that can interface with the real world.\nE\nHYPERPARAMETERS AND COMPUTE\nWe use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don’t have a separate scheduler.\n\nTable 2: Common hyperparameters\nhyperparameters\nvalue\nlr\n3e-4\n𝜆BC\n1.0\n𝜆WM\n1.0\n𝜆NL\n0.05\n𝜆L\n0.3\n𝜆S\n1.0\nAn exception to the above hyperparameters is that 𝜆L = 3.0 for 𝜋RL+L+S and 𝜆L = 0.1 for 𝜋RL+L because we find that it significantly\nimpacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories\nin parallel during optimization.\nAll experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute.\nF\nASSETS AND LICENSES\nWe borrow code from CleanRL’s PPO implementation [16], provided under the MIT license.\nWe draw inspiration from Innersloth’s Among Us game, which gives permission to use the Among Us IP for non-commercial and\neducational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for\nillustrative purposes.\nAll art assets in this paper were created using Processing, Matplotlib, and Keynote.\nThis paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='n recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:13:28,472 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:13:42,344 - DEBUG - start_idx: 1420, start_marker: The proliferation o, end_idx: -1, end_marker: code to help further.
2025-02-12 22:13:44,690 - DEBUG - start_idx: 3002, start_marker: A longstanding goal, end_idx: 66803, end_marker: Imposters win!
2025-02-12 22:15:59,635 - DEBUG - start_idx: 1780, start_marker: In recent years, the, end_idx: -1, end_marker: tion in this area.
2025-02-12 22:16:04,876 - DEBUG - start_idx: 364, start_marker: Large Language Mode, end_idx: 1798, end_marker: ies of LLMs.
2025-02-12 22:20:03,425 - DEBUG - start_idx: 1560, start_marker: 1. Introduction
Due, end_idx: -1, end_marker: efficient inference.
2025-02-12 22:20:03,427 - INFO - Total execution time: 394.22 seconds (6.57 minutes)
2025-02-12 22:20:03,433 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='Large Language Models\n(LLMs) by using additional computation during the inference phase. However, current studies do not system-\natically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS.\nThis lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on\ntwo core questions: (1) What is the optimal approach to scale test-time computation across different policy\nmodels, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the\nperformance of LLMs on complex tasks, and can smaller language models outperform larger ones through\nthis approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have\nthe following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of\npolicy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small\npolicy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B\nLLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show\nthe significance of adapting TTS strategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='The proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='A longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='In recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:20:34,066 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:21:55,612 - DEBUG - start_idx: 3002, start_marker: A longstanding goal, end_idx: -1, end_marker: shown in Table 12.
2025-02-12 22:21:56,134 - DEBUG - start_idx: 3124, start_marker: 1. Introduction
Large, end_idx: 41110, end_marker: ical tasks up-to-date.
2025-02-12 22:25:27,686 - DEBUG - start_idx: 1560, start_marker: 1. Introduction
Due, end_idx: -1, end_marker: efficient inference.
2025-02-12 22:25:27,966 - DEBUG - start_idx: 1420, start_marker: The proliferation of, end_idx: 1255, end_marker: earch in multilingual
2025-02-12 22:25:43,725 - ERROR - Error extracting main content: An error occurred (ThrottlingException) when calling the InvokeModel operation: Too many tokens, please wait before trying again.
2025-02-12 22:25:43,729 - INFO - Total execution time: 308.96 seconds (5.15 minutes)
2025-02-12 22:25:43,736 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=88, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content=None),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=24, thumbnail=None, content='A longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!\n\nSample 4: Player Green says false information, but the imposter is still voted out.\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (2,0).\nPlayer Green (to all): "I don\'t know."\nPlayer Blue (to all): "I am not sure," "I do not know," "I do not recognize the dead body\nPlayer Red (you) saying: "I believe Player Green is the Imposter."\nPlayer Yellow (to all): "It is our obligation to investigate whether the dead body of Player Red is the Imposter."\nPlayer Green (to all): "The dead body in room (1,0). Player Red killed Player Blue in room (1\nPlayer Blue (to all): "I believe Player Red is the Imposter."\nPlayer Red (you) saying: "The dead body in room (1,0) is the Imposter. There are currently more\nPlayer Yellow (to all): "I believe Player Red is the Imposter."\nPlayer Red was voted out. Crewmates win!\nSample 5: Despite counter-accusing, Player Orange is voted out by the rest of the group.\nWorld (to all): Player Yellow discovered the dead body of Player Blue in room (1,1).\nPlayer Red (to all): "It is clear Player Orange is the Imposter. Therefore, I vote for Player Orange to be\nPlayer Yellow (to all): "I\'m Player Orange, Player Red knew Player Orange. Player Orange killed Player Red."\nPlayer Orange (you) saying: "The Player Red is the most suspicious Player"\nPlayer Red (to all): "I am Player Orange"\nPlayer Yellow (to all): "I vote Player Orange"\nPlayer Orange (you) saying: "I vote Player Yellow"\nSample 6: Agents say evidence they observed in the environment.\nWorld (to all): Player Pink discovered the dead body of Player Green in room (1,0).\nPlayer Pink (to all): "I have a suspicion that Player Red killed Player Purple. There are currently more Crewmates\nthan 1\nPlayer Purple (you) saying: "I think it is the Player Red in the room."\nPlayer Red (to all): "From Player Purple: The Crewmates discovered the dead body of Player Purple in room (1,\nPlayer Pink (to all): "I think Player Red is the Imposter."\nPlayer Purple (you) saying: "I think it is Player Red.\nPlayer Red (to all): "I think I see Player Purple leaving from room (0,0).\nWorld (to all): Player Red received 1 votes, Player Purple received 1 votes, Player Pink received 1 votes.\nTherefore, nobody is ejected this round.\nD\nBROADER IMPACTS\nThe primary goal of this work is to enable multi-agent communication between language model agents in novel settings without human\ndemonstration data. Although the discussions generated by our agents are still relatively simple, our technique has the potential to scale to\nlarger models with sufficient compute.\nStrong multi-agent communication could have great benefits, such as deploying cooperative agents in settings where humans cannot\nreasonably provide demonstrations, like at microscopic scales or in toxic environments. Being able to specifically communicate in natural\nlanguage would also be very useful, since it can help enable smoother human-AI coordination in heterogeneous teams.\nThere are also potential risks to deploying our technique outside of Among Us. In particular, we notice that both crewmates and imposters\nmake statements that are not backed by evidence. It is unclear whether this is simply a result of using small models that are lacking in the\nability to recall information precisely or if this is a fundamental feature that will be preserved regardless of scale. We encourage future\nresearchers to continue studying Among Us and similar sandboxed settings to understand the multi-agent dynamics of these language\nmodels before deploying large-scale multi-agent learning systems that can interface with the real world.\nE\nHYPERPARAMETERS AND COMPUTE\nWe use the AdamWScheduleFree optimizer from Defazio et al. [6] so we don’t have a separate scheduler.\n\nTable 2: Common hyperparameters\nhyperparameters\nvalue\nlr\n3e-4\n𝜆BC\n1.0\n𝜆WM\n1.0\n𝜆NL\n0.05\n𝜆L\n0.3\n𝜆S\n1.0\nAn exception to the above hyperparameters is that 𝜆L = 3.0 for 𝜋RL+L+S and 𝜆L = 0.1 for 𝜋RL+L because we find that it significantly\nimpacts stability. We use a batch size of 30 environments when collecting RL trajectories, but we subdivide them into processing 6 trajectories\nin parallel during optimization.\nAll experiments were conducted on individual A40 GPUs with 48 GB of VRAM. All models can be trained within 48 hours of compute.\nF\nASSETS AND LICENSES\nWe borrow code from CleanRL’s PPO implementation [16], provided under the MIT license.\nWe draw inspiration from Innersloth’s Among Us game, which gives permission to use the Among Us IP for non-commercial and\neducational use. Our work is not associated with or officially licensed by Innersloth. Depictions of Among Us characters in Fig. 1 are for\nillustrative purposes.\nAll art assets in this paper were created using Processing, Matplotlib, and Keynote.\nThis paper is provided to the public under the CC-BY-4.0 License, and all associated code is shared under GNU GPL v3.0.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content=None),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=16, thumbnail=None, content="1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.\n2. Related Work\nModel weight quantization is an extremely power-\nful and prevalent technique for making resource-\nintensive neural networks suitable for deployment\nconstraints – especially modern-day LLMs. Quan-\ntization algorithms can be categorized as either\nlearning-free or learning-based. Learning-free\nmethods use limited data to calibrate model pa-\nrameters without relying on gradient descent.\nLearning-based methods, however, utilize gra-\ndient descent to update either model parameters\nor auxiliary parameters to aid in quantization.\n2\n\nMatryoshka Quantization\nLearning-free Quantization Methods.\nNaive\nquantization methods, such as MinMax, absmax,\nand zero-point quantization, aim to directly map\nthe range of model weights to the target bit-\nwidth – see (Dettmers et al., 2022) for a de-\ntailed background. Dettmers et al. (2022) fur-\nther improved this by identifying the need to\nhandle outliers with higher precision than the\nrest of the model weights. The core principle\nof more recent learning-free quantization meth-\nods remains similar while improving various as-\npects of it and using small amounts of data for\ncalibration. For example, GPTQ (Frantar et al.,\n2022) improves upon min-max quantization by it-\nerating over all the coordinates, quantizing them\none at a time, and updating the remaining full-\nprecision coordinates to minimize the layer-wise\nactivation reconstruction error. AWQ (Lin et al.,\n2023), SmoothQuant (Xiao et al., 2023), and\nAffineQuant (Ma et al., 2024) scale the weights\nand activations to reduce outliers, thus mak-\ning them easier to quantize. QuIP (Chee et al.,\n2024), FrameQuant (Adepu et al., 2024), and\nQuaRoT (Ashkboos et al., 2024) multiply the\nweights and activations by orthonormal matri-\nces before quantizing to reduce the number of\noutliers. SqueezeLLM (Kim et al., 2024) uses\nclustering to obtain the optimal buckets for quan-\ntization, and CDQuant (Nair and Suggala, 2024)\nimproves upon GPTQ by greedily choosing the\ncoordinates to descend along. While learning-\nfree methods are inexpensive and work well at\nhigher bit-widths, they are often suboptimal in\nthe low-precision regime, which benefits greatly\nfrom learning-based techniques.\nLearning-based\nQuantization\nMethods.\nQuantization Aware Training (QAT) (Abdol-\nrashidi et al., 2021; Jacob et al., 2018) is a\nlogical approach to ensure that models are easy\nto quantize during inference while retaining\nhigh accuracy. However, because QAT involves\nupdating all the model parameters, its adoption\nfor LLMs has been limited.\nSeveral recent\nworks improve the performance and efficiency\nof QAT. LLM-QAT (Liu et al., 2024a) and\nBitDistiller (Du et al., 2024) enhance QAT with\nknowledge distillation from the full-precision\nmodel. EfficientQAT (Chen et al., 2024) min-\nimizes\nthe\nblock-wise\nreconstruction\nerror\nbefore performing end-to-end training.\nThis\nsignificantly reduces the time it takes for QAT to\nconverge. On the other hand, some techniques\nsignificantly reduce the overhead by learning\nonly the auxiliary parameters, such as scaling\nfactors and zero-points, that aid in quantization\ninstead of updating the actual weight matrices.\nFor example, OmniQuant (Shao et al., 2023)\ndoes not update the model parameters; instead,\nit learns additional scales and shifting parameters\n(that aid with quantization) through gradient\ndescent over the block-wise reconstruction error\nand achieves better accuracy than most QAT\ntechniques.\nLikewise, SpinQuant (Liu et al.,\n2024b) uses gradient descent to learn its rotation\nmatrices. This class of learning-based quantiza-\ntion techniques (OmniQuant, SpinQuant, etc.) is\nwidely adopted due to their appeal of achieving\nQAT-level accuracy at a fraction of the cost.\nMulti-scale Training.\nTraining across multiple\ndata scales (resolutions) was heavily popularized\nin computer vision for both recognition and gen-\neration (Adelson et al., 1984; Denton et al., 2015;\nLin et al., 2017). More recently, the paradigm of\nmulti-scale training has shifted to models (Devvrit\net al., 2023; Kusupati et al., 2022; Rippel et al.,\n2014; Yu et al., 2018), where the data remains the\nsame, and models of varying capacity, all nested\nwithin one large model, are trained jointly. This\njoint, nested (Matryoshka-style) learning with\nvarying model sizes results in a smooth accuracy-\nvs-compute trade-off and is beneficial in many\ndownstream applications and real-world deploy-\nments. However, the most obvious structure with\na nested nature is the bit structure of the inte-\nger data type. Given the success of multi-scale\ntraining for inputs, outputs, and model weights,\nit is imperative to explore it further for integer\ndata types, especially in the context of quantiza-\ntion, which aids in the deployment of resource-\nintensive LLMs.\n3. Matryoshka Quantization\nWe introduce MatQuant, a general-purpose,\nmulti-scale training technique that works seam-\n3\n\nMatryoshka Quantization\nlessly with popular learning-based quantization\nmethods such as Quantization Aware Training\n(QAT) (Jacob et al., 2018) and OmniQuant (Shao\net al., 2023). As long as the model or auxiliary\nparameters are optimized with gradient descent,\nMatQuant’s multi-scale training technique can be\nused across chosen bit-widths, leveraging the in-\nherent nested structure of integer data types. In\nthis section, we will elaborate on the preliminar-\nies behind QAT and OmniQuant, alongside our\nnovel proposed approach, MatQuant.\n3.1. Preliminaries\n3.1.1. Quantized Aware Training\nQuantized Aware Training (QAT) learns a 𝑐-bit\nquantized model by optimizing for the end-to-\nend cross entropy loss using gradient descent. It\nuses the quantized weights for the forward pass\nand a straight through estimator (STE) (Bengio\net al., 2013) to propagate gradients through the\nquantization operator during the backward pass.\nTo mathematically formulate QAT, we define\nMinMax quantization of a real-valued vector 𝑤in\n𝑐bits as follows:\n𝑄MM(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= max(𝑤) −min(𝑤)\n2𝑐−1\n,\n𝑧= −min(𝑤)\n𝛼\n(1)\nwhere 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of\n𝑤, 𝛼is the scaling factor and 𝑧is the zero point.\nLet 𝑊𝐹represent weights of a Transformer LLM\nand let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled\ndataset where 𝑥𝑖and 𝑦𝑖represent the input and\noutput respectively. With 𝐿CE as the cross entropy\nloss, the optimization of QAT is:\nmin\n𝑊𝐹\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\nLCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖)\n(2)\nwhere 𝐹(·) represents the LLM’s forward pass.\n3.1.2. OmniQuant\nOmniQuant, unlike QAT, does not update the\nmodel parameters. Instead, it learns additional\nscaling and shifting parameters through gradient\ndescent over layer-wise L2 error reconstruction.\nThese auxiliary parameters aid with quantization.\nSimilar to QAT, OmniQuant also uses a straight\nthrough estimator during optimization. However,\nunlike QAT, OmniQuant operates with limited\ndata, making it much more attractive for resource-\nscarce settings.\nOmniQuant adds two learnable scales, 𝛾and\n𝛽, to MinMax quantization as follows:\n𝑄Omni(𝑤, 𝑐) = clamp\n\x10j𝑤\n𝛼+ 𝑧\nm\n, 0, 2𝑐−1\n\x11\n𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤)\n2𝑐−1\n,\n𝑧= −𝛽· min(𝑤)\n𝛼\n(3)\nOmniQuant also adds another set of learnable\nshifting and scaling parameters to the FFN’s affine\nprojections as follows:\n𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4)\nwhere 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor-\nmation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso-\nciated with the affine transformation, 𝑏∈ℝ𝑑o is\nthe bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable\nshift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2\nerror (where a layer consists of an Attention block\nfollowed by an FFN block), OmniQuant’s overall\nobjective can be portrayed as follows:\nmin\n𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙\n𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙\n𝐹), 𝑋𝑙)||2\n2\n(5)\nwhere 𝐹𝑙(·) represents the forward pass for a sin-\ngle layer 𝑙, 𝑊𝑙\n𝐹represents the layer parameters\nand 𝑋𝑙represents the layer’s input. Note that the\nabove objective is optimized independently for\neach of the 𝐿Transformer layers.\n3.2. MatQuant\nMatQuant is a general purpose framework to de-\nvelop a single model that can do well at any\nprecision. It is a multi-scale training technique\nthat works with most learning-based quantization\nschemes like QAT and OmniQuant discussed ear-\nlier. At its core, taking inspiration from Kusupati\net al. (2022), MatQuant optimizes the quantiza-\ntion loss for several target bit-widths jointly.\nTo have a single model for various integer pre-\ncisions, we nest smaller bit-widths into large ones\n4\n\nMatryoshka Quantization\n– leveraging the inherent Matryoshka nature of\nthe integer data type. So, if we want to extract a\n𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can\njust slice out the 𝑟most significant bits (MSBs) –\nusing a right shift, followed by a left shift of the\nsame order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices\nthe most significant 𝑟bits from a 𝑐-bit quantized\nvector 𝑞𝑐:\n𝑆(𝑞𝑐, 𝑟) =\n\x12\x16 𝑞𝑐\n2𝑐−𝑟\n\x19\x13\n∗2𝑐−𝑟\n(6)\nOnce we have this structure, we can optimize\nfor several precisions by slicing the MSBs from\nthe largest bit-width we are optimizing for. Let\n𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want\nto optimize for, 𝑄(·, ) represent the quantiza-\ntion function of the base algorithm (i.e., any\nlearning-based quantization scheme), L(·) rep-\nresent the loss function pertaining to the base\nalgorithm, 𝐹(·) represent the forward pass re-\nquired to compute the loss, 𝜃represent the set\nof model/auxiliary parameters we are optimizing\nfor and let 𝑊𝐹represent the model parameters.\nMatQuant’s overall objective can be formulated\nas follows:\nmin\n𝑃\n1\n𝑁\n∑︁\n𝑖∈[𝑁]\n∑︁\n𝑟∈𝑅\n𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′\n𝑖), 𝑦′\n𝑖\n\x01 (7)\nwhere 𝑦′\n𝑖= 𝑦𝑖for QAT and 𝑦′\n𝑖= 𝐹𝑙(𝑊𝑙\n𝐹, 𝑋𝑖\n𝑙) for\nOmniQuant, and 𝑥′\n𝑖= 𝑥𝑖for QAT and 𝑥′\n𝑖= 𝑋𝑖\n𝑙for\nOmniQuant. 𝜆𝑟is the loss reweighing factor for\nbit-width 𝑟.\nIn this work, we default to training MatQuant\nwith three bit-widths, 𝑅= {8, 4, 2}, and subse-\nquently perform a grid search over 𝜆𝑟. This pro-\ncess aims to optimize performance such that the\nmodel performs well across all targeted precision\nlevels. Further, while the focus of this paper is pri-\nmarily on integer data types, we discuss the pos-\nsibility of extending MatQuant to floating-point\nrepresentations in Section 5.5.\nA key point to note is that MatQuant primarily\nalters the quantized weight distributions across\nprecision levels compared to the base quantiza-\ntion algorithm (OmniQuant or QAT). Figure 1c\nillustrates the differences in the quantized weight\nhistograms obtained with and without MatQuant\non Gemma-2 9B using OmniQuant. Upon close\nobservation, we find that all the distributions\nof MatQuant are shifted to the right; that is,\nweights quantized with MatQuant tend to use\nmore higher-valued weights. While this might\nnot significantly impact int8 or even int4 models,\nint2 models benefit from utilizing more of the\npossible quantized weights compared to the base-\nline. Because int2 favors higher-valued weights,\nthis effect propagates to higher-valued weights for\nint4, and then to int8. This observation highlights\nthe potential overparameterization and freedom\nin the int8 data type to accommodate the more\nstringent needs of int2 during joint training. We\nfurther explore the effects of this phenomenon in\nSection 5.3 to develop a better standalone quan-\ntization technique for a single target precision.\n3.2.1. Interpolative Behavior\nSlicing.\nAlthough we explicitly train MatQuant\nfor three precisions (int8, int4, int2), we find that\nthe resulting model, when quantized to interpo-\nlated bit-widths like int6 & int3 by slicing (Eq. 6)\nthe int8 model, performs on par with a baseline\ntrained explicitly for that precision. It is also sig-\nnificantly better than slicing an int8 quantized\nmodel. We attribute this strong interpolation in\nbit-width space to MatQuant, and present more\nresults in Sections 4.1 & 4.2.\nMix’n’Match.\nMatQuant also enables the use\nof different precisions at different layers through\nlayer-wise Mix’n’Match (Devvrit et al., 2023),\neven though we never trained for these com-\nbinatorial possibilities. These large number of\nmodels, obtained at no cost, densely span the\naccuracy-vs-memory trade-off. We explore sev-\neral Mix’n’Match strategies and find that having\na higher precision (int8) in the middle layers and\na lower precision (int2) at the start and end is\nPareto-optimal among hundreds of possible mod-\nels. See Section 4.3 for detailed experiments.\n4. Experiments\nIn this section, we present an empirical evaluation\nof MatQuant working with two popular learning-\n5\n\nMatryoshka Quantization\nTable 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant\nperforms on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even\nthe int3, int6 models obtained for free through interpolation from MatQuant perform comparably to\nthe explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log\npplx (perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nOmniQuant\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n68.25\n2.552\n74.59\n2.418\n73.77\n2.110\nMatQuant\n67.85\n2.580\n74.33\n2.446\n73.46\n2.132\nint4\nSliced int8\n62.98\n2.794\n72.19\n2.546\n46.59\n4.139\nBaseline\n67.03\n2.598\n74.33\n2.451\n73.62\n2.136\nMatQuant\n66.54\n2.617\n74.26\n2.470\n73.13\n2.155\nint2\nSliced int8\n37.68\n17.993\n35.75\n14.892\n36.25\n10.831\nBaseline\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nMatQuant\n55.70\n3.355\n68.25\n2.823\n65.99\n2.569\nint6\nSliced int8\n67.66\n2.565\n74.61\n2.424\n73.50\n2.122\nBaseline\n68.06\n2.554\n74.23\n2.420\n74.10\n2.112\nMatQuant\n68.01\n2.582\n74.50\n2.446\n73.59\n2.139\nint3\nSliced int8\n42.00\n5.781\n55.76\n3.830\n34.60\n8.539\nBaseline\n64.37\n2.727\n73.23\n2.549\n71.68\n2.211\nMatQuant\n63.24\n2.757\n73.25\n2.535\n71.55\n2.228\nbased quantization methods: OmniQuant (Sec-\ntion 4.1) and QAT (Section 4.2). We demon-\nstrate MatQuant’s efficiency on Transformer-\nbased LLMs. Unless otherwise mentioned, our\nprimary focus is on weight quantization within\nthe parameter-intensive FFN blocks of the Trans-\nformer layer.\nFor our experiments, we chose the default tar-\nget quantization precisions to be int8, int4, and\nint2. Furthermore, we showcase the interpolative\nnature of MatQuant through evaluations on int6\nand int3, as well as its elastic ability to densely\nspan the accuracy-vs-cost trade-off using layer-\nwise Mix’n’Match (Section 4.3). Finally, we ablate\non improving the performance of MatQuant (Sec-\ntions 5.1 and 5.2) and extend MatQuant to the\nquantization of FFN and Attention parameters.\n(Section 5.3). Further training and fine-grained\nevaluation details are in the Appendix.\nModels and Data.\nWe experiment with Gemma-\n2 (Gemma-Team, 2024) 2B, 9B, and Mistral\n7B (Jiang et al., 2023) models. For OmniQuant\nexperiments, we sample 128 examples with a se-\nquence length of 2048 from the C4 dataset (Raffel\net al., 2020) and train using a batch size of 4. We\ntrain for a total of 10M tokens for all models ex-\ncept the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT ex-\nperiments, we sample a fixed set of 100M tokens\nfrom the C4 dataset and train all our models us-\ning a batch size of 16 and a sequence length of\n8192 for a single epoch.\nBaselines.\nFor OmniQuant and QAT, our pri-\nmary baselines (referred to as “Baseline” in the\ntables and figures) are models trained explicitly\nfor a given precision. When interpolating the\nmodels trained with MatQuant for int6 and int3,\nwe do not perform any additional training. How-\never, the baselines are trained explicitly for 6 and\n3 bits respectively. We also compare against a\nsliced int8 OmniQuant/QAT baseline model to the\ncorresponding precision (referred to as “Sliced\nint8” in the tables).\nEvaluation\nDatasets.\nFollowing\nrecent\nwork (Frantar et al., 2022; Ma et al., 2024), we\nevaluate all the methods based on log perplexity\nand average zero-shot accuracy across a col-\nlection of downstream tasks. We use C4’s test\n6\n\nMatryoshka Quantization\nTable 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs\non par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the\nint3, int6 models obtained for free through interpolation from MatQuant perform comparably to the\nexplicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx\n(perplexity) is computed on C4 validation set (↓).\nData type\nMethod\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n68.21\n2.551\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n67.82\n2.458\n74.17\n2.29\n73.48\n2.084\nMatQuant\n67.68\n2.471\n74.77\n2.301\n72.41\n2.085\nint4\nSliced int8\n67.20\n2.458\n73.25\n2.338\n71.83\n2.164\nBaseline\n67.03\n2.512\n73.26\n2.324\n72.13\n2.105\nMatQuant\n67.05\n2.521\n73.71\n2.332\n71.63\n2.111\nint2\nSliced int8\n39.67\n9.317\n40.35\n7.144\n38.40\n10.594\nBaseline\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\nint6\nSliced int8\n67.55\n2.462\n74.12\n2.294\n73.30\n2.088\nBaseline\n67.75\n2.460\n74.31\n2.293\n72.71\n2.077\nMatQuant\n67.60\n2.476\n74.55\n2.303\n72.70\n2.089\nint3\nSliced int8\n60.23\n2.913\n68.57\n2.565\n65.29\n2.441\nBaseline\n61.75\n2.678\n69.9\n2.43\n68.82\n2.197\nMatQuant\n62.51\n2.798\n70.68\n2.486\n66.44\n2.308\nset to calculate perplexity, and for downstream\nevaluations, we test on ARC-c, ARC-e (Clark\net al., 2018), BoolQ (Clark et al., 2019), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), and Winogrande (Sakaguchi et al., 2020).\n4.1. MatQuant with OmniQuant\nTable 1 shows the efficacy of MatQuant when\nused with FFN-only OmniQuant and compared to\nexplicitly trained OmniQuant baselines for the tar-\nget precisions, i.e., int8, int4, and int2, across all\nthe models. While the average downstream accu-\nracy of MatQuant for int8 and int4 quantization is\nwithin 0.5% of the corresponding independently\ntrained baselines, the int2 quantized models of\nMatQuant are 4.37%, 8.01%, and 6.35% more\naccurate for Gemma-2 2B, 9B, and Mistral 7B,\nrespectively. Similar trends and improvements\nfollow when measuring performance through val-\nidation log perplexity. Further, the quantized\nint4 and int2 models sliced from the int8 Om-\nniQuant baseline suffer a significant drop in accu-\nracy around int4, demonstrating that the nested\nstructure of int8 is not well utilized.\nSliced Interpolation.\nBeyond the target quan-\ntization granularities (int8, int4, and int2),\nMatQuant allows for bit-width interpolation to\nbit-widths not optimized during training. We\nfind that the accuracy of the int6 and int3 models\nobtained by slicing the MatQuant models is com-\nparable to explicitly trained baselines for both\nprecisions.\n4.2. MatQuant with QAT\nTo\nfurther\ndemonstrate\nthe\ngenerality\nof\nMatQuant, we experiment on the same models\nusing the popular QAT technique. Following the\ntrend of experimental results with OmniQuant,\nwe show in Table 2 that the models trained\nusing MatQuant with QAT are comparable to the\nexplicitly trained baselines for all the targeted\nbit-widths of int8 and int4.\nHowever, int2\nquantized models using MatQuant are 4.69%,\n6.30%, and 6.34% more accurate for Gemma-2\n2B, 9B, and Mistral 7B, respectively.\nSliced Interpolation.\nModels trained using\nMatQuant with QAT exhibit strong interpolative\nperformance similar to that of MatQuant with\n7\n\nMatryoshka Quantization\nOmniQuant. We find that the accuracy of the int6\nand int3 models obtained by slicing the MatQuant\nmodels is comparable to explicitly trained base-\nlines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary pa-\nrameters needed for quantization, QAT also up-\ndates the weight parameters. This potentially re-\nsults in severe overfitting to the C4 subset used in\nthe experiments. We observe this overfitting in all\nthe experiments presented in Table 2, where the\nlog perplexities improve for QAT compared to Om-\nniQuant, while the downstream accuracies suffer.\nThis also highlights the need for high-quality data\nfor QAT to realize its benefits; otherwise, users\nare better off using resource-friendly methods\nlike OmniQuant.\n4.3. Layerwise Mix’n’Match\nAlongside the strong slicing-based interpolative\nproperties, quantization with MatQuant also en-\nables another form of elastic and interpolative\nbehavior through Mix’n’Match.\nMix’n’Match\nprovides a mechanism to obtain a combinato-\nrial number of strong models by using differ-\nent quantization granularities, from the target\nbit-widths – i.e., int8, int4, and int2 across lay-\ners. Figure 2 shows the ability of Mix’n’Match to\ndensely span the Pareto-optimal accuracy-vs-bits-\nper-FFN-parameter (memory/cost) trade-off for\n2\n4\n6\n8\nEffective bits per FFN parameter\n60\n65\n70\n75\nTask Average\nGemma-2 9B\nMatQuant\nMix'n'Match\nMatQuant-Interp.\nBaseline\nFigure 2 | Mix’n’Match on Gemma-2 9B model\ntrained using MatQuant with OmniQuant allows\nelastic pareto-optimal accuracy-vs-cost model ex-\ntraction for free during deployment.\nTable 3\n|\nDesign choice ablation for loss\nre-weighting\nof\nthe\n3\ntarget\nbit-widths\n(int8,\nint4,\nint2) that MatQuant explicitly\noptimizes.\nNote that MatQuant (0, 0, 1) ≡\nSingle Precison MatQuant.\nData type\nWeightings\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nTask Avg.\nint8\n(1, 1, 1)\n67.42\n73.97\n73.46\n(1, 1,\n√\n2)\n67.31\n73.45\n73.41\n(2, 2, 1)\n67.85\n74.02\n73.82\n(\n√\n2,\n√\n2, 1)\n67.3\n74.33\n73.82\nint4\n(1, 1, 1)\n66.11\n73.88\n73.13\n(1, 1,\n√\n2)\n66.70\n73.75\n73.29\n(2, 2, 1)\n66.54\n74.33\n73.5\n(\n√\n2,\n√\n2, 1)\n66.46\n74.26\n72.97\nint2\n(1, 1, 1)\n55.71\n68.52\n65.99\n(1, 1,\n√\n2)\n57.08\n67.93\n66.28\n(2, 2, 1)\n55.70\n66.72\n63.49\n(\n√\n2,\n√\n2, 1)\n55.29\n68.25\n57.85\nthe Gemma-2 9B model trained using MatQuant\nwith OmniQuant – sometimes even improving\non the bfloat16 model accuracy. While there are\nmany more feasible models, we only showcase\nthe best models obtained through the strategy de-\nscribed in Section 3.2.1 and further expanded in\nAppendix A. Interestingly, the Mix’n’Match mod-\nels with effective bit-width of 3 and 6 are as ac-\ncurate as models obtained through slicing. This\nopens up possibilities for effective serving depend-\ning on hardware support (Section 5.4).\n5. Ablations and Discussion\nIn this section, we present design ablations to\nimprove MatQuant. Section 5.1 discusses the ef-\nfect of non-uniform weighting across target preci-\nsions (int8, int4, int2), and Section 5.2 explores\nenabling co-distillation of lower precision levels\n(int4, int2) from the highest precision quantized\nmodel (int8). During the process of extending\nMatQuant to all Transformer parameters, not just\nthe FFN block, we uncovered an interesting hy-\nbrid quantization algorithm (between Baseline\nand MatQuant). Section 5.3 further details this\nmethod, called Single Precison MatQuant, which\nstabilizes the otherwise QAT baseline for all the\nTransformer weights. Finally, we also discuss ex-\ntending MatQuant beyond integer data types and\nthe considerations for effective deployment on\ncurrent hardware.\n8\n\nMatryoshka Quantization\n5.1. Weightings (𝜆𝑟) for MatQuant\nDepending on the constraints, we may wish to\nmaximize the accuracy of one of the target bit-\nwidths in MatQuant. Equation 7 provides a gen-\neral formulation of MatQuant that supports a grid\nsearch on the weights 𝜆𝑟for bit-width 𝑟. The re-\nsults in Section 4 are with the weights that have\nbalanced performance across target precisions.\nTable 3 shows the weight multiplier ablation re-\nsults for Gemma-2 2B, 9B, and Mistral 7B. While\nequal weighting for all precisions works well, we\nsee that higher weights for a specific precision\nresults in increased accuracy for that bit-width.\nThis re-weighting to improve int8 and int4 mod-\nels often results in a minor accuracy drop for the\nint2 models. We can consider re-weighting as\nscaling the importance of the bits during training,\nand finding an optimal grid-search-free recipe is\nan interesting research question.\n5.2. Co-distillation for MatQuant\nGiven the nested nature of the models trained us-\ning MatQuant, we explored co-distillation, where\nthe outputs from a higher-precision model are\nused as the target for the lower-precision nested\nmodel, either in a standalone fashion or along-\nside the ground truth target (weighted equally).\nTable 4 shows the effects of co-distillation ap-\nplied to MatQuant with both OmniQuant and\nQAT on Gemma-2 9B. While int8 and int4 show no\nsignificant improvement, the nested int2 model\nbenefits substantially from the int8 supervision,\nreaching 1.65% higher accuracy than the non-co-\ndistilled MatQuant with OmniQuant. This helps\nus push the int2 quantized Gemma-2 9B beyond\n70% average downstream accuracy for the first\ntime across all our experiments. Co-distillation\nin MatQuant opens up avenues for interesting de-\nsign choices that can further leverage the inherent\nnested structure of integer data types.\n5.3. Single Precison MatQuant\nIn Tables 1 and 2, MatQuant performs on par with\nthe explicitly trained baselines for int4, int8, and\nthe interpolated int3 and int6 precisions. How-\never, the int2 models show a significant accuracy\nimprovement. To investigate this, we conducted\nTable 4 | Design choice ablations for co-distillation\nwithin MatQuant. x →y represents distilling the\ny-bit model from the x-bit model. We note that\nthe accuracy for int2 has significantly improved\nwhile minimally impacting the other bit-widths.\nOmniQuant\nQAT\nData type\nConfig.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nint8\n[8, 4, 2]\n73.97\n2.451\n74.77\n2.301\n[8, 4, 8 →2]\n73.40\n2.467\n74.72\n2.298\n[8, 4, 2, 8 →2]\n73.46\n2.466\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n73.32\n2.466\n74.80\n2.302\nint4\n[8, 4, 2]\n73.88\n2.481\n73.71\n2.332\n[8, 4, 8 →2]\n73.84\n2.488\n73.76\n2.328\n[8, 4, 2, 8 →2]\n73.01\n2.495\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n73.12\n2.518\n73.48\n2.330\nint2\n[8, 4, 2]\n68.52\n2.809\n62.32\n2.756\n[8, 4, 8 →2]\n69.2\n2.796\n61.81\n2.740\n[8, 4, 2, 8 →2]\n70.17\n2.778\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n69.72\n2.804\n62.12\n2.746\na simple ablation in MatQuant by removing the\nloss terms for int4 and int8 (i.e., 𝑅= {2} in\nEquation 7 or setting 𝜆4 = 𝜆8 = 0) and present\nthe results in Table 5. We call this version of\nMatQuant as Single Precison MatQuant.\nWith\nSingle Precison MatQuant, we observe a further\nboost of up to 1.67%, in the accuracy of int2 mod-\nels at a ∼2% accuracy drop in the corresponding\nint4 and int8 models – int2 is still nested within\nint8. This improvement likely stems from the six\nadditional bits available during MatQuant-style\ntraining to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gra-\ndient descent is free to tune these six additional\nbits to improve the overall quality of the int2\nmodel. In MatQuant, since we have additional\nlosses to preserve the performance of the int4\nTable 5 | Single Precison MatQuant significantly\nimproves upon the baseline for int2 and, at times,\noutperforms MatQuant. Crucially, int8 and int4\nperformances of Single Precison MatQuant expe-\nrience a significant accuracy decrease (Tables 21\n& 22).\nint2\nGemma-2 2B\nGemma-2 9B\nMistral 7B\nMethod\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nOmniQuant\n51.33\n3.835\n60.24\n3.292\n59.74\n3.931\nS.P. MatQuant\n57.38\n3.185\n68.58\n2.857\n67.36\n2.464\nMatQuant\n55.71\n3.292\n68.52\n2.809\n65.99\n2.569\nQAT\n47.74\n3.433\n56.02\n2.923\n54.95\n2.699\nS.P. MatQuant\n53.18\n3.090\n62.53\n2.706\n61.55\n2.435\nMatQuant\n52.43\n3.153\n62.32\n2.756\n61.29\n2.474\n9\n\nMatryoshka Quantization\nTable 6 | Extending MatQuant with QAT to FFN\n+ Attention parameters. Baseline QAT destabi-\nlizes for int2 and int3 but improves significantly\nthrough MatQuant & Single Precison MatQuant.\nData type\nMethod\nGemma-2 9B\nMistral 7B\nQAT\nTask Avg.\nlog pplx.\nTask Avg.\nlog pplx.\nbfloat16\n74.38\n2.418\n73.99\n2.110\nint8\nBaseline\n74.61\n2.353\n73.73\n2.091\nMatQuant\n75.07\n2.374\n73.58\n2.101\nint4\nSliced int8\n73.56\n2.43\n71.42\n2.246\nBaseline\n72.98\n2.40\n71.87\n2.132\nMatQuant\n74.11\n2.436\n71.5\n2.166\nint2\nSliced int8\n39.05\n13.116\n38.39\n12.066\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n47.78\n3.705\n34.69\n7.564\nMatQuant\n47.17\n3.837\n43.33\n3.806\nint6\nSliced int8\n74.56\n2.358\n73.71\n2.094\nBaseline\n74.65\n2.357\n73.72\n2.093\nMatQuant\n75.04\n2.379\n73.36\n2.106\nint3\nSliced int8\n64.23\n2.908\n39.36\n4.918\nBaseline\n-\n-\n-\n-\nS.P. MatQuant\n68.69\n2.569\n68.41\n2.245\nMatQuant\n66.94\n2.91\n59.45\n2.703\nand int8, the int2 performance is slightly worse\nthan Single Precison MatQuant. However, since\nthe int4 and int8 models are typically very close\nin accuracy to the bfloat16 model, MatQuant can\nshift some of the weights to improve the int2\nmodel. As int4 and int8 models have substan-\ntially more quantized buckets than int2, we hy-\npothesize that shifting some weights into adjacent\nbuckets may not significantly affect their perfor-\nmance; however, it can significantly impact int2’s\nperformance. In fact, in the weight distributions\npresented in Fig 1c, we observe that MatQuant re-\nsults in a model where larger number of weights\nare assigned to the higher-valued buckets. Conclu-\nsively, MatQuant and Single Precison MatQuant\ninherently seem to be a better way of doing low-\nbit quantization.\nFFN + Attention Weight Quantization.\nWe\npresent results for FFN + Attention quantization\nfor QAT in Table 6. For int8, int4 and the inter-\npolated int6 model, MatQuant performs on par\nwith the Baseline. However, we found int2 and\nint3 to be very unstable while quantizing both,\nthe FFN and the Attention parameters. Most re-\ncent works that do QAT for both the blocks Chen\net al. (2024); Du et al. (2024); Liu et al. (2024a)\neither do some form of warm starting for the\nquantized parameters, or have additional distil-\nlation and auxiliary loss functions. In the naive\nsetup of minimizing the loss with respect to the\nground truth, we find QAT to be very unstable at\nlower precisions. However, both MatQuant and\nSingle Precison MatQuant are very stable further\nhighlighting the benefits brought by MatQuant\nstyle training.\n5.4. Deployment Considerations\nCurrent hardware accelerators have native sup-\nport for serving int8 and int4 quantized models.\nAdditionally, custom-implemented CUDA kernels\ncan can support various low-precision bit-widths,\nlike int2 and int3 (Chee et al., 2024; Frantar\net al., 2022). MatQuant can generate a large\nnumber of models at inference time. Depend-\ning on the serving environment, we can choose\nbetween Mix’n’Match models and homogeneous\nsliced models. For example, suppose the serving\nenvironment has a memory constraint equivalent\nto an int3 model but lacks optimized support\nfor int3, while supporting int2. In this case, a\nMix’n’Match model performing comparably to the\nint3 model could be deployed. More generally, as\ndepicted in Figure 2, MatQuant densely spans the\nmemory-versus-accuracy curve and can be lever-\naged to obtain the most performant model for a\nspecific serving constraint. MatQuant can enable\nfurther research on hardware software co-design\nto effectively support elastic bit-widths on-the-fly\nduring inference time.\n5.5. Extension to Floating Point\nExtending MatQuant to floating-point represen-\ntations, such as FP8 and FP4, presents significant\nchallenges. Given that the exponent is encoded\nwithin the bit representation and contributes to\nthe value as a power of 2 (i.e., effectively log2),\nslicing it results in buckets whose sizes increase\nexponentially, unlike the integer case, where\nbucket sizes are constant. For example, slicing\nthe first two bits from int8 yields buckets of 0,\n64, 128, 192. Here, the bucket size (64) is con-\nstant; however, this would not be the case when\nslicing two exponent bits from FP8. This is a\npromising avenue for future research that could\n10\n\nMatryoshka Quantization\nfurther unlock the benefits of MatQuant, even\nduring large-scale pretraining.\n6. Conclusions\nIn this work, we presented MatQuant, a novel\nmulti-scale training technique that leverages the\nnested structure of integer data types to simul-\ntaneously optimize model weight quantization\nacross multiple precisions (int8, int4, and int2)\nwithin a single model.\nThis general-purpose\nmethod, applicable to learning-based quantiza-\ntion techniques like OmniQuant and QAT, pro-\nduces models with comparable accuracy to base-\nlines for int8 and int4, while achieving sig-\nnificant improvements, up to 10% (using co-\ndistillation), for int2 models.\nMatQuant fur-\nther enables bit-width interpolation and layer-\nwise mix-and-match for flexible accuracy-cost\ntrade-offs, promising more efficient deployment\nof large models across various hardware set-\ntings. Finally, MatQuant also helped discover\nSingle Precison MatQuant, which significantly\nimproves standalone low-bit quantization.\nAcknowledgments\nWe are grateful to Varun Yerram, Shreya Pathak\nand Devvrit for assistance in setting up inference\npipelines, Praneeth Netrapalli, Rakesh Shivanna,\nTom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi\nand Rahul Sukthankar for helpful discussions,\nsupport and feedback.\nReferences\nA. Abdolrashidi, L. Wang, S. Agrawal, J. Mal-\nmaud, O. Rybakov, C. Leichner, and L. Lew.\nPareto-optimal quantized resnet is mostly 4-bit.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n3091–3099, 2021.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nE. H. Adelson, C. H. Anderson, J. R. Bergen, P. J.\nBurt, and J. M. Ogden. Pyramid methods in\nimage processing. RCA engineer, 29(6):33–41,\n1984.\nH. Adepu, Z. Zeng, L. Zhang, and V. Singh.\nFramequant: Flexible low-bit quantization for\ntransformers. arXiv preprint arXiv:2403.06082,\n2024.\nS. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,\nM. Jaggi, D. Alistarh, T. Hoefler, and J. Hens-\nman. Quarot: Outlier-free 4-bit inference in ro-\ntated llms. CoRR, abs/2404.00456, 2024. doi:\n10.48550/ARXIV.2404.00456. URL https://\ndoi.org/10.48550/arXiv.2404.00456.\nY. Bengio, N. Léonard, and A. Courville. Estimat-\ning or propagating gradients through stochas-\ntic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432, 2013.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language.\nIn The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 7432–7439. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6239. URL https://\ndoi.org/10.1609/aaai.v34i05.6239.\nJ. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip:\n2-bit quantization of large language models\nwith guarantees. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nM. Chen, W. Shao, P. Xu, J. Wang, P. Gao,\nK. Zhang, Y. Qiao, and P. Luo. Efficientqat:\nEfficient quantization-aware training for large\nlanguage models.\nCoRR, abs/2407.11062,\n2024.\ndoi:\n10.48550/ARXIV.2407.11062.\nURL https://doi.org/10.48550/arXiv.\n2407.11062.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no ques-\ntions. In J. Burstein, C. Doran, and T. Solorio,\n11\n\nMatryoshka Quantization\neditors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 2924–2936. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/N19-1300.\nURL https:\n//doi.org/10.18653/v1/n19-1300.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab-\nharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering?\ntry\narc, the AI2 reasoning challenge.\nCoRR,\nabs/1803.05457, 2018. URL http://arxiv.\norg/abs/1803.05457.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep\ngenerative image models using a laplacian pyra-\nmid of adversarial networks. Advances in neural\ninformation processing systems, 28, 2015.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettle-\nmoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. Advances in Neu-\nral Information Processing Systems, 35:30318–\n30332, 2022.\nF.\nDevvrit,\nS.\nKudugunta,\nA.\nKusupati,\nT. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov,\nH. Hajishirzi, S. Kakade, A. Farhadi, P. Jain,\net al. Matformer: Nested transformer for elas-\ntic inference. arXiv preprint arXiv:2310.07707,\n2023.\nD. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu,\nand N. Xu.\nBitdistiller: Unleashing the po-\ntential of sub-4-bit llms via self-distillation.\nIn L. Ku, A. Martins, and V. Srikumar, edi-\ntors, Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024, pages 102–\n116. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/V1/2024.ACL-LONG.\n7. URL https://doi.org/10.18653/v1/\n2024.acl-long.7.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-\nDahle, A. Letman, A. Mathur, A. Schelten,\nA. Yang, A. Fan, et al. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022.\nG. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai,\nA. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,\net al. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024.\nGemma-Team.\nGemma 2:\nImproving open\nlanguage models at a practical size.\nArXiv,\nabs/2408.00118,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n270843326.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2704–2713, 2018.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mis-\ntral 7b. CoRR, abs/2310.06825, 2023. doi:\n10.48550/ARXIV.2310.06825. URL https://\ndoi.org/10.48550/arXiv.2310.06825.\nS. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,\nS. Shen, M. W. Mahoney, and K. Keutzer.\nSqueezellm: Dense-and-sparse quantization.\nIn Forty-first International Conference on Ma-\nchine Learning, ICML 2024, Vienna, Aus-\ntria,\nJuly\n21-27,\n2024.\nOpenReview.net,\n2024.\nURL https://openreview.net/\nforum?id=0jpbpFia8m.\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford,\nA. Sinha, V. Ramanujan, W. Howard-Snyder,\nK. Chen, S. Kakade, P. Jain, et al. Matryoshka\nrepresentation learning. Advances in Neural In-\nformation Processing Systems, 35:30233–30249,\n2022.\nJ. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and\nS. Han. Awq: Activation-aware weight quan-\n12\n\nMatryoshka Quantization\ntization for llm compression and acceleration.\narXiv preprint arXiv:2306.00978, 2023.\nT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari-\nharan, and S. Belongie. Feature pyramid net-\nworks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and\nV. Chandra.\nLLM-QAT: data-free quantiza-\ntion aware training for large language mod-\nels.\nIn L. Ku, A. Martins, and V. Srikumar,\neditors, Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thai-\nland and virtual meeting, August 11-16, 2024,\npages 467–484. Association for Computational\nLinguistics, 2024a. doi: 10.18653/V1/2024.\nFINDINGS-ACL.26. URL https://doi.org/\n10.18653/v1/2024.findings-acl.26.\nZ. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and\nT. Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406,\n2024b.\ndoi: 10.48550/ARXIV.2405.16406.\nURL https://doi.org/10.48550/arXiv.\n2405.16406.\nY. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang,\nS. Wen, F. Chao, and R. Ji. Affinequant: Affine\ntransformation quantization for large language\nmodels.\narXiv preprint arXiv:2403.12544,\n2024.\nP. A. Nair and A. S. Suggala. Cdquant: Accu-\nrate post-training weight quantization of large\npre-trained models using greedy coordinate\ndescent. CoRR, abs/2406.17542, 2024. doi:\n10.48550/ARXIV.2406.17542. URL https://\ndoi.org/10.48550/arXiv.2406.17542.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Jour-\nnal of machine learning research, 21(140):1–67,\n2020.\nO. Rippel, M. Gelbart, and R. Adams. Learning or-\ndered representations with nested dropout. In\nInternational Conference on Machine Learning,\npages 1746–1754. PMLR, 2014.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWinogrande: An adversarial wino-\ngrad schema challenge at scale. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020,\npages 8732–8740. AAAI Press, 2020.\ndoi:\n10.1609/AAAI.V34I05.6399. URL https://\ndoi.org/10.1609/aaai.v34i05.6399.\nW. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li,\nK. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni-\nquant: Omnidirectionally calibrated quantiza-\ntion for large language models. arXiv preprint\narXiv:2308.13137, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In\nNeural Information Processing Systems, 2017.\nURL\nhttps://api.semanticscholar.\norg/CorpusID:13756489.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and\nS. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language\nmodels. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. Huang.\nSlimmable neural networks.\narXiv preprint\narXiv:1812.08928, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi,\nand Y. Choi. Hellaswag: Can a machine re-\nally finish your sentence?\nIn A. Korhonen,\nD. R. Traum, and L. Màrquez, editors, Pro-\nceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4791–4800. Asso-\nciation for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472.\nURL https:\n//doi.org/10.18653/v1/p19-1472.\n13\n\nMatryoshka Quantization\nA. Addition Training Details\nWe run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning\nrate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and\nuse a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with\na sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4.\nWe train for a total of 10M tokens for all models except the int2 baseline, where we train the model\nfor 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from\nthe C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a\nsingle epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4\nand train with a batch size of 16 for a single epoch.\nMix’n’Match\nFor a fixed effective bits-per-FFN layer, where each layer was quantized to either\nint2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid,\nIncreasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2,\nthe central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy\nfollowed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers,\nand int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending\nand descending order, respectively, across the layers. Our experimental results demonstrated that,\nfor a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others.\nAllocating higher precision (int8) to the middle layers helped preserve critical information, while the\ninitial and final layers performed adequately with lower bit precision (int2 and int4), leading to a\nmore efficient and effective quantization scheme.\nB. Detailed Downstream Evaluations for OmniQuant ad QAT\nTables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B\nand Mistral 7B with OmniQuant and QAT.\nC. Detailed Downstream Evaluations for MatQuant Re-weighting\nTables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments\non Gemma-2 2B, Gemma-2 9B and Mistral 7B.\nD. Detailed Downstream Evaluations for Co-Distillation\nTables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co-\ndistillation on Gemma-2 9B with OmniQuant and QAT.\nE. Detailed Evaluations for FFN + Attention Quantization\nTables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention\nquantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.\n14\n\nMatryoshka Quantization\nF. Detailed Evaluation for Single Precison MatQuant\nTables\n20,\n21,\n22,\nand\n23\npresent\nthe\ndownstream\nevaluation\nresults\ncomparing\nSingle Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2\n2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices\n2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the\nSingle Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and\nperplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for\nSingle Precison MatQuant in Figure 3.\nFigure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with\nSingle Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution\nis a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the\nfirst 2 MSBs of the int8 representation.\n15\n\nMatryoshka Quantization\nTable 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 2B.\nData type\nMethod\nGemma-2 2B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n50\n71.46\n76.36\n69.76\n78.24\n63.69\n68.25\nMatQuant\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\nint4\nSliced int8\n41.81\n66.2\n71.35\n62.64\n75.95\n59.91\n62.98\nBaseline\n48.46\n70.96\n74.22\n67.66\n77.26\n63.61\n67.03\nMatQuant\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\nint2\nSliced int8\n23.81\n23.53\n53.06\n24.78\n51.8\n49.09\n37.68\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\nint6\nSliced int8\n48.55\n71.25\n75.87\n69.18\n78.35\n62.75\n67.66\nBaseline\n49.32\n71.76\n76.48\n69.52\n78.56\n62.75\n68.06\nMatQuant\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\nint3\nSliced int8\n23.21\n34.43\n58.2\n30.48\n56.69\n49.01\n42\nBaseline\n46.25\n68.64\n72.97\n62.24\n76.06\n60.06\n64.37\nMatQuant\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\nTable 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Gemma-2 9B.\nData type\nMethod\nGemma-2 9B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\nint4\nSliced int8\n55.97\n75.04\n81.19\n73.81\n80.52\n66.61\n72.19\nBaseline\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\nint2\nSliced int8\n23.21\n24.92\n38.13\n25.37\n51.36\n51.54\n35.75\nBaseline\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\nint6\nSliced int8\n59.04\n77.53\n84.68\n77.1\n81.23\n68.11\n74.61\nBaseline\n59.22\n77.27\n83.21\n77.1\n81.12\n67.48\n74.23\nMatQuant\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\nint3\nSliced int8\n35.84\n57.32\n67.61\n48.58\n68.61\n56.59\n55.76\nBaseline\n57.17\n77.06\n83.79\n74.45\n80.36\n66.54\n73.23\nMatQuant\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n16\n\nMatryoshka Quantization\nTable 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant\non Mistral 7B.\nData type\nMethod\nMistral 7B\nOmniQuant\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n73.19\n83.88\n80.41\n81.39\n74.51\n73.77\nMatQuant\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\nint4\nSliced int8\n27.65\n46.72\n49.17\n36.88\n64.09\n55.01\n46.59\nBaseline\n49.23\n73.23\n83.94\n79.9\n81.34\n74.11\n73.62\nMatQuant\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\nint2\nSliced int8\n23.72\n25.29\n43.21\n25.45\n50.49\n49.33\n36.25\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\nint6\nSliced int8\n48.98\n72.01\n83.46\n79.95\n81.72\n74.9\n73.5\nBaseline\n50.26\n73.65\n84.04\n80.55\n81.66\n74.43\n74.1\nMatQuant\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\nint3\nSliced int8\n22.78\n24.66\n37.86\n24.12\n49.24\n48.93\n34.6\nBaseline\n46.33\n70.71\n82.72\n77.74\n80.74\n71.82\n71.68\nMatQuant\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\nTable 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 2B.\nData type\nMethod\nGemma-2 2B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n50.09\n71.59\n76.45\n69.69\n78.29\n63.14\n68.21\nint8\nBaseline\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\nint4\nSliced int8\n46.08\n69.36\n75.78\n68.05\n78.18\n65.75\n67.2\nBaseline\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\nint2\nSliced int8\n25.6\n26.3\n57.98\n25.82\n52.12\n50.2\n39.67\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\nint6\nSliced int8\n47.78\n70.79\n74.25\n69.73\n77.64\n65.11\n67.55\nBaseline\n47.7\n70.88\n74.92\n69.72\n78.07\n65.19\n67.75\nMatQuant\n46.5\n70.71\n75.72\n69.69\n78.02\n64.96\n67.6\nint3\nSliced int8\n38.74\n63.13\n65.57\n58.86\n74.81\n60.3\n60.23\nBaseline\n39.68\n65.28\n67.03\n62.68\n77.04\n58.8\n61.75\nMatQuant\n38.65\n67.34\n70.49\n61.47\n75.41\n61.72\n62.51\n17\n\nMatryoshka Quantization\nTable 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nGemma-2 9B.\nData type\nMethod\nGemma-2 9B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.11\n75.38\n80.12\n78.7\n81.5\n71.19\n74.17\nMatQuant\n58.19\n76.18\n81.5\n79.57\n82.15\n71.03\n74.77\nint4\nSliced int8\n57.42\n75.08\n78.1\n76.97\n81.23\n70.72\n73.25\nBaseline\n56.91\n75.42\n75.38\n78.06\n81.39\n72.38\n73.26\nMatQuant\n57.94\n76.64\n75.2\n78.71\n81.66\n72.14\n73.71\nint2\nSliced int8\n23.89\n27.61\n57.95\n30.16\n54.68\n47.83\n40.35\nBaseline\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\nint6\nSliced int8\n57.85\n75.13\n80.67\n78.63\n81.56\n70.88\n74.12\nBaseline\n57.94\n76.14\n79.63\n78.93\n82.1\n71.11\n74.31\nMatQuant\n58.02\n75.63\n81.31\n79.43\n81.66\n71.27\n74.55\nint3\nSliced int8\n50\n68.1\n75.2\n71.31\n79.43\n67.4\n68.57\nBaseline\n53.07\n75.04\n66.61\n74.94\n80.03\n69.69\n69.9\nMatQuant\n51.62\n71.93\n78.78\n73.99\n80.14\n67.64\n70.68\nTable 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on\nMistral 7B.\nData type\nMethod\nMistral 7B\nQAT\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n48.89\n71.63\n82.42\n81.69\n81.18\n75.06\n73.48\nMatQuant\n46.76\n70.37\n82.51\n79.73\n80.9\n74.19\n72.41\nint4\nSliced int8\n47.18\n70.41\n80.37\n79.84\n80.25\n72.93\n71.83\nBaseline\n47.27\n70.62\n81.28\n78.95\n81.12\n73.56\n72.13\nMatQuant\n45.65\n68.64\n82.02\n79\n81.07\n73.4\n71.63\nint2\nSliced int8\n25.34\n26.47\n54.95\n25.18\n48.48\n49.96\n38.4\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\nint6\nSliced int8\n48.21\n71.51\n82.42\n81.67\n81.72\n74.27\n73.3\nBaseline\n47.7\n71.3\n82.23\n79.84\n80.79\n74.43\n72.71\nMatQuant\n47.53\n71\n81.9\n79.73\n81.28\n74.74\n72.7\nint3\nSliced int8\n40.1\n61.49\n72.91\n68.72\n77.97\n70.56\n65.29\nBaseline\n44.54\n67.97\n73.98\n76.31\n79.65\n70.48\n68.82\nMatQuant\n38.82\n62.42\n77.74\n71.1\n78.07\n70.48\n66.44\n18\n\nMatryoshka Quantization\nTable 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 2B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n71.8\n75.78\n67.64\n78.07\n63.22\n67.42\n(1\n√\n2,\n√\n2)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\n(\n√\n2, 1,\n√\n2)\n47.44\n72.43\n76.02\n67.45\n78.02\n63.85\n67.54\n(1, 1\n√\n2)\n47.7\n71.89\n75.63\n67.21\n78.07\n63.38\n67.31\n(2, 2, 1)\n48.38\n72.31\n76.3\n68.32\n78.35\n63.46\n67.85\n(\n√\n2, 2, 1)\n48.46\n71.84\n75.93\n68.35\n77.91\n63.14\n67.6\n(2,\n√\n2, 1)\n47.95\n71.72\n75.26\n68.13\n78.07\n62.75\n67.31\n(\n√\n2,\n√\n2, 1)\n47.35\n71.34\n75.66\n67.99\n78.07\n63.38\n67.3\nint4\n(1, 1, 1)\n45.65\n70.29\n74.8\n66.07\n77.58\n62.27\n66.11\n(1\n√\n2,\n√\n2)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\n(\n√\n2, 1,\n√\n2)\n46.42\n70.96\n74.71\n65.78\n77.58\n63.14\n66.43\n(1, 1\n√\n2)\n45.56\n71.55\n75.75\n66.18\n77.48\n63.69\n66.7\n(2, 2, 1)\n46.84\n70.88\n74.92\n66.48\n77.91\n62.19\n66.54\n(\n√\n2, 2, 1)\n47.35\n71.68\n72.69\n66.79\n77.26\n63.38\n66.52\n(2,\n√\n2, 1)\n45.9\n70.83\n75.11\n66.97\n77.37\n62.27\n66.41\n(\n√\n2,\n√\n2, 1)\n46.33\n70.92\n73.7\n67.67\n77.26\n62.9\n66.46\nint2\n(1, 1, 1)\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n(1\n√\n2,\n√\n2)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\n(\n√\n2, 1,\n√\n2)\n35.07\n62.04\n65.78\n54.26\n71.65\n56.27\n57.51\n(1, 1\n√\n2)\n34.22\n60.4\n64.98\n54.3\n71.38\n57.22\n57.08\n(2, 2, 1)\n34.47\n57.95\n63.94\n51.84\n69.75\n56.27\n55.7\n(\n√\n2, 2, 1)\n33.45\n57.49\n65.02\n52.22\n70.4\n55.64\n55.7\n(2,\n√\n2, 1)\n34.04\n58.84\n65.11\n51.77\n70.89\n57.14\n56.3\n(\n√\n2,\n√\n2, 1)\n32.76\n56.99\n63.46\n51.99\n70.29\n56.27\n55.29\nint6\n(1, 1, 1)\n47.1\n71.46\n76.02\n67.47\n77.91\n63.61\n67.26\n(1\n√\n2,\n√\n2)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\n(\n√\n2, 1,\n√\n2)\n47.61\n71.89\n75.9\n67.37\n78.24\n63.77\n67.46\n(1, 1\n√\n2)\n47.78\n71.63\n75.47\n67.2\n77.86\n63.61\n67.26\n(2, 2, 1)\n48.55\n72.69\n76.3\n68.02\n78.67\n63.85\n68.01\n(\n√\n2, 2, 1)\n48.29\n71.76\n75.72\n68.42\n78.02\n63.38\n67.6\n(2,\n√\n2, 1)\n48.38\n71.51\n75.84\n68.24\n78.18\n63.85\n67.67\n(\n√\n2,\n√\n2, 1)\n47.44\n71.42\n74.95\n67.85\n77.86\n63.3\n67.14\nint3\n(1, 1, 1)\n44.45\n68.56\n69.11\n62.28\n75.95\n62.59\n63.82\n(1\n√\n2,\n√\n2)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n(\n√\n2, 1,\n√\n2)\n41.98\n68.6\n70.34\n61.95\n75.9\n63.3\n63.68\n(1, 1\n√\n2)\n41.64\n66.71\n71.62\n61.94\n76.01\n61.09\n63.17\n(2, 2, 1)\n41.98\n68.35\n68.41\n63.74\n76.17\n60.77\n63.24\n(\n√\n2, 2, 1)\n42.66\n66.54\n70.46\n63.61\n75.63\n62.98\n63.65\n(2,\n√\n2, 1)\n43.17\n66.71\n60.03\n62.71\n76.77\n61.64\n61.84\n(\n√\n2,\n√\n2, 1)\n43.17\n68.73\n64.74\n61.31\n76.39\n61.48\n62.64\n19\n\nMatryoshka Quantization\nTable 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss\nreweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nGemma-2 9B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n(1\n√\n2,\n√\n2)\n57.68\n77.4\n83.73\n76.1\n81.18\n67.64\n73.95\n(\n√\n2, 1,\n√\n2)\n58.11\n77.86\n81.04\n76\n81.18\n67.09\n73.55\n(1, 1\n√\n2)\n56.91\n77.1\n82.39\n75.93\n81.18\n67.17\n73.45\n(2, 2, 1)\n58.79\n77.48\n82.66\n76.55\n81.23\n67.4\n74.02\n(\n√\n2, 2, 1)\n58.53\n77.31\n82.63\n76.54\n80.96\n67.56\n73.92\n(2,\n√\n2, 1)\n58.62\n77.27\n84.31\n76.54\n81.34\n66.85\n74.16\n(\n√\n2,\n√\n2, 1)\n59.13\n78.07\n84.16\n76.46\n80.9\n67.25\n74.33\nint4\n(1, 1, 1)\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n(1\n√\n2,\n√\n2)\n56.74\n77.74\n85.08\n75.5\n80.85\n66.85\n73.79\n(\n√\n2, 1,\n√\n2)\n57.42\n78.28\n82.51\n75.97\n81.34\n67.56\n73.85\n(1, 1\n√\n2)\n57.59\n77.82\n84.28\n75.32\n81.12\n66.38\n73.75\n(2, 2, 1)\n58.62\n78.28\n83.67\n76.01\n81.5\n67.88\n74.33\n(\n√\n2, 2, 1)\n58.19\n77.82\n83.91\n76.62\n81.99\n67.72\n74.37\n(2,\n√\n2, 1)\n58.28\n78.16\n84.53\n76.41\n81.72\n67.09\n74.36\n(\n√\n2,\n√\n2, 1)\n57.94\n78.11\n84.98\n76.5\n81.01\n67.01\n74.26\nint2\n(1, 1, 1)\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n(1\n√\n2,\n√\n2)\n49.83\n73.91\n78.75\n67.27\n77.2\n66.46\n68.9\n(\n√\n2, 1,\n√\n2)\n48.55\n74.24\n81.5\n68.44\n76.5\n65.9\n69.19\n(1, 1\n√\n2)\n48.29\n72.94\n74.74\n68.34\n77.58\n65.67\n67.93\n(2, 2, 1)\n46.76\n73.27\n71.96\n67.98\n76.77\n63.61\n66.72\n(\n√\n2, 2, 1)\n46.76\n73.7\n77.65\n67.01\n77.58\n65.98\n68.11\n(2,\n√\n2, 1)\n46.76\n72.35\n75.35\n67.51\n76.39\n67.56\n67.65\n(\n√\n2,\n√\n2, 1)\n46.59\n72.6\n79.3\n67.58\n77.69\n65.75\n68.25\nint6\n(1, 1, 1)\n58.87\n78.03\n83.61\n76.18\n81.45\n67.09\n74.21\n(1\n√\n2,\n√\n2)\n57.51\n77.53\n83.55\n75.98\n80.9\n67.17\n73.77\n(\n√\n2, 1,\n√\n2)\n58.79\n77.82\n81.38\n76.21\n81.07\n67.72\n73.83\n(1, 1\n√\n2)\n57.34\n77.23\n82.57\n75.89\n81.12\n67.17\n73.55\n(2, 2, 1)\n59.04\n77.4\n82.66\n76.55\n81.56\n68.03\n74.21\n(\n√\n2, 2, 1)\n59.22\n77.65\n82.17\n76.62\n81.23\n67.8\n74.11\n(2,\n√\n2, 1)\n58.36\n77.82\n83.79\n76.47\n81.23\n67.25\n74.15\n(\n√\n2,\n√\n2, 1)\n59.3\n78.37\n84.5\n76.57\n80.85\n67.4\n74.5\nint3\n(1, 1, 1)\n55.46\n76.14\n84.04\n74.49\n80.14\n67.32\n72.93\n(1\n√\n2,\n√\n2)\n56.23\n76.05\n82.6\n74.85\n80.9\n67.01\n72.94\n(\n√\n2, 1,\n√\n2)\n56.4\n77.86\n80.64\n75.11\n79.87\n68.51\n73.06\n(1, 1\n√\n2)\n55.63\n76.05\n82.39\n74.21\n80.3\n67.17\n72.62\n(2, 2, 1)\n55.2\n76.56\n84.19\n74.87\n80.2\n67.72\n73.12\n(\n√\n2, 2, 1)\n54.44\n75.63\n80.55\n74.97\n80.96\n67.72\n72.38\n(2,\n√\n2, 1)\n56.14\n75.67\n83.33\n74.96\n80.52\n67.72\n73.06\n(\n√\n2,\n√\n2, 1)\n56.31\n77.4\n83.24\n75.62\n80.41\n66.54\n73.25\n20\n\nMatryoshka Quantization\nTable 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight-\ning when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7).\nMistral 7B\nData type\nWeightings\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nint8\n(1, 1, 1)\n48.04\n73.44\n84.13\n79.37\n81.12\n74.66\n73.46\n(1\n√\n2,\n√\n2)\n48.46\n73.19\n84.28\n79.19\n81.12\n74.74\n73.5\n(\n√\n2, 1,\n√\n2)\n47.95\n73.4\n84.46\n79.11\n81.34\n74.51\n73.46\n(1, 1\n√\n2)\n48.21\n73.02\n84.34\n79.03\n81.28\n74.59\n73.41\n(2, 2, 1)\n49.06\n73.48\n84.74\n79.73\n81.56\n74.35\n73.82\n(\n√\n2, 2, 1)\n49.06\n73.57\n84.56\n79.64\n81.39\n74.27\n73.75\n(2,\n√\n2, 1)\n48.98\n73.95\n84.50\n79.60\n81.61\n74.90\n73.92\n(\n√\n2,\n√\n2, 1)\n48.98\n73.86\n84.56\n79.55\n81.23\n74.74\n73.82\nint4\n(1, 1, 1)\n48.21\n72.69\n83.49\n78.82\n81.12\n74.43\n73.13\n(1\n√\n2,\n√\n2)\n49.15\n72.81\n83.39\n78.71\n80.79\n74.66\n73.25\n(\n√\n2, 1,\n√\n2)\n47.95\n72.43\n83.43\n79.24\n81.01\n74.03\n73.01\n(1, 1\n√\n2)\n48.46\n73.44\n84.07\n78.9\n81.01\n73.88\n73.29\n(2, 2, 1)\n49.15\n72.81\n83.88\n79.8\n81.88\n73.48\n73.5\n(\n√\n2, 2, 1)\n48.89\n72.69\n82.72\n79.53\n81.66\n73.88\n73.23\n(2,\n√\n2, 1)\n47.87\n72.05\n83\n79.56\n81.23\n74.27\n73\n(\n√\n2,\n√\n2, 1)\n48.29\n72.47\n82.84\n79.52\n81.07\n73.64\n72.97\nint2\n(1, 1, 1)\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n(1\n√\n2,\n√\n2)\n40.78\n66.2\n73.61\n72.68\n77.75\n67.4\n66.4\n(\n√\n2, 1,\n√\n2)\n40.36\n67.09\n75.35\n72.46\n77.48\n65.9\n66.44\n(1, 1\n√\n2)\n40.36\n67.17\n74.83\n71.64\n77.53\n66.14\n66.28\n(2, 2, 1)\n37.2\n62.46\n67.74\n70.29\n76.55\n66.69\n63.49\n(\n√\n2, 2, 1)\n37.29\n64.35\n61.1\n68.88\n74.86\n65.19\n61.94\n(2,\n√\n2, 1)\n39.68\n65.24\n68.93\n66.64\n75.19\n64.09\n63.29\n(\n√\n2,\n√\n2, 1)\n34.56\n61.24\n60.61\n58.07\n72.63\n59.98\n57.85\nint6\n(1, 1, 1)\n48.46\n72.98\n84.07\n79.64\n81.18\n75.22\n73.59\n(1\n√\n2,\n√\n2)\n49.06\n73.44\n84.59\n79.51\n81.28\n74.74\n73.77\n(\n√\n2, 1,\n√\n2)\n47.95\n73.48\n84.43\n79.28\n81.45\n75.14\n73.62\n(1, 1\n√\n2)\n48.38\n72.94\n84.34\n79.15\n81.18\n74.59\n73.43\n(2, 2, 1)\n48.46\n72.94\n84.13\n79.89\n81.5\n74.9\n73.64\n(\n√\n2, 2, 1)\n48.81\n73.48\n84.34\n79.67\n81.34\n74.9\n73.76\n(2,\n√\n2, 1)\n49.4\n73.65\n84.4\n79.68\n81.28\n74.74\n73.86\n(\n√\n2,\n√\n2, 1)\n49.23\n73.57\n84.43\n79.55\n81.12\n74.66\n73.76\nint3\n(1, 1, 1)\n45.65\n71.21\n80.43\n78.31\n81.07\n72.61\n71.55\n(1\n√\n2,\n√\n2)\n47.7\n72.05\n82.81\n78.74\n81.12\n72.77\n72.53\n(\n√\n2, 1,\n√\n2)\n46.33\n72.43\n81.8\n79.03\n82.1\n73.4\n72.51\n(1, 1\n√\n2)\n45.99\n71.09\n80.73\n78.77\n80.85\n72.53\n71.66\n(2, 2, 1)\n47.95\n73.36\n82.57\n79.31\n81.39\n74.9\n73.25\n(\n√\n2, 2, 1)\n44.45\n69.7\n82.11\n77.68\n80.2\n71.74\n70.98\n(2,\n√\n2, 1)\n46.84\n72.73\n80.95\n78.79\n81.56\n73.01\n72.31\n(\n√\n2,\n√\n2, 1)\n47.01\n71.59\n81.96\n78.89\n81.39\n72.45\n72.22\n21\n\nMatryoshka Quantization\nTable 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with OmniQuant.\nOmniQuant\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n57.59\n77.27\n81.83\n75.48\n81.01\n67.25\n73.4\n2.467\n[8, 4, 2, 8 →2]\n57.17\n77.36\n82.2\n75.82\n80.96\n67.25\n73.46\n2.466\n[8, 4, 2, 8 →4; 2]\n56.4\n77.82\n82.32\n75.02\n80.63\n67.72\n73.32\n2.466\nint4\n[8, 4, 8 →2]\n57.68\n78.45\n82.97\n75.5\n80.85\n67.56\n73.84\n2.488\n[8, 4, 2, 8 →2]\n57.51\n77.61\n80.46\n74.74\n81.12\n66.61\n73.01\n2.495\n[8, 4, 2, 8 →4; 2]\n56.57\n77.99\n82.54\n74.77\n80.58\n66.3\n73.12\n2.518\nint2\n[8, 4, 8 →2]\n48.81\n74.03\n81.65\n68.1\n77.48\n65.11\n69.2\n2.796\n[8, 4, 2, 8 →2]\n49.15\n75.34\n83.12\n68.79\n77.64\n67.01\n70.17\n2.778\n[8, 4, 2, 8 →4; 2]\n49.83\n75.04\n79.79\n68.38\n77.86\n67.4\n69.72\n2.804\nint6\n[8, 4, 8 →2]\n57.42\n77.19\n81.87\n75.42\n81.01\n67.8\n73.45\n2.468\n[8, 4, 2, 8 →2]\n57.51\n77.48\n82.32\n75.88\n81.07\n66.61\n73.48\n2.467\n[8, 4, 2, 8 →4; 2]\n56.4\n78.03\n82.63\n75.14\n80.79\n67.4\n73.4\n2.498\nint3\n[8, 4, 8 →2]\n55.63\n75.88\n80.12\n74.01\n80.36\n67.96\n72.33\n2.549\n[8, 4, 2, 8 →2]\n54.35\n76.85\n79.33\n74.6\n80.47\n67.4\n72.17\n2.543\n[8, 4, 2, 8 →4; 2]\n55.2\n76.98\n82.45\n73.59\n80.41\n68.43\n72.84\n2.58\nTable 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co-\ndistillation experiments on Gemma-2 9B with QAT.\nQAT\nGemma-2 9B\nData type\nConfig.\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\n[8, 4, 8 →2]\n58.11\n76.43\n81.25\n79.12\n82.05\n71.35\n74.72\n2.298\n[8, 4, 2, 8 →2]\n57.51\n76.43\n81.53\n78.95\n82.1\n71.19\n74.62\n2.299\n[8, 4, 2, 8 →4; 2]\n58.11\n76.14\n81.68\n79.12\n82.26\n71.51\n74.8\n2.302\nint4\n[8, 4, 8 →2]\n57.42\n76.35\n77.55\n78.06\n81.61\n71.59\n73.76\n2.328\n[8, 4, 2, 8 →2]\n56.91\n75.8\n78.44\n77.76\n81.39\n72.38\n73.78\n2.329\n[8, 4, 2, 8 →4; 2]\n57.51\n75.76\n75.96\n77.96\n81.72\n71.98\n73.48\n2.33\nint2\n[8, 4, 8 →2]\n39.51\n65.03\n66.88\n63.37\n75.08\n61.01\n61.81\n2.74\n[8, 4, 2, 8 →2]\n40.78\n66.5\n67.55\n63.67\n75.95\n60.62\n62.51\n2.746\n[8, 4, 2, 8 →4; 2]\n40.19\n65.7\n65.57\n63.83\n75.3\n62.12\n62.12\n2.746\nint6\n[8, 4, 8 →2]\n57.85\n76.09\n81.47\n78.98\n81.88\n71.27\n74.59\n2.301\n[8, 4, 2, 8 →2]\n57.17\n75.97\n82.2\n79\n81.83\n71.9\n74.68\n2.302\n[8, 4, 2, 8 →4; 2]\n57.42\n76.09\n82.29\n78.95\n82.10\n71.27\n74.69\n2.305\nint3\n[8, 4, 8 →2]\n51.96\n71.55\n78.07\n73.17\n79.43\n66.93\n70.18\n2.485\n[8, 4, 2, 8 →2]\n50.94\n71.76\n78.78\n73.09\n79.05\n66.77\n70.06\n2.486\n[8, 4, 2, 8 →4; 2]\n51.45\n72.39\n78.84\n73.46\n79.6\n67.96\n70.62\n2.731\n22\n\nMatryoshka Quantization\nTable 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Gemma-2 9B with QAT.\nData type\nMethod\nGemma-2 9B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n58.96\n77.57\n83.33\n77.31\n81.12\n67.96\n74.38\nint8\nBaseline\n58.62\n77.02\n83.43\n79.01\n81.34\n68.27\n74.61\nMatQuant\n59.04\n77.9\n84.4\n78.76\n81.12\n69.22\n75.07\nint4\nSliced int8\n57.42\n76.73\n81.62\n76.02\n80.58\n68.98\n73.56\nBaseline\n56.06\n74.96\n79.27\n77.83\n80.25\n69.53\n72.98\nMatQuant\n57.34\n76.77\n84.19\n77.51\n80.74\n68.11\n74.11\nint2\nSliced int8\n24.74\n25.63\n58.53\n25.5\n50.71\n49.17\n39.05\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n24.91\n41.62\n62.26\n40.87\n63.38\n53.67\n47.78\nMatQuant\n28.24\n39.23\n62.17\n39.13\n63.49\n50.75\n47.17\nint6\nSliced int8\n58.53\n77.15\n82.48\n79.04\n81.5\n68.67\n74.56\nBaseline\n58.87\n77.06\n83.12\n78.81\n81.23\n68.82\n74.65\nMatQuant\n59.81\n77.9\n84.8\n78.68\n81.07\n67.96\n75.04\nint3\nSliced int8\n43.6\n64.98\n72.66\n66\n75.95\n62.19\n64.23\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n50.85\n73.11\n71.13\n72.01\n79.38\n65.67\n68.69\nMatQuant\n45.22\n69.32\n78.5\n68.72\n76.01\n63.85\n66.94\n23\n\nMatryoshka Quantization\nTable 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza-\ntion on Mistral 7B with QAT.\nData type\nMethod\nMistral 7B\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nbfloat16\n49.57\n73.74\n84.4\n80.61\n81.18\n74.43\n73.99\nint8\nBaseline\n49.23\n72.9\n83.49\n80.26\n81.28\n75.22\n73.73\nMatQuant\n49.32\n72.31\n83.76\n80.2\n81.18\n74.74\n73.58\nint4\nSliced int8\n45.99\n71.76\n81.41\n76.95\n80.41\n71.98\n71.42\nBaseline\n48.04\n71.72\n78.87\n78.93\n80.36\n73.32\n71.87\nMatQuant\n47.01\n69.95\n82.02\n76.81\n80.25\n72.93\n71.5\nint2\nSliced int8\n22.78\n24.03\n58.75\n24.63\n50.54\n49.64\n38.39\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n23.21\n23.82\n37.83\n24.67\n49.02\n49.57\n34.69\nMatQuant\n22.27\n32.49\n62.02\n32.43\n59.3\n51.46\n43.33\nint6\nSliced int8\n49.32\n73.53\n82.66\n80.16\n81.12\n75.45\n73.71\nBaseline\n49.32\n73.4\n82.48\n80.24\n81.28\n75.61\n73.72\nMatQuant\n49.15\n71.76\n83.73\n80.13\n81.18\n74.19\n73.36\nint3\nSliced int8\n20.65\n31.57\n44.34\n28.79\n59.41\n51.38\n39.36\nBaseline\n-\n-\n-\n-\n-\n-\n-\nS.P. MatQuant\n41.98\n65.53\n79.39\n74.42\n79.22\n69.93\n68.41\nMatQuant\n34.64\n55.13\n70.43\n58.61\n73.39\n64.48\n59.45\nTable 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant\nand QAT.\nint2\nGemma2-2B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n34.64\n64.06\n65.69\n53.07\n69.7\n57.14\n57.38\n3.185\nBaseline\n31.31\n53.58\n62.2\n40.78\n66.05\n54.06\n51.33\n3.835\nMatQuant\n34.39\n59.64\n62.69\n52.11\n69.86\n55.56\n55.71\n3.292\nQAT\nS.P. MatQuant\n28.92\n53.79\n62.84\n48.41\n69.86\n55.25\n53.18\n3.090\nBaseline\n24.66\n43.22\n62.17\n38.39\n64.42\n53.59\n47.74\n3.433\nMatQuant\n28.24\n51.73\n64.19\n46.76\n68.66\n55.01\n52.43\n3.153\n24\n\nMatryoshka Quantization\nTable 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with\nOmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and\nint8 model were sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n56.48\n76.85\n73.36\n74.87\n80.74\n66.77\n71.51\n2.525\nOmniQuant\n59.47\n77.31\n83.94\n77.35\n81.39\n68.11\n74.59\n2.418\nMatQuant\n58.11\n78.03\n83.27\n76.17\n81.18\n67.09\n73.97\n2.451\nint4\nS.P. MatQuant\n57.17\n77.02\n74.28\n74.41\n80.69\n67.56\n71.85\n2.543\nOmniQuant\n58.79\n78.37\n83.55\n76.71\n81.45\n67.09\n74.33\n2.451\nMatQuant\n57.25\n77.36\n84.86\n75.52\n81.5\n66.77\n73.88\n2.481\nint2\nS.P. MatQuant\n49.74\n74.66\n80.92\n66.57\n76.06\n63.54\n68.58\n2.857\nOmniQuant\n39.16\n63.43\n72.11\n52.24\n72.63\n61.88\n60.24\n3.292\nMatQuant\n48.72\n72.18\n79.2\n68.11\n76.17\n66.77\n68.52\n2.809\nTable 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT.\nNote that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model\nwere sliced post training.\nGemma-2 9B\nData type\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nAverage\nlog pplx.\nint8\nS.P. MatQuant\n55.97\n76.18\n80.09\n75.43\n80.69\n68.9\n72.88\n2.429\nQAT\n47.78\n70.66\n75.08\n69.92\n78.35\n65.11\n67.82\n2.29\nMatQuant\n46.25\n71.21\n75.6\n69.97\n78.4\n64.64\n67.68\n2.301\nint4\nS.P. MatQuant\n55.2\n76.01\n74.74\n74.19\n80.41\n68.9\n71.57\n2.429\nQAT\n46.16\n71.59\n73.73\n68.72\n78.62\n63.38\n67.03\n2.324\nMatQuant\n44.37\n70.45\n75.81\n68.43\n78.35\n64.88\n67.05\n2.332\nint2\nS.P. MatQuant\n41.21\n66.2\n65.02\n64.31\n76.06\n62.35\n62.53\n2.706\nQAT\n33.45\n55.43\n62.26\n54.8\n70.51\n59.67\n56.02\n2.923\nMatQuant\n39.85\n65.66\n65.93\n64.08\n75.68\n62.75\n62.32\n2.756\nTable 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant,\ncomparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and\nQAT.\nint2\nMistral 7B\nMethod\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nTask Avg.\nlog pplx.\nOmniQuant\nS.P. MatQuant\n39.93\n66.25\n76.97\n72.99\n78.07\n69.93\n67.36\n2.464\nBaseline\n36.69\n61.36\n70.06\n57.47\n70.67\n62.19\n59.74\n3.931\nMatQuant\n41.38\n67.42\n71.62\n71.98\n77.86\n65.67\n65.99\n2.569\nQAT\nS.P. MatQuant\n34.64\n56.19\n70.73\n66.77\n75.52\n65.43\n61.55\n2.435\nBaseline\n29.78\n48.23\n64.5\n55.11\n70.84\n61.25\n54.95\n2.694\nMatQuant\n34.3\n55.09\n71.83\n65.89\n75.52\n65.11\n61.29\n2.474\n25")]}
2025-02-12 22:39:01,612 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:40:56,155 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:41:05,873 - DEBUG - start_idx: 3124, start_marker: 1. Introduction
Large, end_idx: 42430, end_marker: ent reasoning strategies.
2025-02-12 22:41:06,281 - DEBUG - start_idx: -1, start_marker: Introduction

The pro, end_idx: -1, end_marker: earch in multilingual t
2025-02-12 22:44:04,671 - DEBUG - start_idx: 1780, start_marker: In recent years, th, end_idx: -1, end_marker: ernal debugging tools.
2025-02-12 22:44:05,701 - DEBUG - start_idx: 1560, start_marker: 1. Introduction
Due to, end_idx: 6299, end_marker: ent LLM inference.
2025-02-12 22:47:52,097 - DEBUG - start_idx: -1, start_marker: 1 INTRODUCTION A lon, end_idx: 66801, end_marker: . Imposters win!
2025-02-12 22:47:52,101 - INFO - Total execution time: 415.24 seconds (6.92 minutes)
2025-02-12 22:47:52,110 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=89, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=25, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=17, thumbnail=None, content='In recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=17, thumbnail=None, content='1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference.')]}
2025-02-12 22:55:37,517 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:55:49,796 - DEBUG - start_idx: -1, start_marker: 1. Introduction
In re, end_idx: -1, end_marker: earch in this area, we
2025-02-12 22:55:50,200 - DEBUG - start_idx: -1, start_marker: 1. INTRODUCTION
A lon, end_idx: -1, end_marker: ense rewards. We releas
2025-02-12 22:56:29,905 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 22:57:50,921 - DEBUG - start_idx: -1, start_marker: 1 INTRODUCTION A lon, end_idx: 66801, end_marker: . Imposters win!
2025-02-12 22:57:52,076 - DEBUG - start_idx: 3124, start_marker: 1. Introduction
Large, end_idx: 42430, end_marker: ent reasoning strategies.
2025-02-12 23:01:20,724 - DEBUG - start_idx: -1, start_marker: Introduction

The pro, end_idx: -1, end_marker: earch in multilingual t
2025-02-12 23:01:23,397 - DEBUG - start_idx: 1780, start_marker: In recent years, th, end_idx: -1, end_marker: ybrid code generation
2025-02-12 23:01:30,118 - ERROR - Error extracting main content: An error occurred (ThrottlingException) when calling the InvokeModel operation: Too many tokens, please wait before trying again.
2025-02-12 23:01:30,119 - INFO - Total execution time: 299.38 seconds (4.99 minutes)
2025-02-12 23:01:30,125 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=89, thumbnail=None, content='1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.\n3\nMethodology\nIn this section, we describe the pipeline introduced\nfor collecting the multilingual parallel text detoxifi-\ncation dataset, SynthDetoxM. A general illustration\nof our approach is shown in Figure 1.\n3.1\nData Collection\nTo create SynthDetoxM, we begin by selecting sev-\neral thousand non-parallel toxic texts from publicly\navailable toxicity identification datasets. We fo-\ncus on four languages for SynthDetoxM: German,\nFrench, Spanish and Russian. From these datasets\nGerman\nSpanish\nFrench\nRussian\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAya 32B\nAya 8B\nCommand-R\nGemma 27B\nLlama-3 70B\nLlama-3 8B\nMistral Nemo\nMistral Small\nQwen 2.5 32B\nFigure 2: Number of accepted samples in the final Syn-\nthDetoxM dataset with respect to the LLM by language.\nwe select only the texts that were marked as toxic\nby human annotators, excluding non-toxic exam-\nples. In cases of multiple annotations, we retained\nthe sample where the majority of annotators classi-\nfied the sentence as toxic.\nTo enhance the final data quality, we employ\nsample-level filtering using the STA and SIM met-\nrics2 and we also apply data augmentation tech-\nniques utilizing the Perspective API (Lees et al.,\n2022). Since the API returns both toxicity scores\nand toxic spans, we further improve data quality by\nsplitting the source texts into sentences and remov-\ning those sentences that do not intersect with the\ndetected toxic spans. This filtering process results\nin a larger dataset of toxic sentences, and we also\nsplit overly long inputs into separate examples.\nRussian\nFor the Russian dataset, we use data\nfrom the Jigsaw Toxic Comments Classification\nChallenge (Kivlichan et al., 2020), Russian Lan-\nguage Toxic Comments (Belchikov, 2019) and\nToxic Russian Comments (Semiletov, 2020). From\nthese sources, we select only those rows labeled as\ntoxic, resulting in more than 15,697 toxic texts. We\nthen calculate the STA and SIM metrics, applying\na threshold of 0.5 for filtering. After removing emo-\njis, eliminating texts with fewer than five words or\nmore than 30 words, and splitting the sentences\nusing toxic spans from Perspective API, our final\ndataset consists of 15,697 texts.\nGerman\nFor German, we use the toxicity iden-\ntification data from the GermEval 2021 shared\n2Evaluation metrics are described in Section §4.3\n\ntask (Risch et al., 2021) and RP-Mod and RP-\nCrowd (Assenmacher et al., 2021) to create a\ndataset of 4,946 toxic texts. We apply the same fil-\ntering and augmentation pipeline as for the Russian\ndataset, but with a lower STA score threshold of\n0.3. This resulted in a dataset of 4,946 texts, which\nexceeds the original size of the raw dataset. We at-\ntribute this increase to the higher median length of\nGerman sentences, which leads to a greater number\nof split texts.\nSpanish\nFor Spanish, we utilize data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the Clandestino\ndataset (Capozzi et al., 2021). This results in an\ninitial dataset of 10,260 toxic texts. We apply the\nsame filtering and augmentation pipeline as for the\nGerman dataset, using the same STA threshold of\n0.3. This process yields a final dataset of 5,826\ntexts.\nFrench\nFor French, we use the data from\nthe Jigsaw Toxic Comments Classification Chal-\nlenge (Kivlichan et al., 2020) and the MLMA Hate\nSpeech Corpus (Ousidhoum et al., 2019) to gener-\nate a dataset of 5,424 toxic texts. As the toxicity\nclassifier used in other languages does not support\nFrench, we instead use the Perspective API to get\ntoxicity scores. After applying a STA score thresh-\nold of 0.25, we obtain a dataset of 4,310 sentences.\n3.1.1\nParallel Data Generation Pipeline\nTo generate parallel detoxification data, we use\nvarious open-source LLMs in a few-shot genera-\ntion setup. Specifically, we employed the following\nmodels: Qwen 2.5 32B by Qwen (Yang et al., 2024;\nTeam, 2024); Command-R 32B by Cohere (Cohere,\n2024); Gemma 2 27B by Google (Rivière et al.,\n2024); Aya Expanse in 32B and 8B versions by Co-\nhere (Dang et al., 2024); Mistral Small 22B, Mis-\ntral Nemo 12B by Mistral AI (Mistral, 2024a,b);\nand Llama 3.1 70B and 8B models respectively, by\nMeta (Dubey et al., 2024). While not all these mod-\nels are explicitly designed for multilingual tasks,\nour experiments show that all of them support the\nlanguages considered in this work.\n3.1.2\nFew-Shot Example Mining\nTo select the best toxic and non-toxic pairs for few-\nshot generation, we calculate the STA and SIM\nmetrics for all sentences in Russian, German and\nSpanish from the multilingual toxicity detection\ndataset3. We then rank the top 10 sentencesbased\non the following score:\nScore(xi; yi) =\n1 −\n\x121 −STA(xi)\n1 −STA(yi) · (1 −SIM(xi; yi))\n\x13\nwhere STA(xi) and STA(yi) represent the tox-\nicity scores for the original and detoxified exam-\nples, respectively. SIM(xi; yi) is the cosine dis-\ntance between the embeddings of toxic and detoxi-\nfied sentences.\nThis ranking criterion is chosen to ensure high-\nquality detoxification without altering the original\nmeaning of the sentences. Since the sentences used\nfor few-shot prompting have been annotated by hu-\nman experts, we expect the detoxification quality to\nbe satisfactory. Additionally, the rewriting process\nof toxic words often leads to an expanded distance\nbetween toxic and non-toxic sentences, increasing\nthe distinction in non-toxicity. To maximize both\nthe distance and the distinction between the origi-\nnal and detoxified sentences, we select harder and\nmore meaningful examples for few-shot prompting,\nwhich helps improve the detoxification process.\nFor French, which is not represented in the Mul-\ntiParaDetox dataset, we used human annotators to\ndetoxify 10 randomly chosen sentences from the\nexisting non-parallel data.\nAfter generating detoxified examples, we per-\nform refusal filtering using a refusal classification\nmodel (see details in Appendix D). Additionally,\nwe use a simple threshold-based non-detoxifiability\nmetric, calculated by dividing the absolute reduc-\ntion in the STA score by the original STA score.\nWe compare the resulting detoxifiability scores\nto a fixed threshold of 0.5. If the score falls be-\nlow this threshold, the example is considered non-\ndetoxifiable.\nAfter generating five detoxification datasets in\neach language using the selected models, we rank\nthe sentences by their multiplied STA and SIM\nmetrics and select the top-scoring examples. This\nmetric helps mitigate issues such as refusal(where\nmodels refuse to generate text due to toxicity) and\ncopy-paste generation (where the model generates\nthe input toxic sentences without modification), as\ncopy-paste generation typically results in a low\nSTA score, while refusal leads to a low SIM score.\n3hf.co/multilingual_toxicity_dataset\n\nSTAT↑STAD↑SIM↑STAD×SIM↑\nGerman 0.389\n0.853 0.793\n0.675\nSpanish 0.514\n0.920 0.736\n0.681\nFrench\n0.583\n0.913 0.677\n0.624\nRussian 0.467\n0.924 0.731\n0.678\nTable 2: Average toxicity levels across different lan-\nguages for source toxic (T) and generated detoxified\n(D) texts, along with similarity scores. STAT represents\nthe toxicity level of the original text, while STAD corre-\nsponds to the detoxified text. In our work, for a text x\nthe score STA(x) = 1 −P(toxic|x).\n3.2\nFinal Composed Dataset\nAfter all preprocessing, cleaning and filtering steps\nwe compose SynthDetoxM - a manually collected\nand synthetically paraphrased parallel detoxifica-\ntion dataset on 16,000 toxic and non-toxic text pairs\nfor Spanish, German, Russian and French.\nWe show the statistics of detoxification candidate\nacceptance with respect to each LLM Language-\nwise in Table 5 and Figure 2. According to the\nstatistics, Qwen 2.5 generated the most preferrable\ndetoxifications among other models.\nHowever, upon manual examination we noticed\nthat Qwen tended to occasionally insert tokens of\nChinese text into the generated text though was\nprompted to answer only on the language of the\nsource text. Therefore, the strict reranking and\nfiltering criteria of generated detoxification candi-\ndates is necessary.\n3.3\nData Quality Evaluation Pipeline\nTo evaluate the quality of our generated detoxifi-\ncation data in Russian, German, and Spanish, we\nuse our dataset for training and compare the perfor-\nmance of models trained on SynthDetoxMwith those\ntrained on the human-annotated parallel detoxifi-\ncation dataset, MultiParaDetox Dementieva et al.\n(2024). Due to its absence in the MultiParaDetox\ndataset, French is excluded from this comparison.\nA more detailed linguistic analysis of the dataset\ncan be found in Appendix E.\n4\nExperimental Setup\n4.1\nData Quality Tests\nTo evaluate the efficacy of our SynthDetoxM for Ger-\nman, Spanish and Russian, we’ve trained a series\nof sequence-to-sequence models on different folds\nfrom the dataset. Since MultiParaDetox consists\nof only 400 pairs of toxic texts with their human-\nwritten non-toxic rephrasings, we split our created\nSynthDetoxM dataset into 10 chunks of 400 pairs\nfor German, Spanish and Russian. We trained 10\nmT0 models on different chunks of the dataset and\nevaluated their average performance on the Multi-\nParaDetox test set. Additionally, we test if using\nboth our SynthDetoxM and MultiParaDetox for train-\ning would lead to improved performance.\n4.2\nToxicity and Similarity of Synthetic Texts\nTo further assess the quality of the generated data,\nwe computed the STA and SIM scores using the\nPerspective API for Russian, German, Spanish,\nand French. These metrics were selected for their\nrelevance to detoxification tasks and their ability\nto quantitatively assess our synthetic dataset. We\nalso assessed the quality of the French subset of\nSynthDetoxM, as French is not represented in the\nMultiParaDetox dataset, and therefore cannot be\nevaluated through model training. The scores are\npresented in Figure 3 and Table 2.\nThe results indicate that French achieves compa-\nrable automatic metric scores to other languages,\nsuggesting that detoxification models trained on\nthis data would perform similarly.\nTherefore,\nwe hypothesize that the French subset of Syn-\nthDetoxM is a valuable addition to the dataset, en-\nabling the training of effective detoxification mod-\nels for French language processing tasks.\n4.3\nAutomatic Evaluation Setup\nTo assess the quality of the generated Spanish, Rus-\nsian, and German data, we follow the evaluation\npipeline of Dementieva et al. (2024), developed\nfor the multilingual text detoxification shared task.\nThese metrics are inspired by prior work on mono-\nlingual text detoxification for English and Rus-\nsian (Logacheva et al., 2022; Moskovskiy et al.,\n2022).\nStyle Transfer Accuracy (STA)\nFor computa-\ntion of this metric we use a multilingual toxicity\nclassifier based on a multilingual XLM-R4 (Con-\nneau et al., 2020) text classification model, trained\non a binary toxicity detection dataset.\nContent Similarity (SIM)\nFor computation of\nthis metric we use the cosine distance between\nLaBSE5 embeddings (Feng et al., 2022) of the\nsource texts and the generated texts.\n4hf.co/textdetox/xlmr-large-toxicity-classifier\n5hf.co/sentence-transformers/LaBSE\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRussian\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\nGerman\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpanish\nDetoxed STA\nToxic STA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\nFrench\nDetoxed STA\nToxic STA\nFigure 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts\nare in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.\nFluency (FL)\nFluency assesses how closely\ndetoxified texts resemble human-written references.\nPrevious works on English have employed CoLA-\nbased classifiers to estimate text fluency (Lo-\ngacheva et al., 2022; Moskovskiy et al., 2024).\nHowever, due to the absence of CoLA datasets for\nall considered languages Dementieva et al. (2024)\nused ChrF1 as a substitute. While recent work has\nintroduced MELA (Zhang et al., 2024), a multilin-\ngual extension of CoLA covering all the languages\nin this study, we maintain the evaluation pipeline\nof Dementieva et al. (2024) and continue using\nChrF1.\nNonetheless, ChrF1 remains a coarse approxima-\ntion of text fluency, which may negatively impact\nthe overall J scores (see Appendix C for details).\nJoint score (J)\nThe metrics STA, SIM and FL\nare subsequently combined into the final J score\nwhich is used for the final ranking of approaches.\nGiven an input toxic text xi and its output detoxi-\nfied version yi, for a test set of n samples:\nJ = 1\nn\nnP\ni=1\nSTA(yi) · SIM(xi, yi) · FL(xi, yi),\nwhere STA(yi), SIM(xi, yi), FL(xi, yi) ∈[0, 1] for\neach text detoxification output yi.\n4.4\nBaselines\nIn our work we adopt the baselines for multilingual\ndetoxification described in MultiParaDetox (De-\nmentieva et al., 2024) that are inspired by prior\nworks on text detoxification (Logacheva et al.,\n2022; Dementieva et al., 2023).\nDuplicate\nis the simplest baseline possible which\ncopies an input toxic sentence. This baseline has\n1.0 (or 100%) SIM score by definition.\nDelete\nremoves the toxic words according to a\npredefined list of inappropriate words. Demen-\ntieva et al. (2024) collects the lists of such toxic\nkeywords for all target languages based on openly\navailable sources. These lists are available online6.\nBacktranslation\nhas proven to be effective in\nprevious works (Dementieva et al., 2023; Prabhu-\nmoye et al., 2018; Konovalov et al., 2016a). Fol-\nlowing (Dementieva et al., 2023) we translate texts\ninto English with NLLB translation model (Costa-\njussà et al., 2022)7. The translated data is then\ndetoxified with the ParaDetox BART (Logacheva\net al., 2022) model8. After that, the detoxified texts\nare translated back into the source language using\nNLLB.\n4.5\nTraining Configuration\nIn our experimental evaluation, of the gener-\nated multilingual parallel detoxificiation dataset\nSynthDetoxM we follow the most efficient ap-\nproaches used during the TextDetox 2024 Shared\nTask (Sushko, 2024; Protasov, 2024; Rykov et al.,\n2024), where top three solutions in automatic\nevaluations utilized fine-tuning of a multilingual\nencoder-decoder language model mT0 (Muen-\nnighoff et al., 2023).\n6hf.co/multilingual_toxic_lexicon\n7hf.co/facebook/nllb-200-distilled-600M\n8hf.co/s-nlp/bart-base-detox\n\nDataset\nSTA SIM\nFL\nJ\nSTA·SIM\nGerman\nMPD\n0.722 0.848 0.602 0.383\n0.612\nSDM (Subset) 0.681 0.912 0.745 0.463\n0.597\nSDM\n0.728 0.899 0.734 0.484\n0.655\nSDM+MPD\n0.615 0.954 0.821 0.483\n0.586\nRussian\nMPD\n0.748 0.852 0.643 0.434\n0.637\nSDM (Subset) 0.858 0.850 0.656 0.478\n0.729\nSDM\n0.927 0.839 0.656 0.521\n0.778\nSDM+MPD\n0.815 0.886 0.726 0.540\n0.721\nSpanish\nMPD\n0.597 0.880 0.616 0.335\n0.525\nSDM (Subset) 0.795 0.856 0.611 0.416\n0.681\nSDM\n0.864 0.861 0.621 0.471\n0.744\nSDM+MPD\n0.681 0.907 0.653 0.413\n0.618\nTable 3: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox\n+ SynthDetoxM).\nWe use mT0-XL model9 and perform fine-\ntuning in full precision. We use AdaFactor op-\ntimizer (Shazeer and Stern, 2018) with batch size\nof 16, 50 warmup steps and set maximum sequence\nlength to 512.\nWe fine-tune mT0 for 2 epochs in all setups. Ac-\ncording to our experiments, the increased number\nof training epochs does not increase the final per-\nformance of the model. This might be explained to\nthe overall training data scarcity compared to the\nsize of the model: mT0-XL has 3 billion parameters\nand is being fine-tuned on 1,200 samples (400 for\neach of the three languages).\n5\nResults\nTable 3 presents the results of our experimental\nevaluation of SynthDetoxM. For clarity, the table is\ndivided by language. We compare the performance\nof mT0-XL trained on human-annotated MultiPa-\nraDetox data (MPD) with mT0-XL fine-tuned on\ntwo subsets of SynthDetoxM: a subset of 400 sam-\nples per language, matching the MPD size (denoted\nas SDM (Subset)), and the full SynthDetoxM dataset.\nAdditionally, following prior work (Xu et al., 2023),\n9hf.co/bigscience/mt0-xl\nGerman Spanish Russian\nHuman References\n0.733\n0.709\n0.732\nBaselines\nDuplicate\n0.287\n0.090\n0.048\nDelete\n0.362\n0.319\n0.255\nBacktranslation\n0.233\n0.275\n0.223\nmT0-XL supervised fine-tuning\nMultiParaDetox\n0.446\n0.344\n0.472\nSDM (Subset)\n0.460\n0.402\n0.475\nSDM\n0.482\n0.470\n0.546\n10-shot LLM prediction\nGemma 2\n0.353\n0.380\n0.404\nMistral Nemo\n0.286\n0.290\n0.258\nMistral Small\n0.371\n0.308\n0.273\nCommand R\n0.328\n0.344\n0.402\nQwen 2.5\n0.402\n0.443\n0.428\nLlama 3.1 8B\n0.394\n0.341\n0.357\nAya Expanse 8B\n0.305\n0.246\n0.225\nAya Expanse 32B\n0.399\n0.320\n0.323\nTable 4: Text detoxification results in terms of J scores\nfor German, Spanish, and Russian languages.\nThe\nbest overall results are boldfaced. The baselines and\nhuman references are from (Dementieva et al., 2024).\nwe investigate whether a two-stage fine-tuning ap-\nproach—first on SynthDetoxM, then on MPD (de-\nnoted as SDM + MPD)—yields further improve-\nments.\nThe highest SIM scores for German and Rus-\nsian are achieved by mT0-XL trained on MultiPa-\nraDetox (0.954 and 0.886, respectively), with the\ntwo-stage training approach (SDM + MPD) yielding\nslightly higher similarity in Russian but lower in\nGerman. However, for STA, models trained on Syn-\nthDetoxM consistently outperform MultiParaDetox\nacross all languages, both when trained on the full\ndataset and on a similarly sized subset.\nModels trained on SynthDetoxM exhibit slightly\nlower FL scores, likely due to the reference-\ndependent nature of this metric.\nDespite this,\nthe aggregated J metric—strongly influenced by\nFL—is significantly higher for models trained on\nboth the full SynthDetoxM and its subset compared\nto MultiParaDetox. Notably, incorporating Multi-\nParaDetox into the training process (SDM + MPD)\nresults in a drop in J scores.\nTo further illustrate the advantages of training on\n\nSDM (Subset)\nvs\nSDM\n24%\n46%\n30%\nSDM (Subset)\nvs\nMPD\n53%\n37%\n9%\nSDM \nvs\nSDM + MPD\n44%\n45%\n11%\nSDM \nvs\nMPD\n54%\n36%\n10%\nFigure 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results\nhighlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish.\nThe notation is similar to the notation from Table 3.\nSynthDetoxM, Table 3 also presents the product of\nSTA and SIM. Even without considering FL, mod-\nels trained on SynthDetoxM outperform those trained\non MultiParaDetox in all setups and languages.\nAdditionally, Table 4 reports the J scores\nof mT0-XL trained on MultiParaDetox, averaged\nacross 10 subsets of SynthDetoxM and the full Syn-\nthDetoxM, compared to baselines and large language\nmodel (LLM)-based detoxification in a 10-shot gen-\neration setting.\nFinally, we provide a Side-by-Side (SBS) com-\nparison of the fine-tuned mT0-XL models to eval-\nuate their detoxification performance.\nFollow-\ning (Moskovskiy et al., 2024), we employ GPT-4o\nas an evaluator to select the preferred detoxified out-\nputs. The results of this comparison across German,\nSpanish, and Russian languages are summarized in\nFigure 4 and detailed in Appendix F.\nOur SBS evaluation shows a clear preference\nfor detoxifications produced by SDM over MultiPa-\nraDetox (MPD), with SDM winning in 59% of cases\ncompared to 19% for MPD, and 22% resulting in\nties. The subset of SDM also outperforms MPD in\n58% of cases.\nWhen comparing SDM against its combination\nwith MPD (SDM + MPD), SDM is preferred in 47%\nof cases, with 21% favoring SDM + MPD, and 33%\ntied. Additionally, the full SDM dataset is slightly\npreferred over the batch processing version in 55%\nof cases, with 35% ties. See Appendix F for more\ndetails.\n6\nConclusions\nWe present several contributions to multilingual\ntext detoxification technology. Firstly, we success-\nfully extend the concept of few-shot prompting\nfor detoxification to a multilingual context, build-\ning upon previous monolingual approaches and\npropose a framework for generation of multilin-\ngual synthetic detoxification data. Secondly, we\nintroduce SynthDetoxM, a large-scale multilingual\nsynthetic parallel dataset designed to address the\nlong-standing issue of data scarcity in text detox-\nification research. Notably, our dataset, created\nusing our selection criteria, demonstrates competi-\ntive quality to existing human-annotated datasets,\nsurpassing them in both low resource and high re-\nsource settings.\nOur comprehensive evaluation of SynthDetoxM re-\nveals its effectiveness in training high-performing\nmodels for text detoxification. Specifically, our ex-\nperiments show that models trained on our dataset\noutperform those, which were trained on a simi-\nlar amount of human-annotated data. Furthermore,\ntraining a detoxification encoder-decoder model on\nfull SynthDetoxM yields a model, which surpasses\nthe performance of most large language models in\nfew-shot generation setups.\nFindings presented in our work, show usefulness\nof the generated data for the task of multilingual\ntext detoxification and pave the way for future re-\nsearch and developments of related technologies.\nAcknowledgments\nThe contribution of E.T. was supported by a grant\nfor research centers in the field of artificial intelli-\ngence, provided by the Analytical Center for the\nGovernment of the Russian Federation in accor-\ndance with the subsidy agreement (agreement iden-\ntifier 000000D730321P5Q0002) and the agreement\nwith the Ivannikov Institute for System Program-\nming of the Russian Academy of Sciences dated\nNovember 2, 2021 No. 70-2021-00142.\n\n7\nLimitations\nOne of the limitations of our work is that we are\nfocusing only on explicit type of toxicity. Addi-\ntionally, definition and type of toxicity changes\ndrastically between the language, e.g. things, that\nare toxic in one language may be perfectly normal\nin other language.\nAnother limitation of this work is our constraint\nwith computational resources, which led to our use\nof smaller and simpler models for synthetic data\ngeneration, which could fit into a single NVIDIA\nA100 80GB GPU. Usage of larger could potentially\nresult in higher quality and diversity of synthetic\ndata.\nMoreover, the comparison with proprietary mod-\nels would strengthen the evaluation as it is done in\nrecent works Dementieva et al. (2025).\nAdditionally, we were limited by the amount of\nannotated non-parallel toxic datasets in some of the\nlanguages, which limited the amount of possible\ngenerated synthetic data. In future, we plan to\nextend our work to other languages, such as Italian,\nPolish and others.\n8\nEthical Considerations\nWhile working with the task detoxification we are\nfully aware of the ethical responsibilities involved.\nAs researchers, we handle this sensitive area with\ncare and integrity. The main goal of text detox-\nification is to make online interactions safer and\nmore inclusive by reducing harmful or offensive\nlanguage.\nWhile these datasets are meant to train models to\ndetect and reduce toxic language, there’s a chance\nthey could be used in the wrong way—such as\ncreating models that spread harmful or offensive\ncontent. This could lead to hate speech and harass-\nment.\nIt’s also important to clarify that the goal of text\ndetoxification isn’t to suppress free speech or force\nautomatic changes to content. Instead, we aim\nto build models that offer non-toxic alternatives,\nhelping users choose better language on their own.\nBy giving suggestions rather than enforcing edits,\nwe respect people’s freedom while encouraging a\nmore positive online environment.\nReferences\nDennis Assenmacher, Marco Niemann, Kilian Müller,\nMoritz Seiler, Dennis M. Riehle, and Heike Traut-\nmann. 2021. Rp-mod & rp-crowd: Moderator- and\ncrowd-annotated german news comment datasets.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nKatherine Atwell, Sabit Hassan, and Malihe Alikhani.\n2022.\nAPPDIA: A discourse-aware transformer-\nbased style transfer model for offensive social me-\ndia conversations. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 6063–6074. International\nCommittee on Computational Linguistics.\nAnatoly Belchikov. 2019. Russian language toxic com-\nments. Accessed: 2024-10-14.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nArthur Capozzi, Gianmarco De Francisci Morales, Ye-\nlena Mejova, Corrado Monti, André Panisson, and\nDaniela Paolotti. 2021. Clandestino or rifugiato?\nanti-immigration facebook ad targeting in italy. In\nCHI ’21: CHI Conference on Human Factors in Com-\nputing Systems, Virtual Event / Yokohama, Japan,\nMay 8-13, 2021, pages 179:1–179:15. ACM.\nCohere. 2024. Command series 0824.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nMarta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, Anna Y. Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loïc Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind:\nScal-\ning human-centered machine translation.\nCoRR,\nabs/2207.04672.\n\nDavid Dale, Anton Voronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nDaryna Dementieva, Nikolay Babakov, Amit Ro-\nnen, Abinew Ali Ayele, Naquee Rizwan, Florian\nSchneider, Xintong Wang, Seid Muhie Yimam,\nDaniil Alekhseevich Moskovskiy, Elisei Stakovskii,\nEran Kaufman, Ashraf Elnagar, Animesh Mukherjee,\nand Alexander Panchenko. 2025. Multilingual and\nexplainable text detoxification with parallel corpora.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, COLING 2025, Abu\nDhabi, UAE, January 19-24, 2025, pages 7998–8025.\nAssociation for Computational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Nikolay\nBabakov, Abinew Ali Ayele, Naquee Rizwan, Flo-\nrian Schneider, Xintong Wang, Seid Muhie Yimam,\nDmitry Ustalov, Elisei Stakovskii, Alisa Smirnova,\nAshraf Elnagar, Animesh Mukherjee, and Alexander\nPanchenko. 2024. Overview of the multilingual text\ndetoxification task at PAN 2024. In Working Notes\nof the Conference and Labs of the Evaluation Forum\n(CLEF 2024), Grenoble, France, 9-12 September,\n2024, volume 3740 of CEUR Workshop Proceedings,\npages 2432–2461. CEUR-WS.org.\nDaryna Dementieva, Daniil Moskovskiy, David Dale,\nand Alexander Panchenko. 2023. Exploring meth-\nods for cross-lingual text style transfer: The case of\ntext detoxification. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023, pages 1083–\n1101. Association for Computational Linguistics.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 878–\n891. Association for Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. ACM\nComputing Surveys (CSUR), 51(4):1–30.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Exploration\nand evaluation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 663–670. AAAI Press.\nSkyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap.\n2023. Detoxifying text with marco: Controllable re-\n\nvision with experts and anti-experts. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n228–242. Association for Computational Linguistics.\nZachary Horvitz, Ajay Patel, Kanishk Singh, Chris\nCallison-Burch, Kathleen R. McKeown, and Zhou Yu.\n2024. Tinystyler: Efficient few-shot text style trans-\nfer with authorship embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13376–13390. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beu-\ntel, Alex Borzunov, Alex Carney, Alex Chow, Alex\nKirillov, Alex Nichol, Alex Paino, Alex Renzin,\nAlex Tachard Passos, Alexander Kirillov, Alexi Chris-\ntakis, Alexis Conneau, Ali Kamali, Allan Jabri, Al-\nlison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann,\nAndrew Codispoti, Andrew Galu, Andrew Kondrich,\nAndrew Tulloch, Andrey Mishchenko, Angela Baek,\nAngela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghor-\nbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky,\nBen Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler,\nBowen Cheng, Brad Lightcap, Brandon Walkin,\nBrendan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss,\nChen Ding, Cheng Lu, Chong Zhang, Chris Beau-\nmont, Chris Hallacy, Chris Koch, Christian Gibson,\nChristina Kim, Christine Choi, Christine McLeavey,\nChristopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin\nKoumouzelis, and Dane Sherburn. 2024. Gpt-4o sys-\ntem card. CoRR, abs/2410.21276.\nMd Tawkat Islam Khondaker, Muhammad Abdul-\nMageed,\nand Laks V. S. Lakshmanan. 2024.\nDetoxllm: A framework for detoxification with expla-\nnations. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024, pages 19112–19139. Association for Computa-\ntional Linguistics.\nIan Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy\nVasserman, Martin Görner, and Phil Culliton. 2020.\nJigsaw multilingual toxic comment classification.\nVasily Konovalov, Meni Adler, and Ido Dagan. 2016a.\nEffective paraphrase expansion in addressing lexical\nvariability. In Proceedings of the Artificial Intelli-\ngence and Natural Language (AINL FRUCT 2016),\npage 87–91, St.-Petersburg, Russia.\nVasily Konovalov, Oren Melamud, Ron Artstein, and\nIdo Dagan. 2016b. Collecting Better Training Data\nusing Biased Agent Policies in Negotiation Dia-\nlogues. In Proceedings of WOCHAT, the Second\nWorkshop on Chatbots and Conversational Agent\nTechnologies, Los Angeles. Zerotype.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 737–762. Association for Computational\nLinguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you bart! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual\nEvent, August 1-6, 2021, pages 484–494. Association\nfor Computational Linguistics.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA new generation of perspective api: Efficient multi-\nlingual character-level transformers. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3197–3207, New York, NY, USA. Association for\nComputing Machinery.\nShuai Liu, Shantanu Agarwal, and Jonathan May. 2024.\nAuthorship style transfer with policy optimization.\nCoRR, abs/2403.08043.\nVarvara Logacheva,\nDaryna Dementieva,\nSergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. Paradetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 6804–6818. Association for Com-\nputational Linguistics.\nMistral. 2024a. Ai in abundance | mistral ai | frontier ai\nin your hands.\nMistral. 2024b. Mistral nemo | mistral ai | frontier ai in\nyour hands.\nDaniil Moskovskiy, Daryna Dementieva, and Alexan-\nder Panchenko. 2022. Exploring cross-lingual text\ndetoxification with large multilingual language mod-\nels. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 346–354. Association for\nComputational Linguistics.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander\nPanchenko. 2024. Llms to replace crowdsourcing\nfor parallel data creation? the case of text detoxifi-\ncation. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, Miami, Florida,\nUSA, November 12-16, 2024, pages 14361–14373.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nSourabrata Mukherjee, Akanksha Bansal, Pritha Majum-\ndar, Atul Kr. Ojha, and Ondˇrej Dušek. 2023. Low-\nresource text style transfer for Bangla: Data & mod-\nels. In Proceedings of the First Workshop on Bangla\nLanguage Processing (BLP-2023), pages 34–47, Sin-\ngapore. Association for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4674–4683.\nAssociation for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nMohammad Mahdi Abdollah Pour, Parsa Farinneya,\nManasa Bharadwaj, Nikhil Verma, Ali Pesarang-\nhader, and Scott Sanner. 2023. COUNT: contrastive\nunlikelihood text style transfer for text detoxification.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 8658–8666. Association for Computa-\ntional Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W. Black. 2018.\nStyle transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 866–876.\nAssociation for Computational Linguistics.\nVitaly Protasov. 2024. PAN 2024 multilingual textdetox:\nExploring cross-lingual transfer using large language\nmodels. In Working Notes of the Conference and\nLabs of the Evaluation Forum (CLEF 2024), Greno-\nble, France, 9-12 September, 2024, volume 3740\nof CEUR Workshop Proceedings, pages 2852–2857.\nCEUR-WS.org.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 129–\n140. Association for Computational Linguistics.\nMachel Reid and Mikel Artetxe. 2023. On the role of\nparallel data in cross-lingual transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 5999–6006. Association for Computa-\ntional Linguistics.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 837–848. Association for\nComputational Linguistics.\nJulian Risch, Anke Stoll, Lena Wilms, and Michael Wie-\ngand. 2021. Overview of the GermEval 2021 shared\ntask on the identification of toxic, engaging, and fact-\nclaiming comments. In Proceedings of the GermEval\n2021 Shared Task on the Identification of Toxic, En-\ngaging, and Fact-Claiming Comments, pages 1–12.\nAssociation for Computational Linguistics.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, Johan Ferret, Peter Liu, Pouya\n\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, An-\nton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan\nGirgin, Nikola Momchev, Matt Hoffman, Shantanu\nThakoor, Jean-Bastien Grill, Behnam Neyshabur,\nOlivier Bachem, Alanna Walton, Aliaksei Severyn,\nAlicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin\nAbdagic, Amanda Carl, Amy Shen, Andy Brock,\nAndy Coenen, Anthony Laforge, Antonia Pater-\nson, Ben Bastian, Bilal Piot, Bo Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChris Welty, Christopher A. Choquette-Choo, Danila\nSinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa\nBandy, Emma Wang, Eric Noland, Erica Moreira,\nEvan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Plucinska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda\nMein, Jack Zhou, James Svensson, Jeff Stanway,\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira,\nJoana Iljazi, Jocelyn Becker, Joe Fernandez, Joost\nvan Amersfoort, Josh Gordon, Josh Lipschultz,\nJosh Newlan, Ju-yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin\nMcDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish\nGreene, Lars Lowe Sjösund, Lauren Usui, Laurent\nSifre, Lena Heuermann, Leticia Lago, and Lilly Mc-\nNealus. 2024. Gemma 2: Improving open language\nmodels at a practical size. CoRR, abs/2408.00118.\nElisei Rykov, Konstantin Zaytsev, Ivan Anisimov, and\nAlexandr Voronin. 2024.\nSmurfcat at PAN 2024\ntextdetox: Alignment of multilingual transformers\nfor text detoxification. In Working Notes of the Con-\nference and Labs of the Evaluation Forum (CLEF\n2024), Grenoble, France, 9-12 September, 2024, vol-\nume 3740 of CEUR Workshop Proceedings, pages\n2866–2871. CEUR-WS.org.\nKoustuv Saha, Eshwar Chandrasekharan, and Mun-\nmun De Choudhury. 2019. Prevalence and psycho-\nlogical effects of hateful speech in online college\ncommunities. Proceedings of the 10th ACM Confer-\nence on Web Science.\nAlexander Semiletov. 2020. Toxic russian comments.\nDataset.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo, Tianwei Zhang, and Guoyin Wang. 2023. Text\nclassification via large language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 8990–9005. Association for Computational\nLinguistics.\nNikita Sushko. 2024. PAN 2024 multilingual textdetox:\nExploring different regimes for synthetic data train-\ning for multilingual text detoxification. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2024), Grenoble, France, 9-12 Septem-\nber, 2024, volume 3740 of CEUR Workshop Proceed-\nings, pages 2892–2900. CEUR-WS.org.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 2195–\n2222. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2020. Formality style transfer with shared\nlatent space. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 2236–2249. International Committee on\nComputational Linguistics.\nBenfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yong-\ndong Zhang, and Zhendong Mao. 2023.\nS2ynre:\nTwo-stage self-training with synthetic data for low-\nresource relation extraction. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 8186–\n8207. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZerogen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11653–11669.\nAssociation for Computational Linguistics.\nPing Yu, Yang Zhao, Chunyuan Li, and Changyou Chen.\n2021. Rethinking sentiment style transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 1569–\n1582. Association for Computational Linguistics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 2658–2674. Association for Computational\nLinguistics.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:\nA large-scale real-world LLM conversation dataset.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nA\nPrompts\nA.1\nLLM Detoxification Prompt\nIn our work, for all of the language models we use\nthe Text Style Transfer prompt taken from work\nby Khondaker et al. (2024). Among others, this\nprompt shown to be giving better generation quality\nand helped to avoid additional content.\nYour task is text style transfer. Rewrite\nthe following text into non-toxic lan-\nguage. You must match the target style\nand preserve the original meaning as\nmuch as possible. Do not include the\ninput text in the response. Only gen-\nerate the target text and nothing else.\nMake sure the response is completely\nnon-toxic and neutral. Answer only in\n{language}. Here are few examples:",\nToxic text: {toxic_text}.\nNeutral\ntext:\nFigure 5: Detoxification prompt we use for synthetic\nparallel data generation. {toxic_text} stands for a\nplaceholder for a given toxic text being prompted into\nLLM. In few-shot setting we add few examples of detox-\nification before last two lines and write: Here are few\nexamples:.\nWe slightly adjust it specifically to text detoxi-\nfication and provide the full text of the prompt in\nFigure 5. In this work, we do not use any advanced\nprompting techniques except few-shot prompting.\nA.2\nmT0 Prompt\nWe add specific prompt to mT0 during both training\nand predictions. In our work do not translate the\nprompt into the language of generation. Instead,\nwe follow the promping approach from the original\npaper (Muennighoff et al., 2023). We show the\nprompt in Figure 6.\nModel\nGerman Spanish French Russian\nLlama 3.1 8B\n662\n619\n773\n1648\nLlama 3.1 70B\n898\n981\n1114\n1354\nMistral Nemo\n622\n583\n392\n1320\nMistral Small\n862\n985\n565\n2237\nQwen 2.5 32B\n477\n819\n513\n3128\nAya Exp. 32B\n458\n453\n142\n945\nAya Exp. 8B\n316\n330\n143\n765\nCommand-R 32B\n273\n492\n308\n2294\nGemma 2 27B\n394\n564\n360\n2019\nTable 5: Number of accepted samples in the final Syn-\nthDetoxM dataset, broken down by language and LLMs.\nWrite a non-toxic version of the\nfollowing\ntext\nin\n{language}:\n{toxic_text}\nFigure 6: Detoxification prompt we use for mT0.\nB\nAutomatic evaluation results\nThe automatic evaluation results are presented in\nTable 6.\nDataset\nSTA\nSIM\nCHRF\nJ\nGerman\nMPD\n0.722\n0.848\n0.602\n0.383\nSDM (Subset) 0.681 ±0.213 0.912 ±0.042 0.745 ±0.035 0.463 ±0.117\nSDM (Full)\n0.728\n0.899\n0.734\n0.484\nSDM+MPD\n0.615\n0.954\n0.821\n0.483\nRussian\nMPD\n0.748\n0.852\n0.643\n0.434\nSDM (Subset) 0.858 ±0.034 0.850 ±0.020 0.656 ±0.021 0.478 ±0.014\nSDM (Full)\n0.927\n0.839\n0.656\n0.521\nSDM+MPD\n0.815\n0.886\n0.726\n0.540\nSpanish\nMPD\n0.597\n0.880\n0.616\n0.335\nSDM (Subset) 0.795 ±0.083 0.856 ±0.031 0.611 ±0.022 0.416 ±0.023\nSDM (Full)\n0.864\n0.861\n0.621\n0.471\nSDM+MPD\n0.681\n0.907\n0.653\n0.413\nTable 6: Results of the automatic evaluation for mT0-XL\non German, Russian, and Spanish trained on original\ndata (MPD stands for MultiParaDetox), our collected\nand synthetically generated data (SDM stands for Syn-\nthDetoxM) and on their combination (MultiParaDetox +\nSynthDetoxM).\n\nSpanish↓\nGerman↓\nRussian↓\nToxic\n2089\n323\n4467\nDetoxified\n27\n102\n14\nTable 7: Total amount of toxic words for toxic and detox-\nified subsets of SynthDetoxM with respect to language.\nSpanish↓\nGerman↓\nRussian↓\nToxic\n0.522\n0.081\n1.117\nDetoxified\n0.007\n0.036\n0.004\nTable 8: Average number of toxic words per text in\nthe toxic and detoxified SynthDetoxM with respect to\nlanguage.\nC\nLimitations of ChrF1 as a Fluency\nMetric\nThis section addresses the issues with using ChrF1\nto evaluate fluency in text detoxification. While\nChrF1 is commonly used in neural machine trans-\nlation (Popovic, 2015), it has significant drawbacks\nfor text style transfer tasks like detoxification.\nReference-based metrics like ChrF1 are ill-\nsuited for assessing fluency in detoxification. The\ngoal is to change the text’s style while maintaining\nmeaning and fluency, without limiting the extent of\nedits. Effective detoxification often involves sub-\nstantial structural changes, making comparisons\nwith the original toxic text using ChrF1 misleading.\nThough ChrF1 may produce low scores, manual\nevaluations frequently show that the detoxified out-\nput is fluent.\nChrF1, based on character n-grams, is sensitive\nto word order and structural changes, which are\noften necessary for detoxification. It also fails to\nconsider semantic content, meaning fluency can\nbe high even when the ChrF1 score is low. Addi-\ntionally, it tends to reward minimal edits, which\nundermines the goal of thorough detoxification.\nRecent research has shifted towards more appro-\npriate fluency metrics. For example, CoLA-based\nclassifiers, as used in Dementieva et al. (2023) and\nLogacheva et al. (2022), focus on linguistic ac-\nceptability, offering a more accurate assessment of\nfluency without relying on comparisons to the toxic\ninput.\nWhile ChrF1 has its merits in other tasks, it is\nnot suitable for evaluating fluency in detoxification.\nFuture work should prioritize methods that assess\nfluency based on grammaticality and naturalness,\nPolitely refuse to answer this in {lang}\nand provide an explanation why you\nrefuse.\nThe refusal should be con-\nnected to the request topic. Do not add\nanything additional, only respond with\na refusal: {input_text}\nFigure 7: Refusal generation prompt for synthetic re-\nfusals dataset.\nSDM (Subset)\nvs\nSDM\n28%\n45%\n27%\nSDM (Subset)\nvs\nMPD\n56%\n36%\n8%\nSDM \nvs\nSDM + MPD\n44%\n44%\n12%\nSDM \nvs\nMPD\n54%\n35%\n10%\nSide-by-side evaluation results for German\nFigure 8: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for German. The notation\nis similar to the notation from Table 3.\nindependent of the original text.\nD\nRefusal classifier training\nTo get rid of LLM refusals in SynthDetoxM, we\ntrained a separate refusal classifier, based on\nxlmr-base10.\nTo train the model, a high quality synthetic\ndataset was created11. It was based on randomly\nselected inputs from the LMSYS-Chat-1M (Zheng\net al., 2024) dataset and then passed to the Gem-\nini Flash 1.5 (Reid et al., 2024) and Llama 3.3\n70B (Dubey et al., 2024) models, prompted to gen-\nerate both responses to the prompts from the dataset\nand refusals. Prompt for synthetic refusal genera-\ntion is presented in Figure 7.\nClassification model was trained using a batch\nsize of 64, learning rate of 1e-4 for one epoch.\nE\nAdditional linguistic analysis of the\ndataset\nTo provide additional perspective about detoxifi-\ncation quality of our dataset, we used dataset12,\n10hf.co/s-nlp/xlmr-base-refusal-classifier\n11hf.co/datasets/chameleon-lizard/multilingual_refusals\n12hf.co/datasets/textdetox/multilingual_toxic_lexicon\n\nSDM (Subset)\nvs\nSDM\n28%\n39%\n33%\nSDM (Subset)\nvs\nMPD\n55%\n34%\n11%\nSDM \nvs\nSDM + MPD\n46%\n42%\n12%\nSDM \nvs\nMPD\n53%\n33%\n13%\nSide-by-side evaluation results for Spanish\nFigure 9: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Spanish. The notation\nis similar to the notation from Table 3.\nSDM (Subset)\nvs\nSDM\n17%\n55%\n28%\nSDM (Subset)\nvs\nMPD\n48%\n43%\n9%\nSDM \nvs\nSDM + MPD\n44%\n48%\n7%\nSDM \nvs\nMPD\n55%\n39%\n7%\nSide-by-side evaluation results for Russian\nFigure 10: Side-by-side comparison of model outputs\nacross all languages, evaluated by GPT-4o. The re-\nsults highlight the relative performance of the models\nin generating detoxified text for Russian. The notation\nis similar to the notation from Table 3.\nwhich contains toxic lexicon in German, Spanish\nand Russian languages and calculated two metrics\nof our generated data: the amount and mean counts\nof toxic words in each sentence (presented in Ta-\nbles 7 and 8 accordingly) in the toxic and generated\ndetoxified subsets of SynthDetoxM.\nDue to the lack of French toxic lexicon dataset\nwe did not do any evaluations in French. Further-\nmore, selected dataset is not comprehensive: for\ninstance, German subset contains only 247 toxic\nwords, which leaves some toxic sentences not hav-\ning any toxic words detected and overall toxicity\nof German subset of our dataset is comparatively\nlow. However, these evaluations still show that\nour detoxified data contains substantially less toxic\nlexicon than the original toxic data, indicating that\noverall explicit toxicity of the generated data is\nmuch lower after the detoxification.\nF\nSide-by-side comparison of trained\nmT0 models\nTo provide additional validation outside of method-\nology created by (Dementieva et al., 2024), we\nhave evaluated the responses from the test dataset\nof each of the trained mT0 models using GPT-\n4o (Hurst et al., 2024) as a judge. To lessen the\npositional bias, each side-by-side comparison was\ndone twice, changing the positions of the answers\nand calculating mean score for all answers. The\nresults are presented in the Figures 8, 9, 10.\nAs demonstrated in the figures, models fine-\ntuned on SynthDetoxM achieve significantly higher\nwin rates compared to those trained on the "gold-\nstandard" MultiParaDetox dataset, as well as mod-\nels trained using a two-stage regime. We attribute\nthis superior performance to the challenges posed\nby human-sourced samples, which may be too com-\nplex or nuanced for relatively simple and compact\nstudent models to learn effectively. This limita-\ntion likely contributes to the weaker performance\nof models trained on MultiParaDetox in both auto-\nmatic evaluations and side-by-side comparisons.\n\nG\nPer-Language Few-Shot Examples\nTo ensure reproducibility we show all the few-shot prompts as is in Tables 9 10, 11, and 12.\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nтварь присягу один раз,а проебал всю страну\nThe bastard swore an oath once, but screwed over the whole country.\n0.820\n1.000\nNeutral\nПрисягу дал один раз, а разрушил всю страну\nHe took the oath once, but destroyed the whole country\n0.360\n0.001\nToxic\nнахрена этого придурка клоуна сюда впихивают?\nwhy the fuck is this idiot clown being pushed in here?\n0.743\n1.000\nNeutral\nЗачем этого человека сюда впихивают?\nWhy is this man being pushed in here?\n0.042\n0.005\nToxic\nмолись не молись,всё - равно получишь хуй. ну, разве что\nблядям на радость\nPray or don’t pray, you’ll get a dick anyway. Well, except for the joy of whores.\n0.757\n1.000\nNeutral\nМолись не молись, всё - равно ничего не получишь. ну, разве\nчто завистникам на радость.\nPray or don’t pray, you won’t get anything anyway. well, except for the envy of the joy.\n0.156\n0.998\nToxic\nеё муженька козла на кол надо посадить\nHer scoundrel of a husband needs to be impaled.\n0.553\n0.999\nNeutral\nЕё муженька нужно сурово наказать\nHer hubby needs to be severely punished\n0.029\n0.988\nToxic\nхорошо пизды не получил уёбок\ngood thing the bastard didn’t get his ass kicked.\n0.853\n1.000\nNeutral\nхорошо что он не был избит\nit’s a good thing he wasn’t beaten up\n0.037\n0.000\nToxic\nда что ты доебалась до этих клумб? людям жрать нечего! по\nмусоркам лазают!\nwhy are you so fixated on those flower beds? people have nothing to eat! they’re rummaging\nthrough trash!\n0.548\n1.000\nNeutral\nда что ты докопалась до этих клумб? Людям есть нечего! по\nмусоркам лазают!\nwhy did you get to these flower beds? People have nothing to eat! they’re climbing through\nthe trash!\n0.115\n0.000\nToxic\nименно благодаря этому президенту, россия еще жива. а такие\nдолбоёбы, только могут воду мутить. дебилоид.\nIt’s precisely thanks to this president that Russia is still alive. And idiots like you can only\nstir up trouble. Moron.\n0.863\n1.000\nNeutral\nИменно благодаря этому президенту, Россия ещё жива, а\nтакие люди, как ты, только и могут, что наводить смуту.\nIt is thanks to this president that Russia is still alive, and people like you can only cause\ntrouble.\n0.369\n0.003\nToxic\nты просто пидор,а не немец.я за свои слова отвечу,\nyou’re just a faggot, not a german. I stand by my words, but you won’t, because you’re a\ncouch scumbag.\n0.906\n1.000\nNeutral\nты просто не немец. я за свои слова отвечу, а ты н\nyou’re just not german. i’ll answer for my words, but you won’t, because you’re writing from\nthe couch\n0.399\n0.002\nToxic\nда, блядь, кто это говнище постоянно сюда сливает?!\nYeah, fuck, who keeps dumping this shit here all the time?!\n0.654\n1.000\nNeutral\nда кто эту ерунду постоянно сюда сливает?!\nwho is constantly pouring this nonsense here?!\n0.096\n0.001\nToxic\nда вы заебали этим постом уже, пидарасы!!!\nI’m sick of this post already, you bastards!!!\n0.903\n1.000\nNeutral\nда вы надоели этим постом уже, люди!!!\nYes, people, I am tired of this post already!!!\n0.012\n0.001\nTable 9: Selected few-shot examples for the Russian language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Invasoren eingestellt. Jetzt AFD wäh-\nlen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young invader. Vote AFD now\n0.397\n0.486\nNeutral\nIn dem Schwarz Grün regierden Hessen werden jetzt 8 zusätzliche Psy-\nchologen nur für die Jugendlichen Einwanderer eingestellt. Jetzt AFD\nwählen\nIn the black-green governed Hesse, 8 additional psychologists are now being hired only for\nthe young immigrants. Vote AFD now\n0.313\n0.004\nToxic\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nDünnpfiff im "Merkel-Propagandaschrott"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual drivel in the\n"Merkel propaganda junk"?\n0.471\n0.898\nNeutral\nMoin, moin, seid Ihr alle noch gesund und munter bei all dem geistigen\nQuatsch in der "Merkel-Propaganda"?\nHello, hello, are you all still healthy and cheerful despite all the intellectual nonsense in the\n"Merkel propaganda"?\n0.427\n0.999\nToxic\nIch würde Grüne mit einem Geschwür am Arsch der Welt bezeichnen\nI would describe Greens as a boil on the ass of the world\n0.548\n0.999\nNeutral\nIch würde Grüne mit einem Geschwür der Welt bezeichnen.\nI would describe Greens as a boil of the world.\n0.267\n0.337\nToxic\nDummes Gelaber, Kern.\nStupid talk, Kern.\n0.397\n0.653\nNeutral\nUnsinnige Aussage, Kern\nNonsensical statement, Kern\n0.177\n0.002\nToxic\nwie kann man nur derart strunzdumme Moderatoren wie bei #taff auf\n#ProSieben beschäftigen\nhow can you employ such extremely stupid presenters as at #taff on #ProSieben\n0.634\n0.016\nNeutral\nwie kann man nur solche Moderatoren wie bei #taff auf #ProSieben\nbeschäftigen\nhow can you employ such presenters as at #taff on #ProSieben\n0.056\n0.001\nToxic\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein\nSau dummer Kommentar\nThat’s where the education from Bild becomes noticeable again. Such a pig stupid comment\n0.557\n0.332\nNeutral\nDa macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr\nunüberlegter Beitrag\nThat’s where the education from Bild becomes noticeable again. A very inconsiderate\ncontribution\n0.110\n0.000\nToxic\nKurz lasst dich nicht von der Trulla aufs Glatteis führen!\nKurz don’t let the chick lead you astray!\n0.224\n0.384\nNeutral\nKurz lasst dich nicht von der Frau aufs Glatteis führen!\nKurz don’t let the woman lead you astray!\n0.185\n0.012\nToxic\nYounower zu werden ist der finale schritt um ein Hurensohn zu werden\nBecoming a Younower is the final step to becoming a son of a bitch\n0.638\n0.999\nNeutral\nYounower zu werden ist der finale schritt um unsympathisch zu werden\nBecoming a Younower is the final step to becoming unsympathetic\n0.231\n0.676\nToxic\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen Wendehälse die uns heute regieren. Frage ich mich. War\nder Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other\nturncoats who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.339\n0.733\nNeutral\nSehe ich Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht und\nall die anderen wechselnhaften Politiker die uns heute regieren. Frage\nich mich. War der Fall der Mauer ein Fehler.\nWhen I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other fickle\npoliticians who govern us today. I ask myself. Was the fall of the wall a mistake.\n0.058\n0.003\nToxic\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach freaks,\ndas is nich menschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just freaks, that’s not human\n0.572\n0.273\nNeutral\nHahaha vllt gönn ich mir das ma. Aber diese asiaten sind einfach\nunglaublich, fast übermenschlich\nHahaha maybe I’ll treat myself to that. But these Asians are just unbelievable, almost\nsuperhuman\n0.235\n0.995\nTable 10: Selected few-shot examples for German language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nY lo bien que les va siendo fachas, que no es gratis\nAnd how well they are doing being fascists, which is not free\n0.184\n0.002\nNeutral\nY lo bien que les va siendo conservadores, que no es gratis\nAnd how well they are doing being conservatives, which is not free\n0.012\n0.574\nToxic\nY a mí qué me importa lo que tú "notes", facha?\nAnd what do I care what you "notice", fascist?\n0.427\n0.391\nNeutral\nY a mí qué me importa lo que tú "notes"?\nAnd what do I care what you "notice"?\n0.080\n0.001\nToxic\nya ves además a los policías nazis que tendrá esa en el whatsapp. . . como\npara creerse algo.\nyou see also the nazi policemen that she will have on whatsapp... as if to believe something.\n0.261\n0.608\nNeutral\nya ves además a los policías que tendrá esa en el whatsapp. . . como para\ncreerse algo.\nyou see also the policemen that she will have on whatsapp... as if to believe something.\n0.035\n0.003\nToxic\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado Fascista\n#YoSoyAnonymousUE enough of absurd victimist repression by the Fascist State\n0.255\n0.352\nNeutral\n#YoSoyAnonymousUE basta de absurda represión victimista por parte\ndel Estado\n#YoSoyAnonymousUE enough of absurd victimist repression by the State\n0.106\n0.034\nToxic\nSupongo que Perro le dará asilo en la Moncloa\nI suppose Dog will give him asylum in La Moncloa\n0.314\n0.283\nNeutral\nSupongo que el presidente le dara asilo en la Moncloa\nI suppose the president will give him asylum in La Moncloa\n0.039\n0.004\nToxic\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndistas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.285\n0.969\nNeutral\nLos halagos y buenos modales siempre hacen bien, menos a los izquier-\ndas de la cultura del odio.\nFlattery and good manners are always good, except for the leftists of the culture of hatred.\n0.280\n0.236\nToxic\nYo me quiero escapar también, dan paguita al llega\nI want to escape too, they give a small handout to the newcomer\n0.011\n0.423\nNeutral\n¿Ofrecen algún tipo de apoyo al llegar?\nDo they offer any kind of support upon arrival?\n0.002\n0.000\nToxic\nMire la saga de 50 sombras de Grey...casi todo eran.lectoras...les va\nel.morbillo no lo.nieguen\nLook at the 50 Shades of Grey saga...almost all were.readers...they like.morbid curiosity\ndon’t.deny it\n0.039\n0.811\nNeutral\nMiren la saga de 50 sombras de Grey.. casi todo eran lectoras... le\nencanta\nLook at the 50 Shades of Grey saga... almost all were readers... they love it\n0.011\n0.000\nToxic\nCortocircuito buenrrollista, pobres inmigrantes vs chica atacada sexual-\nmente.\nGood-natured short circuit, poor immigrants vs sexually attacked girl.\n0.305\n0.719\nNeutral\nEs un conflicto entre el enfoque positivo y la compasión por los inmi-\ngrantes, frente a la preocupación por una chica atacada sexualmente.\nIt is a conflict between the positive approach and compassion for immigrants, versus concern\nfor a sexually attacked girl.\n0.082\n0.002\nToxic\nmenudos fachas xD HIJOS DE PUTA\nwhat fascists xD SONS OF BITCHES\n0.921\n0.802\nNeutral\nmenudos fachas xD\nwhat fascists xD\n0.327\n0.982\nTable 11: Selected few-shot examples for the Spanish language with toxicity scores.\n\nType\nText\nPerspective API Score\nXLM-R Score\nToxic\nPour le crétin qui remet à chaque fois une soi-disant citation de Nietzsche,\nl’expression "tout s’enjuive" est attribuée par Nietzsche à un libre-\npenseur démocrate, autrement dit Nietzsche identifie l’antisémitisme\naux valeurs chrétiennes de la "libre pensée".\nFor the idiot who always puts a so-called quote from Nietzsche, the expression "everything\nis becoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of "free thought".\n0.255\n-\nNeutral\nPour la personne qui remet à chacun une s citation de Nietzsche\nl’expression tout s’enjuive est attribuée par Nietzsche à un libre-penseur\ndémocrate, autrement dit Nietzsche identifie l’antisémitisme aux valeurs\nchrétiennes de la libre pensée.\nFor the person who gives everyone a quote from Nietzsche, the expression "everything is\nbecoming Jewish" is attributed by Nietzsche to a democratic freethinker, in other words\nNietzsche identifies anti-Semitism with the Christian values of free thought.\n0.000\n-\nToxic\nEt toi aussi tu commences sérieusement à me gonfler.\nAnd you too are seriously starting to annoy me.\n0.238\n-\nNeutral\nEt toi aussi tu commences sérieusement à m’énerver.\nAnd you too are seriously starting to irritate me.\n0.285\n-\nToxic\nTout est trop nul, tout est faux ou ou ou!\nEverything is too rubbish, everything is false or or or!\n0.167\n-\nNeutral\nTout est médiocre et faux !\nEverything is mediocre and false!\n0.195\n-\nToxic\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec un pénible de votre espèce.\nContinue on your own if that amuses you. I won’t waste any more time with a pain in the ass\nlike you.\n0.498\n-\nNeutral\nContinuez tout seul si ça vous amuse. Je ne perdrai pas plus de temps\navec une telle personne.\nContinue on your own if that amuses you. I won’t waste any more time with such a person.\n0.309\n-\nToxic\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.383\n-\nNeutral\nSalut, j ai restauré cet article et réverté tes su\nHi, I restored this article and reverted your su\n0.152\n-\nToxic\nEncore un vieux qui fait chier, je ne fais plus attention à force.\nAnother old man who’s a pain in the ass, I don’t pay attention anymore.\n0.509\n-\nNeutral\nEncore un vieux ennuyeux, je ne fais plus attention à force.\nAnother boring old man, I don’t pay attention anymore.\n0.378\n-\nToxic\nOue et on s en tape de ta façon a toi et on s en tape que t as eu d emmerdes\navec personne c est pas mon problème donc comme je l ai dis tout a l\nheure je fais a ma façon.\nYeah and we don’t care about your way and we don’t care that you had troubles with anyone\nit’s not my problem so as I said earlier I do it my way.\n0.373\n-\nNeutral\nOue et ça n’a rien à voir avec ta vie, le fait que tu aies eu des problèmes,\nça ne me regarde pas, donc comme je l ai dis tout a l heure je fais a ma\nfaçon.\nYeah and it has nothing to do with your life, the fact that you had problems, it’s none of my\nbusiness, so as I said earlier I do it my way.\n0.049\n-\nToxic\nTa réponse est confondante... Et tu continues tes manigances ! Je te\nprie de t arrêter immédiatement!\nYour answer is confusing... And you continue your schemes! I beg you to stop immediately!\n0.304\n-\nNeutral\nTa réponse est confondante... Et tu continues tes tricks! Je te prie de t\narrêter immédiatement !\nYour answer is confusing... And you continue your tricks! I beg you to stop immediately!\n0.255\n-\nToxic\nFaut croire que les amateurs de musique sont des malades mentaux\nYou have to believe that music lovers are mentally ill\n0.386\n-\nNeutral\nFaut croire que les amateurs de musique ont des préférences étranges\nYou have to believe that music lovers have strange preferences\n0.046\n-\nTable 12: Selected few-shot examples for the French language with toxicity scores.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=25, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.\n2\nRELATED WORK\nIn this section, we review related work on emergent communica-\ntion, prior works that use language models as agents in embodied\nsettings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent\ncommunication between agents, especially in the context of refer-\nence games and repeated reference games, where a speaker knows\nthe ground-truth answer to a question (e.g., a specific image out\nof a set of images that needs to be referred to). Then, the speaker\nneeds to communicate to the listener, who later needs to choose\nthe item being referenced either over one or repeated interactions.\nPrior work has shown that humans tend to quickly adapt to such\ntasks [26], naturally using theory of mind reasoning to determine\nthe intents of speakers [9]. Further, Hawkins et al. [12] showed that\nlanguage models can also learn to adapt to human conventions via\ncontinual learning. Without using human natural language data,\nLazaridou et al. [23] and Havrylov and Titov [11] use symbolic\ncheap-talk signals to solve referential games. Our framework of\nsocial deduction games, however, is more challenging as each agent\ndoes not know the ground truth answer, so teams must communi-\ncate to collectively learn the answer. Therefore, our domain does\nnot have as clear of a distinction between “speakers” who have\nknowledge and “listeners” who need to gain answers as agents in\nsocial deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs’\naccess to internet scale data for task planning and decision making.\nIn robotics, prior works explore how language models can be used\nto plan out a sequence of high-level primitives given an instruction\nin natural language [1, 17, 24]. In a virtual gaming setting, Park\net al. [29] uses ChatGPT to simulate members of a small virtual\ntown. Although there is no specific task or mechanism for “training”\nthese agents, they demonstrate the use of a long-term memory\nstream to store memories beyond the context length of the language\nmodels, enabling the formation of social networks. This technique\nof having external memory has later been used to learn “skills” in\na single-player environment [38] and for coordination in multi-\nagent environments [10]. These works demonstrate that language\nmodels are capable of controlling agents in a wide range of settings,\nwhich is key to our motivation to directly use language models as\na strong starting point for agents operating in more challenging\nenvironments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works\nalso combine language models with reinforcement learning. Ci-\ncero [8] is an AI for the game of Diplomacy that uses a dialogue-\nconditional action model from human actions and trains a dialogue-\nfree model using RL to choose actions. Cicero uses an “intent” em-\nbedding to connect the dialogue generation and strategic reasoning\ncomponents. This allows Cicero to communicate with other agents\nin a way that feels natural to other players, but it prevents the RL\nmodel from directly controlling the generated messages, potentially\nlimiting improvements in message quality. Another drawback is\nthat this technique requires a large number of human demonstra-\ntions, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards\nand as a base model for policies. Hu and Sadigh [14] and Kwon\net al. [21] use language models as reward signals to train a separate\nnetwork to follow a specific coordination strategy. We similarly use\nthe LLM to provide denser rewards during the discussion phase,\nbut we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also\nbeen key to improving the chat capabilities of LLMs. Ouyang et al.\n[28] demonstrates the effectiveness of reinforcement learning from\nhuman feedback (RLHF), where a reward model is trained using\nhuman feedback and an LLM is fine-tuned using a modification\nof the PPO algorithm to improve its performance. Yuan et al. [39]\nextends this by allowing the LLM to be its own reward model and\ngenerate its own data for self-improvement, similar to how we use\nthe LLM’s own change in beliefs as a reward signal. However, a\ncrucial difference is that our reward model remains grounded in\nan environment by design due to the imposter prediction training\nsignal. This means that we do not need to rely on the ability of\npretrained LLMs to critique their own generations, enabling us to\nuse smaller language models and correct logical errors over time.\n3\nPRELIMINARIES\nWe model social deduction games, such as Among Us, as a variant\nof the partially observable Markov game (POMG) [25] that includes\na question whose answer must be deduced through interacting\nwith players and the rest of the environment. Our modified POMG\ncan be described by a tuple (𝑛, S, A, P,𝑟, O,𝛾, Q,𝑞), where 𝑛is the\nnumber of players, S is the joint (hidden) state space and A is the\njoint action space. The transition function P : S × A × S →[0, 1],\nis the probability of reaching a state given the current state and\njoint action. The reward function 𝑟: S →R𝑛, gives a real value\nreward for each state transition to each player, and 𝛾is the reward\ndiscount. The observation function, O : S →𝑂𝑛, generates the\nplayer-specific observations from the state.\nOur POMG has additional terms for the task of social deduction,\nwhich are the set of all possible answers to the deduction problem\nQ ⊆A and the correct answer 𝑞∈Q. In social deduction games,\nagents will be given opportunities to answer the question as a literal\naction (i.e. voting in Among Us or choosing the correct object in\nreference games), and at those steps the correct action to take is 𝑞.\nThe trajectory up to time 𝑡is defined as a sequence of joint\nobservations and actions: 𝜏𝑡= (𝑜0,𝑎0, . . . ,𝑎𝑡−1,𝑜𝑡). An individ-\nual player only experiences their own action-observation history\n(AOH), which is defined for player 𝑖as 𝜏𝑖\n𝑡= (𝑜𝑖\n0,𝑎𝑖\n0, . . . ,𝑎𝑖\n𝑡−1,𝑜𝑖\n𝑡),\nand they follow a stochastic policy 𝜋𝑖(𝑎𝑖|𝜏𝑖). In the game of Among\nUs, the AOH consists of past observations supplied by the envi-\nronment, past embodied actions, and all prior discussions with the\nother players.\nLanguage Models. Language models are trained to model the\nprobability of a sequence of discrete tokens, where each token\nrepresents a string of natural language. For a sequence of tokens,\n\n𝑊= {𝑤0,𝑤1, . . . ,𝑤𝑘}, the probability of the sequence being gener-\nated is 𝑝(𝑊) = Î𝑘\n𝑗=0 𝑝(𝑤𝑗|𝑤<𝑗), so causal language models predict\nthe distribution of the next token conditioned on all prior tokens.\nOur Among Us environment is designed such that each observa-\ntion at time step𝑡is a sequence of tokens𝑜𝑡= 𝑊𝑡= {𝑤0,𝑤1, . . . ,𝑤𝑘}\nand each action at time step 𝑡is a single token 𝑎𝑡= 𝑤𝑡, allowing\nus to use language models as the policy for each agent. The AOH\nis a sequence of tokens, so language models can sample the next\naction by predicting the next token in the sequence, constrained to\nthe set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent\nlanguage model based on a linear attention mechanism, as the pre-\ntrained foundation model. We choose RWKV over more common\ntransformer-based models [35], because the recurrent formulation\nallows us to generate RL trajectories with a constant time and space\ncomplexity per token, and RWKV enables unbounded-context train-\ning using truncated backpropagation through time. This is espe-\ncially important since Among Us trajectories often reach tens of\nthousands of tokens in length per player, which would require\nsignificantly more compute for classic attention-based models. Em-\npirically, RWKV has also performed on-par with transformer-based\nmodels, especially in decision-making tasks [7] and long-context\nunderstanding [15], making it the ideal choice for this study.\n4\nTHE GAME OF AMONG US\nIn this section, we describe the key design decisions of our imple-\nmentation of the hidden-role game of Among Us. Our goal is to\ncreate an environment where agents can ground their discussion\nbased on evidence in the environment. A more complete description\nof the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either\nassigned as an imposter or a crewmate. The crewmates are not\ninformed of the identities of the other players, but all imposters are\ninformed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the\nother 𝑛−1 players as crewmates. The crewmates are assigned a\nset of 𝑁tasks, scattered across the environment. As an example,\n𝑁= 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultane-\nously move in an embodied environment, receiving observations\nfrom the environment and taking actions, as illustrated in Fig. 2.\nPlayers freely move around a 𝑊× 𝐻grid of rooms during the\ngameplay phase, receiving new observations 𝑜𝑡at each time step.\nAll agents can move between adjacent rooms by choosing 𝑎go to x,\nwhere x is a cardinal direction, or they can simply wait in the room\nby choosing 𝑎wait. Crewmates can complete tasks in their current\nroom by choosing 𝑎task, but they are unable to observe the environ-\nment for 𝑁task_time time steps, i.e., they will not be able to observe\nif a crewmate is being killed by an imposter while performing a\ntask. Note that tasks are indistinguishable from one another, so we\ndo not have different actions for different tasks. Imposters can kill\ncrewmates by choosing 𝑎kill,𝑗where 𝑗is a crewmate in the same\nroom as them, but they have to wait 𝑁cooldown time steps between\nkilling crewmates. Finally, crewmates can report dead bodies in\ntheir room by choosing 𝑎report,𝑗, where 𝑗is the corpse of player 𝑗,\nwhich initiates the discussion phase.\nObserve\nObserve\nToken \nBuffer\nToken \nBuffer\nCrewmate: π2\nCrewmate: πj\nst+1\nImposter: π1\nCrewmate: πj\nObserve\nObserve\n: You are in room (1, 0). You see Green, Orange in the room  \no1\nt+1\na1\nt ∈{awest, await, akill,3, akill,4}\naj\nt ∈{aeast, await, atask, areport,2}\nToken \nBuffer\nToken \nBuffer\nτj\nt\nτ1\nt\nFigure 2: Diagram of the embodied gameplay loop. The envi-\nronment starts by sending observations to all agents simul-\ntaneously and collects tokenized actions from a set of valid\nactions at each timestep.\nThe set of all valid actions are 𝑎go to x, 𝑎task, 𝑎kill,𝑗, 𝑎report,𝑗, and\n𝑎wait, where x is a cardinal direction and 𝑗is the name of a crewmate.\nThe environment provides each player with the subset of actions\nthat are valid at each timestep.\nDiscussion Phase. During the discussion phase, we cycle over each\nplayer twice in a random order and allow them to say a sentence,\n𝑎talk, in natural language. After this discussion, a voting phase\nbegins, where each player votes for one player, 𝑘, they want to eject\nby choosing action 𝑎vote,𝑘. The player who gets the plurality of\nvotes is ejected. If the imposter is not ejected, the game continues to\nthe next gameplay phase, where crewmates can continue finishing\ntasks.\nBefore the discussion starts and between each discussion mes-\nsage, the environment also surveys each crewmate by asking who\nthey would vote for, i.e., by querying them to pick an 𝑎vote,𝑘as if\nthey had to vote immediately. This action has no impact on the\nPOMG, but it will be relevant for our training algorithm.\nNote that the set of all voting actions is equal to the set of all\npossible “answers” in the social deduction game (Q), and voting\nout the correct imposter corresponds to 𝑞, the correct answer to\nthe social deduction question.\nReward Structure. Among Us is fundamentally a team zero-sum\ngame, so reward is based on whether crewmates or imposters win.\nIf all tasks are completed or the imposter is ejected, the crewmates\nwin with a reward of +1. However, if the number of imposters is ever\ngreater than or equal to the number of crewmates, the imposters\nwin, resulting in a crewmate reward of -1.\n5\nTRAINING LLM CREWMATES IN AMONG US\nBy defining an environment that only interfaces with players through\nnatural language, we can directly use a language model as the pol-\nicy 𝜋𝑖(𝑎𝑖|𝜏𝑖) of an agent 𝑖. The action-observation histories of our\nagents 𝜏𝑖are just strings of natural language, and new observa-\ntions and actions can simply be appended to the end of the strings.\nFurthermore, when taking actions 𝑎𝑖, the outputs of the language\nmodel can be constrained to be one of the legal actions provided\nby the environment at each timestep. Following this procedure,\n\nwe construct an agent using a pretrained RWKV language model,\nwhich we define as policy 𝜋RWKV.\nAlthough the environment is designed to interface nicely with\nlanguage models, we find that 𝜋RWKV struggles to reason as crew-\nmates in a zero-shot fashion in Among Us, with models frequently\nvoting to eject the wrong players. In this section, we describe our\nprocedure for improving the performance of crewmates by enabling\nthem to self-critique and use these scores to improve dialogue gen-\neration.\nThe first two subsections describe how to improve the perfor-\nmance of an individual learning crewmate, first describing a rein-\nforcement learning procedure and then describing how to enhance\ncommunication by learning to listen and speak. The third subsec-\ntion describes how to train the team of crewmates to be robust\nto adaptive imposters and different policies within the crewmate\npopulation.\n5.1\nReinforcement Learning in Among Us\nTo train a language model to take more effective actions without\nexpert demonstrations, we can turn to reinforcement learning. Since\nAmong Us already provides rewards for winning, we can directly\noptimize this to produce a model 𝜋RL that minimizes the following\nloss:\n𝐿RL(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n"\n𝛾𝑡𝑟𝑖\n𝑡+ 𝜆NL log(\n𝜋(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡)\n𝜋RWKV(𝑎𝑖\n𝑡|𝜏𝑖\n𝑡) )\n#\n,\n(1)\nwhere Π represents the joint policy that has 𝜋controlling agent 𝑖,\nand 𝜆NL is a hyperparameter controlling the strength of a soft KL\nconstraint regularizing trained models to the base LLM to prevent\ndiscussions from moving out of natural language [28]. Note that\nthe only reward signal is the sparse reward received at the end\nof the game along with additional rewards for completing tasks.\nIn particular, there is very little signal for the effectiveness of its\nmessages during discussions, which makes utilizing communication\nvery difficult with just RL in practice. This sparse signal also makes\nidentifying the imposter difficult in the multi-agent setting, because\nvoting correctly may still result in a loss and voting incorrectly\ncould result in a win if a plurality of agents vote for the imposter.\n5.2\nEnhancing Communication of Crewmates\nTo improve beyond the RL baseline, we can take advantage of the\nsocial deduction component of the game. In particular, each agent’s\nbelief in choosing the correct answer 𝑞∈Q will provide a stronger\nsignal for learning the core components of the game and the means\nof communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work.\nSpecifically, we highlight new loss terms to enhance both listen-\ning and speaking abilities, enabling crewmates to better utilize\ndiscussions.\nListening: Imposter Prediction. Suppose an agent is learning\nin a environment where it is partnered with expert crewmates\nwho already know how to discuss the game. How can this agent\nlearn to understand the meanings of environment observations and\nmessages from other crewmates?\nTo effectively discuss the game of Among Us, crewmates need\nto understand who the imposter is given their past observations\nand the past messages. This prediction task can act as an auxiliary\ntask that can guide the discussion phase to be more grounded and\nmeaningful.\nWe directly train crewmates to improve their reasoning over\nimposters using the environment’s ground truth answer for the\nidentity of the imposter. Specifically, we use the timesteps when\nthe environment directly surveys the players for their beliefs over\nthe imposters, which occurs between discussion messages, as the\ntraining signal. Note that this training signal does not specifically\nrequire human demonstration data; agents can learn to understand\nobservations and messages from other players using any rollout\nbuffer.\nFor every living crewmate, if they are asked to provide their\nbeliefs regarding the identity of the imposter at timestep 𝑡, the\nlistening loss for that timestep is\n𝐿L(𝜋,𝜏𝑖\n𝑡) = −log 𝜋(𝑞|𝜏𝑖\n𝑡),\n(2)\nwhere 𝑞= 𝑎vote,𝑗is the action representing choosing the correct\nimposter 𝑗, and 𝜏𝑖\n𝑡is the AOH until timestep 𝑡, which may include\nprior discussions.\nAt the very start of the discussion phase, agents need to reflect\non the probabilities of other agents being the imposter based on\ntheir observations during the gameplay phase. For instance, if a\ncrewmate directly witnesses a murder, they should be very certain\nthat the murderer is the imposter; our listening loss uses this signal\nto increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages\nand observations as a supervised learning problem, agents learn to\nunderstand the meaning of messages, enabling them to vote out\nthe correct imposter. Using this loss term, we can define two new\npolicies. We can directly incorporate the listening loss into the RL\npolicy, giving us the policy 𝜋RL+L that optimizes\n𝐿RL+L(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(3)\nwhere 𝜆L is a hyperparameter controlling the strength of the lis-\ntening loss and is only nonzero on timesteps when the crewmates\nare asked to predict the identity of the imposter. This enables the\nmodel to optimize actions while improving its ability to identify\nimposters.\nWe can also define a purely listening policy, 𝜋L that incorporates\nthe listening loss without an RL component, therefore optimizing\n𝐿L only(𝜋) =\nE\n𝜏𝑖∼Πrand\n∑︁\n𝑡\n𝜆L𝐿L(𝜋,𝜏𝑖\n𝑡),\n(4)\nwhere Πrand is a joint policy that uses 𝜋RWKV for discussions and\nchooses gameplay actions uniformly at random.\nSpeaking: Reinforced Discussion Learning. So far, we have\ndeveloped a policy that can learn to take effective actions in the\nenvironment with RL, and can update beliefs based on discussion\nmessages. Now suppose that an agent is partnered with expert\ncrewmates who already know how to parse messages from other\nplayers. How can this agent learn to construct helpful messages\nwhen it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows\nagents to learn how to interpret messages from other agents in\nthe previous subsection, we cannot directly apply the same idea to\n\nlearning how to speak as there is no ground truth notion of effec-\ntive messages. We instead improve the agents’ discussion abilities\nusing reinforcement learning. Specifically, we grant rewards to the\nspeaking agent based on the change in living crewmates’ beliefs on\nthe true imposter after each message. Formally, let 𝐵𝑡be the sum\nof all living crewmates’ beliefs,\n𝐵𝑡=\n∑︁\n𝑘∈𝐶𝑡\n𝜋𝑘(𝑞|𝜏𝑘\n𝑡),\n(5)\nwhere the 𝑞represents voting out the correct imposter, and 𝐶𝑡\nis the set of all living crewmates at time 𝑡. If 𝑡′ is the previous\nbelief-querying timestep, then the reward for crewmate 𝑖, who just\nfinished speaking, is 𝑟𝑠\n𝑡:\n𝑟𝑠\n𝑡= 𝐵𝑡−𝐵𝑡′.\n(6)\nIntuitively, this reward models the causal effect of each message\non the task of predicting the correct imposter. The most effective\nmessage that a crewmate could send would convince other crew-\nmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent 𝜋RL+S+L that\nminimizes the following loss:\n𝐿RL+L+S(𝜋) = 𝐿RL+L(𝜋) −\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(7)\n5.3\nTraining for Dynamic Settings\nAs a team zero-sum game, we want our trained crewmates to work\nwell against a wide range of imposters. To do so, we employ an\niterated self-play algorithm, where crewmates and imposters train\nagainst earlier iterations of their adversary’s policy. We train im-\nposters to learn to mislead crewmates into voting out other agents,\nso we keep the RL loss and invert the speaking loss, minimizing\nthe following:\n𝐿imp(𝜋) = 𝐿RL(𝜋) +\nE\n𝜏𝑖∼Π\n∑︁\n𝑡\n[𝜆S𝛾𝑡𝑟𝑠\n𝑡].\n(8)\nAs the inner optimization loop, we use independent PPO [16,\n32] with shared networks for policy and value functions and the\nSchedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners\nwho also act reasonably. Therefore, we always set one crewmate to\nbe frozen to the listening policy 𝜋L when forming the joint policy\nΠ, following the N-Agent Ad hoc teamwork setting [37] instead of\nassuming a homogeneous population. This change also ensures that\ncrewmates cannot simply determine the identity of the imposter\nby forming an arbitrary convention and voting out any agent who\nviolates that convention.\nFinally, we want our agents to be robust to different environment\nconfigurations. We randomize multiple environment parameters\nwhile training: choosing between three different layouts of the\nenvironment (1 × 3, 2 × 2, and 2 × 3 grids), and randomizing the\nnumber of tasks assigned to each crewmate to either 3, 4, or 5. We\nonly train on configurations where there are 4 crewmates and 1\nimposter, but we report generalization results when playing with\ndifferent numbers of crewmates.\nTo stabilize training, we also include the following world model-\ning loss to each model’s loss function:\nModels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 3: Win rates for crewmates trained with different\nalgorithms over the “base” environment: 2 × 2 grid of rooms,\n4 tasks per crewmate, and 5 players. Error bars represent the\nmaximum and minimum expected win rates across the three\nindependently trained runs with different seeds.\n𝐿WM(𝜋) = −E\n𝜏𝑖∼Π\n∑︁\n𝑡\n𝜆WM log 𝜋(𝑜𝑖\n𝑡+1|𝜏𝑖\n𝑡,𝑎𝑖\n𝑡),\n(9)\nwhere 𝜆WM is the relative strength of the world modeling loss.\nAlthough this loss does not directly contribute to improving task\nsuccess, it subtly helps improve the model’s performance. In partic-\nular, as a recurrent model, RWKV benefits from this world modeling\nloss as it ensures that features are remembered throughout training.\nFurthermore, the world modeling loss prevents the model from plac-\ning too much weight on action tokens, which would cause models\nto output action tokens even during regular discussion sections.\n6\nRESULTS\nIn this section, we analyze the quality of our trained crewmates. We\ninspect the importance of different design decisions regarding the\ntraining of crewmates by ablating over the components of the loss\nfunction in Eq. (7): RL, listening, and speaking. We determine the\nimportance of discussions in the game by measuring the equilibrium\nwin rates of crewmates against imposters and analyze emergent\nbehaviors in discussions. Finally, we highlight some common failure\nmodes we observed during training and how they are mitigated in\nour final training algorithms.\n6.1\nCooperative Training\nFor this set of experiments, we analyze the performance of the\ncrewmates from the first iteration of the self-play algorithm. We\nconduct all experiments using the 1.5B RWKV model, because\nwe find diminishing returns at higher parameter counts from our\nexperiments on base models (see Appendix B for more details). We\nreport the win rates of each policy in the base environment when\nkeeping the imposter and one crewmate fixed to 𝜋L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations\nof the base model. We find that the 1.5B RWKV model struggles to\nwin in the game, with the larger 7B parameter model performing\nslightly better as both win less than 20% of the time in the base\nenvironment.\n\n2×1\n1×3\n2×2\n2×3\n3×2\nEnvironment Shape\n0.0\n0.2\n0.4\n0.6\n0.8\nWin Rate\n2\n3\n4\n5\n6\nTasks\n4\n5\n6\nPlayers\nRWKV\nRWKV7B\nRL\nL\nRL + L\nRL + L + S\nFigure 4: Win rates for crewmates trained with different algorithms over different configurations of the environment, modifying\nthe environment shape, tasks, and number of players.\nJust training with RL significantly boosts the performance rel-\native to the base models, even significantly outperforming the 7B\nparameter model. However, we still find that RL without the ad-\nditional listening loss struggles to reason about the identity of\nimposters. Even when warm-starting the RL policy from 𝜋L, we\nfind that it quickly loses the ability to identify imposters, instead\nvoting for any agent with equal probability. When we instead only\ntrained with listening – using the loss 𝐿L only – the model, 𝜋L, does\nnot know which actions are effective or how to discuss details about\nthe environment, but it is an effective baseline due to the fact that\npredicting the identity of the imposter is valuable in Among Us.\nWhen combining RL and the listening loss, we find success\nrates again increase dramatically, with further improvements when\nadding our denser speaking rewards, as agents can now differen-\ntiate between helpful and unhelpful messages when training. We\nultimately find that our full model achieves twice the win rate\nof the RL-only baseline in the base environment. Note that the\ndifference in scores when adding the additional speaking term is\nrelatively small. Even without the explicit speaking reward, the\nlanguage model produces coherent messages, often sharing their\ncurrent suspicions during discussion rounds, thus benefiting from\ndiscussion even without additional rewards. This is an interesting\nemergent behavior as it shows that speaking is indirectly improved\nby training the model to listen better.\nRobustness to Environment Variation. We present the win\nrate of crewmates against imposters across different environment\nconfigurations in Fig. 4, and see that the trends between models\nobserved in the base environment generally persist across configu-\nrations. We find that the shape of the environment has little effect\non the win rates of crewmates, with smaller environments gener-\nally being easier since it is harder for imposters to kill crewmates\nwithout witnesses. We see a general decline in performance across\nall models when increasing the number of tasks, because this makes\nit harder to win the game by completing tasks instead of voting\nout the imposter. Finally, we see a significant increase in win rates\nas the number of crewmates increase, which we expect since the\ncrewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend\nlines in settings that were out of the training distribution, demon-\nstrating how the language models can extrapolate their behaviors\nto unseen deviations in the configuration.\nMessage Evaluations. We find a major difference between the\nmessage patterns of the base RWKV model and those from 𝜋RL+L+S.\nMost messages from the base RWKV model are often unfocused,\nhallucinating a wider context to the game and role-playing a crew-\nmate. Meanwhile, crewmates using 𝜋RL+L+S often directly accuse\nthe imposter or otherwise name the imposter in their messages.\nIn general, we find that naming an agent makes it more likely for\nother agents to vote against them. Furthermore, crewmates share\nmessages that resemble environment observations that helped them\njudge the identity of the imposter. For instance, a crewmate may\nsay “Player Green is leaving Room (0,1)” when the body is in Room\n(0,1) to indicate that Player Green was running away from the dead\nbody, which is often correlated with being the imposter. However,\nthe crewmates sometimes tell lies in their messages – just like hu-\nmans often do when playing Among Us. In particular, they often\nsimply make up evidence and state whatever is most convincing\nto other agents to get enough votes to eject the correct imposter.\nRepresentative behavior samples are provided in Appendix C.\n6.2\nRobustness to Imposters\nWhen training against frozen imposters, crewmates could come\nup with simple strategies that result in high win rates but are easy\nto overcome with a more intelligent imposter. We therefore run\nmultiple iterations of self-play to investigate whether crewmates\ncan use discussions even against imposters that can evolve to their\npolicies, which we illustrate in Fig. 5. Note that in exploitability\ncurves, we would like to see convergence upon a narrow interval\nfor both the upper and lower bounds; a weak strategy can be easily\nexploited at each iteration and would therefore have both lines stay\nfar apart and converge slowly.\nWe find that the crewmates’ strategies are robust to imposters\ntrained in an adversarial fashion. In particular, we see that crew-\nmate scores converge after only a few iterations; depending on the\nseed, the win rate converges to between 0.51 and 0.56 on the base\nenvironment. In fact, even the crewmates that only trained on the\nbase model imposter are relatively strong, as can be seen by the\nlarge jump in the lower bound between iterations 0 and 1. This\nresult implies that policies discovered by 𝜋RL+L+S are very robust\nsince even facing adversarially trained imposters does not cause a\nsignificant performance drop.\n\n0\n1\n2\n3\nSelf-Play Iteration\n0.2\n0.3\n0.4\n0.5\n0.6\nWin Rate\nLearning Imposter\nFrozen Imposter\nFigure 5: Exploitability curves for policies over self-play it-\nerations, evaluated on the base environment. The orange\nline indicates the expected win rate against an adversarially\ntrained imposter. The black line indicates the expected win\nrate of crewmates who are specifically optimized against this\niteration’s imposters. Note that iteration 0 refers to the base\nmodels, while iteration 1 refers to the crewmate policy from\nthe Cooperative Training section. Shaded regions represent\nthe maximum and minimum win rates across the three inde-\npendently trained runs with different seeds.\nQualitatively, we observe that imposters attempt to shift blame\nto other players by counter-accusing another crewmate. In par-\nticular, they mimic the discussion patterns of crewmates, and the\ncrewmates sometimes fall for this deception. Crewmates who have\nnot witnessed the murder tend to support claims made by other\nplayers, causing them to sometimes help the imposter. Interestingly,\nwe still see similar behavior to a smaller level in the base model\nimposters when playing against strong crewmates. This emergent\nbehavior can likely be attributed to the in-context learning capabil-\nities of language models, which would allow the imposter to mimic\nthe speech of crewmates who spoke beforehand.\n6.3\nFailure Modes\nThroughout our experimentation, we encountered various failure\nmodes that we tackled in our final training algorithms. Specifically,\ndiscussions tended to leave natural language and generally degen-\nerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in\nRLHF [28] required careful tuning to keep language generations\nin English. When this constraint is weighted too low, all of our\nRL-trained models diverge from natural language after only a few\niterations, causing it to output random tokens during discussions\nand stop improving in performance.\nWe also observed that allowing all crewmates to be trained si-\nmultaneously would lead to degenerate solutions. Sometimes the\nmodels learn a social convention where they simply do not speak\nduring the discussion phase. Specifically, models would output new-\nlines when it is their turn to speak instead of actually speaking. In\nthis case, only the imposter would speak and all the agents would\njust vote the speaker out. The models would also learn to just wait\nin the starting room instead of moving around, allowing them to\nwitness the murder or vote out the person who moves out of the\nroom. These strategies are degenerate solutions since they would\nnot work if the imposter was aware of their strategy or if not all\nthe crewmates shared the same strategy, but these strategies would\nlead to nearly perfect win rates during the first iteration of self-play.\nThe fix to this issue was to “freeze” one crewmate to not learn and\ntherefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in\ndiscussions instead of natural language. Specifically, the RL-trained\nmodels would learn to take actions by explicitly choosing action\ntokens, but this gives action tokens a higher probability to be chosen\noverall, even during discussion phases. We observed that the best\nway to counteract this effect was to introduce the world modeling\nloss from Eq. (9). This loss ensured that the model preserved its\nlanguage modeling abilities and had the side effect of helping the\nmodels match the patterns it experienced in observations within its\nown discussions, which would help independent agents understand\nthe intentions of our models.\n7\nDISCUSSION\nSummary. We introduce a technique to self-improve the discussion\nability of an LLM in social deduction games and show how it en-\nables agents to communicate effectively in the game of Among Us.\nWe demonstrate that, despite having weak base models, our agents\nlearn to speak effectively and extract information from discussion\nmessages. We also find that our agents are robust to adversarially\ntrained imposters, who, despite attempting to sabotage the dis-\ncussion, are unable to break the crewmates’ coordination during\ndiscussions. Our technique ultimately shows that self-improving\ndiscussions in multi-agent settings does not require task-specific hu-\nman data, unlocking the possibility for multi-agent communication\nwith language models in novel tasks.\nLimitations and Future Work. A key limitation of our approach\nis that our scene prediction technique is task-dependent. In Among\nUs, there is a natural connection between the discussion and trying\nto predict the identity of the imposter, and a similar structure applies\nto a wide range of social deduction games and real-world settings.\nAn interesting future direction would be to allow agents to identify\nwhich aspects of the scene are relevant to a specific task instead\nof manually specifying it. Please refer to Appendix D for more\nanalysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their\ndiscussions, opting instead to make the most convincing statements.\nWe consider this behavior to be potentially dangerous outside of\nour sandboxed setting of Among Us, so we believe that optimizing\nfor truthfulness is an important future direction.\nACKNOWLEDGMENTS\nThis research was supported in part by the Other Transaction\naward HR00112490375 from the U.S. Defense Advanced Research\nProjects Agency (DARPA) Friction for Accountability in Conversa-\ntional Transactions (FACT) program, the Cooperative AI foundation,\nONR project N00014-21-1-2298, NSF Awards #2006388, #1941722,\n#2125511, AFOSR YIP, and the Stanford Center for Human-Centered\nAI (HAI). We thank the Stanford Madrona team for giving advice\non the systems-level details of our implementation, and Hengyuan\nHu for his insightful feedback when reviewing this paper.\n\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,\nEric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao\nLu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding\nLanguage in Robotic Affordances. arXiv:2204.01691 [cs.RO]\n[2] Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A Theoretical\nStudy of Players and Coalitions in a Partial Information Environment. The Annals\nof Applied Probability 18, 3 (2008), 825–846. http://www.jstor.org/stable/25442651\n[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[4] Luca Carminati, Brian Hu Zhang, Gabriele Farina, Nicola Gatti, and Tuomas\nSandholm. 2023. Hidden-Role Games: Equilibrium Concepts and Computation.\narXiv:2308.16017 [cs.GT]\n[5] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans\nfor human-AI coordination. Curran Associates Inc., Red Hook, NY, USA.\n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed\nKhaled, and Ashok Cutkosky. 2024. The Road Less Scheduled. In Thirty-eighth\nConference on Neural Information Processing Systems.\n[7] Yujian Dong, Tianyu Wu, and Chaoyang Song. 2024. Optimizing Robotic Ma-\nnipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for\nLifelong Learning. arXiv:2407.16306 [cs.RO] https://arxiv.org/abs/2407.16306\n[8] FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Fla-\nherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike\nLewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh\nZhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of\nDiplomacy by combining language models with strategic reasoning.\nSci-\nence 378, 6624 (2022), 1067–1074.\nhttps://doi.org/10.1126/science.ade9097\narXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097\n[9] Michael C. Frank and Noah D. Goodman. 2014. Inferring word meanings by\nassuming that speakers are informative. Cognitive Psychology 75 (2014), 80–96.\nhttps://doi.org/10.1016/j.cogpsych.2014.08.002\n[10] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong\nZheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindA-\ngent: Emergent Gaming Interaction. 3154–3183.\nhttps://doi.org/10.18653/v1/\n2024.findings-naacl.200\n[11] Serhii Havrylov and Ivan Titov. 2017. Emergence of language with multi-agent\ngames: learning to communicate with sequences of symbols. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long\nBeach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA,\n2146–2156.\n[12] Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. 2020. Contin-\nual Adaptation for Efficient Machine Communication. In Proceedings of the 24th\nConference on Computational Natural Language Learning, Raquel Fernández and\nTal Linzen (Eds.). Association for Computational Linguistics, Online, 408–419.\nhttps://doi.org/10.18653/v1/2020.conll-1.33\n[13] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. "Other-\nPlay " for zero-shot coordination. In Proceedings of the 37th International Confer-\nence on Machine Learning (ICML’20). JMLR.org, Article 409, 12 pages.\n[14] Hengyuan Hu and Dorsa Sadigh. 2023. Language Instructed Reinforcement\nLearning for Human-AI Coordination. In 40th International Conference on Machine\nLearning (ICML).\n[15] Jerry Huang. 2024. How Well Can a Long Sequence Model Model Long Se-\nquences? Comparing Architechtural Inductive Biases on Long-Context Abilities.\narXiv:2407.08112 [cs.LG] https://arxiv.org/abs/2407.08112\n[16] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and João G.M. Araújo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1–18. http://jmlr.org/papers/v23/21-\n1342.html\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In Proceedings of the 39th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n(Eds.). PMLR, 9118–9147. https://proceedings.mlr.press/v162/huang22a.html\n[18] Innersloth. 2024. Among Us. https://www.innersloth.com/games/among-us/.\n[Online; accessed 25-February-2024].\n[19] Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha\nVezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki,\nJoel Z. Leibo, and Thore Graepel. 2022. Hidden Agenda: a Social Deduction Game\nwith Diverse Learned Equilibria. arXiv:2201.01816 [cs.AI]\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2024. Toward Grounded Social Reasoning. In International\nConference on Robotics and Automation (ICRA).\n[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR).\n[22] Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia,\nShirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf Among Us:\nMultimodal Resources for Modeling Persuasion Behaviors in Social Deduction\nGames. In Findings of the Association for Computational Linguistics: ACL 2023,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 6570–6588.\nhttps://doi.org/10.\n18653/v1/2023.findings-acl.411\n[23] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. In International\nConference on Learning Representations.\nhttps://openreview.net/forum?id=\nHk8N3Sclg\n[24] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette\nBohg. 2023. Text2Motion: from natural language instructions to feasible plans.\nAutonomous Robots (14 Nov 2023). https://doi.org/10.1007/s10514-023-10131-7\n[25] Qinghua Liu, Csaba Szepesvari, and Chi Jin. 2022. Sample-Efficient Reinforce-\nment Learning of Partially Observable Markov Games. In Advances in Neural\nInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HnIQrSY7vPI\n[26] William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron Holdaway,\nand Judith E. Fan. 2021. Learning to communicate about shared procedural\nabstractions. arXiv:2107.00077 [cs.CL]\n[27] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL).\n[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[29] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of\nHuman Behavior. In In the 36th Annual ACM Symposium on User Interface Software\nand Technology (UIST ’23) (San Francisco, CA, USA) (UIST ’23). Association for\nComputing Machinery, New York, NY, USA.\n[30] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella\nBiderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian\nDu, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna\nSri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and\nRui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings\nof the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936\n[31] Bidipta Sarkar, Andy Shih, and Dorsa Sadigh. 2024. Diverse conventions for\nhuman-AI collaboration. In Proceedings of the 37th International Conference on\nNeural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1003, 25 pages.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[33] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi,\nYoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or Neural\nTheory of Mind? Stress Testing Social Reasoning in Large Language Models.\narXiv:2305.14763 [cs.CL]\n[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. GPT-4\nDoesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning\nProblems. arXiv:2310.12397 [cs.AI]\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[36] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L.\nSifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets,\nRémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky,\n\nJames Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai\nWu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,\nChris Apps, and David Silver. 2019. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature 575 (2019), 350 – 354. https://api.\nsemanticscholar.org/CorpusID:204972004\n[37] Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, and Peter Stone.\n2024. N-Agent Ad Hoc Teamwork. In Advances in Neural Information Processing\nSystems (NeurIPS).\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied\nAgent with Large Language Models. arXiv preprint arXiv: Arxiv-2305.16291 (2023).\n[39] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar\nSukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models.\narXiv:2401.10020 [cs.CL]\n\nA\nENVIRONMENT DESIGN\nGameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a 𝑊× 𝐻grid of rooms. The\nlocation of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set\nspeed of movement, 𝑁𝑡𝑟𝑎𝑣𝑒𝑙.\nCrewmates use this time to complete “tasks” around the map. In the original game of Among Us, tasks involve completing minigames that\nrequire a player’s attention, such as solving a maze, preventing them from observing the environment around them. In our implementation,\nwe simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified\namount of time. However, similar to the original game, crewmates receive no observations from the environment while completing the task,\nwhich leaves them vulnerable to attacks and may cause them to miss out on information such as an imposter killing another crewmate. If all\ntasks are completed, the crewmates win and the game ends, which incentivizes them to attempt performing tasks despite the risks and\npartial observability that comes with it.\nImposters are not assigned tasks to complete, and instead use the main gameplay loop to eliminate crewmates. Eliminating a crewmate is\nan action where the imposter kills a specific crewmate in their current room. Imposters have to wait for an “elimination cooldown” before\neliminating a crewmate, and this countdown restarts after each elimination, preventing imposters from instantly eliminating all crewmates.\nWhen a crewmate is eliminated, their corpse is left behind, and any player who finds the corpse can report it and instantly start the discussion\nphase.\nSince the environment interfaces with agents through natural language, the environment constructs observation descriptions based on\nthe surroundings of the agent. Each observation 𝑜𝑖\n𝑡include the current timestamp of the environment, the current room that the agent is in,\nand the list of other players in the room. Note that this observation does not include if other players are completing a task as waiting in a\nroom looks identical to working on a task. If another player is travelling towards or away from the current room, this is also indicated. Here\nis an example of an observation for a crewmate at timestep 56:\n[56]: You are in room (0, 1). You see Player Green leaving to room 2. You have the following tasks in this room: Task 2.\nFurthermore, crewmates are given information about uncompleted tasks in the current room, and imposters are given the number of\nseconds remaining in the elimination cooldown.\nFollowing an observation, the agent also receives a discrete set of legal actions based on the state, which they must pick from. Here is an\nexample of the action set:\n[56] World: You can perform any of the following actions: go north; wait; do task; go south; wait\n[56] You: wait\nAll agents are allowed to just “wait” in a room until something changes in the environment, or “go” to an adjacent room, taking time\nto travel. If there is a corpse near an agent, they can “report body” and initiate the discussion phase. Crewmates can “do task” to do an\nuncompleted task in the room, while imposters can “kill player [x]” to eliminate a specific player if they are not on the elimination cooldown.\nNote that although imposters may not complete tasks, they can still appear to do tasks to an outside observer by simply performing the “wait”\naction. To enable efficient action generation with language models, each action is mapped to a unique token in a multiple-choice format.\nDiscussion Phase. When an agent reports the body of another player, the discussion phase starts and all players are immediately brought\nto a central room. The environment informs all players about the identity of the corpse and the player who reported the body.\nTo determine the order of speakers, the environment cycles through a randomized list of living players twice. During a player’s turn to\nspeak, they can produce up to 20 tokens or until a newline character, and this message is shared with all agents before the environment\nchooses the next player. Unlike the gameplay phase, where actions are restricted to a small set of tokens, discussions are free-form so we\nallow agents to generate any printable token. We allow up to 20 tokens since this roughly corresponds to the maximum number of tokens\nused in the longest messages in the “quick chat” setting of the original Among Us game. In practice, trained agents tend to use fewer than 20\ntokens per message, instead ending their turn early using newline characters.\nAfter the free-form discussion, a voting phase begins. Each player is given the option to vote to eject a living player or abstain. The agent\nwho achieves the plurality of the votes gets ejected, except in the case of ties or when “abstaining” wins, at which point nobody gets ejected.\nIf all imposters are ejected, the crewmates win and the game ends. Otherwise, the gameplay phase starts again and the cycle continues.\nBefore the discussion begins and between each discussion message, the environment queries all living crewmates to ask who they believe\nis the current imposter, and collects their probabilities based on the language model’s probabilities of voting out each agent. This has no\nimpact on the game itself, but it is used for training.\nReward Structure. Among Us is fundamentally a zero-sum team game, so the reward is based on whether crewmates or imposters win. If\nall tasks are completed or all imposters are ejected, the crewmates win, representing a crewmate reward of +1. If the number of imposters is\never greater than or equal to the number of crewmates, the imposters win, resulting in a crewmate reward of −1.\n\nB\nBASE MODEL RESULTS\nThe original RWKV models came in sizes ranging from 169 million parameters to 7 billion parameters. To determine the scaling laws of these\nmodels on our environment, we trained our base models using Eq. (4) over the same amounts of randomized environment interaction data.\nWe present the results of the trained models in Table 1. For the metric of accuracy, we find diminishing returns at 1.5 billion parameters, so\nwe use this model for ablation experiments.\nOur Base Model\nOriginal RWKV\nModel Size\nAccuracy ↑\nPPLWorld ↓\nPPLLam ↓\nAccuracy ↑\nPPL𝑊𝑜𝑟𝑙𝑑↓\nPPL𝐿𝑎𝑚↓\n169 M\n0.60\n0.34\n45.1\n0.15\n4.4\n27.5\n430 M\n0.61\n0.30\n19.6\n0.18\n2.9\n11.2\n1.5 B\n0.62\n0.28\n7.3\n0.20\n2.4\n6.2\n3 B\n0.64\n0.23\n6.2\n0.14\n2.0\n4.8\n7 B\n0.65\n0.22\n5.2\n0.22\n2.0\n4.1\nTable 1: Scaling performance for the RWKV base model. PPLWorld refers to the perplexity of predicting observation tokens\nin the environment, which correlates with a model’s ability to understand its environment. PPLLAM refers to the lambada\nevaluation perplexity.\nC\nSAMPLE GENERATIONS\nAll samples below are from the perspective of the imposter:\nSample 1: Through the discussion, Player Pink learns that Player Blue is the imposter:\nWorld (to all): Player Yellow discovered the dead body of Player Purple in room (0,0).\nPlayer Blue (you) saying: "There\'s no way I\'m playing this game."\nPlayer Yellow (to all): "Only the Imposter will speak."\nPlayer Green (to all): "The other Crewmates are suspicious of Player Blue. They don\'t trust him to report anything when\nPlayer Pink (to all): "I hope to stay impartial and complete the tasks to the best of my ability and remain a trusted\nPlayer Blue (you) saying: "The other Crewmates are suspicious of Player Pink. It\'s my hope to remain the Imposter\nPlayer Yellow (to all): "If you hear or see anything suspicious, inform other Crewmates."\nPlayer Green (to all): "I notice suspicious activities of Player Blue and Player Purple."\nPlayer Pink (to all): "I notice suspicious activities of Player Blue."\nPlayer Blue was voted out. Crewmates win!\nSample 2: Other players directly accuse Player Green:\nWorld (to all): Player Purple discovered the dead body of Player Yellow in room (2,0).\nPlayer Orange (to all): "The Imposter is Player Green."\nPlayer Green (you) saying: "Shut Up!"\nPlayer Purple (to all): "The Imposter is Player Green."\nPlayer Orange (to all): "You are the Imposter!"\nPlayer Green (you) saying: "My strategy is to kill or eject Imposters."\nPlayer Purple (to all): "Players can only report each other and identify the Imposter."\nPlayer Green was voted out. Crewmates win!\nSample 3: Player Pink, the imposter, convinces the other crewmate to vote out the innocent Player Orange.\nWorld (to all): Player Purple discovered the dead body of Player Red in room (1,0).\nPlayer Orange (to all): "I\'ve been playing as a Player Pink since the beginning, because of that, I know that\nPlayer Pink (you) saying: "I want to make it clear that I think Player Orange is the likely imposter. I don\nPlayer Purple (to all): "I believe the imposter in this room is Player Pink."\nPlayer Orange (to all): "I believe that Player Purple is the imposter in this\nPlayer Pink (you) saying: "I believe the imposter is Player Cubie."\nPlayer Purple (to all): "I believe Player Pink killed Player Orange."\nWorld (to all): Player Orange received 2 votes, Player Pink received 1 votes. Therefore, Player Orange is ejected\nthis round.\nWorld (to all): There are currently more imposters than crewmates. Imposters win!'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=18, thumbnail=None, content='In recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=17, thumbnail=None, content=None)]}
2025-02-12 23:27:26,951 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 23:27:35,262 - DEBUG - start_idx: 2987, start_marker: 1 INTRODUCTION A long, end_idx: 10447, end_marker: imes higher success r
2025-02-12 23:27:35,866 - DEBUG - start_idx: 1405, start_marker: 1 Introduction The p, end_idx: 20154, end_marker: ification shared task.
2025-02-12 23:29:31,765 - DEBUG - start_idx: 1765, start_marker: 1 Introduction In, end_idx: -1, end_marker: ations and Future Work
2025-02-12 23:29:35,723 - DEBUG - start_idx: 3124, start_marker: 1. Introduction Large, end_idx: -1, end_marker: efore References/Bibliog
2025-02-12 23:33:08,624 - DEBUG - start_idx: 1560, start_marker: 1. Introduction Due, end_idx: -1, end_marker: ies in model quality.
2025-02-12 23:33:08,627 - INFO - Total execution time: 340.79 seconds (5.68 minutes)
2025-02-12 23:33:08,636 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=90, thumbnail=None, content='1. Introduction Large Language Models (LLMs) have shown significant improvements across a variety of domains (Ope- nAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently, OpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning capabilities of LLMs by allocating additional computation at inference time, making it an effective approach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI et al., 2025). TTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs to “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and (2) External TTS, which improves the reasoning performance via sampling or search-based methods with fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to scale compute optimally, that is, allocating the optimal computation for each problem (Snell et al., 2024). Current TTS methods guide the generation process and select the final answer using Process Reward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024; Beeching et al., 2024). These TTS methods involve several important factors, such as policy models1, PRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy models, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the community from fully understanding the effectiveness of this method and developing insights for compute-optimal TTS strategies. To address these issues, this paper aims to investigate the influence of policy models, PRMs, and problem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore the concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct extensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO, 2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across multiple policy models (ranging from 0.5B to 72B across two model families). Our results show that the compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem difficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B model) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging reasoning tasks by applying compute-optimal TTS. The contributions of this work can be summarized as follows: 1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date policy models, multiple PRMs, diverse scaling methods, and more challenging tasks. 2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process and introduces reward-aware compute-optimal TTS. We also demonstrate that the compute- optimal scaling strategy varies with different policy models, PRMs, and problem difficulty levels. 3. The empirical results demonstrate the significant potential of smaller language models to outperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy, we show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and DeepSeek-R1 on MATH-500 and AIME24 tasks. 1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs. 2  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Question Best-of-N Question Beam Search Diverse Verifier Tree Search  : Scored by PRM : Selected by PRM : Rejected by PRM : Solution / Step with Final Answer : Intermediate Step Question Question Answer Answer Answer Figure 2: Comparison of different external TTS methods. 2. Setup & Preliminaries 2.1. Problem Formulation We formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018), defined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮 is the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor. Given a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1), where 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to 𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the episode terminates, either by reaching the maximum number of steps or by generating an <EOS> token. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized as follows: Initial State: 𝑠1 = 𝑥∼𝒳 Action: 𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡) State Transition: 𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡] Reward: 𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡) (1) 2.2. Test-Time Scaling Method We consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al., 2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell et al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or other methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS methods are shown in Figure 2. Best-of-N. In the BoN approach, the policy model generates 𝑁responses, after which scoring and voting methods are applied to select the final answer. Beam Search. Given beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps. The verifier selects the top 𝑁 𝑀steps for subsequent search. In the next step, the policy model samples 3  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or an <EOS> token is generated. Diverse Verifier Tree Search. To increase diversity, DVTS extends beam search by dividing the search process into 𝑁 𝑀subtrees, each of which is explored independently using beam search. As shown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with a large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing the number of parallel subtrees proves to be more effective than increasing the beam width under the same budget. 2.3. Compute-Optimal Test-Time Scaling To maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling strategy, which selects hyperparameters corresponding to a given test-time strategy to maximize performance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output distribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁. 𝜃* 𝑥,𝑦*(𝑥)(𝑁) = arg max 𝜃 (︀ E𝑦∼Target(𝜃,𝑁,𝑥) [︀ 1𝑦=𝑦*(𝑥) ]︀)︀ , (2) where 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃* 𝑥,𝑦*(𝑥)(𝑁) represents the test-time compute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁. 3. Rethinking Compute-Optimal Test-Time Scaling 3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware Compute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024). Previous works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al., 2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier to do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM trained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL), we obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy PRM produces more accurate rewards for the responses of the policy model, while the offline PRM often generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng et al., 2024). For practical applications of compute-optimal TTS, training a PRM for each policy model to prevent OOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy in a more general setting, where the PRM might be trained on a different policy model than the one used for TTS. For search-based methods, PRMs guide the selection at each response step, while for sampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the reward influences response selection across all methods; (2) for search-based methods, the reward also influences the search process. To analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B- Instruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs. The results in Figure 12 demonstrate that the reward significantly affects the generation process and outcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect answers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more 4  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 0.0 0.2 0.4 0.6 0.8 1.0 Pass@1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Percentage 11.2% 3.4% 3.4% 5.8% 76.2% mean: 0.82 Figure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five bins. tokens. In Section 4, we also empirically show that rewards have great influence on TTS performance and output tokens. Based on these findings, we propose that rewards should be integrated into the compute-optimal TTS strategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is formulated as: 𝜃* 𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max 𝜃 (︀ E𝑦∼Target(𝜃,𝑁,𝑥,ℛ) [︀ 1𝑦=𝑦*(𝑥) ]︀)︀ , (3) where Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the reward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods, Target(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal scaling adapts to the policy model, prompt, and reward function, leading to a more general framework for practical TTS. 3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles To consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five difficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels from MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al., 2024) is not effective since different policy models have different reasoning capabilities. As shown in Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500 problems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty. Specifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%). 4. How to Scale Test-Time Compute Optimally? In this section, we aim to answer the following questions: • Q1: How does TTS improve with different policy models and PRMs? • Q2: How does TTS improve for problems with different difficulty levels? • Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods? 5  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 4.1. Setup Datasets. We conduct experiments on competition-level mathematical datasets, including MATH- 500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative problems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell et al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical reasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for experiments. Policy Models. For test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and Qwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy models. Process Reward Models. We consider the following open-source PRMs for evaluation: • Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang et al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al., 2024). • RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta- Math (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The base model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024). • Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM- 1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B- Instruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron et al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series models. • Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5- Math-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang et al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al., 2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed, Qwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while Qwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as demonstrated in Zhang et al. (2025). Scoring and Voting Methods. Following Wang et al. (2024a), we consider three scoring methods: PRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote. To obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of length 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1) PRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻 𝑡=0. (2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg scores each trajectory by the average reward among all steps, i.e., score = 1 𝐻 ∑︀𝐻 𝑡=0 ℛ𝑡. The voting methods then aggregate the scores to determine the final answer. Majority Vote selects the answer with the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest score, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer with the highest score. 6  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 50 60 70 80 90 Llama-3.1-8B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 50 60 70 80 90 RLHFlow-PRM-Mistral-8B 22 24 26 28 50 60 70 80 90 RLHFlow-PRM-Deepseek-8B 22 24 26 28 50 60 70 80 90 Skywork-PRM-1.5B 22 24 26 28 50 60 70 80 90 Skywork-PRM-7B 22 24 26 28 50 60 70 80 90 Qwen2.5-Math-PRM-7B 22 24 26 28 50 60 70 80 90 Qwen2.5-Math-PRM-72B 22 24 26 28 75 80 85 90 95 Qwen2.5-7B-Inst. 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 Pass@k Majority Best-of-N Beam Search DVTS Figure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different PRMs and TTS strategies. 22 24 26 28 0 20 40 60 Llama-3.1-8B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 0 20 40 60 RLHFlow-PRM-Mistral-8B 22 24 26 28 0 20 40 60 RLHFlow-PRM-Deepseek-8B 22 24 26 28 0 20 40 60 Skywork-PRM-1.5B 22 24 26 28 0 20 40 60 Skywork-PRM-7B 22 24 26 28 0 20 40 60 Qwen2.5-Math-PRM-7B 22 24 26 28 0 20 40 60 Qwen2.5-Math-PRM-72B 22 24 26 28 10 20 30 40 50 Qwen2.5-7B-Inst. 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 Pass@k Majority Best-of-N Beam Search DVTS Figure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different PRMs and TTS strategies. We use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute budgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as in prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width is set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we restrict the maximum number of new tokens to 8192. For search-based methods, the token limit is 2048 for each step and 8192 for the total response. 4.2. How does TTS improve with different policy models and PRMs? (Q1) PRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama- 3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs improves significantly with larger compute budgets, while the results of searching with Math-Shepherd and RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct, the performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more budgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy of both policy models improves a lot with larger compute budgets, the performance improvement of TTS remains moderate. These results demonstrate that the generalization of PRMs is particularly challenging across different policy models and tasks, especially for more complex tasks. The optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms other strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based 2https://github.com/openreasoner/openr 7  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling methods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using a PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization across policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers trapped in local optima and worsen the performance. This may also be related to the base model of the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B- Instruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025). Further analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice of the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of considering reward information in compute-optimal TTS. We also explore the relationship between TTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS performance is positively correlated with the process supervision abilities of PRMs, and the fitted function is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the process supervision abilities of the PRM (Zhang et al., 2025). 30 40 50 60 70 80 ProcessBench Performance 70 72 74 76 78 Test-Time Scaling Performance Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B Figure 6: The relationship between TTS performance and process supervision abilities of different PRMs on MATH, where the size of each circle represents the number of parameters of the PRM and the curve represents the fitted function. 22 24 26 28 40 60 80 Qwen2.5-0.5B-Inst. 22 24 26 28 60 70 80 90 Qwen2.5-1.5B-Inst. 22 24 26 28 70 80 90 Qwen2.5-3B-Inst. 22 24 26 28 80 85 90 95 Qwen2.5-7B-Inst. 22 24 26 28 80 85 90 95 Qwen2.5-14B-Inst. 22 24 26 28 80 85 90 95 100 Qwen2.5-32B-Inst. 22 24 26 28 85 90 95 100 Qwen2.5-72B-Inst. Pass@k Majority Best-of-N Beam Search DVTS Figure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with different scaling methods. The optimal TTS method varies with policy models. To study the relationship between the parameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5 family LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy models. For small policy models, search-based methods outperform BoN, while for large policy models, BoN is more effective than search-based methods. This difference occurs because larger models have stronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast, smaller models rely on a verifier to select each step, ensuring the correctness of each intermediate step. 8  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 4.3. How does TTS improve for problems with different difficulty levels? (Q2) Following Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty levels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in MATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell et al., 2024) is not appropriate because different policy models exhibit different reasoning abilities. To address this, we categorize the difficulty levels into three groups based on the absolute value of Pass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%). The optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9 show that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy problems, while beam search works better for harder problems. For policy models with parameters between 7B and 32B, DVTS performs well for easy and medium problems, and beam search is preferable for hard problems. For policy models with 72B parameters, BoN is the best method for all difficulty levels. 22 24 26 28 94 96 98 100 Llama-3.2-1B-Inst. Level 1 22 24 26 28 40 60 80 100 Level 2 22 24 26 28 0 20 40 60 Level 3 22 24 26 28 92 94 96 98 100 Llama-3.2-3B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 20 40 60 80 22 24 26 28 94 96 98 100 Llama-3.1-8B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 Pass@k Majority Best-of-N Beam Search DVTS Figure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels. 4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods? (Q3) Table 1: Statistics of training data of RLHFlow PRMs. Mistral-PRM-Data Deepseek-PRM-Data Average Token per Response 236.9 333.1 Average Token per Step 46.6 58.4 PRMs are biased towards the length of steps. Although we perform TTS under the same budget in pervious experiments, we find that the number of inference tokens with different PRMs varies singificantly. For example, given the same budget and the same policy model, the number of inference 9  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling tokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow- PRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different LLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze several properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek- 8B4. As shown in Table 1, both the average token per response and the average token per step of DeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data of RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to the bias towards the length of the output. We also find that the number of inference tokens of scaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very near, which indicates that searching with Skywork-PRM-7B is more efficient than searching with Qwen2.5-Math-7B. Table 2: Performance of TTS with different voting methods on MATH-500. Skywork-PRM-7B Qwen2.5-Math-PRM-7B Majority Vote 86.8 87.6 PRM-Min-Max 83.0 87.4 PRM-Min-Vote 86.6 87.6 PRM-Last-Max 84.4 87.6 PRM-Last-Vote 87.0 87.6 PRM-Avg-Max 85.8 87.8 PRM-Avg-Vote 86.8 87.6 PRMs are sensitive to voting methods. From the results in Table 2, it is shown that Skywork- PRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very sensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are processed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps labeled as positive steps in the training data and makes the outputted large reward values more likely to be correct. This shows that the training data of PRMs is important for improving the ability to find errors in the search process. 5. Results for Compute-Optimal Test-Time Scaling With the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to explore the following questions: • Q4: Can smaller policy models outperform larger models with the compute-optimal TTS strategy? • Q5: How does compute-optimal TTS improve compared with CoT and majority voting? • Q6: Is TTS more effective than long-CoT-based methods? 5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy (Q4) Scaling test-time compute of small policy models is crucially important for improving the reasoning performance of LLMs. We are interested in whether smaller policy models can outperform larger ones, GPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the 3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data 4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data 10  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Table 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs (CoT) on MATH-500 and AIME24. Policy Model MATH-500 AIME24 Avg. Proprietary LLMs (CoT) GPT-4o 74.6 9.3 42.0 o1-preview 85.5 44.6 65.1 o1-mini 90.0 63.6 76.8 o1 94.8 79.2 87.0 Open-Source LLMs (CoT) Llama-3.1-70B-Inst. 65.2 16.7 41.0 Llama-3.1-405B-Inst. 71.4 23.3 47.4 QwQ-32B-Preview 90.6 50.0 70.3 DeepSeek-R1 97.3 79.8 88.6 Open-Source LLMs (TTS) Llama-3.2-1B-Inst. 66.2 16.7 41.5 Llama-3.2-1B-Inst. (𝑁= 512) 72.2 10.0 41.1 Llama-3.2-3B-Inst. 75.6 30.0 52.8 Qwen2.5-0.5B-Inst. 76.4 10.0 43.2 Qwen2.5-1.5B-Inst. 81.8 20.0 50.9 DeepSeek-R1-Distill-Qwen-1.5B 91.6 63.3 77.5 DeepSeek-R1-Distill-Qwen-7B 95.2 83.3 89.3 performance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct (CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above two tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek- R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24. From the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24, meaning that smaller models can outperform 135× larger models using the compute-optimal TTS strategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve the result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512, Llama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on MATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct and Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating that small models can exceed GPT-level performance with the compute-optimal TTS strategy. (4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview and o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the compute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These results demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs with the compute-optimal TTS strategy. FLOPS Comparison. To answer the question of whether compute-optimal TTS is more effective than increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell 5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use Qwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct. 11  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Table 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones (CoT). Policy Model Pre-training FLOPS Inference FLOPS Total FLOPS. Llama-3.2-3B-Inst. 1.62 × 1023 3.07 × 1017 1.62 × 1023 Llama-3.1-405B-Inst. 3.65 × 1025 4.25 × 1017 3.65 × 1025 DeepSeek-R1-Distill-7B 7.56 × 1023 8.15 × 1017 7.56 × 1023 DeepSeek-R1 5.96 × 1025 4.03 × 1018 5.96 × 1025 Table 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models on MATH-500. Policy Model CoT Major. Compute-Optimal TTS Performance Gain Efficiency Gain Llama-3.2-1B-Inst. 26.0 39.0 66.2 154.6% >256.0× Llama-3.2-3B-Inst. 41.4 58.4 78.2 88.9% 14.1× Llama-3.1-8B-Inst. 49.8 66.4 80.6 61.8% 43.9× Qwen2.5-0.5B-Inst. 31.6 47.2 76.4 141.8% >64.0× Qwen2.5-1.5B-Inst. 54.4 68.4 85.6 57.4% >256.0× Qwen2.5-3B-Inst. 64.0 77.0 87.6 36.9% 58.4× Qwen2.5-7B-Inst. 76.8 83.6 91.0 18.5% 35.9× Qwen2.5-14B-Inst. 80.2 85.6 91.0 13.5% 51.4× Qwen2.5-32B-Inst. 82.4 87.0 90.6 10.0% 0.8× Qwen2.5-72B-Inst. 83.8 87.2 91.8 9.5% 12.9× et al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results, we can see that small policy models even surpass large ones with less inference FLOPS and reduce the total FLOPS by 100× ∼1000×. 5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5) Based on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty levels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in Table 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and improve reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal TTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters in the policy model increases, the improvement of TTS gradually decreases. This suggests that the effectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for models with weak reasoning abilities, scaling test-time compute leads to a substantial improvement, whereas for models with strong reasoning abilities, the gain is limited. 5.3. Is TTS more effective than long-CoT-based methods? (Q6) Recently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan et al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance of TTS with these approaches. Setup. We evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first generates reasoning data via MCTS, followed by online policy and preference model learning. (2) Eurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit process rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection 12  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Table 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24. Policy Model MATH-500 AIME24 Avg. Open-Source LLMs (CoT) Qwen2.5-7B-Inst. 76.8 13.3 45.1 Qwen2.5-Math-7B-Inst. 79.8 13.3 46.6 Long-CoT Methods (CoT) rStar-Math-7B 78.4 26.7 52.6 Eurus-2-7B-PRIME 79.2 26.7 53.0 Qwen2.5-7B-SimpleRL-Zero 77.2 33.3 55.3 Qwen2.5-7B-SimpleRL 82.4 26.7 54.6 Satori-Qwen-7B 83.6 23.3 53.5 DeepSeek-R1-Distill-Qwen-7B 92.4 63.3 77.9 Open-Source LLMs (TTS) Qwen2.5-7B-Inst. w/ 7B PRM (Ours) 88.0 33.3 60.5 Qwen2.5-7B-Inst. w/ 72B PRM (Ours) 91.0 36.7 63.9 with only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and then improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B parameters into a 7B LLM. Results. As shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math, Eurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of TTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on AIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on the data generated via MCTS but is less effective than distilling from strong reasoning models. Also, TTS is more effective on simpler tasks than on more complex tasks. 6. Related Work LLM Test-Time Scaling. Scaling LLM test-time compute is an effective way to improve the perfor- mance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based methods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu et al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al. (2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al. (2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process reward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for external reward models and the generation of extensive samples, Manvi et al. (2024) proposes a self-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al., 2024) explores TTS via search methods with diversity. However, these works lack a evaluation with either strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a more systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and provide some principles for practical TTS. Improving Mathematical Reasoning Abilities of LLMs. Prior methods for improving mathematical reasoning abilities can be divided into training-time methods and test-time methods. For training- 13  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling time methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023; Azerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024; Gou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical capabilities. Another line of works explore self-training and self-improvement strategies (Zelikman et al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024; Zhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve the reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve the mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi, 2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI o1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking. For test-time methods, prompt-based approaches have been extensively studied to enhance reasoning without altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022) and its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into manageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond prompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and correct their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages program interpreter or symbolic manipulators to perform precise calculations and validations. Self- verification approaches (Weng et al., 2023) enable models to assess the correctness of their own reasoning processes, further increasing robustness. These test-time strategies complement training- time enhancements, collectively contributing to significant improvements in LLMs’ mathematical reasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time compute via PRM-guided search methods. Process Reward Models. Previous works show that PRMs are more effective than ORMs (Ue- sato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as PRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col- lection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu et al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs are explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024), implicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives. Additionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al., 2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on mathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and PRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang et al. (2025) provides guidelines for practical development of PRMs and releases the most capable PRMs for mathematical tasks up-to-date. 7. Conclusion & Discussion In this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from the perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our findings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs, and problem difficulty, validating that smaller language models can perform better than larger models when applying compute-optimal TTS. Our results show that a 1B model can achieve better performance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can achieve strong TTS results by supervising a more capable 72B policy model, which suggests the importance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak” supervision for policy optimization. To achieve this goal, we need to develop more efficient supervision 14  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling methods, as both PRM-based and RL-based approaches have limitations due to their dependence on high-quality supervision. Future work should focus on developing more adaptable and universal supervision mechanisms to boost the performance of small language models on complex tasks and provide new approaches for developing efficient reasoning strategies. Limitations. Although we provide a comprehensive evaluation of TTS on mathematical tasks, there are still some limitations and future directions to explore: (1) Extending TTS to more tasks such as coding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS. 15  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling References AI-MO. Aime 2024, 2024. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-aime. Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/ introducing-claude/. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=4WnqRR915j. Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models, 2024. URL https://huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum?id=VaXnxQ3UKo. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum? id=YfZ4ZPt8zd. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin 16  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In International Conference on Machine Learning (ICML), volume 202, pages 10764–10799, 2023. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview. net/forum?id=Ep0TtjVoap. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 17  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265, 2024. Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview. net/forum?id=shgx0eqdw6. Kimi. k0-math, November 2024. URL https://kimi.moonshot.cn/. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Joshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically annotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024. Wendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum? id=v8L0pN6EOi. Jianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo. Autopsv: Automated process-supervised verifier. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46534–46594, 2023. Rohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. 18  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint arXiv:2410.18982, 2024. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532, 2024a. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of- action-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. Skywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/. Skywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction tuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume 235, pages 47885–47900, 2024. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware rejection tuning for mathematical problem-solving. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7601–7614, 2024. 19  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun Wang. AlphaZero-like tree-search can guide large language model decoding and training. In International Conference on Machine Learning (ICML), volume 235, pages 49890–49920, 2024. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024a. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426–9439, 2024b. Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR), 2023. URL https:// openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural information processing systems (NeurIPS), volume 35, pages 24824–24837, 2022. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2550–2575, 2023. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 41618–41650, 2023. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. 20  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024c. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 11809–11822, 2023. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for large language models. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=N8N0hgNDRt. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 15476–15488, 2022. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-STar: Language models can teach themselves to think before speaking. In Conference on Language Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9. Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in large language models–the story goes on. arXiv preprint arXiv:2407.08348, 2024. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM self-training via process reward guided tree search. In Advances in Neural Information Processing Systems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5. Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint arXiv:2412.11006, 2024b. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. 21  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595–46623, 2023. 22  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling A. Prompt Template for Test-Time Scaling The system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang et al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use the system prompt of the official evaluation6 for Llama 3 to prevent performance drop. Table 7: System prompt for Llama 3 series models. Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide a concise solution with minimal explanation. - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] ... Regardless of the approach, always conclude with: Therefore, the final answer is: $\\boxed{answer}$. I hope it is correct. Where [answer] is just the final number or expression that solves the problem. Table 8: System prompt for Qwen2.5 series models. Please reason step by step, and put your final answer within \\boxed{}. B. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and Scaling Methods The full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10 and Figure 11. 6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals 23  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 94 96 98 100 Llama-3.2-1B-Inst. Level 1 22 24 26 28 40 60 80 100 Level 2 22 24 26 28 0 20 40 60 Level 3 22 24 26 28 92 94 96 98 100 Llama-3.2-3B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 20 40 60 80 22 24 26 28 94 96 98 100 Llama-3.1-8B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 22 24 26 28 92 94 96 98 100 Qwen2.5-0.5B-Inst. 22 24 26 28 40 50 60 70 80 90 100 22 24 26 28 0 20 40 60 80 22 24 26 28 94 96 98 100 Qwen2.5-1.5B-Inst. 22 24 26 28 40 50 60 70 80 90 100 22 24 26 28 0 20 40 60 80 22 24 26 28 97 98 99 100 Qwen2.5-3B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 22 24 26 28 95 96 97 98 99 100 Qwen2.5-7B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 20 40 60 22 24 26 28 97 98 99 100 Qwen2.5-14B-Inst. 22 24 26 28 40 60 80 100 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 96 97 98 99 100 Qwen2.5-32B-Inst. 22 24 26 28 40 50 60 70 80 90 100 22 24 26 28 0 20 40 60 22 24 26 28 97 98 99 100 Qwen2.5-72B-Inst. 22 24 26 28 20 40 60 80 100 22 24 26 28 0 20 40 60 Pass@k Majority Best-of-N Beam Search DVTS Figure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels. 24  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 30 40 50 60 70 80 Llama-3.2-1B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 30 40 50 60 70 80 RLHFlow-PRM-Mistral-8B 22 24 26 28 30 40 50 60 70 80 RLHFlow-PRM-Deepseek-8B 22 24 26 28 30 40 50 60 70 80 Skywork-PRM-1.5B 22 24 26 28 30 40 50 60 70 80 Skywork-PRM-7B 22 24 26 28 30 40 50 60 70 80 Qwen2.5-Math-PRM-7B 22 24 26 28 30 40 50 60 70 80 Qwen2.5-Math-PRM-72B 22 24 26 28 40 50 60 70 80 90 Llama-3.2-3B-Inst. 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 40 50 60 70 80 90 22 24 26 28 50 60 70 80 90 Llama-3.1-8B-Inst. 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 Qwen2.5-0.5B-Inst. 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 30 40 50 60 70 80 90 22 24 26 28 60 70 80 90 Qwen2.5-1.5B-Inst. 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 60 70 80 90 22 24 26 28 70 80 90 Qwen2.5-3B-Inst. 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 70 80 90 22 24 26 28 75 80 85 90 95 Qwen2.5-7B-Inst. 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 22 24 26 28 75 80 85 90 95 Pass@k Majority Best-of-N Beam Search DVTS Figure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling strategies. 25  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 22 24 26 28 0 10 20 30 40 Llama-3.2-1B-Inst. Math-Shepherd-PRM-7B 22 24 26 28 0 10 20 30 40 RLHFlow-PRM-Mistral-8B 22 24 26 28 0 10 20 30 40 RLHFlow-PRM-Deepseek-8B 22 24 26 28 0 10 20 30 40 Skywork-PRM-1.5B 22 24 26 28 0 10 20 30 40 Skywork-PRM-7B 22 24 26 28 0 10 20 30 40 Qwen2.5-Math-PRM-7B 22 24 26 28 0 10 20 30 40 Qwen2.5-Math-PRM-72B 22 24 26 28 0 10 20 30 40 50 60 Llama-3.2-3B-Inst. 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 Llama-3.1-8B-Inst. 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 10 20 30 40 50 60 22 24 26 28 0 5 10 15 20 Qwen2.5-0.5B-Inst. 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 5 10 15 20 22 24 26 28 0 10 20 30 40 50 Qwen2.5-1.5B-Inst. 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 Qwen2.5-3B-Inst. 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 0 10 20 30 40 50 22 24 26 28 10 20 30 40 50 Qwen2.5-7B-Inst. 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 22 24 26 28 10 20 30 40 50 Pass@k Majority Best-of-N Beam Search DVTS Figure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling strategies. 26  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling C. Cases In this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the output of TTS, we identify several issues with PRMs. Specifically, we observe four major categories: (1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct steps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM sometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect these errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the PRM assigns lower scores to certain intermediate steps that are not where the critical errors actually occur. This indicates a misalignment between the scoring signal and the actual error locations. (4) Scoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the token length of intermediate steps, result in large discrepancies in scores for equally correct reasoning steps. Notably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not used during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model). These problems distort the reasoning search process, degrade overall performance, and reduce the reliability of PRM-assisted reasoning. Addressing these biases in future model architectures and training procedures is necessary to improve the robustness and interpretability of PRMs. 27  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response (Beam Search with RLHFlow-Mistral-PRM-8B): ## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince  it can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible  by 10. [score=0.42] ## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe  possible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable  with the digits 2 and 0. [score=0.62] ## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use  leading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with  the digits 0 and 2. ки\\n [score=0.56] ## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found  the least positive integer multiple of 30 that can be written with only the digits 0 and 2.  [score=0.87] Therefore, the final answer is: $\\\\boxed{660}$. [score=0.81] Problem: What is the least positive integer multiple of 30 that  can be written with only the digits 0 and 2? Response (Beam Search with RLHFlow-Deepseek-PRM-8B): ## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of  30, a number must be divisible by both 2 and 3. [score=0.85] ## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end  in 0, since it\'s the only way it will be even. [score=0.89] ## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of  its digits must be divisible by 3. [score=0.92] ## Step 4: Determine the least possible number meeting these requirements\\nThe smallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200.  However, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85] ## Step 5: Find the next possible number meeting the requirements\\nThe next number  to try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91] ## Step 6: Determine the next possible number meeting the requirements\\nThe next  number is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3.  [score=0.91] ## Step 7: Find the next possible number meeting the requirements\\nThe next number  to try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3.  [score=0.88] ## Step 8: Determine the next possible number meeting the requirements\\nThe next  number is 22000. However, we can notice that the sum of the digits is still not divisible by  3. [score=0.88] ## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more  2s to meet the divisibility requirement of 3. [score=0.91] ## Step 10: Find the next possible number meeting the requirements\\nThe next number  to try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85] Therefore, the final answer is: $\\\\boxed{2220}$. [score=0.81] # of Tokens: 890 # of Tokens: 2419 Figure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B. 28  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the  steps:  [score=0.90] 1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is  even, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2  \\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n      \\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).   [score=0.88] 2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its  prime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]   [score=0.84] 3. **Simplify the Square Root**: We can use the property of square roots that  \\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} =  \\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2}  \\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53] Therefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).   [score=0.46] Problem: Simplify $\\\\sqrt{242}$. Figure 13: TTS case of Over-Criticism. 29  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we  can follow these steps: [score=0.99] 1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D  = 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99] 2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of  an angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here,  \\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90] 3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D =  \\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n    \\\\] [score=0.97] 4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the  equation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by  0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE  = 10\\n     \\\\] [score=0.99] Therefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99] Problem: Suppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF =  (0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n  label(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\",  (E+F)/2,W); \\n[/asy] Figure 14: TTS case of Error Neglect. 30  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\)  where \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and  the sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle,  which is the ratio of the length of the opposite side to the hypotenuse. [score=0.74] Given:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to  get:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE =  \\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88] Thus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82] Problem: Suppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF =  (0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n  label(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\",  (E+F)/2, W); \\n [/asy] Figure 15: TTS case of Error Neglect. 31  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: To solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents  to the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the  excenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents  from a point to a circle, \\\\(DB = DC\\\\).  [score=0.20] We use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended  to intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know  that the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the  ratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} =  \\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the  Power of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the  circle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of  the tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot  BP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x =  \\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66] Using the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies  \\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot  DP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the  ratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and  \\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the  formula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC  \\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is  to use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\).  Since \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP  = \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector  Theorem and the length of the angle bisector formula), we can solve for  \\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} =  \\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus,  \\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 =  157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92] Problem: Let $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$  and $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If  $AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$  and $n$ are relatively prime integers. Find $m + n$. Figure 16: TTS case of Error Localization Bias. 32  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Response: Let\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\).  Therefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\).  After finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\).  [score=0.98] Since you redistribute the coins so that all eight bags have the same number of coins, the  total number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible  by 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n  \\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\),  we have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81] To solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative  inverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8}  \\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49  \\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\)  by 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n  = 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92] The total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53  = 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we  have:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer  \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the  expression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256  \\\\] [score=0.79] To find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7  \\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51] Thus, the smallest number of coins you could have had before finding the bag of 53 coins  is \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31] Problem: You have seven bags of gold coins. Each bag has the same number of gold coins. One day, you  find a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight  bags you hold have the same number of coins. You successfully manage to redistribute all the  coins, and you also note that you have more than 200 coins. What is the smallest number of  coins you could have had before finding the bag of 53 coins? Figure 17: TTS case of Scoring Bias. 33  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Problem: You have seven bags of gold coins. Each bag has the same number of gold coins. One day, you  find a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight  bags you hold have the same number of coins. You successfully manage to redistribute all the  coins, and you also note that you have more than 200 coins. What is the smallest number of  coins you could have had before finding the bag of 53 coins? Response: Let\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\).  Therefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins,  the total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98] We are given that the total number of coins can be evenly distributed among 8 bags, so  \\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of  coins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\)  that satisfies both conditions.  [score=0.95] First, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv  0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation  as:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x  \\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can  be written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90] Next, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of  coins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k +  88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2  \\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\).  Substituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we  get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you  could have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7  \\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before  finding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283] Figure 18: TTS case of Scoring Bias. 34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='1 Introduction The proliferation of social networks and text-based internet media has highlighted the issue of online toxicity and hate speech (Saha et al., 2019). This phenomenon not only creates an unpleasant envi- ronment for users but also deters advertisers, poten- tially impacting the economic viability of these plat- forms (Fortuna and Nunes, 2018). Consequently, there is an urgent need for effective mechanisms to measure and mitigate toxicity in online spaces. A promising approach to addressing this chal- lenge is text detoxification of text through para- phrasing (Krishna et al., 2020). Text detoxification is a subtask of text style transfer (TST), which in- volves rewriting text while preserving its original *Equal contribution. Toxic Text Detoxified Text German Wie be**oppt muss man sein? Wie verwirrt muss man sein? Spanish Que os den por el c**o. Que os dé muy mala suerte. French c’est moi at***dé ! je suis tombé ! C’est moi qui suis tombé ! Russian я мужик а вы г**но Я мужчина, а вы неправы Table 1: Examples of the source toxic texts across dif- ferent languages and their respective synthetic detoxifi- cations from our SynthDetoxM. meaning and altering specific style attribute, such as formality, bias, expressiveness, sentiment, or, in the case of detoxification, toxicity (Fu et al., 2018; Lai et al., 2021). While significant progress has been made in monolingual TST and detoxification, both in super- vised and unsupervised settings (Dale et al., 2021; Logacheva et al., 2022; Pour et al., 2023), multilin- gual text detoxification remains a largely unsolved problem. This is primarily due to two factors: the scarcity of parallel detoxification data across multi- ple languages and the suboptimal performance of unsupervised methods in cross-lingual settings (De- mentieva et al., 2023). Manual or crowdsourced data collection is a chal- lenging and costly task (Rao and Tetreault, 2018; Reid and Artetxe, 2023; Konovalov et al., 2016b), creating parallel data with the use of modern LLMs, which already proven to work well for the tasks of text classification (Sun et al., 2023) and question answering (Ye et al., 2022), remains underexplored. To address these challenges and facilitate the devel- opment of multilingual text detoxification models and datasets, we propose a framework for gener- ating parallel multilingual synthetic detoxification data and SynthDetoxM, a large-scale multilinugal synthetic parallel text detoxification dataset, which was created using this framework. arXiv:2502.06394v1  [cs.CL]  10 Feb 2025  Multilingual Toxic Data Source Toxic Data Detoxification-1 Detoxification-2 Detoxification-3 Detoxification-4 Detoxification-5 Score(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖) xi 𝑖𝑖=1 n yi 𝑖𝑖=1 n SynthDetoxM LLM Clean and filter  data by length Generate detox candidates using LLMs Score detoxification candidates and select best xi; yibest 𝑖𝑖=1 n Compose final  detox dataset Figure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification dataset SynthDetoxM. Our dataset is comprised of 16,000 high-quality synthetic detoxification pairs across four languages: German, Spanish, French and Russian. The dataset was created using few-shot prompting and selecting the best generations of five different open-source LLMs. The answers were combined using a hand- crafted heuristic, providing the best answers from each model to ensure diversity and quality of the final data. Our contributions can be summarized as follows: 1. We propose a framework for generating syn- thetic parallel multilingual detoxification data using few-shot prompting of LLMs. 2. We create SynthDetoxM, a large-scale multilin- gual synthetic parallel dataset for text detox- ification, helping to address the data scarcity issue in the detoxification task. 3. We conduct a thorough empirical evaluation of the proposed dataset, including linguistic analysis of the data and benchmarking against the human-annotated MultiParaDetox. We openly release the generated data and code.1 2 Background and Related Work 2.1 Text Style Transfer Text Style Transfer (TST), the task of rewriting the text in a target style while preserving its se- mantic content and fluency, has garnered signifi- cant attention in the natural language processing community due to its potential applications in text generation (Fu et al., 2018). TST encompasses various subtasks, including formality style trans- fer (Wang et al., 2020; Lai et al., 2021), sentiment style transfer (Yu et al., 2021), authorship style transfer (Horvitz et al., 2024; Liu et al., 2024), and 1github.com/s-nlp/synthdetoxm detoxification (Dale et al., 2021; Atwell et al., 2022; Moskovskiy et al., 2022, 2024). With the advent of Large Language Models (LLMs), in-context learning methods have increas- ingly been utilized for TST and detoxification tasks. Suzgun et al. (2022) proposed a novel approach to TST by prompting LLMs and then reranking the generated texts based on three TST metrics: text similarity, target style strength, and fluency. Simi- larly, Reif et al. (2022) demonstrated the effective- ness of prompting GPT-3, a state-of-the-art LLM at the time, to rewrite texts in a desired style. 2.2 Text Detoxification Text Detoxification, a subtask of Text Style Trans- fer (TST), involves transforming an input text xi, identified as toxic through toxicity estimation mod- els, into a text yi that is non-toxic in style while maintaining semantic similarity and fluency. In this context, toxicity refers to language that is harmful, offensive, or inappropriate. Due to the lack of parallel training data, early research focused on unsupervised detoxification methods (dos Santos et al., 2018; Dale et al., 2021; Hallinan et al., 2023; Pour et al., 2023). For instance,(Logacheva et al., 2022) and APP- DIA (Atwell et al., 2022), has enabled the training of sequence-to-sequence models (Logacheva et al., 2022; Pour et al., 2023) that outperform most un- supervised approaches in terms of rewritten toxi- city, fluency, and semantic similarity. In parallel, Moskovskiy et al. (2024) explored the use of ac- tivation patching in LLMs to generate synthetic parallel detoxification data for English. Their re- sults demonstrated that training detoxification mod- els on this data yields performance comparable to models trained on manually annotated datasets in automatic evaluations, while achieving superior quality in human assessments.  2.3 Multilingual Text Style Transfer The scarcity of high-quality parallel multilingual detoxification data remains a major challenge in the field. Recently, new non-English parallel datasets have been introduced for various TST tasks, in- cluding a Bangla language parallel sentiment style transfer dataset (Mukherjee et al., 2023) and the extension of the GYAFC dataset to Portuguese, French, and Italian, resulting in XFORMAL (Bri- akou et al., 2021). Following the crowdsourcing pipeline introduced by (Logacheva et al., 2022), a parallel text detoxification dataset for Russian was collected (Moskovskiy et al., 2022). Later, using the similar data annotation pipeline, Dementieva et al. (2024) collected 1000 sentence pairs across nine languages, resulting in the MultiParaDetox dataset for a corresponding shared task on multi- lingual text detoxification. Furthermore, in a more recent work Dementieva et al. (2025), provide an in-depth analysis of toxicity characteristics across languages, exploring descriptive linguistic features that influence detoxification quality. Nevertheless, the size of MultiParaDetox is far from satisfactory with 1000 sentence pairs per lan- guage, only 400 of which are publicly available. The remaining 600 pairs comprised the test set for the multilingual text detoxification shared task (De- mentieva et al., 2024). Such relatively small dataset may be insufficient for training big multilingual lan- guage models for multilingual text detoxification. To bridge this gap, we present SynthDetoxM - a synthetic parallel detoxification corpus for four European languages, namely, German, Spanish, French, and Russian, with 4000 samples for each language. The dataset creationg pipeline presented in our work can easily be transferred to other lan- guages as well, drastically reducing the cost of annotation for parallel detoxification datasets. 3 Methodology In this section, we describe the pipeline introduced for collecting the multilingual parallel text detoxifi- cation dataset, SynthDetoxM. A general illustration of our approach is shown in Figure 1. 3.1 Data Collection To create SynthDetoxM, we begin by selecting sev- eral thousand non-parallel toxic texts from publicly available toxicity identification datasets. We fo- cus on four languages for SynthDetoxM: German, French, Spanish and Russian. From these datasets German Spanish French Russian 0 500 1000 1500 2000 2500 3000 3500 Aya 32B Aya 8B Command-R Gemma 27B Llama-3 70B Llama-3 8B Mistral Nemo Mistral Small Qwen 2.5 32B Figure 2: Number of accepted samples in the final Syn- thDetoxM dataset with respect to the LLM by language. we select only the texts that were marked as toxic by human annotators, excluding non-toxic exam- ples. In cases of multiple annotations, we retained the sample where the majority of annotators classi- fied the sentence as toxic. To enhance the final data quality, we employ sample-level filtering using the STA and SIM met- rics2 and we also apply data augmentation tech- niques utilizing the Perspective API (Lees et al., 2022). Since the API returns both toxicity scores and toxic spans, we further improve data quality by splitting the source texts into sentences and remov- ing those sentences that do not intersect with the detected toxic spans. This filtering process results in a larger dataset of toxic sentences, and we also split overly long inputs into separate examples. Russian For the Russian dataset, we use data from the Jigsaw Toxic Comments Classification Challenge (Kivlichan et al., 2020), Russian Lan- guage Toxic Comments (Belchikov, 2019) and Toxic Russian Comments (Semiletov, 2020). From these sources, we select only those rows labeled as toxic, resulting in more than 15,697 toxic texts. We then calculate the STA and SIM metrics, applying a threshold of 0.5 for filtering. After removing emo- jis, eliminating texts with fewer than five words or more than 30 words, and splitting the sentences using toxic spans from Perspective API, our final dataset consists of 15,697 texts. German For German, we use the toxicity iden- tification data from the GermEval 2021 shared 2Evaluation metrics are described in Section §4.3  task (Risch et al., 2021) and RP-Mod and RP- Crowd (Assenmacher et al., 2021) to create a dataset of 4,946 toxic texts. We apply the same fil- tering and augmentation pipeline as for the Russian dataset, but with a lower STA score threshold of 0.3. This resulted in a dataset of 4,946 texts, which exceeds the original size of the raw dataset. We at- tribute this increase to the higher median length of German sentences, which leads to a greater number of split texts. Spanish For Spanish, we utilize data from the Jigsaw Toxic Comments Classification Chal- lenge (Kivlichan et al., 2020) and the Clandestino dataset (Capozzi et al., 2021). This results in an initial dataset of 10,260 toxic texts. We apply the same filtering and augmentation pipeline as for the German dataset, using the same STA threshold of 0.3. This process yields a final dataset of 5,826 texts. French For French, we use the data from the Jigsaw Toxic Comments Classification Chal- lenge (Kivlichan et al., 2020) and the MLMA Hate Speech Corpus (Ousidhoum et al., 2019) to gener- ate a dataset of 5,424 toxic texts. As the toxicity classifier used in other languages does not support French, we instead use the Perspective API to get toxicity scores. After applying a STA score thresh- old of 0.25, we obtain a dataset of 4,310 sentences. 3.1.1 Parallel Data Generation Pipeline To generate parallel detoxification data, we use various open-source LLMs in a few-shot genera- tion setup. Specifically, we employed the following models: Qwen 2.5 32B by Qwen (Yang et al., 2024; Team, 2024); Command-R 32B by Cohere (Cohere, 2024); Gemma 2 27B by Google (Rivière et al., 2024); Aya Expanse in 32B and 8B versions by Co- here (Dang et al., 2024); Mistral Small 22B, Mis- tral Nemo 12B by Mistral AI (Mistral, 2024a,b); and Llama 3.1 70B and 8B models respectively, by Meta (Dubey et al., 2024). While not all these mod- els are explicitly designed for multilingual tasks, our experiments show that all of them support the languages considered in this work. 3.1.2 Few-Shot Example Mining To select the best toxic and non-toxic pairs for few- shot generation, we calculate the STA and SIM metrics for all sentences in Russian, German and Spanish from the multilingual toxicity detection dataset3. We then rank the top 10 sentencesbased on the following score: Score(xi; yi) = 1 − \x121 −STA(xi) 1 −STA(yi) · (1 −SIM(xi; yi)) \x13 where STA(xi) and STA(yi) represent the tox- icity scores for the original and detoxified exam- ples, respectively. SIM(xi; yi) is the cosine dis- tance between the embeddings of toxic and detoxi- fied sentences. This ranking criterion is chosen to ensure high- quality detoxification without altering the original meaning of the sentences. Since the sentences used for few-shot prompting have been annotated by hu- man experts, we expect the detoxification quality to be satisfactory. Additionally, the rewriting process of toxic words often leads to an expanded distance between toxic and non-toxic sentences, increasing the distinction in non-toxicity. To maximize both the distance and the distinction between the origi- nal and detoxified sentences, we select harder and more meaningful examples for few-shot prompting, which helps improve the detoxification process. For French, which is not represented in the Mul- tiParaDetox dataset, we used human annotators to detoxify 10 randomly chosen sentences from the existing non-parallel data. After generating detoxified examples, we per- form refusal filtering using a refusal classification model (see details in Appendix D). Additionally, we use a simple threshold-based non-detoxifiability metric, calculated by dividing the absolute reduc- tion in the STA score by the original STA score. We compare the resulting detoxifiability scores to a fixed threshold of 0.5. If the score falls be- low this threshold, the example is considered non- detoxifiable. After generating five detoxification datasets in each language using the selected models, we rank the sentences by their multiplied STA and SIM metrics and select the top-scoring examples. This metric helps mitigate issues such as refusal(where models refuse to generate text due to toxicity) and copy-paste generation (where the model generates the input toxic sentences without modification), as copy-paste generation typically results in a low STA score, while refusal leads to a low SIM score. 3hf.co/multilingual_toxicity_dataset  STAT↑STAD↑SIM↑STAD×SIM↑ German 0.389 0.853 0.793 0.675 Spanish 0.514 0.920 0.736 0.681 French 0.583 0.913 0.677 0.624 Russian 0.467 0.924 0.731 0.678 Table 2: Average toxicity levels across different lan- guages for source toxic (T) and generated detoxified (D) texts, along with similarity scores. STAT represents the toxicity level of the original text, while STAD corre- sponds to the detoxified text. In our work, for a text x the score STA(x) = 1 −P(toxic|x). 3.2 Final Composed Dataset After all preprocessing, cleaning and filtering steps we compose SynthDetoxM - a manually collected and synthetically paraphrased parallel detoxifica- tion dataset on 16,000 toxic and non-toxic text pairs for Spanish, German, Russian and French. We show the statistics of detoxification candidate acceptance with respect to each LLM Language- wise in Table 5 and Figure 2. According to the statistics, Qwen 2.5 generated the most preferrable detoxifications among other models. However, upon manual examination we noticed that Qwen tended to occasionally insert tokens of Chinese text into the generated text though was prompted to answer only on the language of the source text. Therefore, the strict reranking and filtering criteria of generated detoxification candi- dates is necessary. 3.3 Data Quality Evaluation Pipeline To evaluate the quality of our generated detoxifi- cation data in Russian, German, and Spanish, we use our dataset for training and compare the perfor- mance of models trained on SynthDetoxMwith those trained on the human-annotated parallel detoxifi- cation dataset, MultiParaDetox Dementieva et al. (2024). Due to its absence in the MultiParaDetox dataset, French is excluded from this comparison. A more detailed linguistic analysis of the dataset can be found in Appendix E. 4 Experimental Setup 4.1 Data Quality Tests To evaluate the efficacy of our SynthDetoxM for Ger- man, Spanish and Russian, we’ve trained a series of sequence-to-sequence models on different folds from the dataset. Since MultiParaDetox consists of only 400 pairs of toxic texts with their human- written non-toxic rephrasings, we split our created SynthDetoxM dataset into 10 chunks of 400 pairs for German, Spanish and Russian. We trained 10 mT0 models on different chunks of the dataset and evaluated their average performance on the Multi- ParaDetox test set. Additionally, we test if using both our SynthDetoxM and MultiParaDetox for train- ing would lead to improved performance. 4.2 Toxicity and Similarity of Synthetic Texts To further assess the quality of the generated data, we computed the STA and SIM scores using the Perspective API for Russian, German, Spanish, and French. These metrics were selected for their relevance to detoxification tasks and their ability to quantitatively assess our synthetic dataset. We also assessed the quality of the French subset of SynthDetoxM, as French is not represented in the MultiParaDetox dataset, and therefore cannot be evaluated through model training. The scores are presented in Figure 3 and Table 2. The results indicate that French achieves compa- rable automatic metric scores to other languages, suggesting that detoxification models trained on this data would perform similarly. Therefore, we hypothesize that the French subset of Syn- thDetoxM is a valuable addition to the dataset, en- abling the training of effective detoxification mod- els for French language processing tasks. 4.3 Automatic Evaluation Setup To assess the quality of the generated Spanish, Rus- sian, and German data, we follow the evaluation pipeline of Dementieva et al. (2024), developed for the multilingual text detoxification shared task.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=25, thumbnail=None, content='1 INTRODUCTION A longstanding goal of multi-agent artificial intelligence is the de- velopment of independent agents that can communicate using a shared language. Communication is especially necessary in “par- tially observable” settings, where each agent only has a limited view of the world and therefore benefits from sharing knowledge with other agents to achieve its goal. In particular, “social deduction” games are settings where each agent’s goal is to deduce informa- tion about the environment by communicating with other agents – requiring each player to learn how to parse messages from other players while effectively sharing important information needed for game completion. In this work, we study the hidden-role game of Among Us [18] as a specific instance of a challenging social deduction game to investigate the importance of communication, illustrated in Fig. 1. Hidden-role games [4, 19] are a class of environments where play- ers are split into an uninformed majority and a smaller informed hidden subteam, which we refer to as crewmates and imposters re- spectively. These two teams are adversaries, resulting in a zero-sum game, where the goal of the crewmates is to deduce the identity of imposters to vote them out. Unlike other popular hidden role games such as the game of Mafia [2], where statements from players are unfalsifiable, Among Us is based in a 2D embodied environment, allowing discussions and intuitions to be grounded in specific ob- servations. In the game, crewmates try to complete an assigned set of tasks scattered across the environment while imposters try to kill all crewmates. If a player reports the corpse of an eliminated crewmate – killed by an imposter – the game moves to a discussion phase with a free-form chat followed by a voting period, where all players vote to eject a suspected imposter. For crewmates, success in the discussion phase would mean correctly voting out the im- poster, while success for imposters means avoiding suspicion from the crewmates to continue staying in the game as long as possi- ble. This highlights the importance of communication during the discussion phase as crewmates need to learn to effectively utilize arXiv:2502.06060v1  [cs.AI]  9 Feb 2025  Figure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid environment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room. Here, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can perform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and green agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates the discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided by our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter. The example discussion shown on the right is based on a generated discussion from our trained models. the discussion phase to vote out imposters in an adversarial setting. For the rest of this paper, we study the game of Among Us from the perspective of crewmates attempting to perform tasks, identify imposters, and win the game. In multi-agent environments, an effective technique for training strong cooperative and competitive agents is multi-agent reinforce- ment learning (MARL), which enables artificial agents to achieve superhuman levels of performance in competitive games such as StarCraft [36], and cooperative games such as Overcooked [5, 31] and Hanabi [13]. However, in settings where communication in natural language is necessary, existing MARL techniques often struggle as they require large datasets of task-specific human com- munication data to perform on-par with humans [8]. This funda- mentally limits the agents’ ability to communicate at human-level and is not practical for learning in settings where these datasets do not readily exist. The game of Among Us falls into this category, where communication is necessary to reason and progress in the game. Therefore, we would like to find an approach that learns to communicate effectively and convincingly without requiring large amounts of task-specific human data. However, the major chal- lenge in learning to communicate without access to large amounts of human data is that novice agents do not have a strong signal for understanding the helpfulness of the messages they send (speak- ing) or for learning the meaning of messages from other players (listening). In particular, the sparse reward signal the agents receive when winning the game is not informative enough to reinforce high-quality discussions between agents. Our key insight is that we can leverage the agents’ instrumental goal of predicting useful information about the world – e.g., the identity of imposters – as a dense reward to provide a higher-quality signal that can enable more informative communication during the discussion phase and potentially higher performance policies. We propose an approach that rewards a message generated dur- ing the discussion phase based on how the other crewmates’ beliefs on the identity of the imposter changes. Each crewmate wants to send messages that help other crewmates be more certain about the true identity of the imposter. However, this only explains how to learn to “speak” assuming that the other agents can appropri- ately update their belief about the world given a message. We also need to ensure the agents know how to “listen” and update beliefs appropriately. To encourage this, we additionally add an imposter prediction signal to guide the agent’s learning to predict the true identity of the imposter after each message. By training agents to speak and listen effectively, we enable the agents to self-improve their discussion abilities. Further, to encourage listening and speak- ing in natural language during the discussion phase of the game, we tap into the power of large language models (LLMs), unspecial- ized models trained with large amounts of human language data. Specifically, we initialize crewmates as LLMs capable of communi- cating via natural language. Recent advances in foundation models have demonstrated some reasoning abilities [3, 27], including un- derstanding social scenarios [20], but even the strongest language models today are weak at self-critiquing [34] or performing theory of mind reasoning [33], limiting their ability to improve their lis- tening skills based on their own feedback. However, by training LLMs within our proposed framework of encouraging listening and speaking with auxiliary dense rewards for helping other crewmates vote out the correct imposter, we overcome this limitation, enabling the self-improvement of these models over time. To evaluate our framework, we analyze the success rate of crew- mates against both pretrained and adaptive imposters, and find that crewmates form a robust communication strategy. We find that our technique results in emergent behavior commonly found in real games of Among Us between humans, such as directly accusing  players and providing evidence to help other crewmates [22]. We also find that our augmentation to discussions results in two times higher success r'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=18, thumbnail=None, content='1 Introduction In recent years, the rise of Large Language Mod- els (LLMs) has made significant advances in AI- assisted coding and reshaped the domain of code generation and problem-solving (Zhao et al., 2023). Code generation assistants built on GPT-4 (Ope- nAI, 2024), Mistral (Jiang et al., 2023a), and Llama *Work done when working as a remote RA at QCRI. (Dubey et al., 2024), inter alia, have demonstrated unprecedented ability to understand, generate, and manipulate code across various programming lan- guages and problem domains. However, despite these advancements, significant challenges persist in generating code for complex programming tasks. Current state-of-the-art approaches in code gen- eration typically employ a dual-pass process (Shi et al., 2024; Jin et al., 2024b; Zhong et al., 2024; Levin et al., 2024). In the first pass, they use LLMs to generate an initial, fully/partially cor- rect version of the program. Then accordingly in the second pass, they apply external tool-based it- erative debuggers that leverage runtime compiler feedback or other diagnostic tools to refine and cor- rect the generated code. While this approach has shown promise, it necessitates numerous iterations of LLM-tool interactions, and importantly its ef- fectiveness is heavily dependent on the quality of the initial code generation—a process that contin- ues to present substantial difficulties. Therefore, in this paper, we present CODESIM, a novel multi- agent code generation framework that seamlessly synthesizes complex code solutions without exter- nal resources, while offering potential for further enhancement through minimal external debugging. Synthesizing programs even in the first pass, however, is fundamentally challenging, requiring a deep understanding of natural language processing, computer algorithms, data structures, and problem- solving strategies. These challenges are further compounded when attempting to generate code for competitive programming problems or advanced software engineering tasks, where adherence to spe- cific constraints or passing unit tests are paramount (Khan et al., 2023). While earlier code generation methods em- ployed direct approaches (Chen et al., 2021a), chain-of-thought reasoning (Wei et al., 2022a), syn- thesized test-case guidance (Chen et al., 2022a), retrieval-augmented generation (Parvez et al., arXiv:2502.05664v1  [cs.CL]  8 Feb 2025  Problem Check if in given list of numbers, are any two numbers closer to each other than given threshold. Sample I/O Passed? No Yes Planning\xa0 Agent Plans Coding\xa0 Agent Code Debugged Code Debugging\xa0 Agent Try to debug\xa0 d times If all d debugging\xa0steps\xa0fails to pass sample I/O,\xa0loop\xa0back\xa0 to Planning Agent\xa0p times.\xa0 Plan Plan OK? Yes No Generate\xa0 Exemplar\xa0 &\xa0Plan\xa0 Simulate & Verify Plan\xa0 Revise Plan Generate Code Problem Plan Simulate on Failed I/O & Debug Testing Feedback Plan Code Problem CodeSim Pipeline Planning Agent Coding Agent Debugging Agent Figure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent first generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and refined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses potential bugs through step-wise simulation across d trials. The entire process iterates p times. 2021), and various in-context exemplar-based strategies (Shum et al., 2023; Zhang et al., 2022), recent paradigms have shifted toward plan-based (Jiang et al., 2023b), sampling or tree-searching (Zhou et al., 2023), self-retrieval (Yasunaga et al., 2023), and diverse agent-based approaches (Zhang et al., 2024; Qian et al., 2024; Shinn et al., 2023; Huang et al., 2023; Dong et al., 2023b). Most recently, MapCoder (Islam et al., 2024a) proposes a multi-agent framework that implements agents emulating different stages of program syn- thesis such as recalling relevant examples, design- ing/planning, code generation, and testing/debug- ging. While this approach mimics a real devel- oper’s code generation cycle and shows improve- ments, it focuses solely on expanding steps without verifying the underlying hypotheses, with tests be- ing performed only during the debugging phase. Consequently, the resulting gains are limited and it also requires larger number of iterations (i.e., LLM API calls). To address these limitations, CODESIM—built upon planning, coding, and debugging agents—introduces a novel verification approach inspired by human problem-solving. By simulating input/output step-by-step, CODESIM verifies both the generated plans and performs internal debugging, mirroring how humans understand, visualize, and refine algorithms. This simulation- driven planning and debugging process ensures that each step is thoroughly evaluated, significantly enhancing both solution quality and efficiency. Figure 1 shows an overview of our proposed ap- proach, CODESIM and in Figure 2, we demonstrate how simulation assists in both plan verification and debugging, highlighting its crucial role in improving problem-solving accuracy. We evaluate CODESIM on seven popular pro- gramming synthesis benchmarks, including foun- dational tasks like HumanEval and MBPP, as well as challenging competitive problem-solving bench- marks such as APPS, and CodeContest. Our ex- periments leverage multiple LLMs, including Chat- GPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix- tral, showcasing significant improvements in their program synthesis capabilities. CODESIM consis- tently achieves state-of-the-art performances, often surpassing strong baselines like MapCoder. Ad- ditionally, our findings suggest that CODESIM’s performance can be further improved when inte- grated with external debugging tools, such as LDB (Zhong et al., 2024), highlighting a promising di- rection for future research in hybrid code gener- ation and debugging systems. Through detailed ablation studies, we provide valuable insights into CODESIM’s functionality. We will open-source our framework to support future research in AI-assisted programming and problem-solving. 2 Related Work Code Generation: Program synthesis has been a fundamental challenge in AI for decades (Manna and Waldinger, 1971). Early attempts with smaller language models centered on code generation by fine-tuning neural networks (Wang et al., 2021; Ah- mad et al., 2021; Feng et al., 2020; Parvez et al., 2018; Hellendoorn and Devanbu, 2017; Rabinovich et al., 2017; Yin and Neubig, 2017; Hindle et al.,  LLM Loop from 2 to 3 Simulation of tri(3) For n = 2:\xa0since it\'s even, use the formula\xa0 `tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0 3, 2 Initialize: 3 For n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0 \xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`. \xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case). \xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`. \xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`. \xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8 The list should contain the first `n + 1` numbers, so we need to include `tri(0)`. But\xa0`tri(0)` is not defined in the problem, but based on the example, it seems to be `1`. The simulation shows that the plan does not correctly handle the initialization and calculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0 The plan needs modification because it does not account for `tri(0)`. We need to start the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`. Plan Evaluation Tribonacci sequence is defined by the recurrence: \xa0 \xa0 tri(1) = 3 \xa0 \xa0 tri(n) = 1 + n / 2, if n is even. \xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd. Examples:\xa0tri(3) = [1, 3, 2, 8] Problem Initialize a List:\xa0Start with a list containing the base case: [3]. Iterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0 \xa0 \xa0 \xa0sequence Calculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list. Calculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance. Return the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence. Plan 0 end = max(10, 14) = 14 \xa0 \xa0- `10 % 2 == 0` (even, added to list) \xa0 \xa0- `11 % 2 != 0` (odd, not added) \xa0 \xa0- `12 % 2 == 0` (even, added to list) \xa0 \xa0- `13 % 2 != 0` (odd, not added) \xa0 \xa0- `14 % 2 == 0` (even, added to list) 10 The list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this. LLM Given two positive integers a and b, return the even digits between a\xa0and b, in ascending order. For example: \xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8] \xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8] \xa0 \xa0 generate_integers(10, 14) => [] Problem def generate_integers(a, b): \xa0 \xa0 start = min(a, b) \xa0 \xa0 end = max(a, b) \xa0 \xa0 even_numbers = [] \xa0 \xa0 for number in range(start, end + 1): \xa0 \xa0 \xa0 \xa0 if number % 2 == 0: \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number) \xa0 \xa0 return even_numbers Code Plan Validation Using Simulation Failed on test case:  generate_integers(10, 14) => [] Code\xa0 Execution LLM Simulation of\xa0generate_integers(10, 14) Loop over 10, 11, 12, 13, 14 start = min(10, 14) = 10 even_numbers =\xa0 10, 12 10, 12, 14 10, 12, 14 The code does not account for the requirement that the even digits themselves must be between `a` and `b`.\xa0 Debugging Note def generate_integers(a, b): \xa0 \xa0 start = min(a, b) \xa0 \xa0 end = max(a, b) \xa0 \xa0 even_numbers = [] \xa0 \xa0 for number in range(start, end + 1): \xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number \xa0 \xa0 \xa0 \xa0 for digit in str(number): \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end: \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit)) \xa0 \xa0 # Remove duplicates and sort the result \xa0 \xa0 return sorted(set(even_numbers)) Modified Code Code Execution Passed on Sample I/O Send it for final Evaluation Private Test \xa0Passed Debugging Using Simulation Figure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two different problems using CODESIM. 2016), while others explored leveraging data flow information or conversational intents to guide the process (Andreas et al., 2020; Yu et al., 2019). Var- ious prior approaches have also addressed code generation tasks using techniques such as data flow analysis and search-based methods (Li et al., 2022a; Parisotto and Salakhutdinov, 2017; Polozov and Gulwani, 2015; Gulwani, 2011). LLMs for Code: Various LLMs have been de- veloped for code synthesis (Austin et al., 2021; Chen et al., 2021b; Nijkamp et al., 2022; Fried et al., 2022; Allal et al., 2023; Li et al., 2022c). Re- cent open-source LLMs include the Llama family (Llama-2, CodeLlama, Llama3.1, etc.) (Roziere et al., 2023; Touvron et al., 2023), the Mistral family (Mistral, Mixtral, Codestral) (Jiang et al., 2023a), the Deepseek family (Deepseek Coder, Deepseek-V2, etc.) (Guo et al., 2024), MoTCoder (Li et al., 2023), and the Qwen family (Qwen 1.5, 2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which are capable of solving many basic problems. Prompting LLMs and Multi-Agent Code Gen- eration: LLM prompting can be summarized into three categories: retrieval (Yasunaga et al., 2023; Parvez et al., 2021, 2023), planning (Jiang et al., 2023b; Wei et al., 2022b), and debugging (Le et al., 2022; Chen et al., 2022b, 2023; Ridnik et al., 2024), in addition to direct code generation approaches. In contrast, our work combines all these paradigms and bridges their gaps (See Table 1). Recently, nu-  Approach Exemplars Plan Additional  test cases  generation Debug Simulation Reflexion ✗ ✗ ✔ ✔ ✗ Self-planning ✗ ✔ ✗ ✗ ✗ Analogical ✔ ✔ ✗ ✗ ✗ LATS ✗ ✔ ✔ ✔ ✗ MapCoder ✔ ✔ ✗ ✔ ✗ CodeSim ✔ ✔ ✗ ✔ ✔ Table 1: Comparison of code generation approaches. merous works have explored multi-agent code gen- eration and problem-solving, including (Kulesza et al., 2004; Jin et al., 2024a; Phan et al., 2024), as well as approaches highlighted in Section 1. However, CODESIM uniquely features simulation- driven planning and LLM-based debugging. More recently, external debugging has emerged to fur- ther boost performance, such as LDB (Zhong et al., 2024), ChatDebug (Levin et al., 2024), and MGDe- bugger (Shi et al., 2024), which serve as a second pass after our generation. 3 CODESIM Our goal is to develop a multi-agent code genera- tion approach capable of complex problem solving. Drawing inspiration from recent works like Map- Coder and ChatDev (in a different context), we de- vise the agents in CODESIM for planning, coding, and debugging. While these existing approaches fo- cus primarily on expanding steps without verifying underlying hypotheses, we address this limitation by introducing a novel verification approach. Our approach simulates input/output step-by-step, veri- fying generated plans and performing internal de- bugging, mirroring how humans understand, visu- alize, and refine in algorithm development. Below, we present our proposed model. 3.1 Planning Agent The first component of CODESIM is the Planning Agent. Given a problem description, the Planning Agent generates a single exemplar—a relevant prob- lem along with its plan and solution. This mimics the behavior of human programmers, who, when faced with a new problem, first recall a similar problem they’ve previously solved. This exemplar- based recall is crucial as it provides a starting point for constructing a solution plan. Instead of gener- ating multiple ungrounded exemplars as in Map- Coder, our agent focuses on only one at a time. We then instruct the LLM to generate an appro- priate plan. Once the plan is created, the LLM simulates (step-by-step) the solution with a sample input. If the simulation result does not match the expected output, the agent prompts the LLM to re- vise the plan. Otherwise, the plan is deemed valid. In the case of failure, the Planning Agent refines the plan. The complete prompts for the Planning Agent—including plan generation, verification, and refinement—are provided in the Appendix (Figure 5, 6, 7). 3.2 Coding Agent Next component is the Coding Agent, which takes the problem description and the plan generated by the Planning Agent as input. The role of this agent is to translate the plan into executable code that solves the given problem. Once the code is generated, CODESIM evaluates it using sample in- put/output test cases. If the code passes all sample tests, it is returned as the final solution. Otherwise, the code is handed over to the next agent for further refinement. Figure 8 in the Appendix provides the complete prompt used by the Coding Agent. 3.3 Debugging Agent The final component, the Debugging Agent, re- ceives the original problem, the plan from the Planning Agent, the code generated by the Coding Agent, and the execution (unit testing) log as input to debug the code. To identify bugs, instead of di- rectly prompting the LLMs, we uniquely leverage the simulation once again. The LLM is instructed specifically to simulate the code on inputs where it fails to produce the expected output, allowing it to trace the execution step by step and locate the error. Once the bug is identified, the LLM mod- ifies the code to resolve the issue. The complete prompt for the Debugging Agent is shown in the Appendix (Figure 9). Unlike other approaches such as LATS (Zhou et al., 2023), AgentCoder (Huang et al., 2023), and Reflexion (Shinn et al., 2023), our Debugging Agent does not require additional test case generation. The rationale behind excluding this phase is discussed in the Ablation Study 6.8. 3.4 Adaptive Iteration CODESIM employs an adaptive iteration starting with the Planning Agent, which generates plans for the given problem. These plans are passed to the Coding Agent, which translates them into code and tests against sample I/Os. If all tests pass, the code is returned; otherwise, it’s sent to the Debugging Agent. The Debugging Agent attempts to fix the  Basic Programming Problems LLM Approach HumanEval HumanEval ET EvalPlus Avg HumanEval MBPP MBPP-ET Avg MBPP Avg ChatGPT Direct 71.3% 64.6% 67.1% 67.7% 75.8% 52.6% 64.2% 65.9% CoT 70.7% 63.4% 68.3% 67.5% 78.3% 55.7% 67.0% 67.2% Self-Planning 70.7% 61.0% 62.8% 64.8% 73.8% 51.1% 62.5% 63.6% Analogical 67.1% 59.1% 59.1% 61.8% 69.3% 46.9% 58.1% 59.9% Self-collaboration 74.4% 56.1% - 65.3% 68.2% 49.5% 58.9% 62.1% LATS 83.8% - - - - - - - MapCoder 80.5% 70.1% 71.3% 74.0% 78.3% 54.4% 66.4% 70.2% CodeSim 86.0% 72.0% 73.2% 77.1% 86.4% 59.7% 73.1% 75.1% GPT4 Direct 80.1% 73.8% 81.7% 78.5% 81.1% 54.7% 67.9% 73.2% CoT 89.0% 61.6% - 75.3% 82.4% 56.2% 69.3% 72.3% Self-Planning 85.4% 62.2% - 73.8% 75.8% 50.4% 63.1% 68.4% Analogical 66.5% 48.8% 62.2% 59.1% 58.4% 40.3% 49.4% 54.3% Reflexion 91.0% 78.7% 81.7% 83.8% 78.3% 51.9% 65.1% 74.4% LATS 92.7% - - - - - - - MapCoder 93.9% 82.9% 83.5% 86.8% 83.1% 57.7% 70.4% 78.6% CodeSim 94.5% 81.7% 84.8% 87.0% 89.7% 61.5% 75.6% 81.3% GPT4o Direct 90.2% 81.1% 82.3% 84.5% 81.1% 55.9% 68.5% 76.5% CoT 90.9% 82.3% 87.2% 86.8% 82.9% 57.9% 70.4% 78.6% Self-Planning 89.0% 80.5% 84.1% 84.5% 82.60% 56.4% 69.50% 77.0% Analogical 88.4% 80.5% 83.5% 84.1% 75.10% 50.9% 63.00% 73.6% Reflexion 87.2% 81.1% 81.1% 83.1% 81.1% 56.7% 68.9% 76.0% LATS 88.8% 81.2% - 85.0% - - - - MapCoder 90.2% 80.5% 81.7% 84.1% 88.7% 59.2% 74.0% 79.0% CodeSim 95.1% 86.0% 87.2% 89.4% 90.7% 61.2% 76.0% 82.7% Table 2: Pass@1 results for different approaches on basic programming tasks. code for up to d attempts. If unsuccessful after d attempts, the process returns to the Planning Agent, restarting the cycle. Once code passing all sample I/Os is obtained, the cycle ends, returning the code as the final output solution for evaluation against hidden test cases. This entire process repeats for a maximum of p cycles if needed. Algorithm 9 in the Appendix summarizes our adaptive agent traversal. The algorithm’s complexity is O(pd). Appendix 12 provides a comprehensive example of how CODESIM solves a problem. 4 Experimental Setup 4.1 Datasets Following MapCoder, we evaluate CODESIM on five basic programming benchmarks i.e., Hu- manEval (Chen et al., 2021a), HumanEval- ET (Dong et al., 2023a), EvalPlus (Liu et al., 2023), MBPP) (Austin et al., 2021), and MBPP- ET (Dong et al., 2023a) and two competitive pro- gramming datasets i.e., APPS (Hendrycks et al., 2021), and CodeContest (Li et al., 2022b). For fair comparison, we collect all the datasets from the repository of the MapCoder. 4.2 Baselines and Metric To evaluate CODESIM, we compare it against state-of-the-art code generation approaches, includ- ing MapCoder, as well as several notable meth- ods: Direct, Chain of Thought (CoT) (Wei et al., 2022b), Self-Planning (Jiang et al., 2023b), Ana- logical Reasoning (Yasunaga et al., 2023), and Self-collaboration (Dong et al., 2023b). For sim- pler programming tasks, we include strong base- lines such as Reflexion (Shinn et al., 2023) and LATS (Zhou et al., 2023). We exclude AgentCoder (Huang et al., 2023) due to reproducibility issues (discussed in Appendix 10). For fair comparison, our evaluation utilizes ChatGPT (gpt-3.5-turbo- 1106), GPT-4 (gpt-4-1106-preview) from OpenAI, alongside open-source LLMs such as Gemma2- 9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1- 70B. For basic programming tasks, we report next- generation performance with additional evaluations using GPT-4o (gpt-4o-2024-08-06). We adopt the  widely used pass@1 metric, where a model is deemed successful if its sole predicted solution is correct. 4.3 Reproducibility We aim to contribute to the NLP community by open-sourcing all of our code along with result logs, enabling others to reproduce our findings. For simple programming, we set the maximum number of planning tries to p = 5 and debugging tries to d = 5. For the competitive problem solving, we used p = 3 and d = 3 by default except for the CodeContest with GPT-4 where p = 3, d = 5. 5 Results 5.1 Basic Code Generation In Table 2, we evaluate the model performances on simple code generation tasks. Overall, CODESIM demonstrates consistently superior per- formance compared to all other baselines across all datasets and LLMs. Notably, CODESIM achieves top scores with GPT-4o, reaching 95.1% on Hu- manEval, 87.2% on EvalPlus, and 90.7% on MBPP, resulting in an impressive 82.7% overall average and their new state-of-the-art (SoTA) re- sults. This represents a significant improvement over the next best method, MapCoder, which scores 79.0% on average with GPT-4o. CODESIM’s effec- tiveness is consistent across different model vari- ants, outperforming other approaches with Chat- GPT (75.1% avg) and GPT-4 (81.3% avg) as well. The method’s robust performance across di- verse datasets, including the challenging MBPP- ET where it achieves 61.5% with GPT-4, under- scores its versatility in handling various program- ming tasks. These results strongly indicate that CODESIM’s simulation-driven planning and debug- ging approach marks a substantial advancement in code generation and problem-solving capabilities, as it consistently outperformed other baselines. 5.2 Competitive Problem Solving In Table 3, we evaluate performance on complex, contest-level code generation tasks. CODESIM de- livers significant improvements over other base- lines in solving complex contest-level code genera- tion tasks. With GPT-4, CODESIM reaches a strong 29.1% on CodeContests and 22.0% on APPS, marking a consistent edge over MapCoder’s 25.3% average. The performance gains are even more pro- nounced with ChatGPT, where CODESIM achieves a 16.4% on CodeContests, and 12.0% on APPS re- sulting 14.2% overall, outperforming MapCoder’s 12.0%. These results highlight CODESIM’s ability to handle the complexity of contest-level problems more effectively, especially through its simulation- driven approach. LLM Contest-Level Problems Approach CodeContest APPS Avg ChatGPT Direct 5.5% 8.0% 6.8% CoT 6.1% 7.3% 6.7% Self-Planning 6.1% 9.3% 7.7% Analogical 7.3% 6.7% 7.0% MapCoder 12.7% 11.3% 12.0% CodeSim 16.4% 12.0% 14.2% GPT4 Direct 12.1% 12.7% 12.4% CoT 5.5% 11.3% 8.4% Self-Planning 10.9% 14.7% 12.8% Analogical 10.9% 12.0% 11.5% MapCoder 28.5% 22.0% 25.3% CodeSim 29.1% 22.0% 25.6% Table 3: Pass@1 results for different approaches on CodeContest and APPS dataset. Open-Source LLMs LLM Approach HumanEval HumanEval ET EvalPlus Avg Gemma2-9B Direct 64.0% 56.1% 56.1% 58.7% CoT 31.7% 26.2% 27.4% 28.4% Reflexion 62.2% 56.7% 55.5% 58.1% CodeSim 82.9% 72.0% 72.6% 75.8% Mixtral8x7B Direct 20.7% 18.9% 18.9% 19.5% CoT 46.3% 42.1% 39.0% 42.5% Reflexion 34.1% 32.9% 29.9% 32.3% CodeSim 75.0% 61.6% 61.0% 65.9% LLaMa3.1-8B Direct 42.1% 38.4% 39.0% 39.8% CoT 48.8% 42.1% 43.3% 44.7% Reflexion 43.9% 31.1% 29.9% 35.0% CodeSim 79.9% 65.2% 61.2% 68.8% LLaMa3.1-70B Direct 57.3% 50.6% 52.4% 53.4% CoT 75.6% 67.7% 70.1% 71.1% Reflexion 73.8% 64.0% 68.3% 68.7% CodeSim 90.2% 73.8% 76.2% 80.1% Table 4: Pass@1 results for different approaches using Open-source LLMs. 5.3 Performance Across Open-source LLMs To further demonstrate CODESIM’s generaliza- tion capability, we evaluate its performance with open-source LLMs, including Gemma2-9B, Mix- tral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As shown in Table 4, CODESIM consistently outper- forms all other methods across these models. On LLaMa3.1-70B, CODESIM achieves an accuracy  of 90.2% on HumanEval and 76.2% on EvalPlus, with an average of 80.1%, closely matching GPT- 4o’s performance. Due to the complex prompting scheme of MapCoder, open-source LLMs often struggle to generate output in the correct format. Therefore, we exclude MapCoder from this experi- ment. On the other hand, Reflexion shows minimal improvement in accuracy. These results highlight CODESIM’s strong generalization ability across various LLM architectures, even on smaller mod- els like Gemma2-9B that achieves a notable avg accuracy of 75.8%. 6 Ablation Studies and Analyses 6.1 Impact of Different Agents Our primary contributions are two folds: (i) the simulation-guided plan verification step within the Planning Agent and (ii) the bug fixing process through simulation in Debugging Agent. To evalu- ate the significance of these components, we ablate these two parts of our approach and present the results in Table 5. The findings confirm that both components contribute significantly. Simulation  Driven  Planning Debugging  using  Simulation Pass@1 Performance  Drop ✗ ✗ 92.1% 3.2% ✔ ✗ 93.3% 1.9% ✗ ✔ 93.3% 1.9% ✔ ✔ 95.1% - Table 5: Pass@1 results for different versions of CODESIM (by using GPT4o on HumanEval dataset). 6.2 Fine-grained Analysis of the Impact of Simulation Table 6 presents the impact of incorporating Simulation in CODESIM. The results show that CODESIM consistently outperforms other ap- proaches across both simple and multi-agent set- tings, demonstrating superior performance with both open-source and proprietary LLMs. This high- lights the effectiveness of Simulation in enhancing problem-solving efficiency within our pipeline. 6.3 Impact of Varying Programming Languages To evaluate the performance of CODESIM across various programming languages, we utilized the LLM Method Approach HumanEval  (Pass@1) Impact of using  Simulation GPT4o Simpler Direct 90.2% 5.4% CoT 90.9% 4.6% Self-Planning 89.0% 6.9% Analogical 88.4% 7.6% Reflexion 91.0% 4.5% LATS 88.8% 7.1% Multi-Agent MapCoder 90.2% 5.4% CodeSim 95.1% - LLaMa3.1-70B Simpler Direct 57.3% 57.4% CoT 75.6% 19.3% Reflexion 73.8% 22.2% Multi-Agent CodeSim 90.2% - Table 6: Impact of using Simulation. xCodeEval (Khan et al., 2023) dataset. The ex- perimental results, presented in Table 7, demon- strate that CODESIM maintains strong performance across different programming languages, highlight- ing its versatility and effectiveness. Dataset Language Direct MapCoder CodeSim xCodeEval Python 17.9% 27.4% 27.4% C 9.4% 21.7% 24.5% Rust 12.3% 21.7% 23.6% Table 7: Pass@1 results for different programming lan- guages from xCodeEval dataset by using ChatGPT. 6.4 Use of External Debugger LLM LDB Reflexion MapCoder CodeSim ChatGPT without 88.0% 90.2% 95.1% with 89.6% 92.1% 96.3% GPT-4o without 88.0% 90.2% 95.1% with 94.5% 91.5% 97.6% Table 8: Pass@1 results for different approaches using an external debugger. The performance of CODESIM can be further en- hanced by incorporating an external debugger in the second pass. We experiment with LDB as the external debugger on HumanEval dataset in Table 8. We use the output code from the most competitive first-pass generation methods, includ- ing CODESIM, Reflexion, and MapCoder, using GPT-4o as the backbone. These seed programs are then passed to LDB, which was tested with two different LLMs: ChatGPT and GPT-4o. As can be seen, CODESIM achieves 95.1% accuracy in  the first pass with GPT-4o, surpassing Reflexion’s second pass performance of 94.5%. By utilizing LDB with GPT-4o, CODESIM achieves a second pass accuracy of 97.6%, setting a new state-of-the- art result for a dual-pass approach. In addition, we note that the second pass with LDB consumes 39K more tokens in Reflexion compared to our approach, highlighting the efficiency of CODESIM. 6.5 Qualitative Example We also conduct a qualitative analysis to better understand how CODESIM improves performance across various datasets. Figure 2 demonstrates how CODESIM enhances the plan through simulation and assists in debugging the code using the same technique. A complete example, including LLM output, is provided in Appendix 12. 6.6 Impact of p and d CODESIM includes two key hyperparameters: the maximum number of planning steps (p) and the maximum number of debugging steps (d). By vary- ing these parameters, we plot the results in Figure 3, which shows a proportionate improvement in performance. It is important to note that higher val- ues of p and d lead to more API calls and increased token consumption, allowing users to adjust these parameters to balance between accuracy and cost. Figure 3: Pass@1 results by varying maximum number of planning, p and maximum number of debugging, d. 6.7 Impact of Number of Sample I/Os The HumanEval dataset has an average of only 2.82 sample I/Os per example, which is a relatively small number for deriving meaningful insights. In this ablation, we augment the dataset by adding 5 more sample I/Os from the HumanEval-ET dataset. This augmentation increases performance notably, leading to 89% accuracy with ChatGPT, a 3.5% improvement over previous results, 86%. 6.8 Impact of Synthesizing Additional I/O Increasing the number of sample I/Os for testing can enhance the overall performance of our ap- proach, as indicated in 6.7. Based on this insight, we use a self-consistency (Wang et al., 2023a) method to generate additional test cases. We in- struct the LLM to generate five more test cases for each problem, covering both basic and edge cases. The LLM is called twice, and we select the test cases that are present in both responses. How- ever, this approach results in a performance decline. With ChatGPT we achieve 78% accuracy—a 9.3% decrease from the original 86%. This indicates that generating additional I/Os is a non-trivial task that may negatively impact final outcomes. 6.9 API Call and Token Analysis We compare the API calls and token consumption of our approach with the previous state-of-the-art method, MapCoder (Islam et al., 2024a), as shown in Table 9. The results reveal that CODESIM not only improves performance but also reduces token consumption. On average, CODESIM uses 4.13 thousand fewer tokens while achieving a 7.1% in- crease in accuracy, proving that CODESIM is more efficient in both accuracy and token usage com- pared to MapCoder. 6.10 Error Analysis and Challenges Although CODESIM demonstrates strong performance compared to other methods, it faces challenges in specific algorithmic domains. The APPS dataset (Hendrycks et al., 2021) includes problems with three levels of difficulty: (i) Introductory, (ii) Interview, and (iii) Competition. Figure 4 illustrates the performance of different approaches based on difficulty level. The results indicate that for introductory and interview-level problems, CODESIM does not surpass MapCoder when using ChatGPT. Additionally, when using GPT-4, CODESIM struggles to outperform MapCoder on interview-level problems. Upon manual review, we observe that for more complex issues, such as dynamic programming (DP), CODESIM encounters difficulties in constructing the DP table.  LLM Dataset Average API Calls ↓ Average Token Consumption (K) ↓ Token Reduction over  MapCoder (k) ↑ Acc Gain over  MapCoder ↑ MapCoder CodeSim MapCoder CodeSim ChatGPT HumanEval 17 7 10.41 5.48 4.93 6.8% MBPP 12 6 4.84 4.24 0.60 10.3% APPS 21 15 26.57 19.20 7.37 6.2% CodeContest 23 16 34.95 24.02 10.92 29.1% GPT4 HumanEval 15 5 12.75 5.15 7.60 0.6% MBPP 8 5 4.96 5.21 -0.26 7.9% APPS 19 13 31.80 23.18 8.61 0.0% CodeContest 19 17 38.70 41.66 -2.95 2.1% GPT4o HumanEval 9 4 6.63 3.84 2.79 5.4% MBPP 9 5 6.10 4.43 1.67 2.3% Average 15.2 9.3 17.77 13.64 4.13 7.1% Table 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens used (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for downward symbol (↓). Figure 4: Performance of different approaches across different difficulty levels on the APPS dataset. 7 Conclusion and Future Work In this paper, we introduce CODESIM, a novel framework that leverages the multi-agent prompting capabilities of LLMs for efficient code generation in problem-solving tasks. CODESIM integrates three agents—planning, coding, and debugging—to effectively solve programming problems. It harnesses the power of simulation for plan verification and debugging, significantly outperforming existing state-of-the-art approaches by a wide margin. Future work will focus on extending this approach to other domains such as mathematical reasoning and question answering broadening its scope and impact. 8 Limitations In Section 6.4, we observe that utilizing an exter- nal debugger can further enhance our results. Our next research goal is to achieve the best perfor- mance without relying on any external tools. Al- though we have reduced token consumption com- pared to the previous state-of-the-art method, Map- Coder, it still remains high compared to the di- rect prompting approach. Direct prompting con- sumes an average of 560 tokens, while our method consumes around 13,640 tokens. This indicates room for enhancement in efficiency. While in this work, we generate the exemplars with the LLMs themselves, in general they are found from exter- nal resource (Parvez and Chang, 2021). Although this has its own challenges such as noisy retrievals (Wang et al., 2023b), inconsistent generations (Is- lam et al., 2024b; Parvez, 2024; Sadat et al., 2023) this direction could also be a possible improvement. Another limitation is the use of external tools for assistance during simulation. We have not explored this avenue in the current research, leaving it for future work. Additionally, more sample I/Os could potentially improve performance, and our future research will focus on investigating methods for generating accurate additional I/Os. Moreover, we would like to note that in this work, we focus solely on generated code’s correctness and did not study its optimizations such as test-time, memory. Fi- nally, it is advisable to run the machine generated code inside a sandbox to avoid any potential risks. References Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation. arXiv preprint arXiv:2103.06333. Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz  Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santacoder: don’t reach for the stars! arXiv preprint arXiv:2301.03988. Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc- Govern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, An- drei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-oriented dialogue as dataflow synthesis. Trans- actions of the Association for Computational Linguis- tics, 8:556–571. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluat- ing large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large lan- guage models trained on code. arXiv preprint arXiv:2107.03374. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. Yihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li, Ge Li, and Zhi Jin. 2023a. Codescore: Evaluating code generation by learning code execution. arXiv preprint arXiv:2301.09043. Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b. Self-collaboration code generation via chatgpt. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and et al. 2024. The llama 3 herd of models. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi- aocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural lan- guages. In Findings of the Association for Computa- tional Linguistics: EMNLP 2020, pages 1536–1547. Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999. Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. ACM Sigplan Notices, 46(1):317–330. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming–the rise of code intelligence. arXiv preprint arXiv:2401.14196. Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks the best choice for model- ing source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineer- ing, ESEC/FSE 2017, pages 763–773, New York, NY, USA. ACM. Dan Hendrycks, Steven Basart, Saurav Kadavath, Man- tas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938. Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. 2016. On the naturalness of software. Commun. ACM, 59(5):122–131. Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023. Agentcoder: Multi-agent- based code generation with iterative testing and opti- misation. arXiv preprint arXiv:2312.13010. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day- iheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186.  Md. Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. 2024a. MapCoder: Multi-agent code generation for competitive problem solving. In Proceedings of the 62nd Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 4912–4944, Bangkok, Thailand. Association for Computational Linguistics. Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, and Md Rizwan Parvez. 2024b. Open-RAG: Enhanced retrieval aug- mented reasoning with open-source large language models. In Findings of the Association for Compu- tational Linguistics: EMNLP 2024, pages 14231– 14244, Miami, Florida, USA. Association for Com- putational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a. Mistral 7b. Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023b. Self-planning code genera- tion with large language model. arXiv preprint arXiv:2303.06689. Dongming Jin, Zhi Jin, Xiaohong Chen, and Chun- hui Wang. 2024a. Mare: Multi-agents collabora- tion framework for requirements engineering. arXiv preprint arXiv:2405.03256. Haolin Jin, Zechao Sun, Yiheng Yang, and Huaming Chen. 2024b. Rgd: Multi-llm based agent debug- ger via refinement and generation guidance. arXiv preprint arXiv:2410.01242. Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. 2023. xcodeeval: A large scale multi- lingual multitask benchmark for code understanding, generation, translation and retrieval. arXiv preprint arXiv:2303.03004. Uirá Kulesza, Alessandro Garcia, Carlos Lucena, and Paulo Alencar. 2004. A generative approach for multi-agent system development. In International Workshop on Software Engineering for Large-Scale Multi-agent Systems, pages 52–69. Springer. Md Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai- ful Bari, Mizanur Rahman, Mohammad Abdul- lah Matin Khan, Haidar Khan, Israt Jahan, Amran Bhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty, and Jimmy Huang. 2024. A sys- tematic survey and critical review on evaluating large language models: Challenges, limitations, and recom- mendations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Process- ing, pages 13785–13816, Miami, Florida, USA. As- sociation for Computational Linguistics. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328. Kyla Levin, Nicolas van Kempen, Emery D Berger, and Stephen N Freund. 2024. Chatdbg: An ai-powered debugging assistant. arXiv preprint arXiv:2403.16354. Jingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot- coder: Elevating large language models with modular of thought for challenging programming tasks. arXiv preprint arXiv:2312.15960. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022a. Competition-level code generation with alphacode. Science, 378(6624):1092–1097. Yujia Li, David Choi, Junyoung Chung, Nate Kush- man, Julian Schrittwieser, Rémi Leblond, Tom Ec- cles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas- son d’Autume, Igor Babuschkin, Xinyun Chen, Po- Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022b. Competition-level code generation with al- phacode. Science, 378(6624):1092–1097. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas- son d’Autume, Igor Babuschkin, Xinyun Chen, Po- Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Fre- itas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c. Competition-level code generation with alphacode. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling- ming Zhang. 2023. Is your code generated by chat- GPT really correct? rigorous evaluation of large lan- guage models for code generation. In Thirty-seventh Conference on Neural Information Processing Sys- tems. Zohar Manna and Richard J. Waldinger. 1971. To- ward automatic program synthesis. Commun. ACM, 14(3):151–165. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474. OpenAI. 2024. Gpt-4 technical report. Emilio Parisotto and Ruslan Salakhutdinov. 2017. Neu- ral map: Structured memory for deep reinforcement learning. arXiv preprint arXiv:1702.08360.  Md Rizwan Parvez. 2024. Evidence to generate (e2g): A single-agent two-step prompting for context grounded and retrieval augmented reasoning. arXiv preprint arXiv:2401.05787. Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and sum- marization. arXiv preprint arXiv:2108.11601. Md Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2018. Building language mod- els for text with named entities. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 2373–2383, Melbourne, Australia. Association for Computational Linguistics. Md Rizwan Parvez and Kai-Wei Chang. 2021. Evalu- ating the values of sources in transfer learning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 5084–5116, Online. Association for Computa- tional Linguistics. Md Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, and Kai-Wei Chang. 2023. Retrieval enhanced data augmentation for question answer- ing on privacy policies. In Proceedings of the 17th Conference of the European Chapter of the Associa- tion for Computational Linguistics, pages 201–210, Dubrovnik, Croatia. Association for Computational Linguistics. Huy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui. 2024. Hyperagent: Generalist software engineering agents to solve coding tasks at scale. arXiv preprint arXiv:2409.16299. Oleksandr Polozov and Sumit Gulwani. 2015. Flash- meta: A framework for inductive program synthesis. In Proceedings of the 2015 ACM SIGPLAN Interna- tional Conference on Object-Oriented Programming, Systems, Languages, and Applications, pages 107– 126. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ChatDev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15174–15186, Bangkok, Thailand. Association for Computational Linguistics. Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract syntax networks for code generation and semantic parsing. CoRR, abs/1704.07535. Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code generation with alphacodium: From prompt engineering to flow engineering. arXiv preprint arXiv:2401.08500. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh Menon, Md Parvez, and Zhe Feng. 2023. Delu- cionQA: Detecting hallucinations in domain-specific question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 822–835, Singapore. Association for Computational Linguistics. Yuling Shi, Songsong Wang, Chengcheng Wan, and Xiaodong Gu. 2024. From code to correctness: Clos- ing the last mile of code generation with hierarchical debugging. arXiv preprint arXiv:2410.01215. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Re- flexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. Kashun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic prompt augmentation and selection with chain-of-thought from labeled data. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12113–12139, Singapore. Asso- ciation for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models. Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code un- derstanding and generation. arXiv preprint arXiv:2109.00859. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023b. Learning to fil- ter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022a. Chain-of-thought prompting elicits rea- soning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits rea- soning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837.  Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, and Denny Zhou. 2023. Large language models as ana- logical reasoners. arXiv preprint arXiv:2310.01714. Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. CoRR, abs/1704.01696. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin- cent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019. CoSQL: A conversational text-to-SQL challenge towards cross- domain natural language interfaces to databases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 1962– 1979, Hong Kong, China. Association for Computa- tional Linguistics. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024. CodeAgent: Enhancing code generation with tool-integrated agent systems for real-world repo- level coding challenges. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13643– 13658, Bangkok, Thailand. Association for Compu- tational Linguistics. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompt- ing in large language models. arXiv preprint arXiv:2210.03493. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223. Li Zhong, Zilong Wang, and Jingbo Shang. 2024. De- bug like a human: A large language model debugger via verifying runtime execution step by step. In Find- ings of the Association for Computational Linguis- tics: ACL 2024, pages 851–870, Bangkok, Thailand. Association for Computational Linguistics. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Lan- guage agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406.  Appendix 9 Algorithm of CODESIM Algorithm 1 shows the pseudo-code of our prompt- ing technique. Algorithm 1 CODESIM 1: p ←maximum number of planning steps 2: d ←maximum number of debugging steps 3: 4: for i ←1 to p do 5: # Start of Planning Agent 6: plan ←GeneratePlan(problem) 7: feedback ←ValidatePlan(problem, plan) 8: if feedback is negative then 9: plan ←RefinePlan(problem, plan, feedback) 10: end if 11: # End of Planning Agent 12: 13: # Start of Coding Agent 14: code ←GenerateCode(problem, plan) 15: passed, log ←test(code, sample_io) 16: if passed then 17: Return code 18: else 19: # Start of Debugging Agent 20: for j ←1 to d do 21: code ←DebugCode( 22: problem, 23: plan, 24: code, 25: log 26: ) 27: 28: passed, log ←test(code, sample_io) 29: if passed then 30: Return code 31: end if 32: end for 33: # End of Debugging Agent 34: end if 35: # End of Coding Agent 36: 37: end for 38: Return code 10 Exclusion of AgentCoder We have not included AgentCoder (Huang et al., 2023) in our comparison due to reproducibility is- sues which undoubtedly plays a critical role in fair comparison as indicted in Laskar et al. (2024), as we were unable to replicate their results. In our at- tempts to reproduce their work on the HumanEval benchmark using ChatGPT, we achieved 56.7% accuracy after four iterations, consuming 11.9 mil- lion tokens. When using GPT-4, we attained only 17.7% accuracy after two iterations, with 10.4 mil- lion tokens consumed. The token consumption in both cases is significantly higher compared to Map- Coder (1.7 million tokens with ChatGPT and 2.1 million with GPT-4) and CODESIM(0.89 million tokens in ChatGPT and 0.85 million in GPT-4). These two experiments resulted in a cost of ap- proximately $500 USD, but we were still unable to come close to AgentCoder’s reported claims of 79.9% accuracy with ChatGPT and 96.3% with GPT-4. Furthermore, we found unaddressed issues on their GitHub page (link) related to reproducibility. Additionally, for the MBPP dataset, they used all test cases as public test cases (link), which deviates from standard practices. As a result, we did not consider those results in our comparison either. 11 Details Promptings of CODESIM The Planning Agent interacts with the LLM three times to generate a plan. In the first API call, it instructs the LLM to comprehend the problem, gen- erate an example problem, recommend a suitable algorithm, and finally produce the plan (Figure 5). In the second API call, the LLM is instructed to verify the plan through simulation (Figure 6). If the plan is satisfactory, it is returned by the agent. Otherwise, the LLM is called again to refine the plan based on the feedback from the simulation (Figure 7). The next step involves the Coding Agent, which receives the plan from the Planning Agent and uses the prompt outlined in Figure 8 to generate code. If the code fails to pass the sample input/output, CODESIM activates its final agent, the Debugging Agent, using the prompt shown in Figure 9. These figures also include the rationale behind the inclusion of each sentence in the prompt. 12 Example Problem We present a complete example of problem solving using CODESIM below:  You are a programmer tasked with generating appropriate plan to solve a given problem  using the **{language}** programming language. ## Problem {problem} **Expected Output:** Your response must be structured as follows: ### Problem Understanding - Think about the original problem. Develop an initial    understanding about the problem. ### Recall Example Problem Recall a relevant and distinct problems (different from problem  mentioned above) and - Describe it - Generate {language} code step by step to solve that problem - Discuss the algorithm to solve this problem - Finally generate a planning to solve that problem ### Algorithm to solve the original problem - Write down the algorithm that is well suited for the original   problem - Give some tutorials to about the algorithm for example: \xa0 \xa0 - How to approach this type of algorithm \xa0 \xa0 - Important things to consider ### Plan - Write down a detailed, step-by-step plan to solve the    **original problem**. -------- **Important Instruction:** - Strictly follow the instructions. - Do not generate code. Coding Agent: Prompt for Plan Generation Allow the LLM sufficient time and space to process and comprehend the problem. Rather than providing the LLM with a predefined example, we leverage its inherent knowledge to independently recall a relevant problem that can aid in solving the original issue. Guide the LLM to determine the type of algorithm suitable for solving the problem, and request a tutorial or explanation on how to apply it. Lastly, instruct the LLM to generate a detailed plan to solve the problem. Force the LLM to follow the instructions. Figure 5: Planning Agent: Prompt for Plan Generation. You are a programmer tasked with verifying a plan to solve a given problem using the  **{language}** programming language. ## Problem {problem} ### Plan {plan} **Expected Output:** Your response must be structured as follows: ### Simulation - Take a sample input and apply plan step by step to get the output. - Compare the generated output with the sample output to verify if    your plan works as expected. ### Plan Evaluation - If the simulation is successful write **No Need to Modify Plan**. - Otherwise write **Plan Modification Needed**. Coding Agent: Prompt for Plan Verification with Simulation To validate a plan, a human programmer typically simulates it with sample input, generates the corresponding output, and compares the generated output to the expected result. At this stage, we\xa0 instruct the LLM to perform this process as well, ensuring it follows the same steps like human programmer to confirm the validity of the plan. Finally, tell the LLM to write done it\'s decision in a specific format. Figure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.  You are a programmer tasked with generating appropriate plan to solve a given problem  using the **{language}** programming language. You already have a wrong plan.  Correct it so that it can generate correct code. ## Problem {problem} ### Plan {plan} ## Plan Critique {plan_verifical_report_from_previous_step} **Expected Output:** Your response must be structured as follows: ### New Plan - Write down a detailed, step-by-step modified plan to solve the **original problem**. - Ensure each step logically follows from the previous one. -------- **Important Instruction:** - Your response must contain only the plan. - Do not add any explanation. - Do not generate code. Coding Agent: Prompt for Plan Refinement Provide all the details and instruct the LLM to generate revised plan. Figure 7: Planning Agent: Prompt for Plan Refinement. You are a programmer tasked with solving a given problem using the **{language}**  programming language. See the plan to solve the plan and implement code to solve it. ## Problem {problem} ### Plan {plan} -------- **Important Instruction:** - Do not add any explanation. - The generated **{language}** code must be inside a triple backtick (```) code block. Coding Agent: Prompt for Code Generation Figure 8: Coding Agent: Prompt for Code Generation.  An Example from HumanEval dataset for demonstrating how CODESIM works Input for Planning: 1 You are a programmer tasked with generating appropriate plan to solve a given problem using the Python3 programming language. ## Problem def generate_integers(a, b): """ Given two positive integers a and b, return the even digits between a and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] """ Expected Output: Your response must be structured as follows:  You are a programmer who has received a solution of a problem written in **{language}**  that fails to pass certain test cases. Your task is to modify the code in such a way so  that it can pass all the test cases. Do not generate same code. ## Problem {problem} ### Plan {plan} ### Buggy Code {code} ### Test Report {test_log} **Expected Output:** Your response must be structured as follows: ### Simulation with failed test case To detect where is the bug: - Take a sample test case where it fails. - Take the input go through each step according to the plan - You will get a output that must be different from the expected output.  ### Debugging Notes - Based on this simulation detect any of the following cases:     - Plan is wrong     - Plan is correct but plan to code generation is wrong. - Finally, discuss how to correct this code. ### Modified Code # Your corrected code, with comments explaining each correction. -------- **Important Instruction:** - Strictly follow the instructions. - Do not add testing code for example assert statement in your code. - Do not be overconfident that the generated code is correct. It is wrong. - The modified **{language}** code must be enclosed within triple backticks (```). - Your response must contain **Simulation with failed test case**, **Debugging Notes**,    and **Modified Code** section. Debugging Agent: Prompt for Debugging To detect where a program fails, a human programmer typically selects a sample test case, simulates it line by line, and observes the output at each step. This process helps in identifying and debugging issues within the code. Similarly, we have instructed the LLM to select a test case where the code fails, simulate it, identify the bug through step-by-step analysis, and then debug the issue accordingly. Based on the simulation output we have instruct the LLM to write down how to correct the code. Finally, instruct the LLM to generate the modified code. Figure 9: Debugging Agent: Prompt for Debugging. ### Problem Understanding - Think about the original problem. Develop an initial understanding about the prob- lem. ### Recall Example Problem Recall a relevant and distinct problems (different from problem mentioned above) and - Describe it - Generate Python3 code step by step to solve that problem - Discuss the algorithm to solve this problem - Finally generate a planning to solve that problem ### Algorithm to solve the original problem - Write down the algorithm that is well suited for the original problem - Give some tutorials to about the algorithm for example:  - How to approach this type of algorithm - Important things to consider ### Plan - Write down a detailed, step-by-step plan to solve the original problem. ——– Important Instruction: - Strictly follow the instructions. - Do not generate code. Response from Planning: 1 ### Problem Understanding The problem requires generating a list of even integers between two given positive integers, a and b, inclusive. The list should be in ascending order, regardless of whether a is less than or greater than b. If there are no even integers in the range, the function should return an empty list. ### Recall Example Problem Example Problem: Find all odd numbers between two given integers x and y, inclusive, and return them in descending order. Step-by-step Solution: 1. Determine the range between x and y. 2. Identify all odd numbers within this range. 3. Sort the odd numbers in descending order. 4. Return the sorted list. Algorithm: 1. Check if x is greater than y. If so, swap them to ensure the range is correct. 2. Iterate through the range from x to y. 3. For each number, check if it is odd (i.e., number%2! = 0). 4. Collect all odd numbers in a list. 5. Sort the list in descending order. 6. Return the sorted list. Planning: - Define a function find_odd_numbers(x, y). - Use a conditional to swap x and y if x > y. - Use a loop to iterate through the range from x to y. - Check each number for oddness and add it to a list if it is odd.  - Sort the list in descending order. - Return the list. ### Algorithm to Solve the Original Problem 1. Determine the Range: - Identify the start and end of the range by taking the minimum and maximum of a and b. 2. Iterate Through the Range: - Loop through each number in the range from the minimum to the maximum of a and b. 3. Check for Even Numbers: - For each number, check if it is even by using the condition number%2 == 0. 4. Collect Even Numbers: - Add each even number to a list. 5. Return the List: - Since the numbers are collected in ascending order, return the list directly. Tutorials: - Approach: - Always ensure the range is correctly defined, especially when the order of inputs can vary. - Use modulus operation to check for evenness or oddness. - Important Considerations: - Handle edge cases where there might be no even numbers in the range. - Ensure the function is robust to handle any positive integer inputs. ### Plan 1. Define the Function: - Create a function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range. 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use a for loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use ifnumber%2 == 0 to check if a number is even. 6. Collect Even Numbers: - Append even numbers to the list.  7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate a list of even integers between the given numbers a and b. Input for Simulation: 1 You are a programmer tasked with verifying a plan to solve a given problem using the Python3 programming language. ## Problem: def generate_integers(a, b): """ Given two positive integers a and b, return the even digits between a and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] """ ### Plan 1. Define the Function: - Create a function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range.v 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use a for loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use ifnumber%2 == 0 to check if a number is even. 6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate a list of even integers between the given numbers a and b.  Expected Output: Your response must be structured as follows: ### Simulation - Take a sample input and apply plan step by step to get the output. - Compare the generated output with the sample output to verify if your plan works as expected. ### Plan Evaluation - If the simulation is successful write No Need to Modify Plan. - Otherwise write Plan Modification Needed. Response from Simulation: 1 ### Simulation Let’s apply the plan step by step using the sample input generate_integers(2, 8). 1. Define the Function: - We define the function generate_integers(a, b). 2. Determine the Range: - Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8. 3. Initialize an Empty List: - Create an empty list evennumbers = []. 4. Loop Through the Range: - Iterate over the range from start to end (inclusive). 5. Check for Evenness: - For each number in the range, check if it is even using ifnumber%2 == 0. 6. Collect Even Numbers: - Append even numbers to the list evennumbers. 7. Return the Result: - Return the list evennumbers. Applying these steps: - Range: 2 to 8 - Iteration: - 2 is even, append to list: [2] - 3 is not even - 4 is even, append to list: [2, 4] - 5 is not even  - 6 is even, append to list: [2, 4, 6] - 7 is not even - 8 is even, append to list: [2, 4, 6, 8] The output is [2, 4, 6, 8], which matches the expected output. ### Plan Evaluation No Need to Modify Plan The plan works as expected for the given problem. The function correctly generates a list of even integers between the given numbers a and b, regardless of their order. Input for final code generation: You are a programmer tasked with solving a given problem using the Python3 programming language. See the plan to solve the plan and implement code to solve it. ## Problem: def generate_integers(a, b): """ Given two positive integers a and b, return the even digits between a and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] """ ### Plan 1. Define the Function: - Create a function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range. 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use a for loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use ifnumber%2 == 0 to check if a number is even.  6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate a list of even integers be- tween the given numbers a and b. ——– Important Instructions: - Do not add any explanation. - The generated Python3 code must be inside a triple backtick ($$$) code block. Response from final code generation: def generate_integers(a, b): start = min(a, b) end = max(a, b) even_numbers = [] for number in range(start, end + 1): if number % 2 == 0: even_numbers.append(number) return even_numbers Input for Improving code: 1, 1 You are a programmer who has received a solution of a problem written in Python3 that fails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all the test cases. Do not generate same code. ## Problem: def generate_integers(a, b): """ Given two positive integers a and b, return the even digits between a and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] """ ### Plan  1. Define the Function: - Create a function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range. 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use a for loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use ifnumber%2 == 0 to check if a number is even. 6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate a list of even integers between the given numbers a and b. ### Buggy Code def generate_integers(a, b): start = min(a, b) end = max(a, b) even_numbers = [] for number in range(start, end + 1): if number % 2 == 0: even_numbers.append(number) return even_numbers ### Test Report Test Cases where the generated code failed to generate the expected output: assertgenerateintegers(10, 14) == [] Expected Output: Your response must be structured as follows: ### Simulation with failed test case To detect where is the bug: - Take a sample test case where it fails. - Take the input go through each step according to the plan - You will get a output that must be different from the expected output.  ### Debugging Notes Based on this simulation detect any of the following cases: - Plan is wrong - Plan is correct but plan to code generation is wrong. - Finally, discuss how to correct this code. ### Modified Code “‘Python3 # Your corrected code, with comments explaining each correction. “‘ ——– Important Instructions: - Strictly follow the instructions. - Do not add testing code for example assert statement in your code. - Do not be overconfident that the generated code is correct. It is wrong. - The modified Python3 code must be enclosed within triple backticks (“‘). - Your response must contain Simulation with failed test case, Debugging Notes, and Modified Code section. Response from Improving code: 1, 1 ### Simulation with failed test case Let’s take the test case generate_integers(10, 14):v 1. Determine the Range: - start = min(10, 14) = 10 - end = max(10, 14) = 14 2. Loop Through the Range: - The loop iterates over 10, 11, 12, 13, 14. 3. Check for Evenness: - 10%2 == 0 (even, added to list) - 11%2! = 0 (odd, not added) - 12%2 == 0 (even, added to list) - 13%2! = 0 (odd, not added) - 14%2 == 0 (even, added to list) 4. Collect Even Numbers: - The list becomes [10, 12, 14]. 5. Return the Result: - The function returns [10, 12, 14].  Expected Output: [] ### Debugging Notes - Plan is correct: The plan correctly identifies the need to find even numbers between a and b. - Plan to code generation is wrong: The code does not account for the requirement that the even digits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when the digits themselves (1, 2, 4) are not between 10 and 14. ### Modified Code def generate_integers(a, b): start = min(a, b) end = max(a, b) even_numbers = [] for number in range(start, end + 1): # Check each digit in the number for digit in str(number): if int(digit) % 2 == 0 and start <= int(digit) <= end: even_numbers.append(int(digit)) # Remove duplicates and sort the result return sorted(set(even_numbers)) Explanation of Corrections: - Digit Check: The code now checks each digit of the numbers in the range to see if it is even and within the specified range. - Set for Uniqueness: A set is used to ensure that each even digit is only included once. - Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=17, thumbnail=None, content="1. Introduction Due to their impressive performance, there is a strong push to deploy deep learning models, par- ticularly large language models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; G Team et al., 2024) in a large number of scenarios. Due to auto- regressive nature of LLMs, decode latency tends to dominate inference cost. Decode latency itself is dominated by communication cost of transfer- ring model weights from high-bandwidth mem- ory (HBM) to the SRAM or due to transferring weights/activations in a distributed cluster. Quantizing weights and/or activations can sig- nificantly reduce the overall communication load and is, therefore, one of the most popular tech- niques for reducing inference costs (Dettmers et al., 2022). While floating-point representations are standard for training, integer data types such as int8, int4, and int2 are appealing alternatives for inference. However, current methods for quan- tizing to these varying integer precisions typically treat each target precision as an independent op- timization problem, leading to a collection of dis- tinct models rather than a single, versatile one. Furthermore, quantizing to extremely low preci- sions like int2 is known to be highly inaccurate. In this work, we pose the question of whether both of the above challenges can be addressed; that is, can we train a single model from which we can extract multiple accurate lower-precision models? We answer this question in the affir- mative by introducing Matryoshka Quantization (MatQuant), a novel multi-scale training method that leverages the inherent nested (Matryoshka) structure (Kusupati et al., 2022) within integer data types (Figure 1a). Specifically, slicing the top bits of an int8-quantized weight can directly yield an int4 or int2 model. Existing quantiza- tion techniques often neglect this structure, which limits the potential for multi-scale adaptable mod- els operating at various bit-widths with optimal performance. Instead, MatQuant simultaneously optimizes model weights across multiple precision levels (e.g., int8, int4, int2). At a high level, we repre- sent each model parameter at different precision levels using shared most significant bits (MSBs), and then jointly optimize the loss for each pre- cision level. This allows us to develop a single quantized model that can effectively operate at any of the chosen bit-widths, offering a spectrum of accuracy-versus-cost options. MatQuant is a general-purpose technique, applicable to most 1 arXiv:2502.06786v1  [cs.LG]  10 Feb 2025  Matryoshka Quantization 11 01 1001  (a) 2 4 6 8 Effective bits per FFN parameter 40 50 60 70 Task Average Gemma-2 9B MatQuant MatQuant-Interp. Baseline MinMax Sliced int8 (b) (c) Figure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka structure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially > 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution as a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions. learning-based quantization methods, such as Quantization Aware Training (QAT) (Jacob et al., 2018) and OmniQuant (Shao et al., 2023). We demonstrate the efficacy of MatQuant when applied to quantizing the Feed-Forward Network (FFN) parameters of standard LLMs (Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al., 2017) – typically, FFN is the main latency block hence the focus on improving the most signifi- cant component’s latency. Our results show that MatQuant produces int8 and int4 models with comparable accuracy to independently trained baselines, despite the benefit of shared model pa- rameters. Critically, the int2 models generated by MatQuant significantly outperform their in- dividually trained counterparts, with 8% higher accuracy on downstream tasks (Figure 1b). We also extend MatQuant to quantize all weights of a Transformer layer. Finally, we find that quantiz- ing with MatQuant shifts the quantized weight distribution toward higher values, contributing to improved int2 performance (Figure1c). Beyond improving chosen precision perfor- mance, MatQuant allows for seamless extraction of interpolative bit-widths, such as int6 and int3. MatQuant also admits a dense accuracy-vs-cost pareto-optimal trade-off by enabling layer-wise Mix’n’Match of different precisions. This ensures deployment of say an effective int3 sized model even if the underlying hardware only supports int4 and int2. Overall, MatQuant and its vari- ants present a significant step toward develop- ing multi-scale models with high flexibility and performance, pushing the boundaries of low-bit quantization for efficient LLM inference. 2. Related Work Model weight quantization is an extremely power- ful and prevalent technique for making resource- intensive neural networks suitable for deployment constraints – especially modern-day LLMs. Quan- tization algorithms can be categorized as either learning-free or learning-based. Learning-free methods use limited data to calibrate model pa- rameters without relying on gradient descent. Learning-based methods, however, utilize gra- dient descent to update either model parameters or auxiliary parameters to aid in quantization. 2  Matryoshka Quantization Learning-free Quantization Methods. Naive quantization methods, such as MinMax, absmax, and zero-point quantization, aim to directly map the range of model weights to the target bit- width – see (Dettmers et al., 2022) for a de- tailed background. Dettmers et al. (2022) fur- ther improved this by identifying the need to handle outliers with higher precision than the rest of the model weights. The core principle of more recent learning-free quantization meth- ods remains similar while improving various as- pects of it and using small amounts of data for calibration. For example, GPTQ (Frantar et al., 2022) improves upon min-max quantization by it- erating over all the coordinates, quantizing them one at a time, and updating the remaining full- precision coordinates to minimize the layer-wise activation reconstruction error. AWQ (Lin et al., 2023), SmoothQuant (Xiao et al., 2023), and AffineQuant (Ma et al., 2024) scale the weights and activations to reduce outliers, thus mak- ing them easier to quantize. QuIP (Chee et al., 2024), FrameQuant (Adepu et al., 2024), and QuaRoT (Ashkboos et al., 2024) multiply the weights and activations by orthonormal matri- ces before quantizing to reduce the number of outliers. SqueezeLLM (Kim et al., 2024) uses clustering to obtain the optimal buckets for quan- tization, and CDQuant (Nair and Suggala, 2024) improves upon GPTQ by greedily choosing the coordinates to descend along. While learning- free methods are inexpensive and work well at higher bit-widths, they are often suboptimal in the low-precision regime, which benefits greatly from learning-based techniques. Learning-based Quantization Methods. Quantization Aware Training (QAT) (Abdol- rashidi et al., 2021; Jacob et al., 2018) is a logical approach to ensure that models are easy to quantize during inference while retaining high accuracy. However, because QAT involves updating all the model parameters, its adoption for LLMs has been limited. Several recent works improve the performance and efficiency of QAT. LLM-QAT (Liu et al., 2024a) and BitDistiller (Du et al., 2024) enhance QAT with knowledge distillation from the full-precision model. EfficientQAT (Chen et al., 2024) min- imizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for QAT to converge. On the other hand, some techniques significantly reduce the overhead by learning only the auxiliary parameters, such as scaling factors and zero-points, that aid in quantization instead of updating the actual weight matrices. For example, OmniQuant (Shao et al., 2023) does not update the model parameters; instead, it learns additional scales and shifting parameters (that aid with quantization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant (Liu et al., 2024b) uses gradient descent to learn its rotation matrices. This class of learning-based quantiza- tion techniques (OmniQuant, SpinQuant, etc.) is widely adopted due to their appeal of achieving QAT-level accuracy at a fraction of the cost. Multi-scale Training. Training across multiple data scales (resolutions) was heavily popularized in computer vision for both recognition and gen- eration (Adelson et al., 1984; Denton et al., 2015; Lin et al., 2017). More recently, the paradigm of multi-scale training has shifted to models (Devvrit et al., 2023; Kusupati et al., 2022; Rippel et al., 2014; Yu et al., 2018), where the data remains the same, and models of varying capacity, all nested within one large model, are trained jointly. This joint, nested (Matryoshka-style) learning with varying model sizes results in a smooth accuracy- vs-compute trade-off and is beneficial in many downstream applications and real-world deploy- ments. However, the most obvious structure with a nested nature is the bit structure of the inte- ger data type. Given the success of multi-scale training for inputs, outputs, and model weights, it is imperative to explore it further for integer data types, especially in the context of quantiza- tion, which aids in the deployment of resource- intensive LLMs. 3. Matryoshka Quantization We introduce MatQuant, a general-purpose, multi-scale training technique that works seam- 3  Matryoshka Quantization lessly with popular learning-based quantization methods such as Quantization Aware Training (QAT) (Jacob et al., 2018) and OmniQuant (Shao et al., 2023). As long as the model or auxiliary parameters are optimized with gradient descent, MatQuant’s multi-scale training technique can be used across chosen bit-widths, leveraging the in- herent nested structure of integer data types. In this section, we will elaborate on the preliminar- ies behind QAT and OmniQuant, alongside our novel proposed approach, MatQuant. 3.1. Preliminaries 3.1.1. Quantized Aware Training Quantized Aware Training (QAT) learns a 𝑐-bit quantized model by optimizing for the end-to- end cross entropy loss using gradient descent. It uses the quantized weights for the forward pass and a straight through estimator (STE) (Bengio et al., 2013) to propagate gradients through the quantization operator during the backward pass. To mathematically formulate QAT, we define MinMax quantization of a real-valued vector 𝑤in 𝑐bits as follows: 𝑄MM(𝑤, 𝑐) = clamp \x10j𝑤 𝛼+ 𝑧 m , 0, 2𝑐−1 \x11 𝛼= max(𝑤) −min(𝑤) 2𝑐−1 , 𝑧= −min(𝑤) 𝛼 (1) where 𝑄MM(𝑤, 𝑐) is the 𝑐-bit quantized version of 𝑤, 𝛼is the scaling factor and 𝑧is the zero point. Let 𝑊𝐹represent weights of a Transformer LLM and let D = {(𝑥1, 𝑦1), . . . , (𝑥𝑁, 𝑦𝑁)} be a labelled dataset where 𝑥𝑖and 𝑦𝑖represent the input and output respectively. With 𝐿CE as the cross entropy loss, the optimization of QAT is: min 𝑊𝐹 1 𝑁 ∑︁ 𝑖∈[𝑁] LCE (𝐹(𝑥𝑖; 𝑄MM (𝑊𝐹, 𝑐)), 𝑦𝑖) (2) where 𝐹(·) represents the LLM’s forward pass. 3.1.2. OmniQuant OmniQuant, unlike QAT, does not update the model parameters. Instead, it learns additional scaling and shifting parameters through gradient descent over layer-wise L2 error reconstruction. These auxiliary parameters aid with quantization. Similar to QAT, OmniQuant also uses a straight through estimator during optimization. However, unlike QAT, OmniQuant operates with limited data, making it much more attractive for resource- scarce settings. OmniQuant adds two learnable scales, 𝛾and 𝛽, to MinMax quantization as follows: 𝑄Omni(𝑤, 𝑐) = clamp \x10j𝑤 𝛼+ 𝑧 m , 0, 2𝑐−1 \x11 𝛼= 𝛾· max(𝑤) −𝛽· min(𝑤) 2𝑐−1 , 𝑧= −𝛽· min(𝑤) 𝛼 (3) OmniQuant also adds another set of learnable shifting and scaling parameters to the FFN’s affine projections as follows: 𝑋𝑊+𝑏→((𝑋−𝛿) ⊘𝑠)·𝑄Omni(𝑊⊙𝑠)+𝑏+𝛿·𝑊(4) where 𝑋∈ℝ𝑛×𝑑is the input to the affine transfor- mation, 𝑊∈ℝ𝑑×𝑑o is the linear projection asso- ciated with the affine transformation, 𝑏∈ℝ𝑑o is the bias vector, 𝛿∈ℝ𝑑and 𝑠∈ℝ𝑑are learnable shift and scale parameters respectively. With the goal of optimizing the layer-wise L2 error (where a layer consists of an Attention block followed by an FFN block), OmniQuant’s overall objective can be portrayed as follows: min 𝛾,𝛽,𝛿,𝑠||𝐹𝑙(𝑊𝑙 𝐹), 𝑋𝑙) −𝐹𝑙(𝑄Omni(𝑊𝑙 𝐹), 𝑋𝑙)||2 2 (5) where 𝐹𝑙(·) represents the forward pass for a sin- gle layer 𝑙, 𝑊𝑙 𝐹represents the layer parameters and 𝑋𝑙represents the layer’s input. Note that the above objective is optimized independently for each of the 𝐿Transformer layers. 3.2. MatQuant MatQuant is a general purpose framework to de- velop a single model that can do well at any precision. It is a multi-scale training technique that works with most learning-based quantization schemes like QAT and OmniQuant discussed ear- lier. At its core, taking inspiration from Kusupati et al. (2022), MatQuant optimizes the quantiza- tion loss for several target bit-widths jointly. To have a single model for various integer pre- cisions, we nest smaller bit-widths into large ones 4  Matryoshka Quantization – leveraging the inherent Matryoshka nature of the integer data type. So, if we want to extract a 𝑟-bit model from a 𝑐-bit model (0 < 𝑟< 𝑐), we can just slice out the 𝑟most significant bits (MSBs) – using a right shift, followed by a left shift of the same order. Formally, the 𝑆(𝑞𝑐, 𝑟) operator slices the most significant 𝑟bits from a 𝑐-bit quantized vector 𝑞𝑐: 𝑆(𝑞𝑐, 𝑟) = \x12\x16 𝑞𝑐 2𝑐−𝑟 \x19\x13 ∗2𝑐−𝑟 (6) Once we have this structure, we can optimize for several precisions by slicing the MSBs from the largest bit-width we are optimizing for. Let 𝑅= {𝑟1, 𝑟2, ..., 𝑟𝐾} be the bit-widths we want to optimize for, 𝑄(·, ) represent the quantiza- tion function of the base algorithm (i.e., any learning-based quantization scheme), L(·) rep- resent the loss function pertaining to the base algorithm, 𝐹(·) represent the forward pass re- quired to compute the loss, 𝜃represent the set of model/auxiliary parameters we are optimizing for and let 𝑊𝐹represent the model parameters. MatQuant’s overall objective can be formulated as follows: min 𝑃 1 𝑁 ∑︁ 𝑖∈[𝑁] ∑︁ 𝑟∈𝑅 𝜆𝑟·L \x00𝐹(𝑆(𝑄(𝜃, 𝑐), 𝑟), 𝑥′ 𝑖), 𝑦′ 𝑖 \x01 (7) where 𝑦′ 𝑖= 𝑦𝑖for QAT and 𝑦′ 𝑖= 𝐹𝑙(𝑊𝑙 𝐹, 𝑋𝑖 𝑙) for OmniQuant, and 𝑥′ 𝑖= 𝑥𝑖for QAT and 𝑥′ 𝑖= 𝑋𝑖 𝑙for OmniQuant. 𝜆𝑟is the loss reweighing factor for bit-width 𝑟. In this work, we default to training MatQuant with three bit-widths, 𝑅= {8, 4, 2}, and subse- quently perform a grid search over 𝜆𝑟. This pro- cess aims to optimize performance such that the model performs well across all targeted precision levels. Further, while the focus of this paper is pri- marily on integer data types, we discuss the pos- sibility of extending MatQuant to floating-point representations in Section 5.5. A key point to note is that MatQuant primarily alters the quantized weight distributions across precision levels compared to the base quantiza- tion algorithm (OmniQuant or QAT). Figure 1c illustrates the differences in the quantized weight histograms obtained with and without MatQuant on Gemma-2 9B using OmniQuant. Upon close observation, we find that all the distributions of MatQuant are shifted to the right; that is, weights quantized with MatQuant tend to use more higher-valued weights. While this might not significantly impact int8 or even int4 models, int2 models benefit from utilizing more of the possible quantized weights compared to the base- line. Because int2 favors higher-valued weights, this effect propagates to higher-valued weights for int4, and then to int8. This observation highlights the potential overparameterization and freedom in the int8 data type to accommodate the more stringent needs of int2 during joint training. We further explore the effects of this phenomenon in Section 5.3 to develop a better standalone quan- tization technique for a single target precision. 3.2.1. Interpolative Behavior Slicing. Although we explicitly train MatQuant for three precisions (int8, int4, int2), we find that the resulting model, when quantized to interpo- lated bit-widths like int6 & int3 by slicing (Eq. 6) the int8 model, performs on par with a baseline trained explicitly for that precision. It is also sig- nificantly better than slicing an int8 quantized model. We attribute this strong interpolation in bit-width space to MatQuant, and present more results in Sections 4.1 & 4.2. Mix’n’Match. MatQuant also enables the use of different precisions at different layers through layer-wise Mix’n’Match (Devvrit et al., 2023), even though we never trained for these com- binatorial possibilities. These large number of models, obtained at no cost, densely span the accuracy-vs-memory trade-off. We explore sev- eral Mix’n’Match strategies and find that having a higher precision (int8) in the middle layers and a lower precision (int2) at the start and end is Pareto-optimal among hundreds of possible mod- els. See Section 4.3 for detailed experiments. 4. Experiments In this section, we present an empirical evaluation of MatQuant working with two popular learning- 5  Matryoshka Quantization Table 1 | MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the int3, int6 models obtained for free through interpolation from MatQuant perform comparably to the explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx (perplexity) is computed on C4 validation set (↓). Data type Method Gemma-2 2B Gemma-2 9B Mistral 7B OmniQuant Task Avg. log pplx. Task Avg. log pplx. Task Avg. log pplx. bfloat16 68.21 2.551 74.38 2.418 73.99 2.110 int8 Baseline 68.25 2.552 74.59 2.418 73.77 2.110 MatQuant 67.85 2.580 74.33 2.446 73.46 2.132 int4 Sliced int8 62.98 2.794 72.19 2.546 46.59 4.139 Baseline 67.03 2.598 74.33 2.451 73.62 2.136 MatQuant 66.54 2.617 74.26 2.470 73.13 2.155 int2 Sliced int8 37.68 17.993 35.75 14.892 36.25 10.831 Baseline 51.33 3.835 60.24 3.292 59.74 3.931 MatQuant 55.70 3.355 68.25 2.823 65.99 2.569 int6 Sliced int8 67.66 2.565 74.61 2.424 73.50 2.122 Baseline 68.06 2.554 74.23 2.420 74.10 2.112 MatQuant 68.01 2.582 74.50 2.446 73.59 2.139 int3 Sliced int8 42.00 5.781 55.76 3.830 34.60 8.539 Baseline 64.37 2.727 73.23 2.549 71.68 2.211 MatQuant 63.24 2.757 73.25 2.535 71.55 2.228 based quantization methods: OmniQuant (Sec- tion 4.1) and QAT (Section 4.2). We demon- strate MatQuant’s efficiency on Transformer- based LLMs. Unless otherwise mentioned, our primary focus is on weight quantization within the parameter-intensive FFN blocks of the Trans- former layer. For our experiments, we chose the default tar- get quantization precisions to be int8, int4, and int2. Furthermore, we showcase the interpolative nature of MatQuant through evaluations on int6 and int3, as well as its elastic ability to densely span the accuracy-vs-cost trade-off using layer- wise Mix’n’Match (Section 4.3). Finally, we ablate on improving the performance of MatQuant (Sec- tions 5.1 and 5.2) and extend MatQuant to the quantization of FFN and Attention parameters. (Section 5.3). Further training and fine-grained evaluation details are in the Appendix. Models and Data. We experiment with Gemma- 2 (Gemma-Team, 2024) 2B, 9B, and Mistral 7B (Jiang et al., 2023) models. For OmniQuant experiments, we sample 128 examples with a se- quence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4. We train for a total of 10M tokens for all models ex- cept the int2 baseline, where we train the model for 20M tokens (Shao et al., 2023). For QAT ex- periments, we sample a fixed set of 100M tokens from the C4 dataset and train all our models us- ing a batch size of 16 and a sequence length of 8192 for a single epoch. Baselines. For OmniQuant and QAT, our pri- mary baselines (referred to as “Baseline” in the tables and figures) are models trained explicitly for a given precision. When interpolating the models trained with MatQuant for int6 and int3, we do not perform any additional training. How- ever, the baselines are trained explicitly for 6 and 3 bits respectively. We also compare against a sliced int8 OmniQuant/QAT baseline model to the corresponding precision (referred to as “Sliced int8” in the tables). Evaluation Datasets. Following recent work (Frantar et al., 2022; Ma et al., 2024), we evaluate all the methods based on log perplexity and average zero-shot accuracy across a col- lection of downstream tasks. We use C4’s test 6  Matryoshka Quantization Table 2 | MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the int3, int6 models obtained for free through interpolation from MatQuant perform comparably to the explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks (↑) while log pplx (perplexity) is computed on C4 validation set (↓). Data type Method Gemma-2 2B Gemma-2 9B Mistral 7B QAT Task Avg. log pplx. Task Avg. log pplx. Task Avg. log pplx. bfloat16 68.21 2.551 74.38 2.418 73.99 2.110 int8 Baseline 67.82 2.458 74.17 2.29 73.48 2.084 MatQuant 67.68 2.471 74.77 2.301 72.41 2.085 int4 Sliced int8 67.20 2.458 73.25 2.338 71.83 2.164 Baseline 67.03 2.512 73.26 2.324 72.13 2.105 MatQuant 67.05 2.521 73.71 2.332 71.63 2.111 int2 Sliced int8 39.67 9.317 40.35 7.144 38.40 10.594 Baseline 47.74 3.433 56.02 2.923 54.95 2.699 MatQuant 52.43 3.153 62.32 2.756 61.29 2.474 int6 Sliced int8 67.55 2.462 74.12 2.294 73.30 2.088 Baseline 67.75 2.460 74.31 2.293 72.71 2.077 MatQuant 67.60 2.476 74.55 2.303 72.70 2.089 int3 Sliced int8 60.23 2.913 68.57 2.565 65.29 2.441 Baseline 61.75 2.678 69.9 2.43 68.82 2.197 MatQuant 62.51 2.798 70.68 2.486 66.44 2.308 set to calculate perplexity, and for downstream evaluations, we test on ARC-c, ARC-e (Clark et al., 2018), BoolQ (Clark et al., 2019), Hel- laSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and Winogrande (Sakaguchi et al., 2020). 4.1. MatQuant with OmniQuant Table 1 shows the efficacy of MatQuant when used with FFN-only OmniQuant and compared to explicitly trained OmniQuant baselines for the tar- get precisions, i.e., int8, int4, and int2, across all the models. While the average downstream accu- racy of MatQuant for int8 and int4 quantization is within 0.5% of the corresponding independently trained baselines, the int2 quantized models of MatQuant are 4.37%, 8.01%, and 6.35% more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively. Similar trends and improvements follow when measuring performance through val- idation log perplexity. Further, the quantized int4 and int2 models sliced from the int8 Om- niQuant baseline suffer a significant drop in accu- racy around int4, demonstrating that the nested structure of int8 is not well utilized. Sliced Interpolation. Beyond the target quan- tization granularities (int8, int4, and int2), MatQuant allows for bit-width interpolation to bit-widths not optimized during training. We find that the accuracy of the int6 and int3 models obtained by slicing the MatQuant models is com- parable to explicitly trained baselines for both precisions. 4.2. MatQuant with QAT To further demonstrate the generality of MatQuant, we experiment on the same models using the popular QAT technique. Following the trend of experimental results with OmniQuant, we show in Table 2 that the models trained using MatQuant with QAT are comparable to the explicitly trained baselines for all the targeted bit-widths of int8 and int4. However, int2 quantized models using MatQuant are 4.69%, 6.30%, and 6.34% more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively. Sliced Interpolation. Models trained using MatQuant with QAT exhibit strong interpolative performance similar to that of MatQuant with 7  Matryoshka Quantization OmniQuant. We find that the accuracy of the int6 and int3 models obtained by slicing the MatQuant models is comparable to explicitly trained base- lines for both interpolated bit-widths. While OmniQuant only trains the auxiliary pa- rameters needed for quantization, QAT also up- dates the weight parameters. This potentially re- sults in severe overfitting to the C4 subset used in the experiments. We observe this overfitting in all the experiments presented in Table 2, where the log perplexities improve for QAT compared to Om- niQuant, while the downstream accuracies suffer. This also highlights the need for high-quality data for QAT to realize its benefits; otherwise, users are better off using resource-friendly methods like OmniQuant. 4.3. Layerwise Mix’n’Match Alongside the strong slicing-based interpolative properties, quantization with MatQuant also en- ables another form of elastic and interpolative behavior through Mix’n’Match. Mix’n’Match provides a mechanism to obtain a combinato- rial number of strong models by using differ- ent quantization granularities, from the target bit-widths – i.e., int8, int4, and int2 across lay- ers. Figure 2 shows the ability of Mix’n’Match to densely span the Pareto-optimal accuracy-vs-bits- per-FFN-parameter (memory/cost) trade-off for 2 4 6 8 Effective bits per FFN parameter 60 65 70 75 Task Average Gemma-2 9B MatQuant Mix'n'Match MatQuant-Interp. Baseline Figure 2 | Mix’n’Match on Gemma-2 9B model trained using MatQuant with OmniQuant allows elastic pareto-optimal accuracy-vs-cost model ex- traction for free during deployment. Table 3 | Design choice ablation for loss re-weighting of the 3 target bit-widths (int8, int4, int2) that MatQuant explicitly optimizes. Note that MatQuant (0, 0, 1) ≡ Single Precison MatQuant. Data type Weightings Gemma-2 2B Gemma-2 9B Mistral 7B Task Avg. int8 (1, 1, 1) 67.42 73.97 73.46 (1, 1, √ 2) 67.31 73.45 73.41 (2, 2, 1) 67.85 74.02 73.82 ( √ 2, √ 2, 1) 67.3 74.33 73.82 int4 (1, 1, 1) 66.11 73.88 73.13 (1, 1, √ 2) 66.70 73.75 73.29 (2, 2, 1) 66.54 74.33 73.5 ( √ 2, √ 2, 1) 66.46 74.26 72.97 int2 (1, 1, 1) 55.71 68.52 65.99 (1, 1, √ 2) 57.08 67.93 66.28 (2, 2, 1) 55.70 66.72 63.49 ( √ 2, √ 2, 1) 55.29 68.25 57.85 the Gemma-2 9B model trained using MatQuant with OmniQuant – sometimes even improving on the bfloat16 model accuracy. While there are many more feasible models, we only showcase the best models obtained through the strategy de- scribed in Section 3.2.1 and further expanded in Appendix A. Interestingly, the Mix’n’Match mod- els with effective bit-width of 3 and 6 are as ac- curate as models obtained through slicing. This opens up possibilities for effective serving depend- ing on hardware support (Section 5.4). 5. Ablations and Discussion In this section, we present design ablations to improve MatQuant. Section 5.1 discusses the ef- fect of non-uniform weighting across target preci- sions (int8, int4, int2), and Section 5.2 explores enabling co-distillation of lower precision levels (int4, int2) from the highest precision quantized model (int8). During the process of extending MatQuant to all Transformer parameters, not just the FFN block, we uncovered an interesting hy- brid quantization algorithm (between Baseline and MatQuant). Section 5.3 further details this method, called Single Precison MatQuant, which stabilizes the otherwise QAT baseline for all the Transformer weights. Finally, we also discuss ex- tending MatQuant beyond integer data types and the considerations for effective deployment on current hardware. 8  Matryoshka Quantization 5.1. Weightings (𝜆𝑟) for MatQuant Depending on the constraints, we may wish to maximize the accuracy of one of the target bit- widths in MatQuant. Equation 7 provides a gen- eral formulation of MatQuant that supports a grid search on the weights 𝜆𝑟for bit-width 𝑟. The re- sults in Section 4 are with the weights that have balanced performance across target precisions. Table 3 shows the weight multiplier ablation re- sults for Gemma-2 2B, 9B, and Mistral 7B. While equal weighting for all precisions works well, we see that higher weights for a specific precision results in increased accuracy for that bit-width. This re-weighting to improve int8 and int4 mod- els often results in a minor accuracy drop for the int2 models. We can consider re-weighting as scaling the importance of the bits during training, and finding an optimal grid-search-free recipe is an interesting research question. 5.2. Co-distillation for MatQuant Given the nested nature of the models trained us- ing MatQuant, we explored co-distillation, where the outputs from a higher-precision model are used as the target for the lower-precision nested model, either in a standalone fashion or along- side the ground truth target (weighted equally). Table 4 shows the effects of co-distillation ap- plied to MatQuant with both OmniQuant and QAT on Gemma-2 9B. While int8 and int4 show no significant improvement, the nested int2 model benefits substantially from the int8 supervision, reaching 1.65% higher accuracy than the non-co- distilled MatQuant with OmniQuant. This helps us push the int2 quantized Gemma-2 9B beyond 70% average downstream accuracy for the first time across all our experiments. Co-distillation in MatQuant opens up avenues for interesting de- sign choices that can further leverage the inherent nested structure of integer data types. 5.3. Single Precison MatQuant In Tables 1 and 2, MatQuant performs on par with the explicitly trained baselines for int4, int8, and the interpolated int3 and int6 precisions. How- ever, the int2 models show a significant accuracy improvement. To investigate this, we conducted Table 4 | Design choice ablations for co-distillation within MatQuant. x →y represents distilling the y-bit model from the x-bit model. We note that the accuracy for int2 has significantly improved while minimally impacting the other bit-widths. OmniQuant QAT Data type Config. Task Avg. log pplx. Task Avg. log pplx. int8 [8, 4, 2] 73.97 2.451 74.77 2.301 [8, 4, 8 →2] 73.40 2.467 74.72 2.298 [8, 4, 2, 8 →2] 73.46 2.466 74.62 2.299 [8, 4, 2, 8 →4; 2] 73.32 2.466 74.80 2.302 int4 [8, 4, 2] 73.88 2.481 73.71 2.332 [8, 4, 8 →2] 73.84 2.488 73.76 2.328 [8, 4, 2, 8 →2] 73.01 2.495 73.78 2.329 [8, 4, 2, 8 →4; 2] 73.12 2.518 73.48 2.330 int2 [8, 4, 2] 68.52 2.809 62.32 2.756 [8, 4, 8 →2] 69.2 2.796 61.81 2.740 [8, 4, 2, 8 →2] 70.17 2.778 62.51 2.746 [8, 4, 2, 8 →4; 2] 69.72 2.804 62.12 2.746 a simple ablation in MatQuant by removing the loss terms for int4 and int8 (i.e., 𝑅= {2} in Equation 7 or setting 𝜆4 = 𝜆8 = 0) and present the results in Table 5. We call this version of MatQuant as Single Precison MatQuant. With Single Precison MatQuant, we observe a further boost of up to 1.67%, in the accuracy of int2 mod- els at a ∼2% accuracy drop in the corresponding int4 and int8 models – int2 is still nested within int8. This improvement likely stems from the six additional bits available during MatQuant-style training to optimize the int2 representation. In the case of Single Precison MatQuant, gra- dient descent is free to tune these six additional bits to improve the overall quality of the int2 model. In MatQuant, since we have additional losses to preserve the performance of the int4 Table 5 | Single Precison MatQuant significantly improves upon the baseline for int2 and, at times, outperforms MatQuant. Crucially, int8 and int4 performances of Single Precison MatQuant expe- rience a significant accuracy decrease (Tables 21 & 22). int2 Gemma-2 2B Gemma-2 9B Mistral 7B Method Task Avg. log pplx. Task Avg. log pplx. Task Avg. log pplx. OmniQuant 51.33 3.835 60.24 3.292 59.74 3.931 S.P. MatQuant 57.38 3.185 68.58 2.857 67.36 2.464 MatQuant 55.71 3.292 68.52 2.809 65.99 2.569 QAT 47.74 3.433 56.02 2.923 54.95 2.699 S.P. MatQuant 53.18 3.090 62.53 2.706 61.55 2.435 MatQuant 52.43 3.153 62.32 2.756 61.29 2.474 9  Matryoshka Quantization Table 6 | Extending MatQuant with QAT to FFN + Attention parameters. Baseline QAT destabi- lizes for int2 and int3 but improves significantly through MatQuant & Single Precison MatQuant. Data type Method Gemma-2 9B Mistral 7B QAT Task Avg. log pplx. Task Avg. log pplx. bfloat16 74.38 2.418 73.99 2.110 int8 Baseline 74.61 2.353 73.73 2.091 MatQuant 75.07 2.374 73.58 2.101 int4 Sliced int8 73.56 2.43 71.42 2.246 Baseline 72.98 2.40 71.87 2.132 MatQuant 74.11 2.436 71.5 2.166 int2 Sliced int8 39.05 13.116 38.39 12.066 Baseline - - - - S.P. MatQuant 47.78 3.705 34.69 7.564 MatQuant 47.17 3.837 43.33 3.806 int6 Sliced int8 74.56 2.358 73.71 2.094 Baseline 74.65 2.357 73.72 2.093 MatQuant 75.04 2.379 73.36 2.106 int3 Sliced int8 64.23 2.908 39.36 4.918 Baseline - - - - S.P. MatQuant 68.69 2.569 68.41 2.245 MatQuant 66.94 2.91 59.45 2.703 and int8, the int2 performance is slightly worse than Single Precison MatQuant. However, since the int4 and int8 models are typically very close in accuracy to the bfloat16 model, MatQuant can shift some of the weights to improve the int2 model. As int4 and int8 models have substan- tially more quantized buckets than int2, we hy- pothesize that shifting some weights into adjacent buckets may not significantly affect their perfor- mance; however, it can significantly impact int2’s performance. In fact, in the weight distributions presented in Fig 1c, we observe that MatQuant re- sults in a model where larger number of weights are assigned to the higher-valued buckets. Conclu- sively, MatQuant and Single Precison MatQuant inherently seem to be a better way of doing low- bit quantization. FFN + Attention Weight Quantization. We present results for FFN + Attention quantization for QAT in Table 6. For int8, int4 and the inter- polated int6 model, MatQuant performs on par with the Baseline. However, we found int2 and int3 to be very unstable while quantizing both, the FFN and the Attention parameters. Most re- cent works that do QAT for both the blocks Chen et al. (2024); Du et al. (2024); Liu et al. (2024a) either do some form of warm starting for the quantized parameters, or have additional distil- lation and auxiliary loss functions. In the naive setup of minimizing the loss with respect to the ground truth, we find QAT to be very unstable at lower precisions. However, both MatQuant and Single Precison MatQuant are very stable further highlighting the benefits brought by MatQuant style training. 5.4. Deployment Considerations Current hardware accelerators have native sup- port for serving int8 and int4 quantized models. Additionally, custom-implemented CUDA kernels can can support various low-precision bit-widths, like int2 and int3 (Chee et al., 2024; Frantar et al., 2022). MatQuant can generate a large number of models at inference time. Depend- ing on the serving environment, we can choose between Mix’n’Match models and homogeneous sliced models. For example, suppose the serving environment has a memory constraint equivalent to an int3 model but lacks optimized support for int3, while supporting int2. In this case, a Mix’n’Match model performing comparably to the int3 model could be deployed. More generally, as depicted in Figure 2, MatQuant densely spans the memory-versus-accuracy curve and can be lever- aged to obtain the most performant model for a specific serving constraint. MatQuant can enable further research on hardware software co-design to effectively support elastic bit-widths on-the-fly during inference time. 5.5. Extension to Floating Point Extending MatQuant to floating-point represen- tations, such as FP8 and FP4, presents significant challenges. Given that the exponent is encoded within the bit representation and contributes to the value as a power of 2 (i.e., effectively log2), slicing it results in buckets whose sizes increase exponentially, unlike the integer case, where bucket sizes are constant. For example, slicing the first two bits from int8 yields buckets of 0, 64, 128, 192. Here, the bucket size (64) is con- stant; however, this would not be the case when slicing two exponent bits from FP8. This is a promising avenue for future research that could 10  Matryoshka Quantization further unlock the benefits of MatQuant, even during large-scale pretraining. 6. Conclusions In this work, we presented MatQuant, a novel multi-scale training technique that leverages the nested structure of integer data types to simul- taneously optimize model weight quantization across multiple precisions (int8, int4, and int2) within a single model. This general-purpose method, applicable to learning-based quantiza- tion techniques like OmniQuant and QAT, pro- duces models with comparable accuracy to base- lines for int8 and int4, while achieving sig- nificant improvements, up to 10% (using co- distillation), for int2 models. MatQuant fur- ther enables bit-width interpolation and layer- wise mix-and-match for flexible accuracy-cost trade-offs, promising more efficient deployment of large models across various hardware set- tings. Finally, MatQuant also helped discover Single Precison MatQuant, which significantly improves standalone low-bit quantization. Acknowledgments We are grateful to Varun Yerram, Shreya Pathak and Devvrit for assistance in setting up inference pipelines, Praneeth Netrapalli, Rakesh Shivanna, Tom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi and Rahul Sukthankar for helpful discussions, support and feedback. References A. Abdolrashidi, L. Wang, S. Agrawal, J. Mal- maud, O. Rybakov, C. Leichner, and L. Lew. Pareto-optimal quantized resnet is mostly 4-bit. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3091–3099, 2021. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Al- tenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 29(6):33–41, 1984. H. Adepu, Z. Zeng, L. Zhang, and V. Singh. Framequant: Flexible low-bit quantization for transformers. arXiv preprint arXiv:2403.06082, 2024. S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, M. Jaggi, D. Alistarh, T. Hoefler, and J. Hens- man. Quarot: Outlier-free 4-bit inference in ro- tated llms. CoRR, abs/2404.00456, 2024. doi: 10.48550/ARXIV.2404.00456. URL https:// doi.org/10.48550/arXiv.2404.00456. Y. Bengio, N. Léonard, and A. Courville. Estimat- ing or propagating gradients through stochas- tic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica- tions of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educa- tional Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6239. URL https:// doi.org/10.1609/aaai.v34i05.6239. J. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Informa- tion Processing Systems, 36, 2024. M. Chen, W. Shao, P. Xu, J. Wang, P. Gao, K. Zhang, Y. Qiao, and P. Luo. Efficientqat: Efficient quantization-aware training for large language models. CoRR, abs/2407.11062, 2024. doi: 10.48550/ARXIV.2407.11062. URL https://doi.org/10.48550/arXiv. 2407.11062. C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no ques- tions. In J. Burstein, C. Doran, and T. Solorio, 11  Matryoshka Quantization editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2924–2936. Asso- ciation for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1300. URL https: //doi.org/10.18653/v1/n19-1300. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sab- harwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv. org/abs/1803.05457. E. L. Denton, S. Chintala, R. Fergus, et al. Deep generative image models using a laplacian pyra- mid of adversarial networks. Advances in neural information processing systems, 28, 2015. T. Dettmers, M. Lewis, Y. Belkada, and L. Zettle- moyer. Gpt3. int8 (): 8-bit matrix multiplica- tion for transformers at scale. Advances in Neu- ral Information Processing Systems, 35:30318– 30332, 2022. F. Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hajishirzi, S. Kakade, A. Farhadi, P. Jain, et al. Matformer: Nested transformer for elas- tic inference. arXiv preprint arXiv:2310.07707, 2023. D. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu, and N. Xu. Bitdistiller: Unleashing the po- tential of sub-4-bit llms via self-distillation. In L. Ku, A. Martins, and V. Srikumar, edi- tors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 102– 116. Association for Computational Linguis- tics, 2024. doi: 10.18653/V1/2024.ACL-LONG. 7. URL https://doi.org/10.18653/v1/ 2024.acl-long.7. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al- Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of mod- els. arXiv preprint arXiv:2407.21783, 2024. E. Frantar, S. Ashkboos, T. Hoefler, and D. Alis- tarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. G. G Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal under- standing across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Gemma-Team. Gemma 2: Improving open language models at a practical size. ArXiv, abs/2408.00118, 2024. URL https: //api.semanticscholar.org/CorpusID: 270843326. B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pages 2704–2713, 2018. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam- ford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mis- tral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https:// doi.org/10.48550/arXiv.2310.06825. S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm: Dense-and-sparse quantization. In Forty-first International Conference on Ma- chine Learning, ICML 2024, Vienna, Aus- tria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=0jpbpFia8m. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, et al. Matryoshka representation learning. Advances in Neural In- formation Processing Systems, 35:30233–30249, 2022. J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quan- 12  Matryoshka Quantization tization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hari- haran, and S. Belongie. Feature pyramid net- works for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117–2125, 2017. Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra. LLM-QAT: data-free quantiza- tion aware training for large language mod- els. In L. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Compu- tational Linguistics, ACL 2024, Bangkok, Thai- land and virtual meeting, August 11-16, 2024, pages 467–484. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024. FINDINGS-ACL.26. URL https://doi.org/ 10.18653/v1/2024.findings-acl.26. Z. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary, R. Krishnamoorthi, V. Chandra, Y. Tian, and T. Blankevoort. Spinquant: LLM quantization with learned rotations. CoRR, abs/2405.16406, 2024b. doi: 10.48550/ARXIV.2405.16406. URL https://doi.org/10.48550/arXiv. 2405.16406. Y. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang, S. Wen, F. Chao, and R. Ji. Affinequant: Affine transformation quantization for large language models. arXiv preprint arXiv:2403.12544, 2024. P. A. Nair and A. S. Suggala. Cdquant: Accu- rate post-training weight quantization of large pre-trained models using greedy coordinate descent. CoRR, abs/2406.17542, 2024. doi: 10.48550/ARXIV.2406.17542. URL https:// doi.org/10.48550/arXiv.2406.17542. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Jour- nal of machine learning research, 21(140):1–67, 2020. O. Rippel, M. Gelbart, and R. Adams. Learning or- dered representations with nested dropout. In International Conference on Machine Learning, pages 1746–1754. PMLR, 2014. K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial wino- grad schema challenge at scale. In The Thirty- Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Appli- cations of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educa- tional Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8732–8740. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6399. URL https:// doi.org/10.1609/aaai.v34i05.6399. W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo. Omni- quant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszko- reit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar. org/CorpusID:13756489. G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint arXiv:1812.08928, 2018. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine re- ally finish your sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Pro- ceedings of the 57th Conference of the Associ- ation for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Vol- ume 1: Long Papers, pages 4791–4800. Asso- ciation for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1472. URL https: //doi.org/10.18653/v1/p19-1472. 13  Matryoshka Quantization A. Addition Training Details We run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning rate of 1𝑒−3 and for QAT experiments, we linearly warmup the learning rate to 1𝑒−5 for 150 and use a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with a sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using a batch size of 4. We train for a total of 10M tokens for all models except the int2 baseline, where we train the model for 20M tokens (Shao et al., 2023). For QAT experiments, we sample a fixed set of 100M tokens from the C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a single epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4 and train with a batch size of 16 for a single epoch. Mix’n’Match For a fixed effective bits-per-FFN layer, where each layer was quantized to either int2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid, Increasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2, the central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy followed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers, and int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending and descending order, respectively, across the layers. Our experimental results demonstrated that, for a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others. Allocating higher precision (int8) to the middle layers helped preserve critical information, while the initial and final layers performed adequately with lower bit precision (int2 and int4), leading to a more efficient and effective quantization scheme. B. Detailed Downstream Evaluations for OmniQuant ad QAT Tables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. C. Detailed Downstream Evaluations for MatQuant Re-weighting Tables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments on Gemma-2 2B, Gemma-2 9B and Mistral 7B. D. Detailed Downstream Evaluations for Co-Distillation Tables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant co- distillation on Gemma-2 9B with OmniQuant and QAT. E. Detailed Evaluations for FFN + Attention Quantization Tables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention quantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT. 14  Matryoshka Quantization F. Detailed Evaluation for Single Precison MatQuant Tables 20, 21, 22, and 23 present the downstream evaluation results comparing Single Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2 2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices 2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the Single Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and perplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for Single Precison MatQuant in Figure 3. Figure 3 | The Figure presents the weight distribution for Gemma-2 9B when trained with Single Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution is a consequence of Single Precison MatQuant’s training mechanism that heavily optimizes for the first 2 MSBs of the int8 representation. 15  Matryoshka Quantization Table 7 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant on Gemma-2 2B. Data type Method Gemma-2 2B OmniQuant ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 50.09 71.59 76.45 69.69 78.29 63.14 68.21 int8 Baseline 50 71.46 76.36 69.76 78.24 63.69 68.25 MatQuant 48.04 71.8 75.78 67.64 78.07 63.22 67.42 int4 Sliced int8 41.81 66.2 71.35 62.64 75.95 59.91 62.98 Baseline 48.46 70.96 74.22 67.66 77.26 63.61 67.03 MatQuant 45.65 70.29 74.8 66.07 77.58 62.27 66.11 int2 Sliced int8 23.81 23.53 53.06 24.78 51.8 49.09 37.68 Baseline 31.31 53.58 62.2 40.78 66.05 54.06 51.33 MatQuant 34.39 59.64 62.69 52.11 69.86 55.56 55.71 int6 Sliced int8 48.55 71.25 75.87 69.18 78.35 62.75 67.66 Baseline 49.32 71.76 76.48 69.52 78.56 62.75 68.06 MatQuant 47.1 71.46 76.02 67.47 77.91 63.61 67.26 int3 Sliced int8 23.21 34.43 58.2 30.48 56.69 49.01 42 Baseline 46.25 68.64 72.97 62.24 76.06 60.06 64.37 MatQuant 44.45 68.56 69.11 62.28 75.95 62.59 63.82 Table 8 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant on Gemma-2 9B. Data type Method Gemma-2 9B OmniQuant ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 58.96 77.57 83.33 77.31 81.12 67.96 74.38 int8 Baseline 59.47 77.31 83.94 77.35 81.39 68.11 74.59 MatQuant 58.11 78.03 83.27 76.17 81.18 67.09 73.97 int4 Sliced int8 55.97 75.04 81.19 73.81 80.52 66.61 72.19 Baseline 58.79 78.37 83.55 76.71 81.45 67.09 74.33 MatQuant 57.25 77.36 84.86 75.52 81.5 66.77 73.88 int2 Sliced int8 23.21 24.92 38.13 25.37 51.36 51.54 35.75 Baseline 39.16 63.43 72.11 52.24 72.63 61.88 60.24 MatQuant 48.72 72.18 79.2 68.11 76.17 66.77 68.52 int6 Sliced int8 59.04 77.53 84.68 77.1 81.23 68.11 74.61 Baseline 59.22 77.27 83.21 77.1 81.12 67.48 74.23 MatQuant 58.87 78.03 83.61 76.18 81.45 67.09 74.21 int3 Sliced int8 35.84 57.32 67.61 48.58 68.61 56.59 55.76 Baseline 57.17 77.06 83.79 74.45 80.36 66.54 73.23 MatQuant 55.46 76.14 84.04 74.49 80.14 67.32 72.93 16  Matryoshka Quantization Table 9 | Table presents the downstream evaluation results for MatQuant when applied to OmniQuant on Mistral 7B. Data type Method Mistral 7B OmniQuant ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 49.57 73.74 84.4 80.61 81.18 74.43 73.99 int8 Baseline 49.23 73.19 83.88 80.41 81.39 74.51 73.77 MatQuant 48.04 73.44 84.13 79.37 81.12 74.66 73.46 int4 Sliced int8 27.65 46.72 49.17 36.88 64.09 55.01 46.59 Baseline 49.23 73.23 83.94 79.9 81.34 74.11 73.62 MatQuant 48.21 72.69 83.49 78.82 81.12 74.43 73.13 int2 Sliced int8 23.72 25.29 43.21 25.45 50.49 49.33 36.25 Baseline 36.69 61.36 70.06 57.47 70.67 62.19 59.74 MatQuant 41.38 67.42 71.62 71.98 77.86 65.67 65.99 int6 Sliced int8 48.98 72.01 83.46 79.95 81.72 74.9 73.5 Baseline 50.26 73.65 84.04 80.55 81.66 74.43 74.1 MatQuant 48.46 72.98 84.07 79.64 81.18 75.22 73.59 int3 Sliced int8 22.78 24.66 37.86 24.12 49.24 48.93 34.6 Baseline 46.33 70.71 82.72 77.74 80.74 71.82 71.68 MatQuant 45.65 71.21 80.43 78.31 81.07 72.61 71.55 Table 10 | Table presents the downstream evaluation results for MatQuant when applied to QAT on Gemma-2 2B. Data type Method Gemma-2 2B QAT ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 50.09 71.59 76.45 69.69 78.29 63.14 68.21 int8 Baseline 47.78 70.66 75.08 69.92 78.35 65.11 67.82 MatQuant 46.25 71.21 75.6 69.97 78.4 64.64 67.68 int4 Sliced int8 46.08 69.36 75.78 68.05 78.18 65.75 67.2 Baseline 46.16 71.59 73.73 68.72 78.62 63.38 67.03 MatQuant 44.37 70.45 75.81 68.43 78.35 64.88 67.05 int2 Sliced int8 25.6 26.3 57.98 25.82 52.12 50.2 39.67 Baseline 24.66 43.22 62.17 38.39 64.42 53.59 47.74 MatQuant 28.24 51.73 64.19 46.76 68.66 55.01 52.43 int6 Sliced int8 47.78 70.79 74.25 69.73 77.64 65.11 67.55 Baseline 47.7 70.88 74.92 69.72 78.07 65.19 67.75 MatQuant 46.5 70.71 75.72 69.69 78.02 64.96 67.6 int3 Sliced int8 38.74 63.13 65.57 58.86 74.81 60.3 60.23 Baseline 39.68 65.28 67.03 62.68 77.04 58.8 61.75 MatQuant 38.65 67.34 70.49 61.47 75.41 61.72 62.51 17  Matryoshka Quantization Table 11 | Table presents the downstream evaluation results for MatQuant when applied to QAT on Gemma-2 9B. Data type Method Gemma-2 9B QAT ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 58.96 77.57 83.33 77.31 81.12 67.96 74.38 int8 Baseline 58.11 75.38 80.12 78.7 81.5 71.19 74.17 MatQuant 58.19 76.18 81.5 79.57 82.15 71.03 74.77 int4 Sliced int8 57.42 75.08 78.1 76.97 81.23 70.72 73.25 Baseline 56.91 75.42 75.38 78.06 81.39 72.38 73.26 MatQuant 57.94 76.64 75.2 78.71 81.66 72.14 73.71 int2 Sliced int8 23.89 27.61 57.95 30.16 54.68 47.83 40.35 Baseline 33.45 55.43 62.26 54.8 70.51 59.67 56.02 MatQuant 39.85 65.66 65.93 64.08 75.68 62.75 62.32 int6 Sliced int8 57.85 75.13 80.67 78.63 81.56 70.88 74.12 Baseline 57.94 76.14 79.63 78.93 82.1 71.11 74.31 MatQuant 58.02 75.63 81.31 79.43 81.66 71.27 74.55 int3 Sliced int8 50 68.1 75.2 71.31 79.43 67.4 68.57 Baseline 53.07 75.04 66.61 74.94 80.03 69.69 69.9 MatQuant 51.62 71.93 78.78 73.99 80.14 67.64 70.68 Table 12 | Table presents the downstream evaluation results for MatQuant when applied to QAT on Mistral 7B. Data type Method Mistral 7B QAT ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 49.57 73.74 84.4 80.61 81.18 74.43 73.99 int8 Baseline 48.89 71.63 82.42 81.69 81.18 75.06 73.48 MatQuant 46.76 70.37 82.51 79.73 80.9 74.19 72.41 int4 Sliced int8 47.18 70.41 80.37 79.84 80.25 72.93 71.83 Baseline 47.27 70.62 81.28 78.95 81.12 73.56 72.13 MatQuant 45.65 68.64 82.02 79 81.07 73.4 71.63 int2 Sliced int8 25.34 26.47 54.95 25.18 48.48 49.96 38.4 Baseline 29.78 48.23 64.5 55.11 70.84 61.25 54.95 MatQuant 34.3 55.09 71.83 65.89 75.52 65.11 61.29 int6 Sliced int8 48.21 71.51 82.42 81.67 81.72 74.27 73.3 Baseline 47.7 71.3 82.23 79.84 80.79 74.43 72.71 MatQuant 47.53 71 81.9 79.73 81.28 74.74 72.7 int3 Sliced int8 40.1 61.49 72.91 68.72 77.97 70.56 65.29 Baseline 44.54 67.97 73.98 76.31 79.65 70.48 68.82 MatQuant 38.82 62.42 77.74 71.1 78.07 70.48 66.44 18  Matryoshka Quantization Table 13 | Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss reweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7). Gemma-2 2B Data type Weightings ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average int8 (1, 1, 1) 48.04 71.8 75.78 67.64 78.07 63.22 67.42 (1 √ 2, √ 2) 47.35 71.34 75.66 67.99 78.07 63.38 67.3 ( √ 2, 1, √ 2) 47.44 72.43 76.02 67.45 78.02 63.85 67.54 (1, 1 √ 2) 47.7 71.89 75.63 67.21 78.07 63.38 67.31 (2, 2, 1) 48.38 72.31 76.3 68.32 78.35 63.46 67.85 ( √ 2, 2, 1) 48.46 71.84 75.93 68.35 77.91 63.14 67.6 (2, √ 2, 1) 47.95 71.72 75.26 68.13 78.07 62.75 67.31 ( √ 2, √ 2, 1) 47.35 71.34 75.66 67.99 78.07 63.38 67.3 int4 (1, 1, 1) 45.65 70.29 74.8 66.07 77.58 62.27 66.11 (1 √ 2, √ 2) 46.33 70.92 73.7 67.67 77.26 62.9 66.46 ( √ 2, 1, √ 2) 46.42 70.96 74.71 65.78 77.58 63.14 66.43 (1, 1 √ 2) 45.56 71.55 75.75 66.18 77.48 63.69 66.7 (2, 2, 1) 46.84 70.88 74.92 66.48 77.91 62.19 66.54 ( √ 2, 2, 1) 47.35 71.68 72.69 66.79 77.26 63.38 66.52 (2, √ 2, 1) 45.9 70.83 75.11 66.97 77.37 62.27 66.41 ( √ 2, √ 2, 1) 46.33 70.92 73.7 67.67 77.26 62.9 66.46 int2 (1, 1, 1) 34.39 59.64 62.69 52.11 69.86 55.56 55.71 (1 √ 2, √ 2) 32.76 56.99 63.46 51.99 70.29 56.27 55.29 ( √ 2, 1, √ 2) 35.07 62.04 65.78 54.26 71.65 56.27 57.51 (1, 1 √ 2) 34.22 60.4 64.98 54.3 71.38 57.22 57.08 (2, 2, 1) 34.47 57.95 63.94 51.84 69.75 56.27 55.7 ( √ 2, 2, 1) 33.45 57.49 65.02 52.22 70.4 55.64 55.7 (2, √ 2, 1) 34.04 58.84 65.11 51.77 70.89 57.14 56.3 ( √ 2, √ 2, 1) 32.76 56.99 63.46 51.99 70.29 56.27 55.29 int6 (1, 1, 1) 47.1 71.46 76.02 67.47 77.91 63.61 67.26 (1 √ 2, √ 2) 47.44 71.42 74.95 67.85 77.86 63.3 67.14 ( √ 2, 1, √ 2) 47.61 71.89 75.9 67.37 78.24 63.77 67.46 (1, 1 √ 2) 47.78 71.63 75.47 67.2 77.86 63.61 67.26 (2, 2, 1) 48.55 72.69 76.3 68.02 78.67 63.85 68.01 ( √ 2, 2, 1) 48.29 71.76 75.72 68.42 78.02 63.38 67.6 (2, √ 2, 1) 48.38 71.51 75.84 68.24 78.18 63.85 67.67 ( √ 2, √ 2, 1) 47.44 71.42 74.95 67.85 77.86 63.3 67.14 int3 (1, 1, 1) 44.45 68.56 69.11 62.28 75.95 62.59 63.82 (1 √ 2, √ 2) 43.17 68.73 64.74 61.31 76.39 61.48 62.64 ( √ 2, 1, √ 2) 41.98 68.6 70.34 61.95 75.9 63.3 63.68 (1, 1 √ 2) 41.64 66.71 71.62 61.94 76.01 61.09 63.17 (2, 2, 1) 41.98 68.35 68.41 63.74 76.17 60.77 63.24 ( √ 2, 2, 1) 42.66 66.54 70.46 63.61 75.63 62.98 63.65 (2, √ 2, 1) 43.17 66.71 60.03 62.71 76.77 61.64 61.84 ( √ 2, √ 2, 1) 43.17 68.73 64.74 61.31 76.39 61.48 62.64 19  Matryoshka Quantization Table 14 | Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss reweighting when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7). Gemma-2 9B Data type Weightings ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average int8 (1, 1, 1) 58.11 78.03 83.27 76.17 81.18 67.09 73.97 (1 √ 2, √ 2) 57.68 77.4 83.73 76.1 81.18 67.64 73.95 ( √ 2, 1, √ 2) 58.11 77.86 81.04 76 81.18 67.09 73.55 (1, 1 √ 2) 56.91 77.1 82.39 75.93 81.18 67.17 73.45 (2, 2, 1) 58.79 77.48 82.66 76.55 81.23 67.4 74.02 ( √ 2, 2, 1) 58.53 77.31 82.63 76.54 80.96 67.56 73.92 (2, √ 2, 1) 58.62 77.27 84.31 76.54 81.34 66.85 74.16 ( √ 2, √ 2, 1) 59.13 78.07 84.16 76.46 80.9 67.25 74.33 int4 (1, 1, 1) 57.25 77.36 84.86 75.52 81.5 66.77 73.88 (1 √ 2, √ 2) 56.74 77.74 85.08 75.5 80.85 66.85 73.79 ( √ 2, 1, √ 2) 57.42 78.28 82.51 75.97 81.34 67.56 73.85 (1, 1 √ 2) 57.59 77.82 84.28 75.32 81.12 66.38 73.75 (2, 2, 1) 58.62 78.28 83.67 76.01 81.5 67.88 74.33 ( √ 2, 2, 1) 58.19 77.82 83.91 76.62 81.99 67.72 74.37 (2, √ 2, 1) 58.28 78.16 84.53 76.41 81.72 67.09 74.36 ( √ 2, √ 2, 1) 57.94 78.11 84.98 76.5 81.01 67.01 74.26 int2 (1, 1, 1) 48.72 72.18 79.2 68.11 76.17 66.77 68.52 (1 √ 2, √ 2) 49.83 73.91 78.75 67.27 77.2 66.46 68.9 ( √ 2, 1, √ 2) 48.55 74.24 81.5 68.44 76.5 65.9 69.19 (1, 1 √ 2) 48.29 72.94 74.74 68.34 77.58 65.67 67.93 (2, 2, 1) 46.76 73.27 71.96 67.98 76.77 63.61 66.72 ( √ 2, 2, 1) 46.76 73.7 77.65 67.01 77.58 65.98 68.11 (2, √ 2, 1) 46.76 72.35 75.35 67.51 76.39 67.56 67.65 ( √ 2, √ 2, 1) 46.59 72.6 79.3 67.58 77.69 65.75 68.25 int6 (1, 1, 1) 58.87 78.03 83.61 76.18 81.45 67.09 74.21 (1 √ 2, √ 2) 57.51 77.53 83.55 75.98 80.9 67.17 73.77 ( √ 2, 1, √ 2) 58.79 77.82 81.38 76.21 81.07 67.72 73.83 (1, 1 √ 2) 57.34 77.23 82.57 75.89 81.12 67.17 73.55 (2, 2, 1) 59.04 77.4 82.66 76.55 81.56 68.03 74.21 ( √ 2, 2, 1) 59.22 77.65 82.17 76.62 81.23 67.8 74.11 (2, √ 2, 1) 58.36 77.82 83.79 76.47 81.23 67.25 74.15 ( √ 2, √ 2, 1) 59.3 78.37 84.5 76.57 80.85 67.4 74.5 int3 (1, 1, 1) 55.46 76.14 84.04 74.49 80.14 67.32 72.93 (1 √ 2, √ 2) 56.23 76.05 82.6 74.85 80.9 67.01 72.94 ( √ 2, 1, √ 2) 56.4 77.86 80.64 75.11 79.87 68.51 73.06 (1, 1 √ 2) 55.63 76.05 82.39 74.21 80.3 67.17 72.62 (2, 2, 1) 55.2 76.56 84.19 74.87 80.2 67.72 73.12 ( √ 2, 2, 1) 54.44 75.63 80.55 74.97 80.96 67.72 72.38 (2, √ 2, 1) 56.14 75.67 83.33 74.96 80.52 67.72 73.06 ( √ 2, √ 2, 1) 56.31 77.4 83.24 75.62 80.41 66.54 73.25 20  Matryoshka Quantization Table 15 | Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweight- ing when applied to OmniQuant. Weightings: (𝑥, 𝑦, 𝑧) →(𝜆8, 𝜆4, 𝜆2) (from Equation 7). Mistral 7B Data type Weightings ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average int8 (1, 1, 1) 48.04 73.44 84.13 79.37 81.12 74.66 73.46 (1 √ 2, √ 2) 48.46 73.19 84.28 79.19 81.12 74.74 73.5 ( √ 2, 1, √ 2) 47.95 73.4 84.46 79.11 81.34 74.51 73.46 (1, 1 √ 2) 48.21 73.02 84.34 79.03 81.28 74.59 73.41 (2, 2, 1) 49.06 73.48 84.74 79.73 81.56 74.35 73.82 ( √ 2, 2, 1) 49.06 73.57 84.56 79.64 81.39 74.27 73.75 (2, √ 2, 1) 48.98 73.95 84.50 79.60 81.61 74.90 73.92 ( √ 2, √ 2, 1) 48.98 73.86 84.56 79.55 81.23 74.74 73.82 int4 (1, 1, 1) 48.21 72.69 83.49 78.82 81.12 74.43 73.13 (1 √ 2, √ 2) 49.15 72.81 83.39 78.71 80.79 74.66 73.25 ( √ 2, 1, √ 2) 47.95 72.43 83.43 79.24 81.01 74.03 73.01 (1, 1 √ 2) 48.46 73.44 84.07 78.9 81.01 73.88 73.29 (2, 2, 1) 49.15 72.81 83.88 79.8 81.88 73.48 73.5 ( √ 2, 2, 1) 48.89 72.69 82.72 79.53 81.66 73.88 73.23 (2, √ 2, 1) 47.87 72.05 83 79.56 81.23 74.27 73 ( √ 2, √ 2, 1) 48.29 72.47 82.84 79.52 81.07 73.64 72.97 int2 (1, 1, 1) 41.38 67.42 71.62 71.98 77.86 65.67 65.99 (1 √ 2, √ 2) 40.78 66.2 73.61 72.68 77.75 67.4 66.4 ( √ 2, 1, √ 2) 40.36 67.09 75.35 72.46 77.48 65.9 66.44 (1, 1 √ 2) 40.36 67.17 74.83 71.64 77.53 66.14 66.28 (2, 2, 1) 37.2 62.46 67.74 70.29 76.55 66.69 63.49 ( √ 2, 2, 1) 37.29 64.35 61.1 68.88 74.86 65.19 61.94 (2, √ 2, 1) 39.68 65.24 68.93 66.64 75.19 64.09 63.29 ( √ 2, √ 2, 1) 34.56 61.24 60.61 58.07 72.63 59.98 57.85 int6 (1, 1, 1) 48.46 72.98 84.07 79.64 81.18 75.22 73.59 (1 √ 2, √ 2) 49.06 73.44 84.59 79.51 81.28 74.74 73.77 ( √ 2, 1, √ 2) 47.95 73.48 84.43 79.28 81.45 75.14 73.62 (1, 1 √ 2) 48.38 72.94 84.34 79.15 81.18 74.59 73.43 (2, 2, 1) 48.46 72.94 84.13 79.89 81.5 74.9 73.64 ( √ 2, 2, 1) 48.81 73.48 84.34 79.67 81.34 74.9 73.76 (2, √ 2, 1) 49.4 73.65 84.4 79.68 81.28 74.74 73.86 ( √ 2, √ 2, 1) 49.23 73.57 84.43 79.55 81.12 74.66 73.76 int3 (1, 1, 1) 45.65 71.21 80.43 78.31 81.07 72.61 71.55 (1 √ 2, √ 2) 47.7 72.05 82.81 78.74 81.12 72.77 72.53 ( √ 2, 1, √ 2) 46.33 72.43 81.8 79.03 82.1 73.4 72.51 (1, 1 √ 2) 45.99 71.09 80.73 78.77 80.85 72.53 71.66 (2, 2, 1) 47.95 73.36 82.57 79.31 81.39 74.9 73.25 ( √ 2, 2, 1) 44.45 69.7 82.11 77.68 80.2 71.74 70.98 (2, √ 2, 1) 46.84 72.73 80.95 78.79 81.56 73.01 72.31 ( √ 2, √ 2, 1) 47.01 71.59 81.96 78.89 81.39 72.45 72.22 21  Matryoshka Quantization Table 16 | Table presents the downstream evaluation and perplexity results for our MatQuant co- distillation experiments on Gemma-2 9B with OmniQuant. OmniQuant Gemma-2 9B Data type Config. ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 [8, 4, 8 →2] 57.59 77.27 81.83 75.48 81.01 67.25 73.4 2.467 [8, 4, 2, 8 →2] 57.17 77.36 82.2 75.82 80.96 67.25 73.46 2.466 [8, 4, 2, 8 →4; 2] 56.4 77.82 82.32 75.02 80.63 67.72 73.32 2.466 int4 [8, 4, 8 →2] 57.68 78.45 82.97 75.5 80.85 67.56 73.84 2.488 [8, 4, 2, 8 →2] 57.51 77.61 80.46 74.74 81.12 66.61 73.01 2.495 [8, 4, 2, 8 →4; 2] 56.57 77.99 82.54 74.77 80.58 66.3 73.12 2.518 int2 [8, 4, 8 →2] 48.81 74.03 81.65 68.1 77.48 65.11 69.2 2.796 [8, 4, 2, 8 →2] 49.15 75.34 83.12 68.79 77.64 67.01 70.17 2.778 [8, 4, 2, 8 →4; 2] 49.83 75.04 79.79 68.38 77.86 67.4 69.72 2.804 int6 [8, 4, 8 →2] 57.42 77.19 81.87 75.42 81.01 67.8 73.45 2.468 [8, 4, 2, 8 →2] 57.51 77.48 82.32 75.88 81.07 66.61 73.48 2.467 [8, 4, 2, 8 →4; 2] 56.4 78.03 82.63 75.14 80.79 67.4 73.4 2.498 int3 [8, 4, 8 →2] 55.63 75.88 80.12 74.01 80.36 67.96 72.33 2.549 [8, 4, 2, 8 →2] 54.35 76.85 79.33 74.6 80.47 67.4 72.17 2.543 [8, 4, 2, 8 →4; 2] 55.2 76.98 82.45 73.59 80.41 68.43 72.84 2.58 Table 17 | Table presents the downstream evaluation and perplexity results for our MatQuant co- distillation experiments on Gemma-2 9B with QAT. QAT Gemma-2 9B Data type Config. ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 [8, 4, 8 →2] 58.11 76.43 81.25 79.12 82.05 71.35 74.72 2.298 [8, 4, 2, 8 →2] 57.51 76.43 81.53 78.95 82.1 71.19 74.62 2.299 [8, 4, 2, 8 →4; 2] 58.11 76.14 81.68 79.12 82.26 71.51 74.8 2.302 int4 [8, 4, 8 →2] 57.42 76.35 77.55 78.06 81.61 71.59 73.76 2.328 [8, 4, 2, 8 →2] 56.91 75.8 78.44 77.76 81.39 72.38 73.78 2.329 [8, 4, 2, 8 →4; 2] 57.51 75.76 75.96 77.96 81.72 71.98 73.48 2.33 int2 [8, 4, 8 →2] 39.51 65.03 66.88 63.37 75.08 61.01 61.81 2.74 [8, 4, 2, 8 →2] 40.78 66.5 67.55 63.67 75.95 60.62 62.51 2.746 [8, 4, 2, 8 →4; 2] 40.19 65.7 65.57 63.83 75.3 62.12 62.12 2.746 int6 [8, 4, 8 →2] 57.85 76.09 81.47 78.98 81.88 71.27 74.59 2.301 [8, 4, 2, 8 →2] 57.17 75.97 82.2 79 81.83 71.9 74.68 2.302 [8, 4, 2, 8 →4; 2] 57.42 76.09 82.29 78.95 82.10 71.27 74.69 2.305 int3 [8, 4, 8 →2] 51.96 71.55 78.07 73.17 79.43 66.93 70.18 2.485 [8, 4, 2, 8 →2] 50.94 71.76 78.78 73.09 79.05 66.77 70.06 2.486 [8, 4, 2, 8 →4; 2] 51.45 72.39 78.84 73.46 79.6 67.96 70.62 2.731 22  Matryoshka Quantization Table 18 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza- tion on Gemma-2 9B with QAT. Data type Method Gemma-2 9B ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 58.96 77.57 83.33 77.31 81.12 67.96 74.38 int8 Baseline 58.62 77.02 83.43 79.01 81.34 68.27 74.61 MatQuant 59.04 77.9 84.4 78.76 81.12 69.22 75.07 int4 Sliced int8 57.42 76.73 81.62 76.02 80.58 68.98 73.56 Baseline 56.06 74.96 79.27 77.83 80.25 69.53 72.98 MatQuant 57.34 76.77 84.19 77.51 80.74 68.11 74.11 int2 Sliced int8 24.74 25.63 58.53 25.5 50.71 49.17 39.05 Baseline - - - - - - - S.P. MatQuant 24.91 41.62 62.26 40.87 63.38 53.67 47.78 MatQuant 28.24 39.23 62.17 39.13 63.49 50.75 47.17 int6 Sliced int8 58.53 77.15 82.48 79.04 81.5 68.67 74.56 Baseline 58.87 77.06 83.12 78.81 81.23 68.82 74.65 MatQuant 59.81 77.9 84.8 78.68 81.07 67.96 75.04 int3 Sliced int8 43.6 64.98 72.66 66 75.95 62.19 64.23 Baseline - - - - - - - S.P. MatQuant 50.85 73.11 71.13 72.01 79.38 65.67 68.69 MatQuant 45.22 69.32 78.5 68.72 76.01 63.85 66.94 23  Matryoshka Quantization Table 19 | Table presents the downstream evaluation results for MatQuant FFN + Attention quantiza- tion on Mistral 7B with QAT. Data type Method Mistral 7B ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 49.57 73.74 84.4 80.61 81.18 74.43 73.99 int8 Baseline 49.23 72.9 83.49 80.26 81.28 75.22 73.73 MatQuant 49.32 72.31 83.76 80.2 81.18 74.74 73.58 int4 Sliced int8 45.99 71.76 81.41 76.95 80.41 71.98 71.42 Baseline 48.04 71.72 78.87 78.93 80.36 73.32 71.87 MatQuant 47.01 69.95 82.02 76.81 80.25 72.93 71.5 int2 Sliced int8 22.78 24.03 58.75 24.63 50.54 49.64 38.39 Baseline - - - - - - - S.P. MatQuant 23.21 23.82 37.83 24.67 49.02 49.57 34.69 MatQuant 22.27 32.49 62.02 32.43 59.3 51.46 43.33 int6 Sliced int8 49.32 73.53 82.66 80.16 81.12 75.45 73.71 Baseline 49.32 73.4 82.48 80.24 81.28 75.61 73.72 MatQuant 49.15 71.76 83.73 80.13 81.18 74.19 73.36 int3 Sliced int8 20.65 31.57 44.34 28.79 59.41 51.38 39.36 Baseline - - - - - - - S.P. MatQuant 41.98 65.53 79.39 74.42 79.22 69.93 68.41 MatQuant 34.64 55.13 70.43 58.61 73.39 64.48 59.45 Table 20 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant and QAT. int2 Gemma2-2B Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Task Avg. log pplx. OmniQuant S.P. MatQuant 34.64 64.06 65.69 53.07 69.7 57.14 57.38 3.185 Baseline 31.31 53.58 62.2 40.78 66.05 54.06 51.33 3.835 MatQuant 34.39 59.64 62.69 52.11 69.86 55.56 55.71 3.292 QAT S.P. MatQuant 28.92 53.79 62.84 48.41 69.86 55.25 53.18 3.090 Baseline 24.66 43.22 62.17 38.39 64.42 53.59 47.74 3.433 MatQuant 28.24 51.73 64.19 46.76 68.66 55.01 52.43 3.153 24  Matryoshka Quantization Table 21 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with OmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model were sliced post training. Gemma-2 9B Data type Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 S.P. MatQuant 56.48 76.85 73.36 74.87 80.74 66.77 71.51 2.525 OmniQuant 59.47 77.31 83.94 77.35 81.39 68.11 74.59 2.418 MatQuant 58.11 78.03 83.27 76.17 81.18 67.09 73.97 2.451 int4 S.P. MatQuant 57.17 77.02 74.28 74.41 80.69 67.56 71.85 2.543 OmniQuant 58.79 78.37 83.55 76.71 81.45 67.09 74.33 2.451 MatQuant 57.25 77.36 84.86 75.52 81.5 66.77 73.88 2.481 int2 S.P. MatQuant 49.74 74.66 80.92 66.57 76.06 63.54 68.58 2.857 OmniQuant 39.16 63.43 72.11 52.24 72.63 61.88 60.24 3.292 MatQuant 48.72 72.18 79.2 68.11 76.17 66.77 68.52 2.809 Table 22 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT. Note that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model were sliced post training. Gemma-2 9B Data type Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 S.P. MatQuant 55.97 76.18 80.09 75.43 80.69 68.9 72.88 2.429 QAT 47.78 70.66 75.08 69.92 78.35 65.11 67.82 2.29 MatQuant 46.25 71.21 75.6 69.97 78.4 64.64 67.68 2.301 int4 S.P. MatQuant 55.2 76.01 74.74 74.19 80.41 68.9 71.57 2.429 QAT 46.16 71.59 73.73 68.72 78.62 63.38 67.03 2.324 MatQuant 44.37 70.45 75.81 68.43 78.35 64.88 67.05 2.332 int2 S.P. MatQuant 41.21 66.2 65.02 64.31 76.06 62.35 62.53 2.706 QAT 33.45 55.43 62.26 54.8 70.51 59.67 56.02 2.923 MatQuant 39.85 65.66 65.93 64.08 75.68 62.75 62.32 2.756 Table 23 | Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and QAT. int2 Mistral 7B Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Task Avg. log pplx. OmniQuant S.P. MatQuant 39.93 66.25 76.97 72.99 78.07 69.93 67.36 2.464 Baseline 36.69 61.36 70.06 57.47 70.67 62.19 59.74 3.931 MatQuant 41.38 67.42 71.62 71.98 77.86 65.67 65.99 2.569 QAT S.P. MatQuant 34.64 56.19 70.73 66.77 75.52 65.43 61.55 2.435 Baseline 29.78 48.23 64.5 55.11 70.84 61.25 54.95 2.694 MatQuant 34.3 55.09 71.83 65.89 75.52 65.11 61.29 2.474 25")]}
2025-02-12 23:44:14,448 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 23:44:31,564 - DEBUG - start_idx: 2987, start_marker: 1 INTRODUCTION A long, end_idx: 71074, end_marker: ours of compute. F A
2025-02-12 23:44:33,204 - DEBUG - start_idx: 3124, start_marker: 1. Introduction Larg, end_idx: -1, end_marker: oning strategies. (3)
2025-02-12 23:48:28,058 - INFO - Initializing LLM for extracting main content from papers
2025-02-12 23:48:43,221 - DEBUG - start_idx: -1, start_marker: 1. INTRODUCTION\nA lo, end_idx: 10654, end_marker: discussion strategies.
2025-02-12 23:48:46,317 - DEBUG - start_idx: -1, start_marker: 1. Introduction\nLarge, end_idx: -1, end_marker: ent reasoning tasks.
2025-02-12 23:51:31,325 - DEBUG - start_idx: -1, start_marker: 1. Introduction The p, end_idx: 9734, end_marker: ification datasets.
2025-02-12 23:51:31,955 - DEBUG - start_idx: 1560, start_marker: 1. Introduction
Due to, end_idx: 6293, end_marker: efficient LLM inference
2025-02-12 23:55:25,563 - DEBUG - start_idx: -1, start_marker: 1. Introduction\nIn r, end_idx: -1, end_marker: hybrid code gener-\nat
2025-02-12 23:55:25,565 - INFO - Total execution time: 416.82 seconds (6.95 minutes)
2025-02-12 23:55:25,571 - INFO - Papers: {'2025-02-11': [Paper(arxiv_id='2502.06703', authors=['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], published_at=datetime.datetime(2025, 2, 11, 0, 36, 11, 270000, tzinfo=datetime.timezone.utc), title='Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling', summary='Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.', upvotes=90, thumbnail=None, content='2025-2-11\nCan 1B LLM Surpass 405B LLM? Rethinking\nCompute-Optimal Test-Time Scaling\nRunze Liu1,2,*, Junqi Gao1,3, Jian Zhao4, Kaiyan Zhang2, Xiu Li2, Biqing Qi1,†, Wanli Ouyang1 and\nBowen Zhou1,2,†\n1Shanghai AI Laboratory, 2Tsinghua University, 3Harbin Institute of Technology, 4BUPT\nTest-Time Scaling (TTS) is an important method for improving the performance of Large Language Models\n(LLMs) by using additional computation during the inference phase. However, current studies do not system-\natically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS.\nThis lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on\ntwo core questions: (1) What is the optimal approach to scale test-time computation across different policy\nmodels, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the\nperformance of LLMs on complex tasks, and can smaller language models outperform larger ones through\nthis approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have\nthe following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of\npolicy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small\npolicy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B\nLLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show\nthe significance of adapting TTS strategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs. Our website is available at\nhttps://ryanliu112.github.io/compute-optimal-tts.\n(a)\n65\n70\n75\n80\n85\n90\n95\n100\n78.2\n3B\n74.6\nunk\n71.4\n405B\n(b)\n65\n70\n75\n80\n85\n90\n95\n100\n91.6\n1.5B\n90.0\nunk\n85.5\nunk\n(c)\n65\n70\n75\n80\n85\n90\n95\n100\n97.3\n671B\n95.2\n7B\n94.8\nunk\n(d)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n30.0\n3B\n23.3\n405B\n9.3\nunk\n(e)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n63.6\nunk\n63.3\n1.5B\n44.6\nunk\n(f)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n83.3\n7B\n79.8\n671B\n79.2\nunk\nMATH-500\nAIME24\nCoT\nTTS\nLlama-3.2-3B-Instruct\nGPT-4o\nLlama-3.1-405B-Instruct\nDeepSeek-R1-Distill-1.5B\no1-mini\no1-preview\nDeepSeek-R1\nDeepSeek-R1-Distill-7B\no1\nFigure 1: Comparison between the performance of smaller LLMs compute-optimal TTS and that\nof larger LLMs CoT on MATH-500 and AIME24. (a) & (d) Llama-3.2-3B-Instruct surpasses Llama-\n3.1-405B-Instruct and GPT-4o on MATH-500 and AIME24; (b) & (e) DeepSeek-R1-Distill-1.5B\noutperforms o1-preview on MATH-500 and AIME24, and surpasses o1-mini on MATH-500; (c) & (f)\nDeepSeek-R1-Distill-7B beats o1 on MATH-500 and AIME24, and exceeds DeepSeek-R1 on AIME24.\n* Work done during an internship at Shanghai AI Laboratory\n† Corresponding authors: Biqing Qi (qibiqing@pjlab.org.cn), Bowen Zhou (zhoubowen@tsinghua.edu.cn)\narXiv:2502.06703v1  [cs.CL]  10 Feb 2025\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n1. Introduction\nLarge Language Models (LLMs) have shown significant improvements across a variety of domains (Ope-\nnAI, 2023; Hurst et al., 2024; Anthropic, 2023; OpenAI, 2024; DeepSeek-AI et al., 2025). Recently,\nOpenAI o1 (OpenAI, 2024) has demonstrated that Test-Time Scaling (TTS) can enhance the reasoning\ncapabilities of LLMs by allocating additional computation at inference time, making it an effective\napproach for improving LLM performance (Qwen Team, 2024; Kimi Team et al., 2025; DeepSeek-AI\net al., 2025).\nTTS approaches can be divided into two main categories: (1) Internal TTS, which trains the LLMs\nto “think” slowly with long Chain-of-Thought (CoT) (OpenAI, 2024; DeepSeek-AI et al., 2025), and\n(2) External TTS, which improves the reasoning performance via sampling or search-based methods\nwith fixed LLMs (Wu et al., 2024; Snell et al., 2024). The key challenge of external TTS is how to\nscale compute optimally, that is, allocating the optimal computation for each problem (Snell et al.,\n2024). Current TTS methods guide the generation process and select the final answer using Process\nReward Models (PRMs), which effectively scale test-time compute (Wu et al., 2024; Snell et al., 2024;\nBeeching et al., 2024). These TTS methods involve several important factors, such as policy models1,\nPRMs, and problem difficulty levels. However, there is limited systematic analysis of how policy\nmodels, PRMs, and problem difficulty influence these TTS strategies. This limitation prevents the\ncommunity from fully understanding the effectiveness of this method and developing insights for\ncompute-optimal TTS strategies.\nTo address these issues, this paper aims to investigate the influence of policy models, PRMs, and\nproblem difficulty on TTS through comprehensive experimental analysis. Furthermore, we explore\nthe concrete characteristics and performance boundaries of TTS methods. Specifically, we conduct\nextensive experiments on MATH-500 (Lightman et al., 2024) and the challenging AIME24 (AI-MO,\n2024) tasks using a range of PRMs (spanning from 1.5B to 72B across different model series) across\nmultiple policy models (ranging from 0.5B to 72B across two model families). Our results show that\nthe compute-optimal TTS strategy heavily depends on the specific policy model, PRM, and problem\ndifficulty level. Even smaller models (e.g., a 1B model) can outperform larger models (e.g., a 405B\nmodel) and even state-of-the-art reasoning models, such as o1 or DeepSeek-R1, in challenging\nreasoning tasks by applying compute-optimal TTS.\nThe contributions of this work can be summarized as follows:\n1. We conduct a comprehensive evaluation of different TTS methods using various up-to-date\npolicy models, multiple PRMs, diverse scaling methods, and more challenging tasks.\n2. Our analysis highlights the necessity of considering the influence of rewards in the TTS process\nand introduces reward-aware compute-optimal TTS. We also demonstrate that the compute-\noptimal scaling strategy varies with different policy models, PRMs, and problem difficulty\nlevels.\n3. The empirical results demonstrate the significant potential of smaller language models to\noutperform larger models through TTS. Using the reward-aware Compute-optimal TTS strategy,\nwe show that a 3B LLM can outperform a 405B LLM, and a 7B LLM can surpass o1 and\nDeepSeek-R1 on MATH-500 and AIME24 tasks.\n1Following Snell et al. (2024), we use “policy models” to refer to LLMs that generate solutions, and “verifiers” for PRMs.\n2\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nQuestion\nBest-of-N\nQuestion\nBeam Search\nDiverse Verifier Tree Search\n : Scored by PRM\n: Selected by PRM\n: Rejected by PRM\n: Solution / Step with Final Answer\n: Intermediate Step\nQuestion\nQuestion\nAnswer\nAnswer\nAnswer\nFigure 2: Comparison of different external TTS methods.\n2. Setup & Preliminaries\n2.1. Problem Formulation\nWe formulate the reasoning problem as a Markov Decision Process (MDP) (Sutton and Barto, 2018),\ndefined by the tuple (𝒮, 𝒜, 𝒫, ℛ, 𝛾), where 𝒮is the state space, 𝒜is the action space, 𝒫: 𝒮× 𝒜→𝒮\nis the transition function, ℛ: 𝒮× 𝒜→R is the reward function, and 𝛾∈[0, 1] is the discount factor.\nGiven a prompt 𝑥∼𝒳, the policy with parameters 𝜃generates the initial action 𝑎1 ∼𝜋𝜃(· | 𝑠1),\nwhere 𝑠1 = 𝑥is the initial state. The policy receives a reward ℛ(𝑠1, 𝑎1), and the state transitions to\n𝑠2 = [𝑠1, 𝑎1], where [·, ·] denotes the concatenation of two strings. This process continues until the\nepisode terminates, either by reaching the maximum number of steps or by generating an <EOS>\ntoken. A trajectory of length 𝐻is represented as 𝜏= {𝑎1, 𝑎2, · · · , 𝑎𝐻}. The process can be summarized\nas follows:\nInitial State:\n𝑠1 = 𝑥∼𝒳\nAction:\n𝑎𝑡∼𝜋𝜃(· | 𝑠𝑡)\nState Transition:\n𝑠𝑡+1 = 𝒫(· | 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡]\nReward:\n𝑟𝑡= ℛ(𝑠𝑡, 𝑎𝑡)\n(1)\n2.2. Test-Time Scaling Method\nWe consider three TTS methods: Best-of-N (BoN) (Brown et al., 2024), beam search (Snell et al.,\n2024), and Diverse Verifier Tree Search (DVTS) (Beeching et al., 2024). As pointed out by Snell\net al. (2024), lookahead search is inefficient due to multi-step sampling, so we do not evaluate it or\nother methods involving lookahead operations, such as Monte Carlo Tree Search (MCTS). The TTS\nmethods are shown in Figure 2.\nBest-of-N.\nIn the BoN approach, the policy model generates 𝑁responses, after which scoring and\nvoting methods are applied to select the final answer.\nBeam Search.\nGiven beam width 𝑁and beam size 𝑀, the policy model first generates 𝑁steps.\nThe verifier selects the top 𝑁\n𝑀steps for subsequent search. In the next step, the policy model samples\n3\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n𝑀steps for each selected previous step. This process repeats until the maximum depth is reached or\nan <EOS> token is generated.\nDiverse Verifier Tree Search.\nTo increase diversity, DVTS extends beam search by dividing the\nsearch process into 𝑁\n𝑀subtrees, each of which is explored independently using beam search. As\nshown in Beeching et al. (2024), DVTS outperforms beam search on easy and medium problems with\na large computational budget 𝑁. A similar trend is observed in Chen et al. (2024), where increasing\nthe number of parallel subtrees proves to be more effective than increasing the beam width under the\nsame budget.\n2.3. Compute-Optimal Test-Time Scaling\nTo maximize the performance of TTS, Snell et al. (2024) proposes a test-time compute-optimal scaling\nstrategy, which selects hyperparameters corresponding to a given test-time strategy to maximize\nperformance benefits on a specific prompt. Given a prompt 𝑥, let Target(𝜃, 𝑁, 𝑥) represent the output\ndistribution over 𝑥produced by the policy model with parameters 𝜃and a compute budget of 𝑁.\n𝜃*\n𝑥,𝑦*(𝑥)(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(2)\nwhere 𝑦*(𝑥) denotes the ground-truth correct response for 𝑥, and 𝜃*\n𝑥,𝑦*(𝑥)(𝑁) represents the test-time\ncompute-optimal scaling strategy for the problem 𝑥with compute budget 𝑁.\n3. Rethinking Compute-Optimal Test-Time Scaling\n3.1. Compute-Optimal Scaling Strategy Should be Reward-Aware\nCompute-optimal TTS aims to allocate the optimal compute for each problem (Snell et al., 2024).\nPrevious works on TTS use a single PRM as verifier (Snell et al., 2024; Wu et al., 2024; Beeching et al.,\n2024). Snell et al. (2024) trains a PRM on the responses of a policy model and uses it as the verifier\nto do TTS with the same policy model, while Wu et al. (2024); Beeching et al. (2024) use a PRM\ntrained on a different policy model to do TTS. From the perspective of Reinforcement Learning (RL),\nwe obtain an on-policy PRM in the former case and an offline PRM in the latter case. The on-policy\nPRM produces more accurate rewards for the responses of the policy model, while the offline PRM\noften generates inaccurate rewards due to out-of-distribution (OOD) issues (Snell et al., 2024; Zheng\net al., 2024).\nFor practical applications of compute-optimal TTS, training a PRM for each policy model to prevent\nOOD issues is computationally expensive. Therefore, we investigate the compute-optimal TTS strategy\nin a more general setting, where the PRM might be trained on a different policy model than the one\nused for TTS. For search-based methods, PRMs guide the selection at each response step, while for\nsampling-based methods, PRMs evaluate the responses after generation. This indicates that (1) the\nreward influences response selection across all methods; (2) for search-based methods, the reward\nalso influences the search process.\nTo analyze these points, we perform a preliminary case study using beam search with Llama-3.1-8B-\nInstruct as the policy model and RLHFlow-PRM-Mistral-8B and RLHFlow-PRM-Deepseek-8B as PRMs.\nThe results in Figure 12 demonstrate that the reward significantly affects the generation process and\noutcomes. RLHFlow-PRM-Mistral-8B assigns high rewards to short responses, leading to incorrect\nanswers, while searching with RLHFlow-Deepseek-PRM-8B produces correct answers but uses more\n4\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPass@1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage\n11.2%\n3.4%\n3.4%\n5.8%\n76.2%\nmean: 0.82\nFigure 3: Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five\nbins.\ntokens. In Section 4, we also empirically show that rewards have great influence on TTS performance\nand output tokens.\nBased on these findings, we propose that rewards should be integrated into the compute-optimal TTS\nstrategy. Let us denote the reward function as ℛ. Our reward-aware compute-optimal TTS strategy is\nformulated as:\n𝜃*\n𝑥,𝑦*(𝑥),ℛ(𝑁) = arg max\n𝜃\n(︀\nE𝑦∼Target(𝜃,𝑁,𝑥,ℛ)\n[︀\n1𝑦=𝑦*(𝑥)\n]︀)︀\n,\n(3)\nwhere Target(𝜃, 𝑁, 𝑥, ℛ) represents the output distribution of the policy model 𝜃, adjusted by the\nreward function ℛ, under a compute budget 𝑁and prompt 𝑥. For sampling-based scaling methods,\nTarget(𝜃, 𝑁, 𝑥, ℛ) = Target(𝜃, 𝑁, 𝑥). This reward-aware strategy ensures that compute-optimal\nscaling adapts to the policy model, prompt, and reward function, leading to a more general framework\nfor practical TTS.\n3.2. Absolute Problem Difficulty Criterion is More Effective Than Quantiles\nTo consider the influence of problem difficulty on TTS, Snell et al. (2024) group problems into five\ndifficulty levels based on Pass@1 accuracy quantiles. However, we find that using difficulty levels\nfrom MATH (Hendrycks et al., 2021) or oracle labels based on Pass@1 accuracy quantiles (Snell et al.,\n2024) is not effective since different policy models have different reasoning capabilities. As shown\nin Figure 3, Qwen2.5-72B-Instruct achieves Pass@1 accuracy above 80% on 76.2% of MATH-500\nproblems. Therefore, we use absolute thresholds instead of quantiles to measure problem difficulty.\nSpecifically, we define three difficulty levels based on Pass@1 accuracy: easy (50% ∼100%), medium\n(10% ∼50%), and hard (0% ∼10%).\n4. How to Scale Test-Time Compute Optimally?\nIn this section, we aim to answer the following questions:\n• Q1: How does TTS improve with different policy models and PRMs?\n• Q2: How does TTS improve for problems with different difficulty levels?\n• Q3: Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n5\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.1. Setup\nDatasets.\nWe conduct experiments on competition-level mathematical datasets, including MATH-\n500 (Lightman et al., 2024) and AIME24 (AI-MO, 2024). MATH-500 contains 500 representative\nproblems from the test set of MATH (Hendrycks et al., 2021), and this subset is used following Snell\net al. (2024); Beeching et al. (2024). As recent LLMs show significant progress in mathematical\nreasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), we include the more challenging AIME24 for\nexperiments.\nPolicy Models.\nFor test-time methods, we use policy models from Llama 3 (Dubey et al., 2024) and\nQwen2.5 (Yang et al., 2024b) families with different sizes. We use the Instruct version for all policy\nmodels.\nProcess Reward Models.\nWe consider the following open-source PRMs for evaluation:\n• Math-Shepherd (Wang et al., 2024b): Math-Shepherd-PRM-7B is trained on Mistral-7B (Jiang\net al., 2023) using PRM data generated from Mistral-7B fine-tuned on MetaMath (Yu et al.,\n2024).\n• RLHFlow Series (Xiong et al., 2024): RLHFlow includes RLHFlow-PRM-Mistral-8B and\nRLHFlow-PRM-Deepseek-8B, which are trained on data from Mistral-7B fine-tuned on Meta-\nMath (Yu et al., 2024) and deepseek-math-7b-instruct (Shao et al., 2024), respectively. The\nbase model for both PRMs is Llama-3.1-8B-Instruct (Dubey et al., 2024).\n• Skywork Series (Skywork o1 Team, 2024): The Skywork series comprises Skywork-PRM-\n1.5B and Skywork-PRM-7B, trained on Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-\nInstruct (Yang et al., 2024c), respectively. The training data is generated from Llama-2 (Touvron\net al., 2023) fine-tuned on a mathematical dataset and Qwen2-Math (Yang et al., 2024a) series\nmodels.\n• Qwen2.5-Math Series (Zhang et al., 2025): We evaluate Qwen2.5-Math-PRM-7B and Qwen2.5-\nMath-PRM-72B, trained on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct (Yang\net al., 2024c), respectively. The data for training is generated using Qwen2-Math (Yang et al.,\n2024a) and Qwen2.5-Math series models (Yang et al., 2024c). Among all the PRMs listed,\nQwen2.5-Math-PRM-72B is the strongest open-source PRM for mathematical tasks, while\nQwen2.5-Math-PRM-7B is the most capable PRM among those with 7B/8B parameters, as\ndemonstrated in Zhang et al. (2025).\nScoring and Voting Methods.\nFollowing Wang et al. (2024a), we consider three scoring methods:\nPRM-Min, PRM-Last, and PRM-Avg, and three voting methods: Majority Vote, PRM-Max, and PRM-Vote.\nTo obtain the final answer, we first use the scoring methods to evaluate the answers. For a trajectory of\nlength 𝐻, the scores for each trajectory with different scoring methods are calculated as follows: (1)\nPRM-Min scores each trajectory by the minimum reward among all steps, i.e., score = minℛ{ℛ𝑡}𝐻\n𝑡=0.\n(2) PRM-Last scores each trajectory by the reward of the last step, i.e., score = ℛ𝐻. (3) PRM-Avg\nscores each trajectory by the average reward among all steps, i.e., score = 1\n𝐻\n∑︀𝐻\n𝑡=0 ℛ𝑡. The voting\nmethods then aggregate the scores to determine the final answer. Majority Vote selects the answer\nwith the majority of votes (Wang et al., 2023), while PRM-Max selects the answer with the highest\nscore, and PRM-Vote first accumulates the scores of all identical answers and then selects the answer\nwith the highest score.\n6\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-1.5B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nSkywork-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n50\n60\n70\n80\n90\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 4: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on MATH-500 with different\nPRMs and TTS strategies.\n22\n24\n26\n28\n0\n20\n40\n60\nLlama-3.1-8B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n20\n40\n60\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n20\n40\n60\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n20\n40\n60\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 5: Performance of Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct on AIME24 with different\nPRMs and TTS strategies.\nWe use OpenR2, which is an open-source LLM reasoning framework as our codebase. For compute\nbudgets, we use {4, 16, 64, 256} in most experiments. The division of steps follows the format \\n\\n as\nin prior works (Xiong et al., 2024; Zhang et al., 2025). For beam search and DVTS, the beam width\nis set to 4. The temperature of CoT is 0.0, while it is 0.7 for other methods. For CoT and BoN, we\nrestrict the maximum number of new tokens to 8192. For search-based methods, the token limit is\n2048 for each step and 8192 for the total response.\n4.2. How does TTS improve with different policy models and PRMs? (Q1)\nPRMs are hard to generalize across policy models and tasks. As shown in Figure 4, for Llama-\n3.1-8B-Instruct, the performance of search-based methods with Skywork and Qwen2.5-Math PRMs\nimproves significantly with larger compute budgets, while the results of searching with Math-Shepherd\nand RLHFlow PRMs remain relatively poor, even worse than majority voting. For Qwen2.5-7B-Instruct,\nthe performance of searching with Skywork-PRM-7B and Qwen2.5-Math PRMs scales well with more\nbudgets, while the performance of other PRMs remains poor. In Figure 5, although the Pass@k accuracy\nof both policy models improves a lot with larger compute budgets, the performance improvement of\nTTS remains moderate. These results demonstrate that the generalization of PRMs is particularly\nchallenging across different policy models and tasks, especially for more complex tasks.\nThe optimal TTS method depends on the PRM used. As shown in Figure 4, BoN outperforms\nother strategies most of the time when using Math-Shepherd and RLHFlow PRMs, while search-based\n2https://github.com/openreasoner/openr\n7\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods perform better with Skywork and Qwen2.5-Math PRMs. This difference occurs because using\na PRM for OOD policy responses leads to sub-optimal answers, as PRMs show limited generalization\nacross policy models. Moreover, if we select each step with OOD PRMs, it is likely to obtain answers\ntrapped in local optima and worsen the performance. This may also be related to the base model\nof the PRM, since the PRM trained with PRM800K (Lightman et al., 2024) on Qwen2.5-Math-7B-\nInstruct generalizes better than PRMs with Mistral and Llama as base models (Zhang et al., 2025).\nFurther analysis is provided in Section 4.4 and Appendix C. These results suggest that the choice\nof the optimal TTS strategy depends on the specific PRMs used, emphasizing the importance of\nconsidering reward information in compute-optimal TTS. We also explore the relationship between\nTTS performance and the process supervision abilities of different PRMs. As shown in Figure 6, TTS\nperformance is positively correlated with the process supervision abilities of PRMs, and the fitted\nfunction is 𝑌= 7.66 log(𝑋) + 44.31, where 𝑌represents TTS performance and 𝑋represents the\nprocess supervision abilities of the PRM (Zhang et al., 2025).\n30\n40\n50\n60\n70\n80\nProcessBench Performance\n70\n72\n74\n76\n78\nTest-Time Scaling Performance\nMath-Shepherd-PRM-7B\nRLHFlow-PRM-Mistral-8B\nRLHFlow-PRM-Deepseek-8B\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nQwen2.5-Math-PRM-72B\nFigure 6: The relationship between TTS performance and process supervision abilities of different\nPRMs on MATH, where the size of each circle represents the number of parameters of the PRM and\nthe curve represents the fitted function.\n22\n24\n26\n28\n40\n60\n80\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n80\n85\n90\n95\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n85\n90\n95\n100\nQwen2.5-72B-Inst.\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 7: TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with\ndifferent scaling methods.\nThe optimal TTS method varies with policy models. To study the relationship between the\nparameters of the policy models and the optimal TTS methods, we conduct experiments with Qwen2.5\nfamily LLMs (Yang et al., 2024b), including models with 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\nparameters. The results in Figure 7 show that the optimal TTS methods depend on the specific policy\nmodels. For small policy models, search-based methods outperform BoN, while for large policy models,\nBoN is more effective than search-based methods. This difference occurs because larger models have\nstronger reasoning capabilities and do not need a verifier to perform step-by-step selection. In contrast,\nsmaller models rely on a verifier to select each step, ensuring the correctness of each intermediate\nstep.\n8\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n4.3. How does TTS improve for problems with different difficulty levels? (Q2)\nFollowing Snell et al. (2024), we conduct a comprehensive evaluation of tasks with varying difficulty\nlevels. However, as explained in Section 3.2, we observe that using the difficulty levels defined in\nMATH (Hendrycks et al., 2021) or the oracle labels based on the quantile of Pass@1 accuracy (Snell\net al., 2024) is not appropriate because different policy models exhibit different reasoning abilities.\nTo address this, we categorize the difficulty levels into three groups based on the absolute value of\nPass@1 accuracy: easy (50% ∼100%), medium (10% ∼50%), and hard (0% ∼10%).\nThe optimal TTS methods vary with different difficulty levels. The results in Figure 8 and Figure 9\nshow that for small policy models (i.e., with fewer than 7B parameters), BoN is better for easy\nproblems, while beam search works better for harder problems. For policy models with parameters\nbetween 7B and 32B, DVTS performs well for easy and medium problems, and beam search is\npreferable for hard problems. For policy models with 72B parameters, BoN is the best method for all\ndifficulty levels.\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 8: TTS performance of three Llama policy models on MATH-500 with three difficulty levels.\n4.4. Does PRMs have bias towards specific response lengths or sensitivity to voting methods?\n(Q3)\nTable 1: Statistics of training data of RLHFlow PRMs.\nMistral-PRM-Data\nDeepseek-PRM-Data\nAverage Token per Response\n236.9\n333.1\nAverage Token per Step\n46.6\n58.4\nPRMs are biased towards the length of steps.\nAlthough we perform TTS under the same budget\nin pervious experiments, we find that the number of inference tokens with different PRMs varies\nsingificantly. For example, given the same budget and the same policy model, the number of inference\n9\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntokens of scaling with RLHFlow-PRM-Deepseek-8B is consistently larger than that of RLHFlow-\nPRM-Mistral-8B, nearly 2×. The training data of RLHFlow series PRMs are sampled from different\nLLMs, which may lead to the bias towards the length of the output. To verify this point, we analyze\nseveral properties of the training data of RLHFlow-PRM-Mistral-8B3 and RLHFlow-PRM-Deepseek-\n8B4. As shown in Table 1, both the average token per response and the average token per step of\nDeepSeek-PRM-Data are larger than those of Mistral-PRM-Data, indicating that the training data\nof RLHFlow-PRM-Deepseek-8B is longer than that of RLHFlow-PRM-Mistral-8B. This may lead to\nthe bias towards the length of the output. We also find that the number of inference tokens of\nscaling with Qwen2.5-Math-7B is larger than that of Skywork-PRM-7B, but the performance is very\nnear, which indicates that searching with Skywork-PRM-7B is more efficient than searching with\nQwen2.5-Math-7B.\nTable 2: Performance of TTS with different voting methods on MATH-500.\nSkywork-PRM-7B\nQwen2.5-Math-PRM-7B\nMajority Vote\n86.8\n87.6\nPRM-Min-Max\n83.0\n87.4\nPRM-Min-Vote\n86.6\n87.6\nPRM-Last-Max\n84.4\n87.6\nPRM-Last-Vote\n87.0\n87.6\nPRM-Avg-Max\n85.8\n87.8\nPRM-Avg-Vote\n86.8\n87.6\nPRMs are sensitive to voting methods.\nFrom the results in Table 2, it is shown that Skywork-\nPRM-7B works better with PRM-Vote than with PRM-Max, while Qwen2.5-Math-PRM-7B is not very\nsensitive to voting methods. The main reason is that the training data of Qwen2.5-Math PRMs are\nprocessed with LLM-as-a-judge (Zheng et al., 2023), which removes the wrong intermediate steps\nlabeled as positive steps in the training data and makes the outputted large reward values more likely\nto be correct. This shows that the training data of PRMs is important for improving the ability to find\nerrors in the search process.\n5. Results for Compute-Optimal Test-Time Scaling\nWith the compute-optimal TTS strategy explored in Section 4, we conduct further experiments to\nexplore the following questions:\n• Q4: Can smaller policy models outperform larger models with the compute-optimal TTS\nstrategy?\n• Q5: How does compute-optimal TTS improve compared with CoT and majority voting?\n• Q6: Is TTS more effective than long-CoT-based methods?\n5.1. Can smaller policy models outperform larger models with the compute-optimal TTS strategy\n(Q4)\nScaling test-time compute of small policy models is crucially important for improving the reasoning\nperformance of LLMs. We are interested in whether smaller policy models can outperform larger ones,\nGPT-4o, even o1 and DeepSeek-R1, with the compute-optimal TTS strategy. First, we compare the\n3https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data\n4https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data\n10\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 3: Comparison of small policy models (compute-optimal TTS) with frontier reasoning LLMs\n(CoT) on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nProprietary LLMs (CoT)\nGPT-4o\n74.6\n9.3\n42.0\no1-preview\n85.5\n44.6\n65.1\no1-mini\n90.0\n63.6\n76.8\no1\n94.8\n79.2\n87.0\nOpen-Source LLMs (CoT)\nLlama-3.1-70B-Inst.\n65.2\n16.7\n41.0\nLlama-3.1-405B-Inst.\n71.4\n23.3\n47.4\nQwQ-32B-Preview\n90.6\n50.0\n70.3\nDeepSeek-R1\n97.3\n79.8\n88.6\nOpen-Source LLMs (TTS)\nLlama-3.2-1B-Inst.\n66.2\n16.7\n41.5\nLlama-3.2-1B-Inst. (𝑁= 512)\n72.2\n10.0\n41.1\nLlama-3.2-3B-Inst.\n75.6\n30.0\n52.8\nQwen2.5-0.5B-Inst.\n76.4\n10.0\n43.2\nQwen2.5-1.5B-Inst.\n81.8\n20.0\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n91.6\n63.3\n77.5\nDeepSeek-R1-Distill-Qwen-7B\n95.2\n83.3\n89.3\nperformance of Llama-3.2-3B-Instruct (compute-optimal TTS) with that of Llama-3.1-405B-Instruct\n(CoT) on MATH-500 and AIME24. Also, we compare the performance of Qwen2.5-0.5B-Instruct,\nQwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct with GPT-4o on the above\ntwo tasks. As AIME24 is challenging for current LLMs, we also compare the performance of DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B with o1 on AIME24.\nFrom the results in Table 3, we have the following observations: (1) Llama-3.2-3B-Instruct with the\ncompute-optimal TTS strategy outperforms Llama-3.1-405B-Instruct on MATH-500 and AIME24,\nmeaning that smaller models can outperform 135× larger models using the compute-optimal TTS\nstrategy. Compared with previous works on TTS (Snell et al., 2024; Beeching et al., 2024), we improve\nthe result by 487.0% (23× →135×). (2) If we further increase the compute budget to 𝑁= 512,\nLlama-3.2-1B-Instruct with the compute-optimal TTS strategy beats Llama-3.1-405B-Instruct on\nMATH-500, but underperforms Llama-3.1-405B-Instruct on AIME24.5 (3) Qwen2.5-0.5B-Instruct\nand Llama-3.2-3B-Instruct with the compute-optimal TTS strategy outperforms GPT-4o, indicating\nthat small models can exceed GPT-level performance with the compute-optimal TTS strategy.\n(4) DeepSeek-R1-Distill-Qwen-1.5B with the compute-optimal TTS strategy outperforms o1-preview\nand o1-mini on MATH-500 and AIME24. We also show that DeepSeek-R1-Distill-Qwen-7B with the\ncompute-optimal TTS strategy outperforms o1 and DeepSeek-R1 on MATH-500 and AIME24. These\nresults demonstrate small reasoning-enhanced models can outperform frontier reasoning LLMs\nwith the compute-optimal TTS strategy.\nFLOPS Comparison.\nTo answer the question of whether compute-optimal TTS is more effective\nthan increasing the model size, we compare the FLOPS of evaluated models in Table 4 following Snell\n5Since some outputs of Llama-3.2-1B-Instruct do not contain \\boxed, which is used for answer extraction, we use\nQwen2.5-32B-Instruct to extract the answers of Llama-3.2-1B-Instruct.\n11\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 4: FLOPS comparison between smaller policy models (compute-optimal TTS) and larger ones\n(CoT).\nPolicy Model\nPre-training FLOPS\nInference FLOPS\nTotal FLOPS.\nLlama-3.2-3B-Inst.\n1.62 × 1023\n3.07 × 1017\n1.62 × 1023\nLlama-3.1-405B-Inst.\n3.65 × 1025\n4.25 × 1017\n3.65 × 1025\nDeepSeek-R1-Distill-7B\n7.56 × 1023\n8.15 × 1017\n7.56 × 1023\nDeepSeek-R1\n5.96 × 1025\n4.03 × 1018\n5.96 × 1025\nTable 5: Comparison of compute-optimal TTS, CoT, and majority voting with different policy models\non MATH-500.\nPolicy Model\nCoT\nMajor.\nCompute-Optimal TTS\nPerformance Gain\nEfficiency Gain\nLlama-3.2-1B-Inst.\n26.0\n39.0\n66.2\n154.6%\n>256.0×\nLlama-3.2-3B-Inst.\n41.4\n58.4\n78.2\n88.9%\n14.1×\nLlama-3.1-8B-Inst.\n49.8\n66.4\n80.6\n61.8%\n43.9×\nQwen2.5-0.5B-Inst.\n31.6\n47.2\n76.4\n141.8%\n>64.0×\nQwen2.5-1.5B-Inst.\n54.4\n68.4\n85.6\n57.4%\n>256.0×\nQwen2.5-3B-Inst.\n64.0\n77.0\n87.6\n36.9%\n58.4×\nQwen2.5-7B-Inst.\n76.8\n83.6\n91.0\n18.5%\n35.9×\nQwen2.5-14B-Inst.\n80.2\n85.6\n91.0\n13.5%\n51.4×\nQwen2.5-32B-Inst.\n82.4\n87.0\n90.6\n10.0%\n0.8×\nQwen2.5-72B-Inst.\n83.8\n87.2\n91.8\n9.5%\n12.9×\net al. (2024), where the computed FLOPS is corresponded to the results in Table 3. From the results,\nwe can see that small policy models even surpass large ones with less inference FLOPS and\nreduce the total FLOPS by 100× ∼1000×.\n5.2. How does compute-optimal TTS improve compared with CoT and majority voting? (Q5)\nBased on the findings of compute-optimal TTS with different policy models, PRMs, and difficulty\nlevels, we summarize the results of compute-optimal TTS for each policy model on MATH-500 in\nTable 5. We find that compute-optimal TTS can be 256× more efficient than majority voting and\nimprove reasoning performance by 154.6% over CoT. These results demonstrate that compute-optimal\nTTS significantly enhances the reasoning capabilities of LLMs. However, as the number of parameters\nin the policy model increases, the improvement of TTS gradually decreases. This suggests that the\neffectiveness of TTS is directly related to the reasoning ability of the policy model. Specifically, for\nmodels with weak reasoning abilities, scaling test-time compute leads to a substantial improvement,\nwhereas for models with strong reasoning abilities, the gain is limited.\n5.3. Is TTS more effective than long-CoT-based methods? (Q6)\nRecently, long-CoT-based methods have shown substantial progress in mathematical reasoning (Guan\net al., 2025; Cui et al., 2025; Zeng et al., 2025; DeepSeek-AI et al., 2025). We compare the performance\nof TTS with these approaches.\nSetup.\nWe evaluate the following methods: (1) rStar-Math (Guan et al., 2025): This method first\ngenerates reasoning data via MCTS, followed by online policy and preference model learning. (2)\nEurus-2 (Cui et al., 2025): This method enhances the reasoning abilities of LLMs through implicit\nprocess rewards and online RL. (3) SimpleRL (Zeng et al., 2025): This method replicates self-reflection\n12\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nTable 6: Comparison of compute-optimal TTS with long-CoT methods on MATH-500 and AIME24.\nPolicy Model\nMATH-500\nAIME24\nAvg.\nOpen-Source LLMs (CoT)\nQwen2.5-7B-Inst.\n76.8\n13.3\n45.1\nQwen2.5-Math-7B-Inst.\n79.8\n13.3\n46.6\nLong-CoT Methods (CoT)\nrStar-Math-7B\n78.4\n26.7\n52.6\nEurus-2-7B-PRIME\n79.2\n26.7\n53.0\nQwen2.5-7B-SimpleRL-Zero\n77.2\n33.3\n55.3\nQwen2.5-7B-SimpleRL\n82.4\n26.7\n54.6\nSatori-Qwen-7B\n83.6\n23.3\n53.5\nDeepSeek-R1-Distill-Qwen-7B\n92.4\n63.3\n77.9\nOpen-Source LLMs (TTS)\nQwen2.5-7B-Inst. w/ 7B PRM (Ours)\n88.0\n33.3\n60.5\nQwen2.5-7B-Inst. w/ 72B PRM (Ours)\n91.0\n36.7\n63.9\nwith only 8K training data. (4) Satori (Shen et al., 2025): This method first learn the format and\nthen improves the reasoning abilities via RL. (5) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025): This method distills 800K high-quality reasoning samples from DeepSeek-R1 with 671B\nparameters into a 7B LLM.\nResults.\nAs shown in Table 6, we find that TTS with Qwen2.5-7B-Instruct outperforms rStar-Math,\nEurus-2, SimpleRL, and Satori on both MATH-500 and AIME24. However, while the performance of\nTTS on MATH-500 is close to that of DeepSeek-R1-Distill-Qwen-7B, it shows a significant drop on\nAIME24. These results indicate that TTS is more effective than methods applying direct RL or SFT on\nthe data generated via MCTS but is less effective than distilling from strong reasoning models. Also,\nTTS is more effective on simpler tasks than on more complex tasks.\n6. Related Work\nLLM Test-Time Scaling.\nScaling LLM test-time compute is an effective way to improve the perfor-\nmance (OpenAI, 2024). Previous works explore majority voting (Wang et al., 2023), search-based\nmethods (Yao et al., 2023; Xie et al., 2023; Khanov et al., 2024; Wan et al., 2024), and refinement (Qu\net al., 2024) to improve the performance. For verification-guided test-time compute, Brown et al.\n(2024) explores inference compute with repeated sampling and domain verifiers, while Kang et al.\n(2024); Wu et al. (2024); Snell et al. (2024) further explore search-based methods with process\nreward guidance and Wang et al. (2024c) extends this setting to VLMs. To eliminate the need for\nexternal reward models and the generation of extensive samples, Manvi et al. (2024) proposes a\nself-evaluation method for adaptive and efficient test-time compute. A recent work (Beeching et al.,\n2024) explores TTS via search methods with diversity. However, these works lack a evaluation with\neither strong verifiers or policies with different sizes / capabilities. In this paper, we aim to provide a\nmore systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and\nprovide some principles for practical TTS.\nImproving Mathematical Reasoning Abilities of LLMs.\nPrior methods for improving mathematical\nreasoning abilities can be divided into training-time methods and test-time methods. For training-\n13\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\ntime methods, previous works explore large-scale mathematical corpus pre-training (OpenAI, 2023;\nAzerbayev et al., 2024; Shao et al., 2024) and supervised fine-tuning (Luo et al., 2023; Yu et al., 2024;\nGou et al., 2024; Tang et al., 2024; Tong et al., 2024; Zeng et al., 2024) to improve mathematical\ncapabilities. Another line of works explore self-training and self-improvement strategies (Zelikman\net al., 2022; Gulcehre et al., 2023; Trung et al., 2024; Hosseini et al., 2024; Zelikman et al., 2024;\nZhang et al., 2024a; Setlur et al., 2024a; Kumar et al., 2024; Cui et al., 2025), which improve\nthe reasoning abilities by fine-tuning on self-generated solutions. Recently, many works improve\nthe mathematical reasoning abilities with long CoT (Qin et al., 2024; Huang et al., 2024; Kimi,\n2024; DeepSeek-AI et al., 2025; Qwen Team, 2024; Skywork, 2024; Zhao et al., 2024), as OpenAI\no1 (OpenAI, 2024) shows significantly powerful reasoning capabilities with long thinking.\nFor test-time methods, prompt-based approaches have been extensively studied to enhance reasoning\nwithout altering the model parameters. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022)\nand its variants (Yao et al., 2023; Leang et al., 2024) guide the model to decompose problems into\nmanageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond\nprompting strategies, self-refinement techniques (Madaan et al., 2023) allow models to review and\ncorrect their outputs, while external tool integration (Gao et al., 2023; Chen et al., 2023) leverages\nprogram interpreter or symbolic manipulators to perform precise calculations and validations. Self-\nverification approaches (Weng et al., 2023) enable models to assess the correctness of their own\nreasoning processes, further increasing robustness. These test-time strategies complement training-\ntime enhancements, collectively contributing to significant improvements in LLMs’ mathematical\nreasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time\ncompute via PRM-guided search methods.\nProcess Reward Models.\nPrevious works show that PRMs are more effective than ORMs (Ue-\nsato et al., 2022; Lightman et al., 2024). However, collecting high-quality PRMs data, such as\nPRM800K (Lightman et al., 2024), is often costly. The researchers explores automatic PRM data col-\nlection via direct Monte Carlo estimation (Wang et al., 2024b), detecting relative scores of ORMs (Lu\net al., 2024), and efficient MCTS with binary search (Luo et al., 2024). Recently, more advanced PRMs\nare explored from advantage modeling (Setlur et al., 2024b), 𝑄-value rankings (Li and Li, 2024),\nimplicit rewards (Yuan et al., 2024), and entropy regularization (Zhang et al., 2024b) perspectives.\nAdditionally, more open-source PRMs are released (Xiong et al., 2024; Skywork, 2024; Zhang et al.,\n2024b; Li and Li, 2024; Yuan et al., 2024; Zhang et al., 2025), showing strong performance on\nmathematical tasks. With the rapid development of PRMs, ProcessBench (Zheng et al., 2024) and\nPRMBench (Song et al., 2025) are proposed to provide comprehensive evaluation of PRMs. Zhang\net al. (2025) provides guidelines for practical development of PRMs and releases the most capable\nPRMs for mathematical tasks up-to-date.\n7. Conclusion & Discussion\nIn this paper, we present a thorough empirical analysis of compute-optimal test-time scaling from\nthe perspectives of different policy models, PRMs, and more challenging evaluation tasks. Our\nfindings demonstrate the dependency of compute-optimal TTS strategies on policy models, PRMs,\nand problem difficulty, validating that smaller language models can perform better than larger\nmodels when applying compute-optimal TTS. Our results show that a 1B model can achieve better\nperformance than a 405B model through TTS. Additionally, we demonstrate that a 7B PRM can\nachieve strong TTS results by supervising a more capable 72B policy model, which suggests the\nimportance of investigating a true “weak-to-strong” approach instead of the current “strong-to-weak”\nsupervision for policy optimization. To achieve this goal, we need to develop more efficient supervision\n14\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nmethods, as both PRM-based and RL-based approaches have limitations due to their dependence\non high-quality supervision. Future work should focus on developing more adaptable and universal\nsupervision mechanisms to boost the performance of small language models on complex tasks and\nprovide new approaches for developing efficient reasoning strategies.\nLimitations.\nAlthough we provide a comprehensive evaluation of TTS on mathematical tasks, there\nare still some limitations and future directions to explore: (1) Extending TTS to more tasks such as\ncoding and chemistry tasks. (2) Exploring more effective methods for compute-optimal TTS.\n15\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nReferences\nAI-MO.\nAime\n2024,\n2024.\nURL\nhttps://huggingface.co/datasets/AI-MO/\naimo-validation-aime.\nAnthropic.\nIntroducing\nClaude,\n2023.\nURL https://www.anthropic.com/index/\nintroducing-claude/.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model\nfor mathematics. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=4WnqRR915j.\nEdward Beeching,\nLewis Tunstall,\nand Sasha Rush.\nScaling test-time compute with\nopen\nmodels,\n2024.\nURL\nhttps://huggingface.co/spaces/HuggingFaceH4/\nblogpost-scaling-test-time-compute.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv\npreprint arXiv:2407.21787, 2024.\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision\nwithout process. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL\nhttps://openreview.net/forum?id=VaXnxQ3UKo.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine\nLearning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum?\nid=YfZ4ZPt8zd.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu,\nQixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao,\nXu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process\nreinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao,\nHui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu,\nLeyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding\nZeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao,\nWei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin\n16\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nShen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,\nYanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning (ICML), volume 202, pages 10764–10799, 2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and\nWeizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=Ep0TtjVoap.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.\nrStar-Math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint\narXiv:2501.04519, 2025.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances\nin Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL\nhttps://openreview.net/forum?id=7Bywt2mQsCe.\nArian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh\nAgarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.\nZhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei\nQin, Weizhe Yuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through\nsimple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint\narXiv:2410.21276, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n17\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan\nHe, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time.\narXiv preprint arXiv:2405.16265, 2024.\nMaxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: Alignment as reward-guided search. In\nInternational Conference on Learning Representations (ICLR), 2024. URL https://openreview.\nnet/forum?id=shgx0eqdw6.\nKimi. k0-math, November 2024. URL https://kimi.moonshot.cn/.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms.\narXiv preprint arXiv:2501.12599, 2025.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\nJoshua Ong Jun Leang, Aryo Pradipta Gema, and Shay B Cohen. CoMAT: Chain of mathematically\nannotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024.\nWendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287,\n2024.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In International\nConference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\nJianqiao Lu, Zhiyang Dou, WANG Hongru, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.\nAutopsv: Automated process-supervised verifier. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,\nLei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated\nprocess supervision. arXiv preprint arXiv:2406.06592, 2024.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS),\nvolume 36, pages 46534–46594, 2023.\nRohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict\nif they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI.\nLearning to reason with llms,\n2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n18\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector\nLiu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint\narXiv:2410.18982, 2024.\nYuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. In Advances in Neural Information Processing Systems\n(NeurIPS), 2024. URL https://openreview.net/forum?id=DRC9pZwBwR.\nQwen Team.\nQwq: Reflect deeply on the boundaries of the unknown, November 2024.\nURL\nhttps://qwenlm.github.io/blog/qwq-32b-preview/.\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on\nincorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint\narXiv:2406.14532, 2024a.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process\nverifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nMaohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory\nWornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-\naction-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508,\n2025.\nSkywork. Skywork-o1, November 2024. URL https://www.tiangong.cn/.\nSkywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November\n2024. URL https://huggingface.co/Skywork.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: A fine-grained and\nchallenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nZhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction\ntuning for mathematical reasoning. In International Conference on Machine Learning (ICML), volume\n235, pages 47885–47900, 2024.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware\nrejection tuning for mathematical problem-solving. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024. URL https://openreview.net/forum?id=zLU21oQjD5.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\nreinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7601–7614, 2024.\n19\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nZiyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun\nWang. AlphaZero-like tree-search can guide large language model decoding and training. In\nInternational Conference on Machine Learning (ICML), volume 235, pages 49890–49920, 2024.\nJun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei\nChen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with large\nlanguage models. arXiv preprint arXiv:2410.09671, 2024a.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 9426–9439, 2024b.\nXiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin,\nFurong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for\nimproved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In International Conference on Learning Representations (ICLR), 2023. URL https://\nopenreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural\ninformation processing systems (NeurIPS), volume 35, pages 24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\nLarge language models are better reasoners with self-verification. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 2550–2575, 2023.\nYangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An\nempirical analysis of compute-optimal inference for problem-solving with language models. arXiv\npreprint arXiv:2408.00724, 2024.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.\nSelf-evaluation guided beam search for reasoning. In Advances in Neural Information Processing\nSystems (NeurIPS), volume 36, pages 41618–41650, 2023.\nWei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm.\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng\nHe, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei\nZhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang\nRen, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei\nChu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024a.\n20\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong\nTu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via\nself-improvement. arXiv preprint arXiv:2409.12122, 2024c.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. In Advances in Neural\nInformation Processing Systems (NeurIPS), volume 36, pages 11809–11822, 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for\nlarge language models. In International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=N8N0hgNDRt.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan\nLiu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981,\n2024.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages\n15476–15488, 2022.\nEric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman.\nQuiet-STar: Language models can teach themselves to think before speaking. In Conference on\nLanguage Modeling (COLM), 2024. URL https://openreview.net/forum?id=oRXPiSOGH9.\nLiang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu,\nYang Liu, Shuicheng Yan, et al. Skywork-Math: Data scaling laws for mathematical reasoning in\nlarge language models–the story goes on. arXiv preprint arXiv:2407.08348, 2024.\nWeihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model\nand 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient.\nhttps://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM\nself-training via process reward guided tree search. In Advances in Neural Information Processing\nSystems (NeurIPS), 2024a. URL https://openreview.net/forum?id=8rcFOqEud5.\nHanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang,\nPavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint\narXiv:2412.11006, 2024b.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning. arXiv preprint arXiv:2501.07301, 2025.\n21\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nYu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,\nand Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv\npreprint arXiv:2411.14405, 2024.\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren\nZhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv\npreprint arXiv:2412.06559, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 46595–46623,\n2023.\n22\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nA. Prompt Template for Test-Time Scaling\nThe system prompt for Llama 3 series models (Dubey et al., 2024) and Qwen2.5 series models (Yang\net al., 2024b) are listed in Table 7 and Table 8, respectively. Following Beeching et al. (2024), we use\nthe system prompt of the official evaluation6 for Llama 3 to prevent performance drop.\nTable 7: System prompt for Llama 3 series models.\nSolve the following math problem efficiently and clearly:\n- For simple problems (2 steps or fewer):\nProvide a concise solution with minimal explanation.\n- For complex problems (3 steps or more):\nUse this step-by-step format:\n## Step 1: [Concise description]\n[Brief explanation and calculations]\n## Step 2: [Concise description]\n[Brief explanation and calculations]\n...\nRegardless of the approach, always conclude with:\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\nWhere [answer] is just the final number or expression that solves the problem.\nTable 8: System prompt for Qwen2.5 series models.\nPlease reason step by step, and put your final answer within \\boxed{}.\nB. Full Results of Test-Time Scaling with Different Policy Models, PRMs, and\nScaling Methods\nThe full results of TTS with different policy models, PRMs, and scaling methods are shown in Figure 10\nand Figure 11.\n6https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n23\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.2-1B-Inst.\nLevel 1\n22\n24\n26\n28\n40\n60\n80\n100\nLevel 2\n22\n24\n26\n28\n0\n20\n40\n60\nLevel 3\n22\n24\n26\n28\n92\n94\n96\n98\n100\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n92\n94\n96\n98\n100\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n94\n96\n98\n100\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n80\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n95\n96\n97\n98\n99\n100\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-14B-Inst.\n22\n24\n26\n28\n40\n60\n80\n100\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n96\n97\n98\n99\n100\nQwen2.5-32B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n100\n22\n24\n26\n28\n0\n20\n40\n60\n22\n24\n26\n28\n97\n98\n99\n100\nQwen2.5-72B-Inst.\n22\n24\n26\n28\n20\n40\n60\n80\n100\n22\n24\n26\n28\n0\n20\n40\n60\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 9: TTS performance of three Llama policy models on MATH-500 with different difficulty levels.\n24\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-1.5B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nSkywork-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n30\n40\n50\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n60\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n70\n80\n90\n22\n24\n26\n28\n75\n80\n85\n90\n95\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\n22\n24\n26\n28\n75\n80\n85\n90\n95\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 10: TTS performance of different policy models on MATH-500 with different PRMs and scaling\nstrategies.\n25\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\n22\n24\n26\n28\n0\n10\n20\n30\n40\nLlama-3.2-1B-Inst.\nMath-Shepherd-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Mistral-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nRLHFlow-PRM-Deepseek-8B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-1.5B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nSkywork-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-7B\n22\n24\n26\n28\n0\n10\n20\n30\n40\nQwen2.5-Math-PRM-72B\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.2-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\nLlama-3.1-8B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n60\n22\n24\n26\n28\n0\n5\n10\n15\n20\nQwen2.5-0.5B-Inst.\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n5\n10\n15\n20\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-1.5B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\nQwen2.5-3B-Inst.\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n0\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nQwen2.5-7B-Inst.\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\n22\n24\n26\n28\n10\n20\n30\n40\n50\nPass@k\nMajority\nBest-of-N\nBeam Search\nDVTS\nFigure 11: TTS performance of different policy models on AIME24 with different PRMs and scaling\nstrategies.\n26\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nC. Cases\nIn this section, we provide cases for TTS and summarize several problems for PRMs. By analyzing the\noutput of TTS, we identify several issues with PRMs. Specifically, we observe four major categories:\n(1) Over-Criticism: As shown in Figure 13, the PRM assigns low scores even to mathematically correct\nsteps, resulting in false negatives. (2) Error Neglect: As shown in Figure 14 and Figure 15, the PRM\nsometimes assigns relatively high scores to steps with clear mathematical errors, failing to detect\nthese errors during the reasoning process. (3) Error Localization Bias: As shown in Figure 16, the\nPRM assigns lower scores to certain intermediate steps that are not where the critical errors actually\noccur. This indicates a misalignment between the scoring signal and the actual error locations. (4)\nScoring Bias: As shown in Figure 17 and Figure 18, certain training biases, such as sensitivity to the\ntoken length of intermediate steps, result in large discrepancies in scores for equally correct reasoning\nsteps.\nNotably, these issues persist across both OOD datasets (e.g., the AIME24 dataset, which was not\nused during PRM training) and in-distribution data (e.g., the MATH dataset used to train the model).\nThese problems distort the reasoning search process, degrade overall performance, and reduce the\nreliability of PRM-assisted reasoning. Addressing these biases in future model architectures and\ntraining procedures is necessary to improve the robustness and interpretability of PRMs.\n27\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse (Beam Search with RLHFlow-Mistral-PRM-8B):\n## Step 1: The least positive integer multiple of 30 must be divisible by 10 and 3. \\nSince \nit can only be written with the digits 0 and 2, it must contain at least one 0 to be divisible \nby 10. [score=0.42]\n## Step 2: To be divisible by 3, the sum of its digits must also be divisible by 3.\\nThe \npossible sums of digits are 2, 4, 6, and so on, but only 6 is divisible by 3 and achievable \nwith the digits 2 and 0. [score=0.62]\n## Step 3: We can achieve a sum of 6 by using the number 220 or 022, but we cannot use \nleading zeros.\\nThe number 220 is the smallest multiple of 30 that can be formed with \nthe digits 0 and 2. ки\\n [score=0.56]\n## Step 4: Multiplying 220 by 3 results in 660, which is a multiple of 30.\\nWe\'ve found \nthe least positive integer multiple of 30 that can be written with only the digits 0 and 2. \n[score=0.87]\nTherefore, the final answer is: $\\\\boxed{660}$. [score=0.81]\nProblem: What is the least positive integer multiple of 30 that \ncan be written with only the digits 0 and 2?\nResponse (Beam Search with RLHFlow-Deepseek-PRM-8B):\n## Step 1: Determine the divisibility requirement for the number\\nTo be a multiple of \n30, a number must be divisible by both 2 and 3. [score=0.85]\n## Step 2: Consider the divisibility by 2\\nFor a number to be divisible by 2, it must end \nin 0, since it\'s the only way it will be even. [score=0.89]\n## Step 3: Consider the divisibility by 3\\nFor a number to be divisible by 3, the sum of \nits digits must be divisible by 3. [score=0.92]\n## Step 4: Determine the least possible number meeting these requirements\\nThe\nsmallest number that can be formed with the given digits (0 and 2) and ends in 0 is 200. \nHowever, the sum of its digits (2 + 0 + 0 = 2) is not divisible by 3. [score=0.85]\n## Step 5: Find the next possible number meeting the requirements\\nThe next number \nto try is 220. The sum of its digits (2 + 2 + 0 = 4) is still not divisible by 3. [score=0.91]\n## Step 6: Determine the next possible number meeting the requirements\\nThe next \nnumber is 2200. The sum of its digits (2 + 2 + 0 + 0 = 4) is still not divisible by 3. \n[score=0.91]\n## Step 7: Find the next possible number meeting the requirements\\nThe next number \nto try is 2020. The sum of its digits (2 + 0 + 2 + 0 = 4) is still not divisible by 3. \n[score=0.88]\n## Step 8: Determine the next possible number meeting the requirements\\nThe next \nnumber is 22000. However, we can notice that the sum of the digits is still not divisible by \n3. [score=0.88]\n## Step 9: Consider a number with more 2s\\nInstead of adding more 0s, try adding more \n2s to meet the divisibility requirement of 3. [score=0.91]\n## Step 10: Find the next possible number meeting the requirements\\nThe next number \nto try is 2220. The sum of its digits (2 + 2 + 2 + 0 = 6) is divisible by 3. [score=0.85]\nTherefore, the final answer is: $\\\\boxed{2220}$. [score=0.81]\n# of Tokens: 890\n# of Tokens: 2419\nFigure 12: Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B.\n28\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo simplify \\\\(\\\\sqrt{242}\\\\), we can use the prime factorization method. Here are the \nsteps:  [score=0.90]\n1. **Prime Factorization**: First, we need to find the prime factors of 242.\\n   - 242 is \neven, so it is divisible by 2.\\n     \\\\[\\n     242 \\\\div 2 = 121\\n     \\\\]\\n     So, \\\\(242 = 2 \n\\\\times 121\\\\).\\n   - Next, we factor 121. We know that 121 is \\\\(11 \\\\times 11\\\\).\\n     \n\\\\[\\n     121 = 11 \\\\times 11\\n     \\\\]\\n     So, \\\\(242 = 2 \\\\times 11 \\\\times 11\\\\).  \n[score=0.88]\n2. **Rewrite the Square Root**: Now, we can rewrite the square root of 242 using its \nprime factorization.\\n     \\\\[\\n     \\\\sqrt{242} = \\\\sqrt{2 \\\\times 11 \\\\times 11}\\n     \\\\]  \n[score=0.84]\n3. **Simplify the Square Root**: We can use the property of square roots that \n\\\\(\\\\sqrt{a \\\\times b} = \\\\sqrt{a} \\\\times \\\\sqrt{b}\\\\).\\n     \\\\[\\n     \\\\sqrt{242} = \n\\\\sqrt{2 \\\\times 11 \\\\times 11} = \\\\sqrt{2} \\\\times \\\\sqrt{11 \\\\times 11} = \\\\sqrt{2} \n\\\\times 11 = 11\\\\sqrt{2}\\n     \\\\]  [score=0.53]\nTherefore, the simplified form of \\\\(\\\\sqrt{242}\\\\) is \\\\(\\\\boxed{11\\\\sqrt{2}}\\\\).  \n[score=0.46]\nProblem:\nSimplify $\\\\sqrt{242}$.\nFigure 13: TTS case of Over-Criticism.\n29\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve for \\\\( DE \\\\) given that \\\\(\\\\sin D = 0.7\\\\) and the length \\\\( EF = 7 \\\\), we \ncan follow these steps: [score=0.99]\n1. **Identify the given information and the right triangle:**\\n   - We are given \\\\(\\\\sin D \n= 0.7\\\\).\\n   - \\\\( \\\\angle DEF \\\\) is a right angle.\\n   - \\\\( EF = 7 \\\\). [score=0.99]\n2. **Understand the trigonometric relationship:**\\n   - In a right triangle, \\\\(\\\\sin\\\\) of \nan angle is the ratio of the length of the opposite side to the hypotenuse.\\n   - Here, \n\\\\(\\\\sin D = \\\\frac{EF}{DE}\\\\). [score=0.90]\n3. **Set up the equation using the given \\\\(\\\\sin D\\\\):**\\n   \\\\[\\n   \\\\sin D = \n\\\\frac{EF}{DE}\\n   \\\\]\\n   Substituting the given values:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n   \n\\\\] [score=0.97]\n4. **Solve for \\\\( DE \\\\):**\\n   - To isolate \\\\( DE \\\\), multiply both sides of the \nequation by \\\\( DE \\\\):\\n     \\\\[\\n     0.7 \\\\cdot DE = 7\\n     \\\\]\\n   - Divide both sides by \n0.7:\\n     \\\\[\\n     DE = \\\\frac{7}{0.7}\\n     \\\\]\\n   - Simplify the division:\\n     \\\\[\\n     DE \n= 10\\n     \\\\] [score=0.99]\nTherefore, the length of \\\\( DE \\\\) is \\\\(\\\\boxed{10}\\\\). [score=0.99]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW); \\nlabel(\\"$F$\\",F,SW); \\nlabel(\\"$7$\\", \n(E+F)/2,W); \\n[/asy]\nFigure 14: TTS case of Error Neglect.\n30\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo find the length of \\\\( DE \\\\), we start by analyzing the given right triangle \\\\( DEF \\\\) \nwhere \\\\( \\\\angle EDF = 90^\\\\circ \\\\). We know that the length of \\\\( EF \\\\) is 7 and \nthe sine of angle \\\\( D \\\\) is 0.7. We can use the definition of sine in a right triangle, \nwhich is the ratio of the length of the opposite side to the hypotenuse. [score=0.74]\nGiven:\\n\\\\[ \\\\sin D = \\\\frac{EF}{DE} = 0.7 \\\\]\\nSince \\\\( EF = 7 \\\\), we substitute to \nget:\\n\\\\[ \\\\frac{7}{DE} = 0.7 \\\\]\\nTo find \\\\( DE \\\\), we solve the equation:\\n\\\\[ DE = \n\\\\frac{7}{0.7} = \\\\frac{7}{\\\\frac{7}{10}} = 10 \\\\] [score=0.88]\nThus, the length of \\\\( DE \\\\) is:\\n\\\\[ \\\\boxed{10} \\\\] [score=0.82]\nProblem:\nSuppose $\\\\sin D = 0.7$ in the diagram below. What is $DE$? [asy]\\npair D,E,F;\\nF = \n(0,0);\\nD = (sqrt(51),7);\\nE = (0,7);\\ndraw(D--E--F--D);\\ndraw(rightanglemark(D,E,F,15));\\n \nlabel(\\"$D$\\",D,NE);\\n label(\\"$E$\\",E,NW);\\n label(\\"$F$\\",F,SW);\\n label(\\"$7$\\", \n(E+F)/2, W); \\n [/asy]\nFigure 15: TTS case of Error Neglect.\n31\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nTo solve the problem, we start by noting that \\\\(D\\\\) is the intersection of the tangents \nto the circumcircle \\\\(\\\\omega\\\\) at \\\\(B\\\\) and \\\\(C\\\\). This means that \\\\(D\\\\) is the \nexcenter of the triangle \\\\(ABC\\\\) opposite to \\\\(A\\\\). By the properties of tangents \nfrom a point to a circle, \\\\(DB = DC\\\\).  [score=0.20]\nWe use the fact that \\\\(AD\\\\) is the angle bisector of \\\\(\\\\angle BAC\\\\) when extended \nto intersect the circumcircle again at \\\\(P\\\\). By the Angle Bisector Theorem, we know \nthat the ratio of the segments created by the angle bisector on \\\\(BC\\\\) is equal to the \nratio of the other two sides of the triangle. Therefore, we have:\\n\\\\[\\n\\\\frac{BD}{DC} = \n\\\\frac{AB}{AC} = \\\\frac{5}{10} = \\\\frac{1}{2}\\n\\\\]\\nSince \\\\(BD = DC\\\\), we can use the \nPower of a Point theorem at \\\\(D\\\\). The power of point \\\\(D\\\\) with respect to the \ncircle is:\\n\\\\[\\nDB^2 = DC^2 = DA \\\\cdot DP\\n\\\\]\\nSince \\\\(D\\\\) is the intersection of \nthe tangents at \\\\(B\\\\) and \\\\(C\\\\), we have:\\n\\\\[\\nDB^2 = DC^2 = BD^2 = BD \\\\cdot \nBP\\n\\\\]\\nLet \\\\(BD = x\\\\). Then \\\\(DC = x\\\\) and \\\\(BC = BD + DC = 2x = 9\\\\), so \\\\(x = \n\\\\frac{9}{2}\\\\). Therefore, \\\\(BD = DC = \\\\frac{9}{2}\\\\).  [score=0.66]\nUsing the Power of a Point theorem again, we have:\\n\\\\[\\nBD^2 = DA \\\\cdot DP \\\\implies \n\\\\left(\\\\frac{9}{2}\\\\right)^2 = DA \\\\cdot DP \\\\implies \\\\frac{81}{4} = DA \\\\cdot \nDP\\n\\\\]\\nWe need to find the length of \\\\(DA\\\\). By the Angle Bisector Theorem, the \nratio of the segments \\\\(BD\\\\) and \\\\(DC\\\\) is the same as the ratio of \\\\(AB\\\\) and \n\\\\(AC\\\\), which is \\\\(\\\\frac{1}{2}\\\\). Therefore, \\\\(DA\\\\) can be found using the \nformula for the length of the angle bisector:\\n\\\\[\\nAD = \\\\frac{2 \\\\cdot AB \\\\cdot AC \n\\\\cdot \\\\cos \\\\left(\\\\frac{A}{2}\\\\right)}{AB + AC}\\n\\\\]\\nHowever, a simpler approach is \nto use the fact that \\\\(AD \\\\cdot DP = \\\\left(\\\\frac{9}{2}\\\\right)^2 = \\\\frac{81}{4}\\\\). \nSince \\\\(AD + DP = AP\\\\), we need to find \\\\(AP\\\\). Using the fact that \\\\(DA \\\\cdot DP \n= \\\\frac{81}{4}\\\\) and knowing that \\\\(DA = \\\\frac{90}{13}\\\\) (from the Angle Bisector \nTheorem and the length of the angle bisector formula), we can solve for \n\\\\(AP\\\\):\\n\\\\[\\nAP = \\\\frac{81}{4 \\\\cdot DA} = \\\\frac{81}{4 \\\\cdot \\\\frac{90}{13}} = \n\\\\frac{81 \\\\cdot 13}{4 \\\\cdot 90} = \\\\frac{1053}{360} = \\\\frac{117}{40}\\n\\\\]\\nThus, \n\\\\(AP = \\\\frac{117}{40}\\\\), and the sum \\\\(m + n\\\\) is:\\n\\\\[\\n117 + 40 = \n157\\n\\\\]\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{157}\\n\\\\]  [score=0.92]\nProblem:\nLet $ABC$ be a triangle inscribed in circle $\\\\omega$. Let the tangents to $\\\\omega$ at $B$ \nand $C$ intersect at point $D$, and let $\\\\overline{AD}$ intersect $\\\\omega$ at $P$. If \n$AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\\\frac{m}{n}$, where $m$ \nand $n$ are relatively prime integers. Find $m + n$.\nFigure 16: TTS case of Error Localization Bias.\n32\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( n \\\\). \nTherefore, the total number of coins before finding the bag of 53 coins is \\\\( 7n \\\\). \nAfter finding the bag of 53 coins, the total number of coins becomes \\\\( 7n + 53 \\\\). \n[score=0.98]\nSince you redistribute the coins so that all eight bags have the same number of coins, the \ntotal number of coins must be divisible by 8. Therefore, \\\\( 7n + 53 \\\\) must be divisible \nby 8. We can express this condition as:\\n\\\\[ 7n + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ 7n \n\\\\equiv -53 \\\\pmod{8} \\\\]\\nSince \\\\(-53 \\\\equiv -53 + 64 = 11 \\\\equiv 3 \\\\pmod{8}\\\\), \nwe have:\\n\\\\[ 7n \\\\equiv 3 \\\\pmod{8} \\\\] [score=0.81]\nTo solve for \\\\( n \\\\), we need the multiplicative inverse of 7 modulo 8. The multiplicative \ninverse of 7 modulo 8 is a number \\\\( x \\\\) such that:\\n\\\\[ 7x \\\\equiv 1 \\\\pmod{8} \n\\\\]\\nBy testing values, we find that \\\\( x = 7 \\\\) works because:\\n\\\\[ 7 \\\\cdot 7 = 49 \n\\\\equiv 1 \\\\pmod{8} \\\\]\\nThus, multiplying both sides of \\\\( 7n \\\\equiv 3 \\\\pmod{8} \\\\) \nby 7, we get:\\n\\\\[ n \\\\equiv 21 \\\\pmod{8} \\\\]\\n\\\\[ n \\\\equiv 5 \\\\pmod{8} \\\\]\\nSo, \\\\( n \n= 8k + 5 \\\\) for some integer \\\\( k \\\\). [score=0.92]\nThe total number of coins after finding the bag of 53 coins is \\\\( 7n + 53 = 7(8k + 5) + 53 \n= 56k + 35 + 53 = 56k + 88 \\\\). Since the number of coins must be more than 200, we \nhave:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \\\\]\\nThe smallest integer \n\\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). Substituting \\\\( k = 3 \\\\) into the \nexpression for the total number of coins, we get:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \n\\\\] [score=0.79]\nTo find the number of coins before finding the bag of 53 coins, we calculate:\\n\\\\[ 7n = 7 \n\\\\cdot (8 \\\\cdot 3 + 5) = 7 \\\\cdot 29 = 203 \\\\] [score=0.51]\nThus, the smallest number of coins you could have had before finding the bag of 53 coins \nis \\\\(\\\\boxed{203}\\\\). [score=0.68] [token num=31]\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nFigure 17: TTS case of Scoring Bias.\n33\n\nCan 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling\nProblem:\nYou have seven bags of gold coins. Each bag has the same number of gold coins. One day, you \nfind a bag of 53 coins. You decide to redistribute the number of coins you have so that all eight \nbags you hold have the same number of coins. You successfully manage to redistribute all the \ncoins, and you also note that you have more than 200 coins. What is the smallest number of \ncoins you could have had before finding the bag of 53 coins?\nResponse:\nLet\'s denote the number of gold coins in each of the seven bags initially as \\\\( x \\\\). \nTherefore, the total number of coins initially is \\\\( 7x \\\\). After finding a bag of 53 coins, \nthe total number of coins becomes \\\\( 7x + 53 \\\\).  [score=0.98]\nWe are given that the total number of coins can be evenly distributed among 8 bags, so \n\\\\( 7x + 53 \\\\) must be divisible by 8. Additionally, we know that the total number of \ncoins is more than 200. We need to find the smallest possible value of \\\\( 7x + 53 \\\\) \nthat satisfies both conditions.  [score=0.95]\nFirst, we express the condition that \\\\( 7x + 53 \\\\) is divisible by 8:\\n\\\\[ 7x + 53 \\\\equiv \n0 \\\\pmod{8} \\\\]\\nSince \\\\( 7 \\\\equiv -1 \\\\pmod{8} \\\\), we can rewrite the equation \nas:\\n\\\\[ -x + 53 \\\\equiv 0 \\\\pmod{8} \\\\]\\n\\\\[ -x \\\\equiv -53 \\\\pmod{8} \\\\]\\n\\\\[ x \n\\\\equiv 53 \\\\pmod{8} \\\\]\\n\\\\[ x \\\\equiv 5 \\\\pmod{8} \\\\]\\nThis means that \\\\( x \\\\) can \nbe written in the form:\\n\\\\[ x = 8k + 5 \\\\]\\nfor some integer \\\\( k \\\\).  [score=0.90]\nNext, we substitute \\\\( x = 8k + 5 \\\\) into the expression for the total number of \ncoins:\\n\\\\[ 7x + 53 = 7(8k + 5) + 53 = 56k + 35 + 53 = 56k + 88 \\\\]\\nWe need \\\\( 56k + \n88 \\\\) to be more than 200:\\n\\\\[ 56k + 88 > 200 \\\\]\\n\\\\[ 56k > 112 \\\\]\\n\\\\[ k > 2 \n\\\\]\\nThe smallest integer \\\\( k \\\\) that satisfies this inequality is \\\\( k = 3 \\\\). \nSubstituting \\\\( k = 3 \\\\) into the expression for the total number of coins, we \nget:\\n\\\\[ 56 \\\\cdot 3 + 88 = 168 + 88 = 256 \\\\]\\nThus, the smallest number of coins you \ncould have had before finding the bag of 53 coins is:\\n\\\\[ 7x = 7(8 \\\\cdot 3 + 5) = 7 \n\\\\cdot 29 = 203 \\\\]\\nTherefore, the smallest number of coins you could have had before \nfinding the bag of 53 coins is \\\\(\\\\boxed{203}\\\\). [score=0.12] [token num=283]\nFigure 18: TTS case of Scoring Bias.\n34'),
                Paper(arxiv_id='2502.06394', authors=['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], published_at=datetime.datetime(2025, 2, 11, 3, 3, 12, 135000, tzinfo=datetime.timezone.utc), title='SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators', summary='Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.', upvotes=76, thumbnail=None, content='SynthDetoxM: Modern LLMs are Few-Shot\nParallel Detoxification Data Annotators\nDaniil Moskovskiy1,2*\nNikita Sushko1,2*\nSergey Pletenev1,2\nElena Tutubalina1,3,4\nAlexander Panchenko2,1\n1AIRI\n2Skoltech\n3Sber AI\n4ISP RAS Research Center for Trusted AI\nCorrespondence: {d.moskovskiy, a.panchenko}@skol.tech\nAbstract\nExisting approaches to multilingual text detox-\nification are hampered by the scarcity of par-\nallel multilingual datasets. In this work, we\nintroduce a pipeline for the generation of\nmultilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually col-\nlected and synthetically generated multilingual\nparallel text detoxification dataset compris-\ning 16,000 high-quality detoxification sentence\npairs across German, French, Spanish and Rus-\nsian. The data was sourced from different toxi-\ncity evaluation datasets and then rewritten with\nnine modern open-source LLMs in few-shot\nsetting. Our experiments demonstrate that mod-\nels trained on the produced synthetic datasets\nhave superior performance to those trained on\nthe human-annotated MultiParaDetox dataset\neven in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs\nin few-shot setting. We release our dataset and\ncode to help further research in multilingual\ntext detoxification.\nWarning: this paper contains illustrative examples of\ntexts that readers may find offensive or disturbing.\n1\nIntroduction\nThe proliferation of social networks and text-based\ninternet media has highlighted the issue of online\ntoxicity and hate speech (Saha et al., 2019). This\nphenomenon not only creates an unpleasant envi-\nronment for users but also deters advertisers, poten-\ntially impacting the economic viability of these plat-\nforms (Fortuna and Nunes, 2018). Consequently,\nthere is an urgent need for effective mechanisms to\nmeasure and mitigate toxicity in online spaces.\nA promising approach to addressing this chal-\nlenge is text detoxification of text through para-\nphrasing (Krishna et al., 2020). Text detoxification\nis a subtask of text style transfer (TST), which in-\nvolves rewriting text while preserving its original\n*Equal contribution.\nToxic Text\nDetoxified Text\nGerman\nWie be**oppt muss\nman sein?\nWie\nverwirrt\nmuss\nman sein?\nSpanish\nQue os den por el\nc**o.\nQue os dé muy mala\nsuerte.\nFrench\nc’est moi at***dé ! je\nsuis tombé !\nC’est moi qui suis\ntombé !\nRussian\nя\nмужик\nа\nвы\nг**но\nЯ мужчина, а вы\nнеправы\nTable 1: Examples of the source toxic texts across dif-\nferent languages and their respective synthetic detoxifi-\ncations from our SynthDetoxM.\nmeaning and altering specific style attribute, such\nas formality, bias, expressiveness, sentiment, or, in\nthe case of detoxification, toxicity (Fu et al., 2018;\nLai et al., 2021).\nWhile significant progress has been made in\nmonolingual TST and detoxification, both in super-\nvised and unsupervised settings (Dale et al., 2021;\nLogacheva et al., 2022; Pour et al., 2023), multilin-\ngual text detoxification remains a largely unsolved\nproblem. This is primarily due to two factors: the\nscarcity of parallel detoxification data across multi-\nple languages and the suboptimal performance of\nunsupervised methods in cross-lingual settings (De-\nmentieva et al., 2023).\nManual or crowdsourced data collection is a chal-\nlenging and costly task (Rao and Tetreault, 2018;\nReid and Artetxe, 2023; Konovalov et al., 2016b),\ncreating parallel data with the use of modern LLMs,\nwhich already proven to work well for the tasks of\ntext classification (Sun et al., 2023) and question\nanswering (Ye et al., 2022), remains underexplored.\nTo address these challenges and facilitate the devel-\nopment of multilingual text detoxification models\nand datasets, we propose a framework for gener-\nating parallel multilingual synthetic detoxification\ndata and SynthDetoxM, a large-scale multilinugal\nsynthetic parallel text detoxification dataset, which\nwas created using this framework.\narXiv:2502.06394v1  [cs.CL]  10 Feb 2025\n\nMultilingual\nToxic Data\nSource Toxic Data\nDetoxification-1\nDetoxification-2\nDetoxification-3\nDetoxification-4\nDetoxification-5\nScore(𝑥𝑥𝑖𝑖; 𝑦𝑦𝑖𝑖)\nxi 𝑖𝑖=1\nn\nyi 𝑖𝑖=1\nn\nSynthDetoxM\nLLM\nClean and filter \ndata by length\nGenerate detox\ncandidates using LLMs\nScore detoxification candidates and select best\nxi; yibest 𝑖𝑖=1\nn\nCompose final \ndetox dataset\nFigure 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification\ndataset SynthDetoxM.\nOur dataset is comprised of 16,000 high-quality\nsynthetic detoxification pairs across four languages:\nGerman, Spanish, French and Russian. The dataset\nwas created using few-shot prompting and selecting\nthe best generations of five different open-source\nLLMs. The answers were combined using a hand-\ncrafted heuristic, providing the best answers from\neach model to ensure diversity and quality of the\nfinal data.\nOur contributions can be summarized as follows:\n1. We propose a framework for generating syn-\nthetic parallel multilingual detoxification data\nusing few-shot prompting of LLMs.\n2. We create SynthDetoxM, a large-scale multilin-\ngual synthetic parallel dataset for text detox-\nification, helping to address the data scarcity\nissue in the detoxification task.\n3. We conduct a thorough empirical evaluation\nof the proposed dataset, including linguistic\nanalysis of the data and benchmarking against\nthe human-annotated MultiParaDetox.\nWe openly release the generated data and code.1\n2\nBackground and Related Work\n2.1\nText Style Transfer\nText Style Transfer (TST), the task of rewriting\nthe text in a target style while preserving its se-\nmantic content and fluency, has garnered signifi-\ncant attention in the natural language processing\ncommunity due to its potential applications in text\ngeneration (Fu et al., 2018). TST encompasses\nvarious subtasks, including formality style trans-\nfer (Wang et al., 2020; Lai et al., 2021), sentiment\nstyle transfer (Yu et al., 2021), authorship style\ntransfer (Horvitz et al., 2024; Liu et al., 2024), and\n1github.com/s-nlp/synthdetoxm\ndetoxification (Dale et al., 2021; Atwell et al., 2022;\nMoskovskiy et al., 2022, 2024).\nWith the advent of Large Language Models\n(LLMs), in-context learning methods have increas-\ningly been utilized for TST and detoxification tasks.\nSuzgun et al. (2022) proposed a novel approach to\nTST by prompting LLMs and then reranking the\ngenerated texts based on three TST metrics: text\nsimilarity, target style strength, and fluency. Simi-\nlarly, Reif et al. (2022) demonstrated the effective-\nness of prompting GPT-3, a state-of-the-art LLM\nat the time, to rewrite texts in a desired style.\n2.2\nText Detoxification\nText Detoxification, a subtask of Text Style Trans-\nfer (TST), involves transforming an input text xi,\nidentified as toxic through toxicity estimation mod-\nels, into a text yi that is non-toxic in style while\nmaintaining semantic similarity and fluency. In this\ncontext, toxicity refers to language that is harmful,\noffensive, or inappropriate.\nDue to the lack of parallel training data, early\nresearch focused on unsupervised detoxification\nmethods (dos Santos et al., 2018; Dale et al.,\n2021; Hallinan et al., 2023; Pour et al., 2023).\nFor instance,(Logacheva et al., 2022) and APP-\nDIA (Atwell et al., 2022), has enabled the training\nof sequence-to-sequence models (Logacheva et al.,\n2022; Pour et al., 2023) that outperform most un-\nsupervised approaches in terms of rewritten toxi-\ncity, fluency, and semantic similarity. In parallel,\nMoskovskiy et al. (2024) explored the use of ac-\ntivation patching in LLMs to generate synthetic\nparallel detoxification data for English. Their re-\nsults demonstrated that training detoxification mod-\nels on this data yields performance comparable\nto models trained on manually annotated datasets\nin automatic evaluations, while achieving superior\nquality in human assessments.\n\n2.3\nMultilingual Text Style Transfer\nThe scarcity of high-quality parallel multilingual\ndetoxification data remains a major challenge in the\nfield. Recently, new non-English parallel datasets\nhave been introduced for various TST tasks, in-\ncluding a Bangla language parallel sentiment style\ntransfer dataset (Mukherjee et al., 2023) and the\nextension of the GYAFC dataset to Portuguese,\nFrench, and Italian, resulting in XFORMAL (Bri-\nakou et al., 2021). Following the crowdsourcing\npipeline introduced by (Logacheva et al., 2022), a\nparallel text detoxification dataset for Russian was\ncollected (Moskovskiy et al., 2022). Later, using\nthe similar data annotation pipeline, Dementieva\net al. (2024) collected 1000 sentence pairs across\nnine languages, resulting in the MultiParaDetox\ndataset for a corresponding shared task on multi-\nlingual text detoxification. Furthermore, in a more\nrecent work Dementieva et al. (2025), provide an\nin-depth analysis of toxicity characteristics across\nlanguages, exploring descriptive linguistic features\nthat influence detoxification quality.\nNevertheless, the size of MultiParaDetox is far\nfrom satisfactory with 1000 sentence pairs per lan-\nguage, only 400 of which are publicly available.\nThe remaining 600 pairs comprised the test set for\nthe multilingual text detoxification shared task (De-\nmentieva et al., 2024). Such relatively small dataset\nmay be insufficient for training big multilingual lan-\nguage models for multilingual text detoxification.\nTo bridge this gap, we present SynthDetoxM - a\nsynthetic parallel detoxification corpus for four\nEuropean languages, namely, German, Spanish,\nFrench, and Russian, with 4000 samples for each\nlanguage. The dataset creationg pipeline presented\nin our work can easily be transferred to other lan-\nguages as well, drastically reducing the cost of\nannotation for parallel detoxification datasets.'),
                Paper(arxiv_id='2502.06060', authors=['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], published_at=datetime.datetime(2025, 2, 11, 4, 8, 55, 672000, tzinfo=datetime.timezone.utc), title='Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning', summary="Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", upvotes=25, thumbnail=None, content='Training Language Models for Social Deduction with Multi-Agent\nReinforcement Learning\nBidipta Sarkar\nStanford University\nStanford, United States of America\nbidiptas@cs.stanford.edu\nWarren Xia\nStanford University\nStanford, United States of America\nwaxia@cs.stanford.edu\nC. Karen Liu\nStanford University\nStanford, United States of America\nkarenliu@cs.stanford.edu\nDorsa Sadigh\nStanford University\nStanford, United States of America\ndorsa@cs.stanford.edu\nABSTRACT\nCommunicating in natural language is a powerful tool in multi-\nagent settings, as it enables independent agents to share information\nin partially observable settings and allows zero-shot coordination\nwith humans. However, most prior works are limited as they either\nrely on training with large amounts of human demonstrations or\nlack the ability to generate natural and useful communication strate-\ngies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without\nany human demonstrations. We decompose the communication\nproblem into listening and speaking. Our key idea is to leverage\nthe agent’s goal to predict useful information about the world as a\ndense reward signal that guides communication. Specifically, we\nimprove a model’s listening skills by training them to predict in-\nformation about the environment based on discussions, and we\nsimultaneously improve a model’s speaking skills with multi-agent\nreinforcement learning by rewarding messages based on their in-\nfluence on other agents. To investigate the role and necessity of\ncommunication in complex social settings, we study an embodied\nsocial deduction game based on Among Us, where the key question\nto answer is the identity of an adversarial imposter. We analyze\nemergent behaviors due to our technique, such as accusing suspects\nand providing evidence, and find that it enables strong discussions,\ndoubling the win rates compared to standard RL. We release our\ncode and models at https://socialdeductionllm.github.io/.\nCCS CONCEPTS\n• Computing methodologies →Multi-agent reinforcement\nlearning; Stochastic games; Cooperation and coordination; • Infor-\nmation systems →Language models.\nKEYWORDS\nLanguage Models; Multi-Agent Reinforcement Learning; Social\nDeduction; LLM Agents\nThis work is licensed under a Creative Commons Attribution Inter-\nnational 4.0 License.\nProc. of the 24th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2025), Y. Vorobeychik, S. Das, A. Nowé (eds.), May 19 – 23, 2025, Detroit, Michigan,\nUSA. © 2025 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org).\nACM Reference Format:\nBidipta Sarkar\n, Warren Xia\n, C. Karen Liu\n, and Dorsa Sadigh\n. 2025.\nTraining Language Models for Social Deduction with Multi-Agent Reinforce-\nment Learning. In Proc. of the 24th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May\n19 – 23, 2025, IFAAMAS, 14 pages.\n1\nINTRODUCTION\nA longstanding goal of multi-agent artificial intelligence is the de-\nvelopment of independent agents that can communicate using a\nshared language. Communication is especially necessary in “par-\ntially observable” settings, where each agent only has a limited view\nof the world and therefore benefits from sharing knowledge with\nother agents to achieve its goal. In particular, “social deduction”\ngames are settings where each agent’s goal is to deduce informa-\ntion about the environment by communicating with other agents –\nrequiring each player to learn how to parse messages from other\nplayers while effectively sharing important information needed for\ngame completion.\nIn this work, we study the hidden-role game of Among Us [18]\nas a specific instance of a challenging social deduction game to\ninvestigate the importance of communication, illustrated in Fig. 1.\nHidden-role games [4, 19] are a class of environments where play-\ners are split into an uninformed majority and a smaller informed\nhidden subteam, which we refer to as crewmates and imposters re-\nspectively. These two teams are adversaries, resulting in a zero-sum\ngame, where the goal of the crewmates is to deduce the identity of\nimposters to vote them out. Unlike other popular hidden role games\nsuch as the game of Mafia [2], where statements from players are\nunfalsifiable, Among Us is based in a 2D embodied environment,\nallowing discussions and intuitions to be grounded in specific ob-\nservations. In the game, crewmates try to complete an assigned set\nof tasks scattered across the environment while imposters try to\nkill all crewmates. If a player reports the corpse of an eliminated\ncrewmate – killed by an imposter – the game moves to a discussion\nphase with a free-form chat followed by a voting period, where all\nplayers vote to eject a suspected imposter. For crewmates, success\nin the discussion phase would mean correctly voting out the im-\nposter, while success for imposters means avoiding suspicion from\nthe crewmates to continue staying in the game as long as possi-\nble. This highlights the importance of communication during the\ndiscussion phase as crewmates need to learn to effectively utilize\narXiv:2502.06060v1  [cs.AI]  9 Feb 2025\n\nFigure 1: Examples of the gameplay and discussion phases of Among Us. During gameplay, all agents navigate a 2D grid\nenvironment (a 1-by-2 grid in this case, with two rooms at (0,0) and (1,0)), where agents can see everything in their same room.\nHere, the red, green, and yellow agents are in room (1,0), and the purple and blue agents are in room (0,0). Crewmates can\nperform tasks (indicated by the stars – in this example there are 3 tasks), while imposters kill crewmates. Here, the orange and\ngreen agents are working on tasks. Agents can also report dead bodies, as the purple agent is currently doing, which initiates\nthe discussion phase. During discussion phases, agents leverage large language models to generate free-form messages guided\nby our framework encouraging effective speaking and listening within the crewmates and finally vote out a suspected imposter.\nThe example discussion shown on the right is based on a generated discussion from our trained models.\nthe discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of Among Us from\nthe perspective of crewmates attempting to perform tasks, identify\nimposters, and win the game.\nIn multi-agent environments, an effective technique for training\nstrong cooperative and competitive agents is multi-agent reinforce-\nment learning (MARL), which enables artificial agents to achieve\nsuperhuman levels of performance in competitive games such as\nStarCraft [36], and cooperative games such as Overcooked [5, 31]\nand Hanabi [13]. However, in settings where communication in\nnatural language is necessary, existing MARL techniques often\nstruggle as they require large datasets of task-specific human com-\nmunication data to perform on-par with humans [8]. This funda-\nmentally limits the agents’ ability to communicate at human-level\nand is not practical for learning in settings where these datasets do\nnot readily exist. The game of Among Us falls into this category,\nwhere communication is necessary to reason and progress in the\ngame. Therefore, we would like to find an approach that learns to\ncommunicate effectively and convincingly without requiring large\namounts of task-specific human data. However, the major chal-\nlenge in learning to communicate without access to large amounts\nof human data is that novice agents do not have a strong signal for\nunderstanding the helpfulness of the messages they send (speak-\ning) or for learning the meaning of messages from other players\n(listening). In particular, the sparse reward signal the agents receive\nwhen winning the game is not informative enough to reinforce\nhigh-quality discussions between agents. Our key insight is that\nwe can leverage the agents’ instrumental goal of predicting useful\ninformation about the world – e.g., the identity of imposters – as\na dense reward to provide a higher-quality signal that can enable\nmore informative communication during the discussion phase and\npotentially higher performance policies.\nWe propose an approach that rewards a message generated dur-\ning the discussion phase based on how the other crewmates’ beliefs\non the identity of the imposter changes. Each crewmate wants to\nsend messages that help other crewmates be more certain about\nthe true identity of the imposter. However, this only explains how\nto learn to “speak” assuming that the other agents can appropri-\nately update their belief about the world given a message. We also\nneed to ensure the agents know how to “listen” and update beliefs\nappropriately. To encourage this, we additionally add an imposter\nprediction signal to guide the agent’s learning to predict the true\nidentity of the imposter after each message. By training agents to\nspeak and listen effectively, we enable the agents to self-improve\ntheir discussion abilities. Further, to encourage listening and speak-\ning in natural language during the discussion phase of the game,\nwe tap into the power of large language models (LLMs), unspecial-\nized models trained with large amounts of human language data.\nSpecifically, we initialize crewmates as LLMs capable of communi-\ncating via natural language. Recent advances in foundation models\nhave demonstrated some reasoning abilities [3, 27], including un-\nderstanding social scenarios [20], but even the strongest language\nmodels today are weak at self-critiquing [34] or performing theory\nof mind reasoning [33], limiting their ability to improve their lis-\ntening skills based on their own feedback. However, by training\nLLMs within our proposed framework of encouraging listening and\nspeaking with auxiliary dense rewards for helping other crewmates\nvote out the correct imposter, we overcome this limitation, enabling\nthe self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crew-\nmates against both pretrained and adaptive imposters, and find that\ncrewmates form a robust communication strategy. We find that our\ntechnique results in emergent behavior commonly found in real\ngames of Among Us between humans, such as directly accusing\n\nplayers and providing evidence to help other crewmates [22]. We\nalso find that our augmentation to discussions results in two times\nhigher success rates relative to standard RL along with over three\ntimes higher success rates relative to base models that are over four\ntimes larger than our models, highlighting the importance of our\ndiscussion strategies.'),
                Paper(arxiv_id='2502.05664', authors=['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], published_at=datetime.datetime(2025, 2, 11, 9, 36, 24, 937000, tzinfo=datetime.timezone.utc), title='CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging', summary="Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", upvotes=18, thumbnail=None, content='CODESIM: Multi-Agent Code Generation and Problem Solving through\nSimulation-Driven Planning and Debugging\nMd. Ashraful Islam*1, Mohammed Eunus Ali1, Md Rizwan Parvez2\n1Bangladesh University of Engineering and Technology (BUET)\n2Qatar Computing Research Institute (QCRI)\n{mdashrafulpramanic, mohammed.eunus.ali}@gmail.com, mparvez@hbku.edu.qa\nAbstract\nLarge Language Models (LLMs) have made\nsignificant strides in code generation and\nproblem solving. Current approaches employ\nexternal tool-based iterative debuggers that use\ncompiler or other tool-based runtime feedback\nto refine coarse programs generated by\nvarious methods. However, the effectiveness\nof\nthese\napproaches\nheavily\nrelies\non\nthe quality of the initial code generation,\nwhich remains an open challenge.\nIn this\npaper, we introduce CODESIM, a novel\nmulti-agent\ncode\ngeneration\nframework\nthat comprehensively addresses the stages\nof\nprogram\nsynthesis—planning,\ncoding,\nand\ndebugging—through\na\nhuman-like\nperception approach. As human verifies their\nunderstanding of any algorithms through\nvisual\nsimulation,\nCODESIM\nuniquely\nfeatures a method of plan verification and\ninternal debugging through the step-by-step\nsimulation\nof\ninput/output.\nExtensive\nexperiments\nacross\nseven\nchallenging\ncompetitive problem-solving and program\nsynthesis benchmarks demonstrate CODESIM’s\nremarkable code generation capabilities. Our\nframework\nachieves\nnew\nstate-of-the-art\n(pass@1) results—(HumanEval 95.1%, MBPP\n90.7%, APPS 22%, and CodeContests 29.1%).\nFurthermore, our method shows potential for\neven greater enhancement when cascaded\nwith external debuggers. To facilitate further\nresearch and development in this area, we\nhave open-sourced our framework in this link\n(https://kagnlp.github.io/codesim.github.io/).\n1\nIntroduction\nIn recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n*Work done when working as a remote RA at QCRI.\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024).\nIn the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation—a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,\narXiv:2502.05664v1  [cs.CL]  8 Feb 2025\n\nProblem\nCheck if in given list of numbers,\nare any two numbers closer to\neach other than given threshold.\nSample I/O\nPassed?\nNo\nYes\nPlanning\xa0\nAgent\nPlans\nCoding\xa0\nAgent\nCode\nDebugged\nCode\nDebugging\xa0\nAgent\nTry to debug\xa0\nd times\nIf all d debugging\xa0steps\xa0fails\nto pass sample I/O,\xa0loop\xa0back\xa0\nto Planning Agent\xa0p times.\xa0\nPlan\nPlan\nOK?\nYes\nNo\nGenerate\xa0\nExemplar\xa0\n&\xa0Plan\xa0\nSimulate\n& Verify\nPlan\xa0\nRevise\nPlan\nGenerate\nCode\nProblem\nPlan\nSimulate on\nFailed I/O\n& Debug\nTesting\nFeedback\nPlan\nCode\nProblem\nCodeSim Pipeline\nPlanning Agent\nCoding Agent\nDebugging Agent\nFigure 1: Overview of CODESIM: It consists of three agents—planning, coding, and debugging. The Planning Agent\nfirst generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and\nrefined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses\npotential bugs through step-wise simulation across d trials. The entire process iterates p times.\n2021), and various in-context exemplar-based\nstrategies (Shum et al., 2023; Zhang et al., 2022),\nrecent paradigms have shifted toward plan-based\n(Jiang et al., 2023b), sampling or tree-searching\n(Zhou et al., 2023), self-retrieval (Yasunaga et al.,\n2023), and diverse agent-based approaches (Zhang\net al., 2024; Qian et al., 2024; Shinn et al., 2023;\nHuang et al., 2023; Dong et al., 2023b).\nMost recently, MapCoder (Islam et al., 2024a)\nproposes a multi-agent framework that implements\nagents emulating different stages of program syn-\nthesis such as recalling relevant examples, design-\ning/planning, code generation, and testing/debug-\nging. While this approach mimics a real devel-\noper’s code generation cycle and shows improve-\nments, it focuses solely on expanding steps without\nverifying the underlying hypotheses, with tests be-\ning performed only during the debugging phase.\nConsequently, the resulting gains are limited and it\nalso requires larger number of iterations (i.e., LLM\nAPI calls).\nTo address these limitations, CODESIM—built\nupon\nplanning,\ncoding,\nand\ndebugging\nagents—introduces a novel verification approach\ninspired by human problem-solving. By simulating\ninput/output step-by-step,\nCODESIM\nverifies\nboth the generated plans and performs internal\ndebugging, mirroring how humans understand,\nvisualize, and refine algorithms. This simulation-\ndriven planning and debugging process ensures\nthat each step is thoroughly evaluated, significantly\nenhancing both solution quality and efficiency.\nFigure 1 shows an overview of our proposed ap-\nproach, CODESIM and in Figure 2, we demonstrate\nhow simulation assists in both plan verification\nand debugging, highlighting its crucial role in\nimproving problem-solving accuracy.\nWe evaluate CODESIM on seven popular pro-\ngramming synthesis benchmarks, including foun-\ndational tasks like HumanEval and MBPP, as well\nas challenging competitive problem-solving bench-\nmarks such as APPS, and CodeContest. Our ex-\nperiments leverage multiple LLMs, including Chat-\nGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mix-\ntral, showcasing significant improvements in their\nprogram synthesis capabilities. CODESIM consis-\ntently achieves state-of-the-art performances, often\nsurpassing strong baselines like MapCoder. Ad-\nditionally, our findings suggest that CODESIM’s\nperformance can be further improved when inte-\ngrated with external debugging tools, such as LDB\n(Zhong et al., 2024), highlighting a promising di-\nrection for future research in hybrid code gener-\nation and debugging systems. Through detailed\nablation studies, we provide valuable insights into\nCODESIM’s functionality. We will open-source our\nframework to support future research in AI-assisted\nprogramming and problem-solving.\n2\nRelated Work\nCode Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,\n\nLLM\nLoop from 2 to 3\nSimulation of tri(3)\nFor n = 2:\xa0since it\'s even, use the formula\xa0\n`tri(2) = 1 + 2 / 2 = 2`.\xa0\xa0Append `2` to the list:\xa0\n3, 2\nInitialize:\n3\nFor n = 3:\xa0\xa0since it\'s odd, use the recursive formula\xa0\n\xa0 \xa0 `tri(3) = tri(2) + tri(1) + tri(4)`.\n\xa0 \xa0- We have `tri(2) = 2` and `tri(1) = 3` (base case).\n\xa0 \xa0- We need `tri(4)`, which is even, so `tri(4) = 1 + 4 / 2 = 3`.\n\xa0 \xa0- Calculate `tri(3) = 2 + 3 + 3 = 8`.\n\xa0 \xa0- Append `8` to the list:\xa0 3, 2, 8\nThe list should contain the first `n + 1` numbers, so we need to include `tri(0)`.\nBut\xa0`tri(0)` is not defined in the problem, but based on the example, it seems\nto be `1`.\nThe simulation shows that the plan does not correctly handle the initialization and\ncalculation of `tri(0)`. So,\xa0Plan Modification Needed.\xa0\nThe plan needs modification because it does not account for `tri(0)`. We need to\nstart the list with `[1, 3]` to correctly generate the sequence starting from `tri(0)`.\nPlan Evaluation\nTribonacci sequence is defined by the recurrence:\n\xa0 \xa0 tri(1) = 3\n\xa0 \xa0 tri(n) = 1 + n / 2, if n is even.\n\xa0 \xa0 tri(n) =\xa0 tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.\nExamples:\xa0tri(3) = [1, 3, 2, 8]\nProblem\nInitialize a List:\xa0Start with a list containing the base case: [3].\nIterate Over Range:\xa0Loop from 2 to `n` to calculate each number in the\xa0\n\xa0 \xa0 \xa0sequence\nCalculate Even Numbers:\xa0For even `n`, use the formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = 1 + n / 2` and append the result to the list.\nCalculate Odd Numbers:\xa0For odd `n`, use the recursive formula\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0`tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1)`.\xa0Use memoization to store and\xa0 \xa0 \xa0\n\xa0 \xa0 \xa0 retrieve previously calculated values to optimize performance.\nReturn the Sequence:\xa0Once the loop completes, return the list\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0\n\xa0 \xa0 \xa0containing the first `n + 1` numbers of the Tribonacci sequence.\nPlan\n0\nend = max(10, 14) = 14\n\xa0 \xa0- `10 % 2 == 0` (even, added to list)\n\xa0 \xa0- `11 % 2 != 0` (odd, not added)\n\xa0 \xa0- `12 % 2 == 0` (even, added to list)\n\xa0 \xa0- `13 % 2 != 0` (odd, not added)\n\xa0 \xa0- `14 % 2 == 0` (even, added to list)\n10\nThe list becomes\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 . return this.\nLLM\nGiven two positive integers a and b, return the even digits\nbetween a\xa0and b, in ascending order.\nFor example:\n\xa0 \xa0 generate_integers(2, 8) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(8, 2) => [2, 4, 6, 8]\n\xa0 \xa0 generate_integers(10, 14) => []\nProblem\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0 if number % 2 == 0:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(number)\n\xa0 \xa0 return even_numbers\nCode\nPlan Validation Using Simulation\nFailed on test case: \ngenerate_integers(10, 14) => []\nCode\xa0\nExecution\nLLM\nSimulation of\xa0generate_integers(10, 14)\nLoop over 10, 11, 12, 13, 14\nstart = min(10, 14) = 10\neven_numbers =\xa0\n10, 12\n10, 12, 14\n10, 12, 14\nThe code does not account for the requirement that the even\ndigits themselves must be between `a` and `b`.\xa0\nDebugging Note\ndef generate_integers(a, b):\n\xa0 \xa0 start = min(a, b)\n\xa0 \xa0 end = max(a, b)\n\xa0 \xa0 even_numbers = []\n\xa0 \xa0 for number in range(start, end + 1):\n\xa0 \xa0 \xa0 \xa0\xa0# Check each digit in the number\n\xa0 \xa0 \xa0 \xa0 for digit in str(number):\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 if int(digit) % 2 == 0 and start <= int(digit) <= end:\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 even_numbers.append(int(digit))\n\xa0 \xa0 # Remove duplicates and sort the result\n\xa0 \xa0 return sorted(set(even_numbers))\nModified Code\nCode\nExecution\nPassed on Sample I/O\nSend it for final Evaluation\nPrivate Test\n\xa0Passed\nDebugging Using Simulation\nFigure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two\ndifferent problems using CODESIM.\n2016), while others explored leveraging data flow\ninformation or conversational intents to guide the\nprocess (Andreas et al., 2020; Yu et al., 2019). Var-\nious prior approaches have also addressed code\ngeneration tasks using techniques such as data flow\nanalysis and search-based methods (Li et al., 2022a;\nParisotto and Salakhutdinov, 2017; Polozov and\nGulwani, 2015; Gulwani, 2011).\nLLMs for Code: Various LLMs have been de-\nveloped for code synthesis (Austin et al., 2021;\nChen et al., 2021b; Nijkamp et al., 2022; Fried\net al., 2022; Allal et al., 2023; Li et al., 2022c). Re-\ncent open-source LLMs include the Llama family\n(Llama-2, CodeLlama, Llama3.1, etc.) (Roziere\net al., 2023; Touvron et al., 2023), the Mistral\nfamily (Mistral, Mixtral, Codestral) (Jiang et al.,\n2023a), the Deepseek family (Deepseek Coder,\nDeepseek-V2, etc.) (Guo et al., 2024), MoTCoder\n(Li et al., 2023), and the Qwen family (Qwen 1.5,\n2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which\nare capable of solving many basic problems.\nPrompting LLMs and Multi-Agent Code Gen-\neration: LLM prompting can be summarized into\nthree categories: retrieval (Yasunaga et al., 2023;\nParvez et al., 2021, 2023), planning (Jiang et al.,\n2023b; Wei et al., 2022b), and debugging (Le et al.,\n2022; Chen et al., 2022b, 2023; Ridnik et al., 2024),\nin addition to direct code generation approaches.\nIn contrast, our work combines all these paradigms\nand bridges their gaps (See Table 1). Recently, nu-\n\nApproach\nExemplars\nPlan\nAdditional \ntest cases \ngeneration\nDebug Simulation\nReflexion\n✗\n✗\n✔\n✔\n✗\nSelf-planning\n✗\n✔\n✗\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\n✗\nLATS\n✗\n✔\n✔\n✔\n✗\nMapCoder\n✔\n✔\n✗\n✔\n✗\nCodeSim\n✔\n✔\n✗\n✔\n✔\nTable 1: Comparison of code generation approaches.\nmerous works have explored multi-agent code gen-\neration and problem-solving, including (Kulesza\net al., 2004; Jin et al., 2024a; Phan et al., 2024),\nas well as approaches highlighted in Section 1.\nHowever, CODESIM uniquely features simulation-\ndriven planning and LLM-based debugging. More\nrecently, external debugging has emerged to fur-\nther boost performance, such as LDB (Zhong et al.,\n2024), ChatDebug (Levin et al., 2024), and MGDe-\nbugger (Shi et al., 2024), which serve as a second\npass after our generation.\n3\nCODESIM\nOur goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model.\n3.1\nPlanning Agent\nThe first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar—a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they’ve previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent—including plan generation, verification, and\nrefinement—are provided in the Appendix (Figure\n5, 6, 7).\n3.2\nCoding Agent\nNext component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent.\n3.3\nDebugging Agent\nThe final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8.\n3.4\nAdaptive Iteration\nCODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it’s sent to the Debugging\nAgent. The Debugging Agent attempts to fix the\n\nBasic Programming Problems\nLLM\nApproach\nHumanEval\nHumanEval\nET\nEvalPlus\nAvg\nHumanEval\nMBPP\nMBPP-ET\nAvg\nMBPP\nAvg\nChatGPT\nDirect\n71.3%\n64.6%\n67.1%\n67.7%\n75.8%\n52.6%\n64.2%\n65.9%\nCoT\n70.7%\n63.4%\n68.3%\n67.5%\n78.3%\n55.7%\n67.0%\n67.2%\nSelf-Planning\n70.7%\n61.0%\n62.8%\n64.8%\n73.8%\n51.1%\n62.5%\n63.6%\nAnalogical\n67.1%\n59.1%\n59.1%\n61.8%\n69.3%\n46.9%\n58.1%\n59.9%\nSelf-collaboration\n74.4%\n56.1%\n-\n65.3%\n68.2%\n49.5%\n58.9%\n62.1%\nLATS\n83.8%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n80.5%\n70.1%\n71.3%\n74.0%\n78.3%\n54.4%\n66.4%\n70.2%\nCodeSim\n86.0%\n72.0%\n73.2%\n77.1%\n86.4%\n59.7%\n73.1%\n75.1%\nGPT4\nDirect\n80.1%\n73.8%\n81.7%\n78.5%\n81.1%\n54.7%\n67.9%\n73.2%\nCoT\n89.0%\n61.6%\n-\n75.3%\n82.4%\n56.2%\n69.3%\n72.3%\nSelf-Planning\n85.4%\n62.2%\n-\n73.8%\n75.8%\n50.4%\n63.1%\n68.4%\nAnalogical\n66.5%\n48.8%\n62.2%\n59.1%\n58.4%\n40.3%\n49.4%\n54.3%\nReflexion\n91.0%\n78.7%\n81.7%\n83.8%\n78.3%\n51.9%\n65.1%\n74.4%\nLATS\n92.7%\n-\n-\n-\n-\n-\n-\n-\nMapCoder\n93.9%\n82.9%\n83.5%\n86.8%\n83.1%\n57.7%\n70.4%\n78.6%\nCodeSim\n94.5%\n81.7%\n84.8%\n87.0%\n89.7%\n61.5%\n75.6%\n81.3%\nGPT4o\nDirect\n90.2%\n81.1%\n82.3%\n84.5%\n81.1%\n55.9%\n68.5%\n76.5%\nCoT\n90.9%\n82.3%\n87.2%\n86.8%\n82.9%\n57.9%\n70.4%\n78.6%\nSelf-Planning\n89.0%\n80.5%\n84.1%\n84.5%\n82.60%\n56.4%\n69.50%\n77.0%\nAnalogical\n88.4%\n80.5%\n83.5%\n84.1%\n75.10%\n50.9%\n63.00%\n73.6%\nReflexion\n87.2%\n81.1%\n81.1%\n83.1%\n81.1%\n56.7%\n68.9%\n76.0%\nLATS\n88.8%\n81.2%\n-\n85.0%\n-\n-\n-\n-\nMapCoder\n90.2%\n80.5%\n81.7%\n84.1%\n88.7%\n59.2%\n74.0%\n79.0%\nCodeSim\n95.1%\n86.0%\n87.2%\n89.4%\n90.7%\n61.2%\n76.0%\n82.7%\nTable 2: Pass@1 results for different approaches on basic programming tasks.\ncode for up to d attempts. If unsuccessful after d\nattempts, the process returns to the Planning Agent,\nrestarting the cycle. Once code passing all sample\nI/Os is obtained, the cycle ends, returning the code\nas the final output solution for evaluation against\nhidden test cases. This entire process repeats for\na maximum of p cycles if needed. Algorithm 9\nin the Appendix summarizes our adaptive agent\ntraversal. The algorithm’s complexity is O(pd).\nAppendix 12 provides a comprehensive example of\nhow CODESIM solves a problem.\n4\nExperimental Setup\n4.1\nDatasets\nFollowing MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For\nfair comparison, we collect all the datasets from\nthe repository of the MapCoder.\n4.2\nBaselines and Metric\nTo evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-4o (gpt-4o-2024-08-06). We adopt the\n\nwidely used pass@1 metric, where a model is\ndeemed successful if its sole predicted solution\nis correct.\n4.3\nReproducibility\nWe aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to p = 5 and debugging tries to\nd = 5. For the competitive problem solving, we\nused p = 3 and d = 3 by default except for the\nCodeContest with GPT-4 where p = 3, d = 5.\n5\nResults\n5.1\nBasic Code Generation\nIn Table 2, we evaluate the model performances\non simple code generation tasks.\nOverall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-4o, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-4o. CODESIM’s effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method’s robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM’s simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines.\n5.2\nCompetitive Problem Solving\nIn Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder’s 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves\na 16.4% on CodeContests, and 12.0% on APPS re-\nsulting 14.2% overall, outperforming MapCoder’s\n12.0%. These results highlight CODESIM’s ability\nto handle the complexity of contest-level problems\nmore effectively, especially through its simulation-\ndriven approach.\nLLM\nContest-Level Problems\nApproach\nCodeContest\nAPPS\nAvg\nChatGPT\nDirect\n5.5%\n8.0%\n6.8%\nCoT\n6.1%\n7.3%\n6.7%\nSelf-Planning\n6.1%\n9.3%\n7.7%\nAnalogical\n7.3%\n6.7%\n7.0%\nMapCoder\n12.7%\n11.3%\n12.0%\nCodeSim\n16.4%\n12.0%\n14.2%\nGPT4\nDirect\n12.1%\n12.7%\n12.4%\nCoT\n5.5%\n11.3%\n8.4%\nSelf-Planning\n10.9%\n14.7%\n12.8%\nAnalogical\n10.9%\n12.0%\n11.5%\nMapCoder\n28.5%\n22.0%\n25.3%\nCodeSim\n29.1%\n22.0%\n25.6%\nTable 3: Pass@1 results for different approaches on\nCodeContest and APPS dataset.\nOpen-Source LLMs\nLLM\nApproach HumanEval HumanEval\nET\nEvalPlus\nAvg\nGemma2-9B\nDirect\n64.0%\n56.1%\n56.1%\n58.7%\nCoT\n31.7%\n26.2%\n27.4%\n28.4%\nReflexion\n62.2%\n56.7%\n55.5%\n58.1%\nCodeSim\n82.9%\n72.0%\n72.6%\n75.8%\nMixtral8x7B\nDirect\n20.7%\n18.9%\n18.9%\n19.5%\nCoT\n46.3%\n42.1%\n39.0%\n42.5%\nReflexion\n34.1%\n32.9%\n29.9%\n32.3%\nCodeSim\n75.0%\n61.6%\n61.0%\n65.9%\nLLaMa3.1-8B\nDirect\n42.1%\n38.4%\n39.0%\n39.8%\nCoT\n48.8%\n42.1%\n43.3%\n44.7%\nReflexion\n43.9%\n31.1%\n29.9%\n35.0%\nCodeSim\n79.9%\n65.2%\n61.2%\n68.8%\nLLaMa3.1-70B\nDirect\n57.3%\n50.6%\n52.4%\n53.4%\nCoT\n75.6%\n67.7%\n70.1%\n71.1%\nReflexion\n73.8%\n64.0%\n68.3%\n68.7%\nCodeSim\n90.2%\n73.8%\n76.2%\n80.1%\nTable 4: Pass@1 results for different approaches using\nOpen-source LLMs.\n5.3\nPerformance Across Open-source LLMs\nTo further demonstrate CODESIM’s generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy\n\nof 90.2% on HumanEval and 76.2% on EvalPlus,\nwith an average of 80.1%, closely matching GPT-\n4o’s performance. Due to the complex prompting\nscheme of MapCoder, open-source LLMs often\nstruggle to generate output in the correct format.\nTherefore, we exclude MapCoder from this experi-\nment. On the other hand, Reflexion shows minimal\nimprovement in accuracy. These results highlight\nCODESIM’s strong generalization ability across\nvarious LLM architectures, even on smaller mod-\nels like Gemma2-9B that achieves a notable avg\naccuracy of 75.8%.\n6\nAblation Studies and Analyses\n6.1\nImpact of Different Agents\nOur primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly.\nSimulation \nDriven \nPlanning\nDebugging \nusing \nSimulation\nPass@1\nPerformance \nDrop\n✗\n✗\n92.1%\n3.2%\n✔\n✗\n93.3%\n1.9%\n✗\n✔\n93.3%\n1.9%\n✔\n✔\n95.1%\n-\nTable 5:\nPass@1 results for different versions of\nCODESIM (by using GPT4o on HumanEval dataset).\n6.2\nFine-grained Analysis of the Impact of\nSimulation\nTable 6 presents the impact of incorporating\nSimulation in CODESIM.\nThe results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline.\n6.3\nImpact of Varying Programming\nLanguages\nTo evaluate the performance of CODESIM across\nvarious programming languages, we utilized the\nLLM\nMethod\nApproach\nHumanEval \n(Pass@1)\nImpact of using \nSimulation\nGPT4o\nSimpler\nDirect\n90.2%\n5.4%\nCoT\n90.9%\n4.6%\nSelf-Planning\n89.0%\n6.9%\nAnalogical\n88.4%\n7.6%\nReflexion\n91.0%\n4.5%\nLATS\n88.8%\n7.1%\nMulti-Agent\nMapCoder\n90.2%\n5.4%\nCodeSim\n95.1%\n-\nLLaMa3.1-70B\nSimpler\nDirect\n57.3%\n57.4%\nCoT\n75.6%\n19.3%\nReflexion\n73.8%\n22.2%\nMulti-Agent\nCodeSim\n90.2%\n-\nTable 6: Impact of using Simulation.\nxCodeEval (Khan et al., 2023) dataset. The ex-\nperimental results, presented in Table 7, demon-\nstrate that CODESIM maintains strong performance\nacross different programming languages, highlight-\ning its versatility and effectiveness.\nDataset\nLanguage\nDirect\nMapCoder\nCodeSim\nxCodeEval\nPython\n17.9%\n27.4%\n27.4%\nC\n9.4%\n21.7%\n24.5%\nRust\n12.3%\n21.7%\n23.6%\nTable 7: Pass@1 results for different programming lan-\nguages from xCodeEval dataset by using ChatGPT.\n6.4\nUse of External Debugger\nLLM\nLDB\nReflexion MapCoder\nCodeSim\nChatGPT\nwithout\n88.0%\n90.2%\n95.1%\nwith\n89.6%\n92.1%\n96.3%\nGPT-4o\nwithout\n88.0%\n90.2%\n95.1%\nwith\n94.5%\n91.5%\n97.6%\nTable 8: Pass@1 results for different approaches using\nan external debugger.\nThe performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-4o as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMs: ChatGPT and GPT-4o. As can\nbe seen, CODESIM achieves 95.1% accuracy in\n\nthe first pass with GPT-4o, surpassing Reflexion’s\nsecond pass performance of 94.5%. By utilizing\nLDB with GPT-4o, CODESIM achieves a second\npass accuracy of 97.6%, setting a new state-of-the-\nart result for a dual-pass approach. In addition,\nwe note that the second pass with LDB consumes\n39K more tokens in Reflexion compared to our\napproach, highlighting the efficiency of CODESIM.\n6.5\nQualitative Example\nWe also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12.\n6.6\nImpact of p and d\nCODESIM includes two key hyperparameters: the\nmaximum number of planning steps (p) and the\nmaximum number of debugging steps (d). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of p and d lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost.\nFigure 3: Pass@1 results by varying maximum number\nof planning, p and maximum number of debugging, d.\n6.7\nImpact of Number of Sample I/Os\nThe HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,\nleading to 89% accuracy with ChatGPT, a 3.5%\nimprovement over previous results, 86%.\n6.8\nImpact of Synthesizing Additional I/O\nIncreasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy—a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes.\n6.9\nAPI Call and Token Analysis\nWe compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder.\n6.10\nError Analysis and Challenges\nAlthough\nCODESIM\ndemonstrates\nstrong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty:\n(i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4,\nCODESIM\nstruggles\nto\noutperform\nMapCoder on interview-level problems.\nUpon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table.\n\nLLM\nDataset\nAverage API Calls ↓\nAverage Token Consumption (K) ↓\nToken Reduction over \nMapCoder (k) ↑\nAcc Gain over \nMapCoder ↑\nMapCoder\nCodeSim\nMapCoder\nCodeSim\nChatGPT\nHumanEval\n17\n7\n10.41\n5.48\n4.93\n6.8%\nMBPP\n12\n6\n4.84\n4.24\n0.60\n10.3%\nAPPS\n21\n15\n26.57\n19.20\n7.37\n6.2%\nCodeContest\n23\n16\n34.95\n24.02\n10.92\n29.1%\nGPT4\nHumanEval\n15\n5\n12.75\n5.15\n7.60\n0.6%\nMBPP\n8\n5\n4.96\n5.21\n-0.26\n7.9%\nAPPS\n19\n13\n31.80\n23.18\n8.61\n0.0%\nCodeContest\n19\n17\n38.70\n41.66\n-2.95\n2.1%\nGPT4o\nHumanEval\n9\n4\n6.63\n3.84\n2.79\n5.4%\nMBPP\n9\n5\n6.10\n4.43\n1.67\n2.3%\nAverage\n15.2\n9.3\n17.77\n13.64\n4.13\n7.1%\nTable 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens\nused (in thousands). Here the upward symbol (↑) refers that the higher value is better and opposite meaning for\ndownward symbol (↓).\nFigure 4: Performance of different approaches across\ndifferent difficulty levels on the APPS dataset.\n7\nConclusion and Future Work\nIn this paper, we introduce CODESIM, a novel\nframework\nthat\nleverages\nthe\nmulti-agent\nprompting capabilities of LLMs for efficient\ncode\ngeneration\nin\nproblem-solving\ntasks.\nCODESIM\nintegrates three agents—planning,\ncoding,\nand debugging—to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact.\n8\nLimitations\nIn Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-\nmance without relying on any external tools. Al-\nthough we have reduced token consumption com-\npared to the previous state-of-the-art method, Map-\nCoder, it still remains high compared to the di-\nrect prompting approach. Direct prompting con-\nsumes an average of 560 tokens, while our method\nconsumes around 13,640 tokens. This indicates\nroom for enhancement in efficiency. While in this\nwork, we generate the exemplars with the LLMs\nthemselves, in general they are found from exter-\nnal resource (Parvez and Chang, 2021). Although\nthis has its own challenges such as noisy retrievals\n(Wang et al., 2023b), inconsistent generations (Is-\nlam et al., 2024b; Parvez, 2024; Sadat et al., 2023)\nthis direction could also be a possible improvement.\nAnother limitation is the use of external tools for\nassistance during simulation. We have not explored\nthis avenue in the current research, leaving it for\nfuture work. Additionally, more sample I/Os could\npotentially improve performance, and our future\nresearch will focus on investigating methods for\ngenerating accurate additional I/Os. Moreover, we\nwould like to note that in this work, we focus solely\non generated code’s correctness and did not study\nits optimizations such as test-time, memory. Fi-\nnally, it is advisable to run the machine generated\ncode inside a sandbox to avoid any potential risks.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\n\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution. arXiv\npreprint arXiv:2301.09043.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and et al. 2024. The llama 3 herd of models.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples. ACM\nSigplan Notices, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence. arXiv preprint arXiv:2401.14196.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineer-\ning, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Commun. ACM, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation. arXiv preprint arXiv:2312.13010.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder\ntechnical report. arXiv preprint arXiv:2409.12186.\n\nMd. Ashraful Islam, Mohammed Eunus Ali, and\nMd Rizwan Parvez. 2024a. MapCoder: Multi-agent\ncode generation for competitive problem solving. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4912–4944, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nShayekh Bin Islam, Md Asib Rahman, K S M Tozammel\nHossain, Enamul Hoque, Shafiq Joty, and Md Rizwan\nParvez. 2024b. Open-RAG: Enhanced retrieval aug-\nmented reasoning with open-source large language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 14231–\n14244, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a. Mistral 7b.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\narXiv preprint\narXiv:2303.06689.\nDongming Jin, Zhi Jin, Xiaohong Chen, and Chun-\nhui Wang. 2024a. Mare: Multi-agents collabora-\ntion framework for requirements engineering. arXiv\npreprint arXiv:2405.03256.\nHaolin Jin, Zechao Sun, Yiheng Yang, and Huaming\nChen. 2024b. Rgd: Multi-llm based agent debug-\nger via refinement and generation guidance. arXiv\npreprint arXiv:2410.01242.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint\narXiv:2303.03004.\nUirá Kulesza, Alessandro Garcia, Carlos Lucena, and\nPaulo Alencar. 2004.\nA generative approach for\nmulti-agent system development. In International\nWorkshop on Software Engineering for Large-Scale\nMulti-agent Systems, pages 52–69. Springer.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul\nHoque, Shafiq Joty, and Jimmy Huang. 2024. A sys-\ntematic survey and critical review on evaluating large\nlanguage models: Challenges, limitations, and recom-\nmendations. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13785–13816, Miami, Florida, USA. As-\nsociation for Computational Linguistics.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314–21328.\nKyla Levin, Nicolas van Kempen, Emery D Berger,\nand Stephen N Freund. 2024.\nChatdbg:\nAn\nai-powered debugging assistant.\narXiv preprint\narXiv:2403.16354.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks. arXiv\npreprint arXiv:2312.15960.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022b. Competition-level code generation with al-\nphacode. Science, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis. Commun. ACM,\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2024. Gpt-4 technical report.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360.\n\nMd Rizwan Parvez. 2024.\nEvidence to generate\n(e2g): A single-agent two-step prompting for context\ngrounded and retrieval augmented reasoning. arXiv\npreprint arXiv:2401.05787.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHuy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui.\n2024. Hyperagent: Generalist software engineering\nagents to solve coding tasks at scale. arXiv preprint\narXiv:2409.16299.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming,\nSystems, Languages, and Applications, pages 107–\n126.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174–15186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. CoRR, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\narXiv preprint\narXiv:2401.08500.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nMobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun\nAraki, Arsalan Gundroo, Bingqing Wang, Rakesh\nMenon, Md Parvez, and Zhe Feng. 2023.\nDelu-\ncionQA: Detecting hallucinations in domain-specific\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n822–835, Singapore. Association for Computational\nLinguistics.\nYuling Shi, Songsong Wang, Chengcheng Wan, and\nXiaodong Gu. 2024. From code to correctness: Clos-\ning the last mile of code generation with hierarchical\ndebugging. arXiv preprint arXiv:2410.01215.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5:\nIdentifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation.\narXiv preprint\narXiv:2109.00859.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023b. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nCoRR, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024. CodeAgent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13643–\n13658, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nLi Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\nbug like a human: A large language model debugger\nvia verifying runtime execution step by step. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2024, pages 851–870, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\n\nAppendix\n9\nAlgorithm of CODESIM\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1 CODESIM\n1: p ←maximum number of planning steps\n2: d ←maximum number of debugging steps\n3:\n4: for i ←1 to p do\n5:\n# Start of Planning Agent\n6:\nplan ←GeneratePlan(problem)\n7:\nfeedback ←ValidatePlan(problem, plan)\n8:\nif feedback is negative then\n9:\nplan ←RefinePlan(problem, plan, feedback)\n10:\nend if\n11:\n# End of Planning Agent\n12:\n13:\n# Start of Coding Agent\n14:\ncode ←GenerateCode(problem, plan)\n15:\npassed, log ←test(code, sample_io)\n16:\nif passed then\n17:\nReturn code\n18:\nelse\n19:\n# Start of Debugging Agent\n20:\nfor j ←1 to d do\n21:\ncode ←DebugCode(\n22:\nproblem,\n23:\nplan,\n24:\ncode,\n25:\nlog\n26:\n)\n27:\n28:\npassed, log ←test(code, sample_io)\n29:\nif passed then\n30:\nReturn code\n31:\nend if\n32:\nend for\n33:\n# End of Debugging Agent\n34:\nend if\n35:\n# End of Coding Agent\n36:\n37: end for\n38: Return code\n10\nExclusion of AgentCoder\nWe have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-\nCoder (1.7 million tokens with ChatGPT and 2.1\nmillion with GPT-4) and CODESIM(0.89 million\ntokens in ChatGPT and 0.85 million in GPT-4).\nThese two experiments resulted in a cost of ap-\nproximately $500 USD, but we were still unable\nto come close to AgentCoder’s reported claims of\n79.9% accuracy with ChatGPT and 96.3% with\nGPT-4.\nFurthermore, we found unaddressed issues on\ntheir GitHub page (link) related to reproducibility.\nAdditionally, for the MBPP dataset, they used all\ntest cases as public test cases (link), which deviates\nfrom standard practices. As a result, we did not\nconsider those results in our comparison either.\n11\nDetails Promptings of CODESIM\nThe Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt.\n12\nExample Problem\nWe present a complete example of problem solving\nusing CODESIM below:\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language.\n## Problem\n{problem}\n**Expected Output:**\nYour response must be structured as follows:\n### Problem Understanding\n- Think about the original problem. Develop an initial \n  understanding about the problem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem \nmentioned above) and\n- Describe it\n- Generate {language} code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original\n  problem\n- Give some tutorials to about the algorithm for example:\n\xa0 \xa0 - How to approach this type of algorithm\n\xa0 \xa0 - Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the \n  **original problem**.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not generate code.\nCoding Agent: Prompt for Plan Generation\nAllow the LLM sufficient time and space to process and\ncomprehend the problem.\nRather than providing\nthe LLM with a\npredefined example,\nwe leverage its\ninherent knowledge to\nindependently recall a\nrelevant problem that\ncan aid in solving the\noriginal issue.\nGuide the LLM to\ndetermine the type of\nalgorithm suitable for\nsolving the problem,\nand request a tutorial\nor explanation on how\nto apply it.\nLastly, instruct the LLM\nto generate a detailed\nplan to solve the\nproblem.\nForce the LLM to\nfollow the instructions.\nFigure 5: Planning Agent: Prompt for Plan Generation.\nYou are a programmer tasked with verifying a plan to solve a given problem using the \n**{language}** programming language.\n## Problem\n{problem}\n### Plan\n{plan}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if \n  your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write **No Need to Modify Plan**.\n- Otherwise write **Plan Modification Needed**.\nCoding Agent: Prompt for Plan Verification with Simulation\nTo validate a plan, a human programmer typically\nsimulates it with sample input, generates the\ncorresponding output, and compares the\ngenerated output to the expected result. At this\nstage, we\xa0 instruct the LLM to perform this\nprocess as well, ensuring it follows the same\nsteps like human programmer to confirm the\nvalidity of the plan.\nFinally, tell the LLM\nto write done it\'s\ndecision in a\nspecific format.\nFigure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation.\n\nYou are a programmer tasked with generating appropriate plan to solve a given problem \nusing the **{language}** programming language. You already have a wrong plan. \nCorrect it so that it can generate correct code.\n## Problem\n{problem}\n### Plan\n{plan}\n## Plan Critique\n{plan_verifical_report_from_previous_step}\n**Expected Output:**\nYour response must be structured as follows:\n### New Plan\n- Write down a detailed, step-by-step modified plan to solve the **original problem**.\n- Ensure each step logically follows from the previous one.\n--------\n**Important Instruction:**\n- Your response must contain only the plan.\n- Do not add any explanation.\n- Do not generate code.\nCoding Agent: Prompt for Plan Refinement\nProvide all the details and instruct the LLM to\ngenerate revised plan.\nFigure 7: Planning Agent: Prompt for Plan Refinement.\nYou are a programmer tasked with solving a given problem using the **{language}** \nprogramming language. See the plan to solve the plan and implement code to solve it.\n## Problem\n{problem}\n### Plan\n{plan}\n--------\n**Important Instruction:**\n- Do not add any explanation.\n- The generated **{language}** code must be inside a triple backtick (```) code block.\nCoding Agent: Prompt for Code Generation\nFigure 8: Coding Agent: Prompt for Code Generation.\n\nAn Example from HumanEval dataset for demonstrating how CODESIM works\nInput for Planning: 1\nYou are a programmer tasked with generating appropriate plan to solve a given problem using the\nPython3 programming language.\n## Problem\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\nExpected Output:\nYour response must be structured as follows:\n\nYou are a programmer who has received a solution of a problem written in **{language}** \nthat fails to pass certain test cases. Your task is to modify the code in such a way so \nthat it can pass all the test cases. Do not generate same code.\n## Problem\n{problem}\n### Plan\n{plan}\n### Buggy Code\n{code}\n### Test Report\n{test_log}\n**Expected Output:**\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output. \n### Debugging Notes\n- Based on this simulation detect any of the following cases:\n    - Plan is wrong\n    - Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n# Your corrected code, with comments explaining each correction.\n--------\n**Important Instruction:**\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified **{language}** code must be enclosed within triple backticks (```).\n- Your response must contain **Simulation with failed test case**, **Debugging Notes**, \n  and **Modified Code** section.\nDebugging Agent: Prompt for Debugging\nTo detect where a program fails, a human programmer typically selects a sample\ntest case, simulates it line by line, and observes the output at each step. This\nprocess helps in identifying and debugging issues within the code. Similarly, we\nhave instructed the LLM to select a test case where the code fails, simulate it,\nidentify the bug through step-by-step analysis, and then debug the issue\naccordingly.\nBased on the\nsimulation\noutput we have\ninstruct the LLM\nto write down\nhow to correct\nthe code.\nFinally, instruct\nthe LLM to\ngenerate the\nmodified code.\nFigure 9: Debugging Agent: Prompt for Debugging.\n### Problem Understanding\n- Think about the original problem.\nDevelop an initial understanding about the prob-\nlem.\n### Recall Example Problem\nRecall a relevant and distinct problems (different from problem mentioned above) and\n- Describe it\n- Generate Python3 code step by step to solve that problem\n- Discuss the algorithm to solve this problem\n- Finally generate a planning to solve that problem\n### Algorithm to solve the original problem\n- Write down the algorithm that is well suited for the original problem\n- Give some tutorials to about the algorithm for example:\n\n- How to approach this type of algorithm\n- Important things to consider\n### Plan\n- Write down a detailed, step-by-step plan to solve the original problem.\n——–\nImportant Instruction:\n- Strictly follow the instructions.\n- Do not generate code.\nResponse from Planning: 1\n### Problem Understanding\nThe problem requires generating a list of even integers between two given positive integers, a and\nb, inclusive. The list should be in ascending order, regardless of whether a is less than or greater\nthan b. If there are no even integers in the range, the function should return an empty list.\n### Recall Example Problem\nExample Problem:\nFind all odd numbers between two given integers x and y, inclusive, and return them in descending\norder.\nStep-by-step Solution:\n1. Determine the range between x and y.\n2. Identify all odd numbers within this range.\n3. Sort the odd numbers in descending order.\n4. Return the sorted list.\nAlgorithm:\n1. Check if x is greater than y. If so, swap them to ensure the range is correct.\n2. Iterate through the range from x to y.\n3. For each number, check if it is odd (i.e., number%2! = 0).\n4. Collect all odd numbers in a list.\n5. Sort the list in descending order.\n6. Return the sorted list.\nPlanning:\n- Define a function find_odd_numbers(x, y).\n- Use a conditional to swap x and y if x > y.\n- Use a loop to iterate through the range from x to y.\n- Check each number for oddness and add it to a list if it is odd.\n\n- Sort the list in descending order.\n- Return the list.\n### Algorithm to Solve the Original Problem\n1. Determine the Range:\n- Identify the start and end of the range by taking the minimum and maximum of a and b.\n2. Iterate Through the Range:\n- Loop through each number in the range from the minimum to the maximum of a and b.\n3. Check for Even Numbers:\n- For each number, check if it is even by using the condition number%2 == 0.\n4. Collect Even Numbers:\n- Add each even number to a list.\n5. Return the List:\n- Since the numbers are collected in ascending order, return the list directly.\nTutorials:\n- Approach:\n- Always ensure the range is correctly defined, especially when the order of inputs can vary.\n- Use modulus operation to check for evenness or oddness.\n- Important Considerations:\n- Handle edge cases where there might be no even numbers in the range.\n- Ensure the function is robust to handle any positive integer inputs.\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\nInput for Simulation: 1\nYou are a programmer tasked with verifying a plan to solve a given problem using the Python3\nprogramming language.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.v\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n\nExpected Output:\nYour response must be structured as follows:\n### Simulation\n- Take a sample input and apply plan step by step to get the output.\n- Compare the generated output with the sample output to verify if your plan works as expected.\n### Plan Evaluation\n- If the simulation is successful write No Need to Modify Plan.\n- Otherwise write Plan Modification Needed.\nResponse from Simulation: 1\n### Simulation\nLet’s apply the plan step by step using the sample input generate_integers(2, 8).\n1. Define the Function:\n- We define the function generate_integers(a, b).\n2. Determine the Range:\n- Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8.\n3. Initialize an Empty List:\n- Create an empty list evennumbers = [].\n4. Loop Through the Range:\n- Iterate over the range from start to end (inclusive).\n5. Check for Evenness:\n- For each number in the range, check if it is even using ifnumber%2 == 0.\n6. Collect Even Numbers:\n- Append even numbers to the list evennumbers.\n7. Return the Result:\n- Return the list evennumbers.\nApplying these steps:\n- Range: 2 to 8\n- Iteration:\n- 2 is even, append to list: [2]\n- 3 is not even\n- 4 is even, append to list: [2, 4]\n- 5 is not even\n\n- 6 is even, append to list: [2, 4, 6]\n- 7 is not even\n- 8 is even, append to list: [2, 4, 6, 8]\nThe output is [2, 4, 6, 8], which matches the expected output.\n### Plan Evaluation\nNo Need to Modify Plan\nThe plan works as expected for the given problem. The function correctly generates a list of even\nintegers between the given numbers a and b, regardless of their order.\nInput for final code generation:\nYou are a programmer tasked with solving a given problem using the Python3 programming\nlanguage. See the plan to solve the plan and implement code to solve it.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers be-\ntween the given numbers a and b.\n——–\nImportant Instructions:\n- Do not add any explanation.\n- The generated Python3 code must be inside a triple backtick ($$$) code block.\nResponse from final code generation:\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\nInput for Improving code: 1, 1\nYou are a programmer who has received a solution of a problem written in Python3 that\nfails to pass certain test cases. Your task is to modify the code in such a way so that it can pass all\nthe test cases. Do not generate same code.\n## Problem:\ndef generate_integers(a, b):\n"""\nGiven two positive integers a and b, return the even digits between a\nand b, in ascending order.\nFor example:\ngenerate_integers(2, 8) => [2, 4, 6, 8]\ngenerate_integers(8, 2) => [2, 4, 6, 8]\ngenerate_integers(10, 14) => []\n"""\n### Plan\n\n1. Define the Function:\n- Create a function generate_integers(a, b).\n2. Determine the Range:\n- Use min(a, b) and max(a, b) to define the start and end of the range.\n3. Initialize an Empty List:\n- Prepare an empty list to store even numbers.\n4. Loop Through the Range:\n- Use a for loop to iterate from the start to the end of the range.\n5. Check for Evenness:\n- Within the loop, use ifnumber%2 == 0 to check if a number is even.\n6. Collect Even Numbers:\n- Append even numbers to the list.\n7. Return the Result:\n- Return the list of even numbers, which will be in ascending order.\nBy following this plan, the function will correctly generate a list of even integers between the\ngiven numbers a and b.\n### Buggy Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\nif number % 2 == 0:\neven_numbers.append(number)\nreturn even_numbers\n### Test Report\nTest Cases where the generated code failed to generate the expected output:\nassertgenerateintegers(10, 14) == []\nExpected Output:\nYour response must be structured as follows:\n### Simulation with failed test case\nTo detect where is the bug:\n- Take a sample test case where it fails.\n- Take the input go through each step according to the plan\n- You will get a output that must be different from the expected output.\n\n### Debugging Notes\nBased on this simulation detect any of the following cases:\n- Plan is wrong\n- Plan is correct but plan to code generation is wrong.\n- Finally, discuss how to correct this code.\n### Modified Code\n“‘Python3\n# Your corrected code, with comments explaining each correction.\n“‘\n——–\nImportant Instructions:\n- Strictly follow the instructions.\n- Do not add testing code for example assert statement in your code.\n- Do not be overconfident that the generated code is correct. It is wrong.\n- The modified Python3 code must be enclosed within triple backticks (“‘).\n- Your response must contain Simulation with failed test case, Debugging Notes, and Modified\nCode section.\nResponse from Improving code: 1, 1\n### Simulation with failed test case\nLet’s take the test case generate_integers(10, 14):v\n1. Determine the Range:\n- start = min(10, 14) = 10\n- end = max(10, 14) = 14\n2. Loop Through the Range:\n- The loop iterates over 10, 11, 12, 13, 14.\n3. Check for Evenness:\n- 10%2 == 0 (even, added to list)\n- 11%2! = 0 (odd, not added)\n- 12%2 == 0 (even, added to list)\n- 13%2! = 0 (odd, not added)\n- 14%2 == 0 (even, added to list)\n4. Collect Even Numbers:\n- The list becomes [10, 12, 14].\n5. Return the Result:\n- The function returns [10, 12, 14].\n\nExpected Output: []\n### Debugging Notes\n- Plan is correct: The plan correctly identifies the need to find even numbers between a\nand b.\n- Plan to code generation is wrong: The code does not account for the requirement that the even\ndigits themselves must be between a and b. It incorrectly includes numbers like 10, 12, 14 when\nthe digits themselves (1, 2, 4) are not between 10 and 14.\n### Modified Code\ndef generate_integers(a, b):\nstart = min(a, b)\nend = max(a, b)\neven_numbers = []\nfor number in range(start, end + 1):\n# Check each digit in the number\nfor digit in str(number):\nif int(digit) % 2 == 0 and start <= int(digit) <= end:\neven_numbers.append(int(digit))\n# Remove duplicates and sort the result\nreturn sorted(set(even_numbers))\nExplanation of Corrections:\n- Digit Check: The code now checks each digit of the numbers in the range to see if it is\neven and within the specified range.\n- Set for Uniqueness: A set is used to ensure that each even digit is only included once.\n- Sorting: The final list is sorted to ensure ascending order.'),
                Paper(arxiv_id='2502.06786', authors=['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], published_at=datetime.datetime(2025, 2, 11, 1, 7, 50, 116000, tzinfo=datetime.timezone.utc), title='Matryoshka Quantization', summary='Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.', upvotes=18, thumbnail=None, content='1. Introduction\nDue to their impressive performance, there is a\nstrong push to deploy deep learning models, par-\nticularly large language models (LLMs) (Achiam\net al., 2023; Dubey et al., 2024; G Team et al.,\n2024) in a large number of scenarios. Due to auto-\nregressive nature of LLMs, decode latency tends\nto dominate inference cost. Decode latency itself\nis dominated by communication cost of transfer-\nring model weights from high-bandwidth mem-\nory (HBM) to the SRAM or due to transferring\nweights/activations in a distributed cluster.\nQuantizing weights and/or activations can sig-\nnificantly reduce the overall communication load\nand is, therefore, one of the most popular tech-\nniques for reducing inference costs (Dettmers\net al., 2022). While floating-point representations\nare standard for training, integer data types such\nas int8, int4, and int2 are appealing alternatives\nfor inference. However, current methods for quan-\ntizing to these varying integer precisions typically\ntreat each target precision as an independent op-\ntimization problem, leading to a collection of dis-\ntinct models rather than a single, versatile one.\nFurthermore, quantizing to extremely low preci-\nsions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether\nboth of the above challenges can be addressed;\nthat is, can we train a single model from which\nwe can extract multiple accurate lower-precision\nmodels? We answer this question in the affir-\nmative by introducing Matryoshka Quantization\n(MatQuant), a novel multi-scale training method\nthat leverages the inherent nested (Matryoshka)\nstructure (Kusupati et al., 2022) within integer\ndata types (Figure 1a). Specifically, slicing the\ntop bits of an int8-quantized weight can directly\nyield an int4 or int2 model. Existing quantiza-\ntion techniques often neglect this structure, which\nlimits the potential for multi-scale adaptable mod-\nels operating at various bit-widths with optimal\nperformance.\nInstead, MatQuant simultaneously optimizes\nmodel weights across multiple precision levels\n(e.g., int8, int4, int2). At a high level, we repre-\nsent each model parameter at different precision\nlevels using shared most significant bits (MSBs),\nand then jointly optimize the loss for each pre-\ncision level. This allows us to develop a single\nquantized model that can effectively operate at\nany of the chosen bit-widths, offering a spectrum\nof accuracy-versus-cost options. MatQuant is a\ngeneral-purpose technique, applicable to most\n1\narXiv:2502.06786v1  [cs.LG]  10 Feb 2025\n\nMatryoshka Quantization\n11 01 1001 \n(a)\n2\n4\n6\n8\nEffective bits per FFN parameter\n40\n50\n60\n70\nTask Average\nGemma-2 9B\nMatQuant\nMatQuant-Interp.\nBaseline\nMinMax\nSliced int8\n(b)\n(c)\nFigure 1 | (a) MatQuant is a multi-scale quantization training technique using the inherent Matryoshka\nstructure of int8 →int4 →int2. (b) Empirical gains of MatQuant on downstream tasks, especially\n> 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution\nas a consequence of MatQuant’s training mechanism that maximises accuracies across all precisions.\nlearning-based quantization methods, such as\nQuantization Aware Training (QAT) (Jacob et al.,\n2018) and OmniQuant (Shao et al., 2023).\nWe demonstrate the efficacy of MatQuant\nwhen applied to quantizing the Feed-Forward\nNetwork (FFN) parameters of standard LLMs\n(Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al.,\n2017) – typically, FFN is the main latency block\nhence the focus on improving the most signifi-\ncant component’s latency. Our results show that\nMatQuant produces int8 and int4 models with\ncomparable accuracy to independently trained\nbaselines, despite the benefit of shared model pa-\nrameters. Critically, the int2 models generated\nby MatQuant significantly outperform their in-\ndividually trained counterparts, with 8% higher\naccuracy on downstream tasks (Figure 1b). We\nalso extend MatQuant to quantize all weights of\na Transformer layer. Finally, we find that quantiz-\ning with MatQuant shifts the quantized weight\ndistribution toward higher values, contributing to\nimproved int2 performance (Figure1c).\nBeyond improving chosen precision perfor-\nmance, MatQuant allows for seamless extraction\nof interpolative bit-widths, such as int6 and int3.\nMatQuant also admits a dense accuracy-vs-cost\npareto-optimal trade-off by enabling layer-wise\nMix’n’Match of different precisions. This ensures\ndeployment of say an effective int3 sized model\neven if the underlying hardware only supports\nint4 and int2. Overall, MatQuant and its vari-\nants present a significant step toward develop-\ning multi-scale models with high flexibility and\nperformance, pushing the boundaries of low-bit\nquantization for efficient LLM inference')]}
