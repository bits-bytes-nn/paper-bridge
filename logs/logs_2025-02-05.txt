2025-02-05 00:24:07,278 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:26:21,615 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:26:23,509 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpdtm368p_.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:26:25,070 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmprz7wd8iy.pdf/2501.14677v1.MatAnyone__Stable_Video_Matting_with_Consistent_Memory_Propagation.pdf'
2025-02-05 00:26:26,156 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpnokxk7fk.pdf/2501.19399v1.Scalable_Softmax_Is_Superior_for_Attention.pdf'
2025-02-05 00:26:27,436 - ERROR - Error downloading/parsing paper 2501.19339: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpafvml9a8.pdf/2501.19339v1.PixelWorld__Towards_Perceiving_Everything_as_Pixels.pdf'
2025-02-05 00:27:52,878 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:28:38,830 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp4vv_719v.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:29:04,937 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:29:21,537 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:30:17,627 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:30:19,459 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp129lynah.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:30:21,003 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpqxuvl552.pdf/2501.14677v1.MatAnyone__Stable_Video_Matting_with_Consistent_Memory_Propagation.pdf'
2025-02-05 00:30:22,575 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 20] Not a directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpmezbmp7p.pdf/2501.19399v1.Scalable_Softmax_Is_Superior_for_Attention.pdf'
2025-02-05 00:31:24,685 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:31:30,784 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpxl2qt40z/2501.18119.pdf'
2025-02-05 00:31:31,823 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmprull1onu/2501.14677.pdf'
2025-02-05 00:31:32,943 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp75fkuacw/2501.19399.pdf'
2025-02-05 00:31:34,409 - ERROR - Error downloading/parsing paper 2501.19339: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp8ue33cz5/2501.19339.pdf'
2025-02-05 00:32:40,721 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:32:43,782 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp7ji39_vn/2501.18119.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:32:45,267 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpdi3w9k8q/2501.14677.pdf/2501.14677v1.MatAnyone__Stable_Video_Matting_with_Consistent_Memory_Propagation.pdf'
2025-02-05 00:32:46,269 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp9qww27px/2501.19399.pdf/2501.19399v1.Scalable_Softmax_Is_Superior_for_Attention.pdf'
2025-02-05 00:33:47,379 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:33:49,668 - ERROR - Error downloading/parsing paper 2501.18119: expected str, bytes or os.PathLike object, not BufferedWriter
2025-02-05 00:33:51,912 - ERROR - Error downloading/parsing paper 2501.14677: expected str, bytes or os.PathLike object, not BufferedWriter
2025-02-05 00:34:15,771 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:34:18,049 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpk3q45hmf/2501.18119.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:34:19,554 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpcsi8jj8v/2501.14677.pdf/2501.14677v1.MatAnyone__Stable_Video_Matting_with_Consistent_Memory_Propagation.pdf'
2025-02-05 00:34:20,858 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmprl77yz1i/2501.19399.pdf/2501.19399v1.Scalable_Softmax_Is_Superior_for_Attention.pdf'
2025-02-05 00:34:49,825 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:34:51,747 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp2usneox9/2501.18119.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:34:53,098 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp8h6pk0uv/2501.14677.pdf/2501.14677v1.MatAnyone__Stable_Video_Matting_with_Consistent_Memory_Propagation.pdf'
2025-02-05 00:34:54,743 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmp8jethqj6/2501.19399.pdf/2501.19399v1.Scalable_Softmax_Is_Superior_for_Attention.pdf'
2025-02-05 00:35:36,473 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:35:38,731 - ERROR - Error downloading/parsing paper 2501.18119: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpykgtx123/2501.18119.pdf/2501.18119v1.Self_supervised_Quantized_Representation_for_Seamlessly_Integrating_Knowledge_Graphs_with_Large_Language_Models.pdf'
2025-02-05 00:35:39,852 - ERROR - Error downloading/parsing paper 2501.14677: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpmebyam1q/2501.14677.pdf/2501.14677v1.MatAnyone__Stable_Video_Matting_with_Consistent_Memory_Propagation.pdf'
2025-02-05 00:35:42,400 - ERROR - Error downloading/parsing paper 2501.19399: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpep9ja4nb/2501.19399.pdf/2501.19399v1.Scalable_Softmax_Is_Superior_for_Attention.pdf'
2025-02-05 00:35:43,668 - ERROR - Error downloading/parsing paper 2501.19339: [Errno 2] No such file or directory: '/var/folders/d0/tzkmd7vs1rl_vj2tgyyc7p_80000gn/T/tmpht2bnj9s/2501.19339.pdf/2501.19339v1.PixelWorld__Towards_Perceiving_Everything_as_Pixels.pdf'
2025-02-05 00:37:21,527 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:37:25,363 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-sonnet-20241022-v2:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:41:24,997 - INFO - Papers: {'2025-02-02': [],
 '2025-02-03': [Paper(arxiv_id='2501.18119', authors=['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], published_at=datetime.datetime(2025, 2, 3, 6, 6, 33, 957000, tzinfo=datetime.timezone.utc), title='Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models', summary='Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.', upvotes=15, thumbnail=None, content='Self-supervised Quantized Representation for Seamlessly Integrating\nKnowledge Graphs with Large Language Models\nQika Lin♡, Tianzhe Zhao♢, Kai He♡, Zhen Peng♢, Fangzhi Xu♢\nLing Huang♡, Jingying Ma♡, Mengling Feng♡\n♡National University of Singapore\n♢Xi’an Jiaotong University\nqikalin@foxmail.com\nephfm@nus.edu.sg\nAbstract\nDue to the presence of the natural gap be-\ntween Knowledge Graph (KG) structures and\nthe natural language, the effective integration\nof holistic structural information of KGs with\nLarge Language Models (LLMs) has emerged\nas a significant question. To this end, we pro-\npose a two-stage framework to learn and ap-\nply quantized codes for each entity, aiming for\nthe seamless integration of KGs with LLMs.\nFirstly, a self-supervised quantized representa-\ntion (SSQR) method is proposed to compress\nboth KG structural and semantic knowledge\ninto discrete codes (i.e., tokens) that align the\nformat of language sentences. We further de-\nsign KG instruction-following data by view-\ning these learned codes as features to directly\ninput to LLMs, thereby achieving seamless\nintegration.\nThe experiment results demon-\nstrate that SSQR outperforms existing unsu-\npervised quantized methods, producing more\ndistinguishable codes. Further, the fine-tuned\nLLaMA2 and LLaMA3.1 also have superior\nperformance on KG link prediction and triple\nclassification tasks, utilizing only 16 tokens\nper entity instead of thousands in conventional\nprompting methods.\n1\nIntroduction\nLarge\nLanguage\nModels\n(LLMs),\nsuch\nas\nLLaMA (Touvron et al., 2023a,b) and GPT-4 (Ope-\nnAI, 2023), are initiating considerable transforma-\ntions within the fields of artificial intelligence (AI)\nand natural language processing (NLP). They have\nachieved substantial success (Peng et al., 2023;\nWang et al., 2024; Xu et al., 2024b), and thus,\nhave been regarded as potential pathways towards\nachieving the ultimate goal of artificial general in-\ntelligence (Yang et al., 2024a). However, the spe-\ncific training strategies employed by LLMs render\nthem black-box models and struggle to retrieve the\nrelevant facts necessary for the correct answer (Pan\net al., 2024), resulting in low performance in com-\nplex reasoning scenarios (Xu et al., 2025, 2024a).\nFigure 1: Illustration of different strategies to integrate\nKGs with LLMs. (a) The direct method utilizes (sam-\npled) graph structures and semantic text as inputs. (b)\nOur method for seamlessly integrating KGs with LLMs\nusing learned quantized and discrete codes.\nFurthermore, knowledge hallucination becomes a\nserious issue, which may generate wrong state-\nments that conflict with reality (Bang et al., 2023;\nJi et al., 2023). It presents considerable risks, par-\nticularly in specialized fields like law (Cui et al.,\n2023) and healthcare (Lin et al., 2025; He et al.,\n2025).\nKnowledge Graphs (KGs), also known as knowl-\nedge bases, organizes massive amounts of factual\nknowledge in a structured and interpretable man-\nner by the triple form of (subject, relation, object).\nThey can serve as a vital supplement to LLMs (Pan\net al., 2024), providing an alternative way to ad-\ndress hallucinations and generate more precise an-\nswers using continual fine-tuning (Zhang et al.,\n2024b; Hron et al., 2024) or retrieve-based rea-\nsoning (Sun et al., 2024; Tan et al., 2024; Zhang\net al., 2024a). However, the KGs’ structure is in a\ngraph form, which markedly differs from the dis-\ncrete token format of the natural language in LLMs.\nThus, due to the presence of this natural represen-\ntation gap, the effective integration of comprehen-\nsive structural information of KGs with LLMs has\nemerged as a significant question.\nAs shown in Figure 1 (a), one straightforward\nmethod involves converting relevant triples into\ntextual prompts and then feeding them into LLMs,\ncombined with semantic text. This simple strategy\nwould necessitate a substantial number of tokens,\narXiv:2501.18119v1  [cs.CL]  30 Jan 2025\n\nFigure 2: The statistics of 2-hop sampled neighbors and\nneeded tokens (by LLaMA2) for entities in FB15k-237.\ncausing an enormous resource burden. Supposing\nthe average degree of an entity is d, the number of\nits neighbors grows exponentially and reaches dh in\nthe h-hop. While certain sampling strategies such\nas random walk (Ko et al., 2024) and path prun-\ning (Tan et al., 2024) have been introduced, a con-\nsiderable computational load also exists. As shown\nin Figure 2, when only sample 20% 2-hop neigh-\nbors in FB15k-237 (Toutanova and Chen, 2015)\ndataset, the median and mean number of neighbors\nfor entities are about 10 and 107, which requires\nmedian and mean tokens of about 300 and 3K, re-\nspectively. When with 30% sampling, even the\nmedian needed tokens reach about 2.5K per entity.\nConsidering that KG tasks may involve multiple en-\ntities, even the most advanced long-context LLMs\nmay face challenges in handling them. Meanwhile,\nemploying KGs’ substructures through sampling\ncould disrupt the holistic modeling of the entire\ngraph, potentially resulting in information loss and\nsub-optimal performance for downstream tasks.\nAnother alternate strategy involves integrating\ncontinuous KG embedding with LLMs by a learn-\nable adapter (Zhang et al., 2024b), introducing new\nnetworks in the framework. It requires additional\nprecise alignment between the different latent rep-\nresentation spaces of KG embeddings and LLMs.\nConsidering the above context, we aim to explore\nthe potential to bridge the natural gap between KG\nstructure and natural language and then integrate\nKGs with LLMs. Inspired by the early fusion strat-\negy in multimodal LLMs (Team, 2024), the general\nidea of this study is to learn compressed and dis-\ncrete entity codes (i.e., tokens), rather than continu-\nous embeddings, by quantized techniques to repre-\nsent holistic structural and semantic information of\nentities in KGs. They have the same discrete form\nof natural language, e.g., the quantized codes in\nFigure 1 (b) align the format of language sentences.\nThus, seamlessly integrating KGs with LLMs can\nbe realized by directly inputting the learned codes\ninto LLMs, merely requiring an expansion of the\nLLMs’ tokenizer vocabulary and eliminating the\nneed for any other framework modifications.\nAlthough several studies have conducted quan-\ntized representations on KGs (Galkin et al., 2022;\nChen et al., 2023; Li et al., 2023), they universally\nemploy an unsupervised approach to select anchors\nto represent entities, failing to the holistic struc-\ntural and semantic modeling. In this study, we first\nintroduce a self-supervised quantized representa-\ntion for KGs, aiming to learn discrete codes for\neach entity that can reconstruct KG structures and\nalign with semantic texts. A graph convolutional\nnetwork (GCN) is used as an encoder to model\nneighbor structures of KGs, and vector quantiza-\ntion (Van Den Oord et al., 2017) is implemented for\nthe KG quantized representation learning. Further,\nbased on learned entity codes, we construct specific\ninstructions for KG tasks, which can be seamlessly\nintegrated with LLMs, presenting a new paradigm\nto employ LLMs in KG applications. In summary,\nour contributions lie in the following three folds:\n• We propose a self-supervised quantized repre-\nsentation (SSQR) method that is capable of acquir-\ning both KG structural and semantic knowledge.\nTo our knowledge, this is the first study for KG\nquantization learning in a self-supervised manner.\n• We propose the first study that utilizes the\nderived codes to seamlessly integrate KGs with\nLLMs, which is achieved by viewing codes as input\nfeatures and designing KG instruction-following\ndata. It has extensive potential applications, e.g.,\nKG link prediction and triple classification.\n• From the experiment view, SSQR exhibits su-\nperior performance compared to current unsuper-\nvised quantized methods and the learned codes\nare more distinguishable. Besides, using only 16\ncodes for each entity, the fine-tuned LLaMA2 and\nLLaMA3.1 have superior performance on KG link\nprediction and triple classification tasks.\n2\nQuantized Representation for KGs\nFormally, a KG can be represented as G\n=\n{E, R, F}, which is the combination of entities\nE, relations R, and triples F ⊆E × R × E. Each\ntriple is in the form of (h, r, t). For each entity\ne, it has the structural and semantic information,\nwhere we utilize the entity neighbors N(e) and its\ntextual description Te to describe, respectively. Al-\nthough here we only use one-order neighbors N(e)\n\nFigure 3: The overall architecture of our study. (a) is for SSQR learning. (b) is for instruction tuning for KG tasks,\nwhere the learned quantized representations serve as features. Icons\nand\nrepresent the status of the module\nduring training, indicating if it is frozen or being updated, respectively.\nfor demonstration, our model is capable of captur-\ning high-order structure information by multi-layer\nGCNs. In the following contents, we will detailedly\ndescribe the structural and semantic modeling, as\nwell as the quantized representation for KGs.\nStructural Modeling. Here, we utilize simple but\neffective GCNs (Lin et al., 2022) to embed the\nstructural information of KGs, which follows the\niterative message-passing strategy to update the\nentity embeddings from l-th layer to (l+1)-th:\nel+1\nj\n= Wl\n1ej +\nX\n(ei,r)∈N(ej)\nWl\n2ml\nei,r,ej,\n(1)\nwhere e is the embedding of the entity and the\nW denotes the transformation matrix. m is the\nmessage information of the specific edge. Here, we\nfollow the composition operation (Vashishth et al.,\n2020) for calculation:\nml\nei,r,ej = el\ni ∗vl\nr,\n(2)\nwhere v is the relation embedding and ∗is element-\nwise multiplication for two vectors. Between dif-\nferent layers, relation representation is updated by\nlinear transformation: vl+1 = Wl\nrelvl. After L\nGCN layers, the entity representation eL and rela-\ntion representation vL are all obtained.\nQuantized Representation. Here, we introduce\nthe quantized representation for discrete KG rep-\nresentation. For its implementation, inspired by\nVQ-VAE (Van Den Oord et al., 2017) and VQ-\nGAN (Esser et al., 2021), we first maintain a dis-\ncrete cookbook X = [x1, x2, · · · , xM], where\neach xm ∈Rd is a learnable vector to represent\ncode m. Using this, a d-dimensional vector e can\nbe quantized by matching the nearest code:\nQ(e) = xi, where i = arg min\nm\n∥e −xm∥2\n2, (3)\nwhere Q is quantized function. In this way, each\nvector can be assigned to only one code, which\nmay lack representation capacity and distinguisha-\nbility for KG embedding. So we first transform the\nlearned entity embedding eL to multiple times of\ndimension d, i.e., FFN(eL) ∈RN×d. In this way,\neach entity can be assigned to a code sequence with\nthe length of N. Thus, each entity can be repre-\nsented to [q1, q2, · · · , qN] by Eq. (3), where qn is\nthe code index in the codebook. The quantized\nrepresentation vector can be:\nqe = WqQ(eL), Q(eL) = [xq1, xq2 · · · xqN ]. (4)\nBased on this, the whole model can be optimized\nin an end-to-end manner by the straight-through\ngradient estimator (Van Den Oord et al., 2017):\nLq =\n\r\rsg[eL] −qe\n\r\r2\n2 + β\n\r\reL −sg[qe]\n\r\r2\n2,\n(5)\nwhere sg stands for stop gradient, which is char-\nacterized by its identity function during forward\ncomputation and has zero partial derivatives for\nbackpropagation. ∥sg[eL] −qe\n\r\r2\n2 is codebook loss\nassuring the codes are close to encoder’s outputs\nand ∥eL −sg[qe]\n\r\r2\n2 is commit loss encouraging the\nencoder generating outputs close to codes. β is a\nhyper-parameter to trade off the two terms.\nStructure Reconstruction. To inject the holistic\nstructure information into the quantized represen-\ntations, we hope the learned entity codes can re-\nconstruct KG structures. But directly predicting\nadjacency matrix as is done in research for homo-\ngeneous graphs (Yang et al., 2024b) is inappropri-\nate, because KGs’ structures are more heteroge-\nneous and sparse. Thus, based on quantized em-\nbeddings, we verify the validity of each triplet (h,\n\nr, t) and implicitly reflect the holistic KG, where\nConvE (Dettmers et al., 2018) is implemented:\ns(h, r, t) =\n\x02\nFlat(Conv(¯qh∥¯vL\nr ))\n\x03⊤Wcqt.\n(6)\nFlat and Conv are the flatten and 2D convolution\noperations, respectively. ¯q and ¯v are transforma-\ntion matrices for embeddings q and v. The final\nscore of triple (h, r, t) can be regularized by the\nsigmoid function σ: ˜y = σ(s(h, r, t)). Finally,\ncompared with actual label y, the structure model-\ning can be learned by binary cross-entropy loss:\nLst =−1\n|F|\nX\ni\n[yi log ˜yi+(1−yi) log(1−˜yi)]. (7)\nSemantic Distilling. For semantic modeling, our\ngoal is to ensure that the learned codes for each\nentity can imply the information of its correspond-\ning text descriptions. Considering the substantial\nsuccess of LLMs, we introduce a simple yet effec-\ntive distilling strategy to learn from them. Specif-\nically, we first obtain text embeddings of KG en-\ntities by LLMs: te = LLM(Te). Here, we utilize\nthe text-embedding-3-large as the LLM by Ope-\nnAI API 1 considering its strong ability for text\nembeddings. It embeds each text sequence into a\n3072-dimension vector. Based on this, we make the\nmodel have the ability to align its semantic embed-\nding through the learned quantized output, where\nthe loss of mean square error is utilized:\nLse = −1\n|E|\nX\ni\n\r\rWsqei −tei\n\r\r2\n2.\n(8)\nIn this way, we distil the semantic knowledge from\nthe LLMs to our discrete codes of GCN outputs.\nOn the whole, the entire quantized representa-\ntion model can be updated by the combination of\nquantized, structural, and semantic loss:\nL = Lq + Lst + Lse.\n(9)\n3\nTuning LLMs with SSQR\nEmploying the quantized representation, each en-\ntity in KGs can be illustrated by codes of length N.\nThis can be perceived as the same form of natural\nlanguage, thereby facilitating its seamless integra-\ntion with LLMs. Every learned code can serve as a\nnew token, necessitating only an expansion of the\ntoken vocabulary within the LLM’s tokenizer.\n1https://platform.openai.com/docs/guides/embeddings\nInstruction: This is a knowledge graph completion\ntask, which needs to predict the tail entity for an\nincomplete query triplet.\nInput: The query triplet is (h, r, ?).\nThe quantized representation of entity h is: [Code(h)]\nThe answer candidates and corresponding quantized\nrepresentations are as follows:\nentity 1, [Code(entity 1)]\n· · ·\nentity 20, [Code(entity 20)]\nPlease generate quantized representations of the top-\n3 potential answers, ranked from highest to lowest:\nOutput: 1. [Code(candidate 1)]\n2. [Code(candidate 2)]\n3. [Code(candidate 3)]\nTable 1: Instruction format for link prediction, where\nlearned codes serve as entity features to help ranking.\nThis paradigm can be applied to various KG\ntasks, by constructing corresponding instruction\ndata, where the learned entity codes could act as\nfeatures. For example, the KG link prediction task\ncan be done using the instruction form as shown\nin Table 1. Specifically, the code sequence of en-\ntity e can be Code(e): “[CODEq1] [CODEq2] · · ·\n[CODEqN]”. For each query (h, r, ?), we pro-\nvide the codes Code(h) for query head h. Besides,\nwe give several answer candidates along with their\nassociated codes for LLM ranking. Candidates\ncan be obtained by conventional KG embedding\nmodels, e.g., TransE (Bordes et al., 2013) and\nCompGCN (Vashishth et al., 2020). The goal is\nto predict the actual ranking list of candidates us-\ning their discrete codes. The detailed instructions\nfor triple classification are described in Section C\nof the Appendix. For the LLM fine-tuning, the\nnext token prediction is carried out based on the\ninstruction I and previously generated text tokens:\nLllm = −\nN\nX\nn=1\nlog\n\x00xn|x<n, I\n\x01\n.\n(10)\n4\nExperiments and Analysis\nTo verify the effectiveness of the proposed SSQR\nand its ability to integrate with LLMs, We carry out\nexperiments on the KG link prediction and triple\nclassification tasks, where the popular datasets\nWN18RR (Dettmers et al., 2018) and FB15k-\n237 (Toutanova and Chen, 2015) as well as FB15k-\n237N (Lv et al., 2022) are utilized. For SSQR, a\n2-layer GCN is utilized as the encoder. β is set to\n0.25 in the experiment. The embedding dimension\n\n1.00\n0.28\n1.00\n0.24\n0.17\n1.00\n0.21\n0.14\n0.17\n1.00\n0.24\n0.13\n0.23\n0.24\n1.00\n0.20\n0.20\n0.17\n0.18\n0.23\n1.00\n0.17\n0.18\n0.17\n0.13\n0.19\n0.08\n1.00\n0.12\n0.03\n0.07\n0.15\n0.09\n0.08\n0.16\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(a) Original text embedding.\n1.00\n0.27\n1.00\n0.37\n0.26\n1.00\n0.20\n-0.34 -0.22\n1.00\n0.27\n-0.00\n0.34\n0.14\n1.00\n0.27\n0.12\n0.12\n0.04\n0.01\n1.00\n0.01\n0.14\n0.06\n-0.14 -0.05\n0.08\n1.00\n-0.03 -0.14 -0.36\n0.06\n-0.08\n0.23\n0.22\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(b) SSQR.\n1.00\n0.98\n1.00\n0.98\n0.98\n1.00\n0.99\n0.99\n0.99\n1.00\n1.00\n0.98\n0.98\n0.99\n1.00\n0.99\n0.96\n0.98\n0.97\n0.98\n1.00\n0.98\n0.98\n1.00\n0.99\n0.98\n0.98\n1.00\n0.98\n1.00\n0.99\n0.99\n0.98\n0.97\n0.98\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(c) SSQR w/o GCN.\n1.00\n0.03\n1.00\n0.20\n-0.16\n1.00\n-0.08\n0.05\n0.07\n1.00\n-0.06\n0.32\n0.13\n-0.00\n1.00\n-0.05\n0.07\n0.12\n0.12\n0.18\n1.00\n0.06\n0.40\n0.04\n0.02\n0.30\n0.24\n1.00\n0.14\n0.14\n-0.15\n0.22\n0.08\n0.33\n0.38\n1.00\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n03126385\n10411551\n02423589\n01902568\n07735803\n08677801\n15171008\n00992331\n(d) SSQR w/o semantics.\nFigure 4: The cosine similarity of quantized representations on the WN18RR dataset (sampled 8 entities).\nTable 2: The results of baselines are from Li et al. (2023).\n† means the improvement of SSQR compared to the best\nperformance in each metric. ‡ means the ablation results\ncompared to the results of SSQR.\nModel\nWN18RR\nFB15k-237\nMRR\nHits@10\nMRR\nHits@10\nNodePiece\n0.403\n0.515\n0.256\n0.420\n+RandomEQ\n0.425\n0.522\n0.263\n0.425\nEARL\n0.440\n0.527\n0.310\n0.501\n+RandomEQ\n0.442\n0.536\n0.308\n0.502\nSSQR\n0.483\n0.578\n0.361\n0.545\n∆(↑)†\n9.28%\n7.84%\n16.45%\n8.57%\nw/o GCN\n0.479\n0.577\n0.309\n0.482\n∆(↓)‡\n0.83%\n0.17%\n14.40%\n11.56%\nw/o sem\n0.447\n0.521\n0.347\n0.528\n∆(↓)‡\n7.45%\n9.86%\n3.88%\n3.12%\nis set to 200 as default. The number of codebook\nM and codes for each entity N is set to 2048 and\n32. The maximum number of training epochs is\n800. For LLM fine-tuning, LLaMA2 (7B) and\nLLaMA3.1 (8B) are utilized using M as 2048 and\nN as 16 for computation efficiency. The query for\nhead prediction (?, r, t) is transformed to the tail\nprediction by adding reverse relation of r. The\nmean reciprocal rank (MRR) and Hits@k values\nare set as evaluation metrics for model performance.\nMoreover, the triple classification task employs ac-\ncuracy, precision, recall and F1-score as metrics.\nMore detailed settings are shown in the Appendix.\n4.1\nSSQR Results\nWe compare the performance of our SSQR\nwith three unsupervised methods, i.e., Node-\nPiece (Galkin et al., 2022), EARL (Chen et al.,\n2023), and random entity quantization (RandomEQ\nfor short) (Li et al., 2023), for KG quantized repre-\nsentations. The results are given in Table 2.\nAs can be observed, SSQR achieves signifi-\ncant performance improvement against baselines,\nwhich has 9.28% and 7.84% improvements com-\npared with the previous optimal performance on the\nWN18RR dataset. When at the FB15k-237 dataset,\nthe improvements are even better, i.e., 16.45% and\n8.57%. Although these unsupervised methods are\nsimple and efficient for implementation, they fail\nto capture the structures of KGs. In contrast, our\nproposed self-supervised strategies would provide\nan effective way for quantized representations for\nKG structure learning.\n4.2\nSSQR Result Analysis\nAblation Studies. We carry out the ablation studies\nto verify the effectiveness of each module in SSQR\nas the bottom part of Table 2. Generally, the per-\nformance of link prediction degrades when GCN\nor semantic distilling is removed, but the extent of\ndegradation varies across different datasets. It can\nbe seen that the GCN encoder is more important\nfor the FB15k-237 dataset (14.40% and 11.56%\ndecline), while semantic information has more im-\npact on WN18RR (7.45% and 9.86%). This may\nbe due to the fact that FB15k-237 contains a rich\nKG structure which requires GCN to capture, while\nthe semantic text is more important for WN18RR\nto make up for the defects caused by the lack of\nrich structural information.\nRelevance among Entity Codes. We also calcu-\nlate the cosine similarity of quantized representa-\ntion in Figure 4, including the original text em-\nbedding, SSQR, SSQR w/o GCN, and SSQR w/o\nsemantics. When using only text embeddings, the\nsimilarities are all small positive values. SSQR w/o\nGCN has similarities that are all close to 1. These\nphenomena indicate that entity representations are\nin a small corner of the space (i.e., anisotropic),\nwhere the representation space is not fully utilized\nfor efficient representation. SSQR solves this prob-\nlem to a certain extent, with a greater range and\nvariety of similarities. Removing semantic infor-\nmation would diminish that advantage.\nImpacts of M and N. The number of codebooks\n\n(a) WN18RR dataset.\n(b) FB15k-237 dataset.\nFigure 5: The effects of codebook length (M) and se-\nquence length (N) for each entity.\nand sequence lengths for codes, i.e., M and N, are\nvital hyper-parameters for SSQR. We explore their\nimpacts in Figure 5. Generally, larger M and N\nwould lead to better performance as they increase\nthe modeling ability of SSQR. In the WN8RR\ndataset, N has a greater impact on M, indicating\nthe necessity of a large N for holistic and distin-\nguishable representations. It may be caused by the\nsparser structure and more entities in the WN18RR.\nDistinguishability of SSQR. Following Ran-\ndomEQ, we calculate the general entropy and Jac-\ncard distance to show codes-level and codewords-\nlevel distinguishability, respectively. For general\nentropy, SSQR has 16.76 and 15.27 on WN18RR\nand FB15k-237 datasets, similar to RandomEQ\n(16.75/15.27) and higher than that of NodePiece\n(15.94/15.26) and EARL (8.20/14.50). It shows\nthat our method has more diverse entity codes and\nbetter entity differentiation ability than other quan-\ntization methods. The Jaccard distances of each\nmodel are shown in Figure 6. RandomEQ and\nSSQR have high values that are far better than those\nof NodePiece and EARL. RandomEQ is superior\non the FB15k-237 dataset but SSQR performs bet-\nter on the WN18RR dataset. In summary, SSQR\nexhibits a robust capacity to distinguish different\nentities and effectively represent KGs.\n4.3\nQuantized Representations with LLMs\nLink Prediction.\nFor fine-tuning, we utilize\nthe pre-trained AdaProp (Zhang et al., 2023) to\ngenerate 20 candidates for each query as it has\nstrong and balanced performance on most KG\ntasks. For comparison, we selected the current\nadvanced embedding models, like TransE (Bordes\net al., 2013), CompGCN (Vashishth et al., 2020),\nAdaProp (Zhang et al., 2023), MA-GNN (Xu\net al., 2023), TCRA (Guo et al., 2024a), and Dif-\nfusionE (Cao et al., 2024). Besides, we include\nfive advanced LLM-based methods for more direct\ncomparison, including KICGPT (Wei et al., 2023),\n200\n400\n600\n800\n1000\n0.2\n0.4\n0.6\n0.8\n1.0\nJaccard distance\nRandomEQ\nNodePiece\nEARL\nSSQR\n(a) WN18RR dataset.\n200\n400\n600\n800\n1000\n0.6\n0.7\n0.8\n0.9\n1.0\nJaccard distance\nRandomEQ\nNodePiece\nEARL\nSSQR\n(b) FB15k-237 dataset.\nFigure 6: The mean Jaccard distance between codes of\na specific entity and its k nearest ones.\nCSProm-KG-CD (Li et al., 2024), ARR (Chen\net al., 2024), KG-FIT (Jiang et al., 2024), and\nMKGL (Guo et al., 2024b).\nThe results of link prediction are shown in Ta-\nble 3. It can be observed that SSQR with LLaMA2\nor LLaMA3.1 is obviously superior in KG link pre-\ndiction against general embedding methods. Com-\npared with the previous state-of-the-art MA-GNN,\nSSQR with LLaMA2 achieves about 4.60%, 8.09%,\n4.39%, -0.88% and 18.47%, 32.62%, 18.31%,\n4.92% improvement in two datasets, respectively.\nCompared with LLM-based methods, SSQR-\nLLaMA2 also shows competitive performance. It\nis better than KICGPT, CSProm-KG-CD, and Chat-\nGPT. Even KICGPT achieves good results on the\nFB15k-237 dataset, it can also be raised by 8.98%,\n14.37%, 9.60%, and 7.76%.\nFor the KG-FIT\n(HAKE), it also has 6.87%, 12.30%, 3.87%, and\n-3.16% improvements on the WN18RR dataset.\nAlthough there is a slight deficiency in terms\nof Hits@10, improvements on other metrics are\nhigh. Meanwhile, SSQR-LLaMA3.1 is better than\nSSQR-LLaMA2, demonstrating that learned quan-\ntized representations can be used for a more pow-\nerful LLM to get better performance. From all the\nresults, our methods generally achieve a greater im-\nprovement in the Hits@1 metric, which is caused\nby the candidate selection and ranking strategies we\nused in LLM fine-tuning. The candidate selection\nmodel may have limited ability, but our method has\na strong ability to select the correct answer from\nall candidates. This demonstrates that our method\nhas good scalability and can be further improved\nwith more accurate candidate selection models.\nTriple Classification.\nBeyond the link prediction\ntask, we conduct experiments on triple classifica-\ntion on the FB15k-237N dataset. The results are\nshown in Table 4, where our method outperforms\ngeneral embedding methods and other LLM-based\nbaselines. For the advanced KoPA (Zhang et al.,\n\nTable 3: The experiment results of general embedding methods and LLM-based methods for KG link prediction.\nModel\nWN18RR\nFB15k237\nMRR\nHits@1\nHits@3\nHits@10\nMRR\nHits@1\nHits@3\nHits@10\nGeneral Embedding Methods\nTransE (Bordes et al., 2013)\n0.223\n0.014\n0.401\n0.529\n0.330\n0.231\n0.369\n0.528\nCompGCN (Vashishth et al., 2020)\n0.479\n0.443\n0.494\n0.546\n0.355\n0.264\n0.390\n0.535\nAdaProp (Zhang et al., 2023)\n0.562\n0.499\n–\n0.671\n0.417\n0.331\n–\n0.585\nMA-GNN (Xu et al., 2023)\n0.565\n0.507\n0.592\n0.679\n0.379\n0.282\n0.415\n0.569\nTCRA (Guo et al., 2024a)\n0.496\n0.457\n0.511\n0.574\n0.367\n0.275\n0.403\n0.554\nDiffusionE (Cao et al., 2024)\n0.557\n0.504\n–\n0.658\n0.376\n0.294\n–\n0.539\nLLM-based Methods\nKICGPT (Wei et al., 2023)\n0.549\n0.474\n0.585\n0.641\n0.412\n0.327\n0.448\n0.554\nCSProm-KG-CD (Li et al., 2024)\n0.559\n0.508\n0.578\n0.660\n–\n–\n–\n–\nARR (Chen et al., 2024)\n0.521\n–\n0.607\n–\n0.398\n–\n0.436\n–\nKG-FIT (Jiang et al., 2024)\n0.553\n0.488\n0.595\n0.695\n0.362\n0.275\n0.485\n0.572\nMKGL (Guo et al., 2024b)\n0.552\n0.500\n0.577\n0.656\n0.415\n0.325\n0.454\n0.591\nSSQR-LLaMA2\n0.591\n0.548\n0.618\n0.673\n0.449\n0.374\n0.491\n0.597\nSSQR-LLaMA3.1\n0.598\n0.559\n0.618\n0.675\n0.459\n0.393\n0.491\n0.597\nTable 4: The experiment results of the triple classifica-\ntion on FB15k-237N dataset. The results of baselines\nare taken from Zhang et al. (2024b).\nModel\nAcc\nP\nR\nF1\nTransE (Bordes et al., 2013)\n0.697\n0.708\n0.671\n0.689\nDistMult (Yang et al., 2015)\n0.587\n0.590\n0.568\n0.579\nRotatE (Sun et al., 2019)\n0.684\n0.692\n0.664\n0.678\nAlpacazero-shot\n0.561\n0.533\n0.974\n0.689\nGPT-3.5zero-shot\n0.602\n0.866\n0.240\n0.376\nKG-LLaMA (Yao et al., 2023)\n0.748\n0.674\n0.962\n0.793\nKG-Alpaca (Yao et al., 2023)\n0.699\n0.627\n0.983\n0.766\nKoPA (Zhang et al., 2024b)\n0.777\n0.708\n0.941\n0.808\nSSQR-LLaMA2\n0.794\n0.757\n0.867\n0.808\nw/o SSQR\n0.754\n0.699\n0.891\n0.783\n∆\n-5.13% -7.71% +2.85% -3.07%\nSSQR-LLaMA3.1\n0.798\n0.759\n0.872\n0.811\nw/o SSQR\n0.767\n0.711\n0.901\n0.795\n∆\n-3.77% -6.34% +3.41% -2.03%\n2024b) model, the performance in the F1-score\nmetric is comparable to that of SSQR. However,\nthe accuracies of SSQR show a significant improve-\nment, i.e., 0.794/0.798 vs. 0.777, demonstrating\nthe effectiveness of integrating SSQR with LLMs.\n4.4\nInsights of LLM Fune-tuning\nAblation Studies. We carry out ablation studies\nto verify the effectiveness of quantized represen-\ntations for LLM tuning. The results are shown\nin Table 5 and the bottom part of Table 4, where\nw/o SSQR means only utilizing the entity’s name\nfor fine-tuning and removing learned entity codes.\nFor the link prediction task, there is a large perfor-\nmance drop, especially in the MRR, Hits@1, and\nHits@3 metrics. A similar pattern is also present in\nTable 5: The ablation results for the link prediction task.\nModel\nMRR\nHits@1\nHits@3\nHits@10\nWN18RR\nSSQR-LLaMA2\n0.591\n0.548\n0.618\n0.673\nw/o SSQR\n0.541\n0.495\n0.603\n0.668\n∆(↓)\n8.46%\n9.67%\n2.43%\n0.74%\nFB15k-237\nSSQR-LLaMA2\n0.449\n0.374\n0.491\n0.597\nw/o SSQR\n0.401\n0.322\n0.441\n0.589\n∆(↓)\n10.69%\n13.90%\n10.18%\n1.34%\nthe triple classification task. We observe that when\nunder the w/o SSQR setting, LLMs have overfitting\nissues, where their performance on training sets is\nvery high but fails to generalize to valid and test\nsets. This demonstrates that the learned discrete\ncodes are distinguishable and representative for dif-\nferent entities, thereby allowing their utilization as\nfeatures to assist KG tasks in LLMs.\nImpacts of M and N for LLM Tuning. We ex-\nplore the impacts of M and N for LLM tuning, the\nresults are shown in Figure 7. First, we present\nthe results of Original, which are the original re-\nsults of AdaProp. It is shown that all other results\nare better than those of AdaProp, showing it is\neffective for LLM fine-tuning with quantized repre-\nsentations. The settings with N=16 and M=2048\nhave better results compared to 16-512 and 8-2048,\nindicating large values are needed to represent en-\ntity structural and semantic information, serving\nbetter features for LLMs. N is more important than\nM, which drops more performance, especially on\nthe FB15k-237 dataset (16-512 even not dropping\n\nFigure 7: The impacts of quantized representation for\nKG link prediction task using LLMs on FB15k-237.\na lot), indicating the long quantized feature is ben-\neficial for LLMs to distinguish entities.\nToken Embeddings in LLMs. To view the repre-\nsentation of codewords and actual language tokens\nin LLMs, we display all 2048 codewords and cor-\nrespondingly sample an equal number of language\ntokens. Further, we reduce the embeddings to 2-\ndimensional space using t-SNE (Van der Maaten\nand Hinton, 2008) and plot the results in Figure 8.\nIt is shown that these two types of tokens are gen-\nerally divided into two categories, indicating they\nhave different representation zones. It is consistent\nwith the intuition and indicates LLM can perceive\nthat they are different types of tokens, showing po-\ntential for further exploration of LLM on KG tasks\nusing SSQR.\n5\nRelated Work\nFor parameter-efficient embeddings on large KGs,\nNodePiece (Galkin et al., 2022) introduces an\nanchor-based method to learn a fixed-size entity\nvocabulary, where unsupervised strategies of Per-\nsonalized PageRank (Page, 1999), node degree, and\nrandom are used for anchor selection. Each entity\ncan be represented through k closest anchors and\ntheir respective distances. Further, EARL (Chen\net al., 2023) randomly samples 10% entities as\nanchors and introduces connected relation informa-\ntion to match anchors’ counterparts. To simplify\nthe whole process, Li et al. (2023) introduces ran-\ndom entity quantization (RandomEQ) to randomly\nset anchor entities and randomly select relations\nfor matching. The results show that RandomEQ\nachieves similar results compared to previous cu-\nrated strategies and has more distinguishable ability.\nIn general, these methods are all in an unsupervised\nlearning manner, which could be efficient for large\nKG embedding but fails to model comprehensive\nstructural and semantic information.\nFigure 8: Token embedding virtualization in LLMs\n(WN18RR dataset), where red and blue dots are real\nword tokens and code tokens, respectively.\nCurrently, numerous research studies are ded-\nicated to incorporating KGs with LLMs to max-\nimize and exploit their respective strengths (Pan\net al., 2024). On one hand, using prompt engineer-\ning or retrieve strategies (Wei et al., 2023; Sun et al.,\n2024; Kau et al., 2024), the information of KGs\nbe sampled and instantiated as tokens like natural\nlanguage to input LLMs. On the other hand, the\ntriple-level or sub-graph structures of KG can be\ninputted to the LLMs to inject knowledge (Hron\net al., 2024). However, because of the natural gap\nbetween the graph structure of KGs and the natural\nlanguage, how to seamlessly and effectively inte-\ngrate the whole structural and semantic information\nof KGs with LLMs is an open problem.\n6\nConclusion and Potential Impacts\nFor seamlessly integrating KGs with LLMs, we\nintroduce a self-supervised quantized representa-\ntion method (SSQR). It compresses the structural\nand semantic information of entities in KGs to a\ndiscrete permutation of codewords, which has a\nsimilar format as the natural language and can be di-\nrectly inputted to the LLMs. By specific instruction\ndata and fine-tuning, LLMs can seamlessly learn\nKG’s knowledge, which can be used in KG applica-\ntions. To verify the effectiveness of our method, we\nimplement experiments on KG link prediction and\ntriple classification tasks, which demonstrate the su-\nperiority of our method. This innovative paradigm\npromises to usher in transformative techniques for\nKGs in the era of LLMs. In the future, we will ex-\nplore more applications and make progress towards\nunified frameworks for multiple KG tasks, e.g.,\nKG-based QA (Luo et al., 2024a), KG-based rec-\nommendation (Huang et al., 2023), and language\nmodeling (Luo et al., 2024b).\n\nLimitations\nDespite our SSQR method’s capacity to facilitate\nthe seamless integration of KGs with LLMs, our\nstudy encounters the generalization limitation due\nto the substantial computational burden associated\nwith LLMs. In most recent and our studies, LLMs\nare fine-tuned for a specific KG and the correspond-\ning task, which can not be applied to various KG\ntasks and largely limits the model generalization\nability. In the future, we will try to construct uni-\nfied LLMs for KGs by implementing quantization\nwithin the same discrete space.\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. In IJC-\nNLP, pages 675–718.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In NIPS, pages 2787–2795.\nZongsheng Cao, Jing Li, Zigan Wang, and Jinliang Li.\n2024. Diffusione: Reasoning on knowledge graphs\nvia diffusion-based graph neural networks. In KDD,\npages 222–230.\nMingyang Chen, Wen Zhang, Zhen Yao, Yushan Zhu,\nYang Gao, Jeff Z. Pan, and Huajun Chen. 2023.\nEntity-agnostic representation learning for parameter-\nefficient knowledge graph embedding.\nIn AAAI,\npages 4182–4190.\nZhongwu Chen, Long Bai, Zixuan Li, Zhen Huang,\nXiaolong Jin, and Yong Dou. 2024. A new pipeline\nfor knowledge graph reasoning enhanced by large\nlanguage models without fine-tuning. In EMNLP,\npages 1366–1381.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and\nLi Yuan. 2023. Chatlaw: Open-source legal large\nlanguage model with integrated external knowledge\nbases. CoRR, abs/2306.16092.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In AAAI, pages 1811–1818.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. 2021.\nTaming transformers for high-resolution image syn-\nthesis. In CVPR, pages 12873–12883.\nMikhail Galkin, Etienne G. Denis, Jiapeng Wu, and\nWilliam L. Hamilton. 2022. Nodepiece: Composi-\ntional and parameter-efficient representations of large\nknowledge graphs. In ICLR.\nJingtao Guo, Chunxia Zhang, Lingxi Li, Xiaojun Xue,\nand Zhendong Niu. 2024a. A unified joint approach\nwith topological context learning and rule augmenta-\ntion for knowledge graph completion. In Findings of\nthe ACL, pages 13686–13696.\nLingbing Guo, Zhongpu Bo, Zhuo Chen, Yichi Zhang,\nJiaoyan Chen, Yarong Lan, Mengshu Sun, Zhiqiang\nZhang, Yangyifei Luo, Qian Li, Qiang Zhang, Wen\nZhang, and Huajun Chen. 2024b. MKGL: mastery\nof a three-word language. In NeurIPS.\nKai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan,\nMengling Feng, and Erik Cambria. 2025. A survey\nof large language models for healthcare: from data,\ntechnology, and applications to accountability and\nethics. Information Fusion, page 102963.\nJiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne\nLiu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet,\nJD Co-Reyes, Noah Fiedel, C Daniel Freeman, et al.\n2024. Training language models on the knowledge\ngraph: Insights on hallucinations and their detectabil-\nity. CoRR, abs/2408.07852.\nQing Huang, Zhenyu Wan, Zhenchang Xing, Changjing\nWang, Jieshan Chen, Xiwei Xu, and Qinghua Lu.\n2023. Let’s chat to find the apis: Connecting human,\nLLM and knowledge graph through AI chain. In\nASE, pages 471–483. IEEE.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):248:1–248:38.\nPengcheng Jiang, Lang Cao, Cao Xiao, Parminder Bha-\ntia, Jimeng Sun, and Jiawei Han. 2024. KG-FIT:\nknowledge graph fine-tuning upon open-world knowl-\nedge. In NeurIPS.\nAmanda Kau, Xuzeng He, Aishwarya Nambissan,\nAland Astudillo, Hui Yin, and Amir Aryani. 2024.\nCombining knowledge graphs and large language\nmodels. CoRR, abs/2407.06564.\nYoumin Ko, Hyemin Yang, Taeuk Kim, and Hyunjoon\nKim. 2024. Subgraph-aware training of text-based\nmethods for knowledge graph completion. CoRR,\nabs/2407.12703.\nDawei Li, Zhen Tan, Tianlong Chen, and Huan Liu.\n2024. Contextualization distillation from large lan-\nguage model for knowledge graph completion. In\nFindings of the EACL, pages 458–477.\nJiaang Li, Quan Wang, Yi Liu, Licheng Zhang, and\nZhendong Mao. 2023.\nRandom entity quantiza-\ntion for parameter-efficient compositional knowledge\ngraph representation. In EMNLP, pages 2917–2928.\nQika Lin, Jun Liu, Fangzhi Xu, Yudai Pan, Yifan Zhu,\nLingling Zhang, and Tianzhe Zhao. 2022. Incorporat-\ning context graph with logical reasoning for inductive\nrelation prediction. In SIGIR, pages 893–903.\n\nQika Lin, Yifan Zhu, Xin Mei, Ling Huang, Jingying\nMa, Kai He, Zhen Peng, Erik Cambria, and Mengling\nFeng. 2025. Has multimodal learning delivered uni-\nversal intelligence in healthcare? a comprehensive\nsurvey. Information Fusion, 116:102795.\nYang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. 2024.\nFinetuning generative large language models with\ndiscrimination instructions for knowledge graph com-\npletion. In ISWC, pages 199–217.\nHaoran Luo, Haihong E, Zichen Tang, Shiyao Peng,\nYikai Guo, Wentai Zhang, Chenghao Ma, Guant-\ning Dong, Meina Song, Wei Lin, Yifan Zhu, and\nAnh Tuan Luu. 2024a. Chatkbqa: A generate-then-\nretrieve framework for knowledge base question an-\nswering with fine-tuned large language models. In\nFindings of the ACL, pages 2039–2056.\nXindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and\nWei Hu. 2024b.\nKnowla: Enhancing parameter-\nefficient finetuning with knowledgeable adaptation.\nIn NAACL, pages 7153–7166.\nXin Lv, Yankai Lin, Yixin Cao, Lei Hou, Juanzi Li,\nZhiyuan Liu, Peng Li, and Jie Zhou. 2022. Do pre-\ntrained models benefit knowledge graph completion?\nA reliable evaluation and a reasonable approach. In\nFindings of the ACL, pages 3570–3581.\nOpenAI. 2023.\nGpt-4 technical report.\nCoRR,\nabs/2303.08774.\nLawrence Page. 1999. The pagerank citation ranking:\nBringing order to the web. Technical report, Techni-\ncal Report.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2024. Unifying large\nlanguage models and knowledge graphs: A roadmap.\nIEEE TKDE, 36(7):3580–3599.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. CoRR, abs/2304.03277.\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\nWang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-\nYeung Shum, and Jian Guo. 2024. Think-on-graph:\nDeep and responsible reasoning of large language\nmodel on knowledge graph. In ICLR.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. In ICLR.\nXingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin\nYuan, and Wenjie Zhang. 2024. Paths-over-graph:\nKnowledge graph empowered large language model\nreasoning. CoRR, abs/2410.14211.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nChameleon Team. 2024.\nChameleon:\nMixed-\nmodal early-fusion foundation models.\nCoRR,\nabs/2405.09818.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd Workshop on\nContinuous Vector Space Models and their Composi-\ntionality (CVSC), pages 57–66.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and efficient foun-\ndation language models. CoRR, abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. Advances in Neural\nInformation Processing Systems, pages 6306–6315.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(11).\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha P. Talukdar. 2020. Composition-based multi-\nrelational graph convolutional networks. In ICLR.\nJianing Wang, Qiushi Sun, Xiang Li, and Ming Gao.\n2024.\nBoosting language models reasoning with\nchain-of-knowledge prompting. In ACL, pages 4958–\n4981.\nYanbin Wei, Qiushi Huang, Yu Zhang, and James T.\nKwok. 2023. KICGPT: large language model with\nknowledge in context for knowledge graph comple-\ntion. In Findings of the EMNLP, pages 8667–8683.\nFangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun\nLiu, and Erik Cambria. 2025. Are large language\nmodels really good logical reasoners? a comprehen-\nsive evaluation and beyond. IEEE Transactions on\nKnowledge and Data Engineering.\nFangzhi Xu, Qika Lin, Tianzhe Zhao, Jiawei Han, and\nJun Liu. 2024a. Pathreasoner: Modeling reasoning\npath with equivalent extension for logical question\nanswering. In ACL, pages 13413–13429.\nFangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei\nYuan, Shuai Yuan, Qika Lin, Yu Qiao, and Jun Liu.\n2024b. Symbol-llm: Towards foundational symbol-\ncentric interface for large language models. In ACL,\npages 13091–13116.\nHongcai Xu, Junpeng Bao, and Wenbo Liu. 2023.\nDouble-branch multi-attention based graph neural\nnetwork for knowledge graph completion. In ACL,\npages 15257–15271.\n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,\nand Li Deng. 2015. Embedding entities and relations\nfor learning and inference in knowledge bases. In\nICLR.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-\ntian Han, Qizhang Feng, Haoming Jiang, Shaochen\nZhong, Bing Yin, and Xia Hu. 2024a. Harnessing\nthe power of llms in practice: A survey on chatgpt\nand beyond. ACM TKDD, 18(6):1–32.\nLing Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda\nHong, Wei Qu, Wentao Zhang, Bin Cui, Muhan\nZhang, and Jure Leskovec. 2024b. Vqgraph: Re-\nthinking graph representation space for bridging gnns\nand mlps. In ICLR.\nLiang Yao, Jiazhen Peng, Chengsheng Mao, and\nYuan Luo. 2023.\nExploring large language mod-\nels for knowledge graph completion.\nCoRR,\nabs/2308.13916.\nHeidi C. Zhang, Sina J. Semnani, Farhad Ghassemi,\nJialiang Xu, Shicheng Liu, and Monica S. Lam.\n2024a. SPAGHETTI: open-domain question answer-\ning from heterogeneous data sources with retrieval\nand semantic parsing. In Findings of the ACL, pages\n1663–1678.\nYichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu,\nWen Zhang, and Huajun Chen. 2024b. Making large\nlanguage models perform better in knowledge graph\ncompletion. In ACM MM, pages 233–242. ACM.\nYongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen\nChu, and Bo Han. 2023. Adaprop: Learning adaptive\npropagation for graph neural network based knowl-\nedge graph reasoning. In KDD, pages 3446–3457.\nA\nStatistics of WN18RR Dataset\nBesides the statistic analysis of FB15k-237 in Fig-\nure 2, we also conduct the statistic analysis of\nWN18RR, which is shown in Figure 9. Specifically,\nwe sample 200 entities from the whole KG and\nthere are two settings (50% neighbor sampling and\n100% neighbors). In the first setting of 50%, the\nmedian and mean of neighbors are 4.0 and 10.37,\nwhile the median and mean number of needed to-\nkens are 61.5 and 185.84, respectively. For the set-\nting of 100%, the median and mean of neighbors\nare 33.5 and 79.05, while the median and mean\nnumber of needed tokens are 623.5 and 1492.74,\nrespectively. Compared to our SSQR, which only\nrequires 16 tokens to represent each entity, both\n50% and 100% settings demand a considerably\nhigher number of tokens.\nFigure 9: The statistics of 2-hop sampled neighbors and\nneeded tokens (by LLaMA2) for entities in WN18RR.\nB\nBaselines\nIn this section, we give detailed descriptions of\nvarious baselines utilized in the paper.\nB.1\nQuantized Representations for KGs\n• NodePiece (Galkin et al., 2022): The selec-\ntion of quantized anchors relies on unsupervised\nstrategies, including Personalized PageRank, node\ndegree, and random approaches.\n• EARL (Chen et al., 2023): It randomly sam-\nples 10% entities as quantized anchors and intro-\nduces connected relation information to match an-\nchors’ counterparts.\n• Random entity quantization (RandomEQ for\nshort) (Li et al., 2023): It randomly sets anchor en-\ntities and randomly selects relations for matching.\nB.2\nKG Link Prediction\n• TransE (Bordes et al., 2013): The strategy of\nincorporating translational distance is utilized for\nlearning representations of entities and relations.\n• CompGCN (Vashishth et al., 2020):\nSev-\neral entity-relation composition operations are pro-\nposed to combine the semantic information of\nneighbor entity-relation pairs in GNNs.\n• AdaProp (Zhang et al., 2023): An adaptive\npropagation path is learned to filter out irrelevant\nentities while preserving promising targets in the\nGNN framework.\n• MA-GNN (Xu et al., 2023): A dual-branch,\nmulti-attention-based GNN model is employed to\ndevelop expressive entity representations.\n• TCRA (Guo et al., 2024a): A neuro-symbolic\nmethod that combines topological context learning\nwith rule augmentation.\n• DiffusionE (Cao et al., 2024): Introducing dif-\nfusion process to KG embedding method.\n\n• KICGPT (Wei et al., 2023): The method uti-\nlizes a model based on embeddings as the retriever,\nwhich generates a ranked list of potential entities.\nAn in-context learning strategy is then designed to\nguide ChatGPT in re-ranking these entities through\nmulti-round interactions.\n• CSProm-KG-CD (Li et al., 2024): It converts\ncompact and structured triples into segments en-\nriched with context by LLMs. Following this, two\ncustom auxiliary tasks (reconstruction and contex-\ntualization) are presented, which enable compact\nKGC models to incorporate insights derived from\nthese enhanced triples.\n• ARR (Chen et al., 2024): A three-step (align-\nment, reasoning, and reranking) process designed\nto support and amplify conventional KG embed-\nding models, without necessitating fine-tuning.\nThe results are taken from the setting of LLAMA3-\n70B and RotatE (Sun et al., 2019) model.\n• KG-FIT (Jiang et al., 2024): It involves the\nautomatic construction of a semantically consis-\ntent entity hierarchy through clustering and LLM-\nguided refinement. It also details a fine-tuning tech-\nnique that incorporates knowledge from the hierar-\nchical structure and pre-trained text embeddings of\nentities, thereby improving KG embeddings. The\nresults are from the HAKE model setting.\n• MKGL (Guo et al., 2024b): A context retriever\nis introduced to help LLMs be aware of the tex-\ntual and relational context of KGs. A score re-\ntriever is also used to provide the score information.\nLLaMA2 (7B) is utilized as the base LLM.\nB.3\nKG Triple Classification\n• TransE (Bordes et al., 2013): The strategy of\nincorporating translational distance is utilized for\nlearning representations of entities and relations.\n• DistMult (Yang et al., 2015): It utilizes the\nsemantic matching strategy, where the validity of\na fact is depicted as the matching degree between\nthe representation of entity and relation.\n• RotatE (Sun et al., 2019): It defines each re-\nlation as a rotation from the source entity to the\ntarget entity in a complex vector space.\n• Alpacazero-shot: It carries out zero-shot reason-\ning with Alpaca (Taori et al., 2023) with textual\nsequences for predicting the validity of a triple.\n• GPT-3.5zero-shot: It carries out zero-shot rea-\nsoning with GPT-3.5 2 with textual sequences for\n2https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-\napi-updates/\npredicting the validity of a triple.\n• KG-LLaMA (Yao et al., 2023): It carries out\ninstruction tuning with LLaMA with textual se-\nquences for predicting the validity of a triple.\n• KG-Alpaca (Yao et al., 2023): It carries out in-\nstruction tuning with Alpaca with textual sequences\nfor predicting the validity of a triple.\n• KoPA (Zhang et al., 2024b): It proposes a\nknowledge prefix adapter to effectively integrate\npre-trained KG structural embeddings with LLMs.\nAlpaca-7B is utilized as the LLM backbone.\nC\nExperimental Details\nTable 6: The statistics of WN18RR, FB15k-237, and\nFB15k-237N datasets. The former two are for link pre-\ndiction. FB15k-237N dataset is for triple classification,\nwhere ‘/’ splits the positive and negative samples.\nDataset\nEnt\nRel\nTrain\nValid\nTest\nWN18RR\n40943 11\n86835\n3034\n3134\nFB15k-237\n14541 237 272115\n17535\n20466\nFB15k-237N 13104 93\n87282 7041/7041 8226/8226\nThe statistics of utilized datasets are shown in\nTable 6. For the SSQR learning, the default embed-\nding dimension is set to 200. The GCN layer and\ndropout rate are 2 and 0.2. The training batch is\n1024. For optimization, the learning rate is 0.0005\nand the L2 regularization weight is 1e-8. For LLM\ntuning, we utilize 4 NVIDIA H100 GPUs and the\nlearning rate is set to 2e-5 with 3% warmup ra-\ntio. In the link prediction experiment, we first tune\nLLMs on the instruction data of CompGCN’s train-\ning split to initialize. Then, inspired by Wei et al.\n(2023) and Liu et al. (2024), we divide the valid\nset into two segments in a 9:1 ratio. The larger\npart is utilized to finetune LLMs to learn the rank-\ning preference, while the smaller part is used for\nvalidation. In the triple classification experiment,\nwe only update the embedding layer and the last\nfour Transformer layers of LLMs for tuning effi-\nciency. Meanwhile, M and N are set to 1024 and\n16. In the training instruction data, we randomly se-\nlect negative samples at a rate 16 times of positive\nones. The instruction format of triple classification\nis shown in Table 7.\nD\nEntropy and Jaccard Distance\nAs presented by Li et al. (2023), it is significant\nfor the ability to distinguish different entities for\n\nInstruction: Given a triple in the knowledge graph,\nyou need to predict its validity based on the triple\nitself and entities’ quantized representations.\nInput: The triple is: (h, r, t)\nThe quantized representation of entity h is: [Code(h)]\nThe quantized representation of entity t is: [Code(t)]\nPlease determine the validity of the triple and re-\nspond True or False.\nOutput: True/False\nTable 7: Instruction format for triple classification.\nquantized representations. We follow this study to\ncalculate the entropy at the overall representation\nlevel and the Jaccard distance at the codeword-\nselection level. The greater entropy and Jaccard\ndistance values denote the greater distinguishable\nability. The entropy is calculated as:\nH = −\nX\np\n\x00Code(e)\n\x01\n· log p\n\x00Code(e)\n\x01\n. (11)\np\n\x00Code(e)\n\x01\nis the relative frequency of quantized\nrepresentation of entity e. Moreover, the Jaccard\ndistance is given by:\nJ =\n1\n|E| · k\nX\nei∈E\nX\nej∈kNN(ei)\nd\n\x00Code(ei), Code(ej)\n\x01\n,\n(12)\nd\n\x00Code(ei), Code(ej)\n\x01\n= |CSet(ei) ∪CSet(ej)| −|CSet(ei) ∩CSet(ej)|\n|CSet(ei) ∪CSet(ej)|\n,\n(13)\nwhere kNN(ei) retrieves k entities. Each possesses\ncodes that exhibit the nearest Jaccard distance to\nthe codes of ei. CSet(e) is the set of Code(e) by\nremoving the order information of codewords of\nthe entity representations.\nE\nAdditional Experimental Analysis\nE.1\nTraining Process of SSQR\nWe display the training process SSQR in Figure 10,\nwhere w/o GCN and w/o sem denote ablations for\nthe structural embedding and semantic distilling,\nrespectively. The findings indicate that both struc-\ntural embedding and semantic distilling contribute\npositively to the overall learning of quantized rep-\nresentation. The influence of semantic information\non the FB15k-237 dataset is less significant when\ncompared to its effect on GCN. Differently, seman-\ntic information is more important on the WN18RR\ndataset. This could be attributed to the varying\nlevels of KGs’ sparsity.\nFigure 10: The training process of SSQR, where the\nHits@1 metric is used to show the model performance.\nE.2\nRelevance among Entity Codes on\nFB15k-237 Dataset\nWe also calculate the cosine similarity of quan-\ntized representation on the FB15k-237 dataset in\nFigure 12, which has the same setting as Figure 4.\nThe contents presented in these two figures are also\nsimilar. When utilizing solely text embeddings, the\ncorresponding similarities yield positive yet modest\nvalues. Moreover, the similarities associated with\nSSQR without the use of GCN are typically close\nto 1. These observations suggest that entity repre-\nsentations occupy a limited portion of the existing\nspace, thus failing to maximize the efficiency of\nrepresentation. SSQR addresses this issue to some\ndegree by providing a broader range and diversity\nof similarities.\nE.3\nImpacts of M and N for LLM Tuning on\nFB15k-237 Dataset\nWe also explore the impacts of M and N for LLM\ntuning on the FB15k-237 dataset, the results are\nshown in Figure 11. It can lead to conclusions\nsimilar to Figure 7.\nFigure 11: The impacts of quantized representation for\nKG link prediction task using LLMs on WN18RR.\n\n1.00\n0.10 1.00\n0.10 0.22 1.00\n0.07 0.21 0.09 1.00\n0.15 0.35 0.14 0.13 1.00\n0.13 0.07 0.14 0.10 0.12 1.00\n0.11 0.09 0.07 0.16 0.05 0.10 1.00\n0.08 0.23 0.15 0.34 0.14 0.14 0.14 1.00\n0.21 0.09 0.15 0.17 0.19 0.17 0.11 0.19 1.00\n0.11 0.28 0.28 0.14 0.33 0.08 0.11 0.14 0.16 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n(a) Original text embedding.\n1.00\n0.23 1.00\n-0.03 -0.03 1.00\n0.11 -0.21 -0.26 1.00\n0.04 0.03 0.09 0.10 1.00\n0.21 0.04 0.11 0.23 -0.08 1.00\n0.63 0.23 0.15 -0.03 -0.02 0.15 1.00\n0.37 -0.09 0.02 0.62 0.10 0.14 0.28 1.00\n0.40 -0.08 -0.12 0.34 0.16 0.15 0.42 0.42 1.00\n0.12 0.28 0.18 0.06 0.22 0.11 0.03 0.31 0.11 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b) SSQR.\n1.00\n1.00 1.00\n1.00 1.00 1.00\n1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(c) SSQR w/o GCN.\n1.00\n0.15 1.00\n-0.27 -0.24 1.00\n0.00 -0.11 -0.10 1.00\n0.01 0.03 0.21 0.04 1.00\n-0.11 0.13 0.22 -0.00 -0.08 1.00\n0.55 0.06 -0.04 0.06 0.16 -0.02 1.00\n0.15 -0.29 -0.04 0.47 -0.00 -0.07 0.14 1.00\n0.35 0.11 -0.11 0.39 0.04 0.20 0.26 0.51 1.00\n-0.11 0.11 -0.19 -0.01 0.26 -0.02 0.04 0.19 -0.03 1.00\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n/m/0q307\n/m/073h5b\n/m/02fybl\n/m/09d5h\n/m/083skw\n/m/07f1x\n/m/05xzcz\n/m/0cv_2\n/m/0xynl\n/m/07k2mq\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(d) SSQR w/o semantics.\nFigure 12: The cosine similarity of quantized representations on the FB15k-237 dataset (sampled 10 entities).\nE.4\nToken Embeddings in LLMs on\nFB15k-237 Dataset\nSimilar to Figure 8, we display the real word tokens\nand learned code tokens using t-SNE in Figure 13.\nThe evidence also suggests that these two types\nof tokens typically fall into distinct categories, im-\nplying they each have unique representation areas.\nE.5\nCase Studies\nTo intuitively show the seamlessly integrating KG\ntasks with LLMs, we carry out case studies in Ta-\nble 8, 9, and 10, covering both link prediction and\ntriple classification tasks. It demonstrates that our\nmethod can effectively address both tasks, indicat-\ning the validity and good generalization ability of\nour proposed SSQR.\nFigure 13: Token embedding virtualization in LLMs\n(FB15k-237 dataset), where red and blue dots are real\nword tokens and code tokens, respectively.\n\nInput: This is a knowledge graph completion task, which needs to predict the tail entity for an incomplete query triplet.\nThe query triplet is (radiotherapy, hypernym, ?).\nThe quantized representation of entity radiotherapy is: [2006] [588] [350] [1486] [214] [929] [328] [1424] [1792] [919]\n[944] [740] [438] [843] [147] [628]\nThe answer candidates and corresponding quantized representations are as follows:\ndisease, [156] [1880] [1777] [185] [121] [720] [783] [1713] [945] [1077] [180] [1576] [1574] [1433] [216] [1280]\ntomography, [182] [597] [657] [1486] [404] [468] [732] [564] [833] [1470] [1756] [626] [1674] [843] [1928] [513]\nmedical care, [422] [68] [1329] [1517] [1251] [431] [1479] [1445] [1666] [407] [952] [406] [1337] [388] [1982] [685]\nstatus, [1721] [1906] [1773] [1811] [12] [892] [1625] [1476] [1561] [176] [534] [1463] [1657] [368] [70] [1618]\nphysiological state, [1721] [718] [267] [394] [120] [1105] [885] [1823] [1496] [23] [952] [406] [1559] [1198] [1149] [1800]\nmedical science, [565] [413] [842] [1517] [350] [873] [575] [595] [721] [935] [1554] [175] [708] [1643] [1820] [1775]\ninfection, [565] [1594] [990] [1066] [974] [40] [434] [874] [1401] [371] [1700] [1118] [1709] [52] [71] [1408]\npicturing, [788] [168] [641] [1797] [927] [711] [1608] [123] [1163] [1460] [952] [406] [1752] [1464] [553] [1158]\nmedicine, [1879] [1216] [691] [296] [1743] [892] [1851] [595] [2039] [1428] [426] [740] [399] [579] [433] [1987]\nunhealthiness, [1389] [644] [570] [258] [635] [647] [732] [1139] [1660] [407] [464] [1020] [1574] [1905] [926] [1971]\ngrounds, [1268] [1053] [803] [780] [1194] [285] [328] [289] [1163] [915] [1921] [1020] [524] [1774] [430] [1572]\ndefense reaction, [1881] [1821] [1620] [1703] [435] [995] [908] [1308] [1596] [1598] [401] [2008] [903] [817] [92] [1158]\nradiology, [1478] [588] [1340] [1797] [1436] [1914] [1894] [1424] [634] [1460] [1756] [740] [673] [843] [108] [1088]\nradioscopy, [1005] [1002] [1441] [137] [1436] [1378] [1479] [1649] [1544] [1470] [534] [626] [902] [272] [904] [1874]\ntreat, [396] [2007] [1935] [1305] [1993] [1030] [1690] [1445] [1203] [1417] [1554] [495] [1752] [1001] [1236] [98]\nspecialize, [1005] [1933] [1976] [780] [927] [1728] [575] [105] [1791] [1598] [616] [1118] [1752] [425] [437] [1847]\ntherapy, [396] [816] [81] [488] [336] [1164] [1690] [1288] [900] [915] [1554] [175] [666] [1622] [765] [685]\nspecialism, [384] [816] [599] [394] [435] [789] [1479] [105] [664] [407] [1554] [103] [1752] [1708] [697] [1130]\nsymptom, [1721] [1913] [772] [858] [120] [1150] [1374] [289] [1666] [1417] [944] [2008] [1454] [958] [1169] [1800]\nmedicine, [156] [350] [1599] [1955] [1368] [508] [1527] [1445] [1561] [1460] [426] [1142] [940] [653] [793] [471]\nPlease generate quantized representations of the top-3 potential answer entities, ranked from highest to lowest:\nLLM Output: 1, [396] [816] [81] [488] [336] [1164] [1690] [1288] [900] [915] [1554] [175] [666] [1622] [765] [685]\n2, [156] [1880] [1777] [185] [121] [720] [783] [1713] [945] [1077] [180] [1576] [1574] [1433] [216] [1280]\n3, [182] [597] [657] [1486] [404] [468] [732] [564] [833] [1470] [1756] [626] [1674] [843] [1928] [513]\nGround Truth: [396] [816] [81] [488] [336] [1164] [1690] [1288] [900] [915] [1554] [175] [666] [1622] [765] [685]\nTable 8: Case study on WN18RR for link prediction using LLaMA2. The code of ground truth therapy is ranked to\nthe first position from 17-th.\n\nInput: This is a knowledge graph completion task, which needs to predict the tail entity for an incomplete query triplet.\nThe query triplet is (Valparaiso University, inverse relation of /location/location/contains, ?).\nThe quantized representation of entity Valparaiso University is [527] [1345] [1849] [1227] [1751] [2038] [818] [515] [1417]\n[333] [29] [721] [1691] [798] [1033] [153]\nThe answer candidates and corresponding quantized representations are as follows:\nMinnesota, [1532] [258] [1837] [357] [923] [1994] [638] [555] [771] [1003] [1736] [1473] [1495] [1436] [1313] [20]\nNew York, [661] [1243] [542] [1741] [1907] [1799] [858] [1794] [1916] [458] [1844] [909] [438] [1737] [686] [963]\nCalifornia, [1059] [1286] [1604] [846] [1086] [451] [1087] [1794] [994] [297] [1463] [159] [556] [1836] [407] [963]\nMassachusetts, [202] [1243] [977] [757] [304] [389] [1172] [1308] [1916] [1858] [1323] [11] [841] [1680] [1798] [1885]\nIllinois, [961] [1025] [1267] [174] [643] [1951] [1742] [1794] [1720] [1481] [543] [1883] [695] [1921] [182] [963]\nNew York City, [1458] [326] [1707] [239] [151] [640] [1366] [1794] [610] [458] [1844] [932] [122] [311] [121] [868]\nUnited Kingdom, [51] [193] [1354] [669] [1867] [881] [480] [1271] [392] [1858] [650] [909] [1503] [1126] [1550] [153]\nPennsylvania, [361] [825] [1052] [1655] [1670] [732] [951] [1569] [275] [1995] [543] [4] [753] [351] [331] [637]\nLos Angeles, [1584] [1231] [1707] [1461] [1867] [1466] [265] [1933] [850] [1533] [805] [1128] [1824] [1823] [307] [963]\nFlorida, [2016] [326] [542] [1614] [462] [1433] [1388] [819] [926] [1289] [1321] [563] [1977] [1144] [1268] [662]\nOhio, [1643] [1889] [1604] [88] [1364] [485] [1819] [1569] [54] [1582] [1500] [411] [438] [125] [1636] [20]\nTexas, [2012] [1845] [1207] [412] [531] [1394] [1004] [688] [653] [1671] [1790] [1690] [1732] [1686] [1721] [1205]\nVirginia, [99] [825] [738] [1859] [1287] [1540] [708] [780] [653] [662] [756] [1873] [1514] [1686] [59] [409]\nEngland, [848] [1220] [1052] [590] [175] [451] [529] [1933] [808] [1598] [1790] [1988] [670] [524] [121] [136]\nConnecticut, [633] [1025] [672] [338] [1694] [1799] [1528] [1177] [1949] [458] [1703] [411] [395] [33] [233] [1013]\nIndiana, [497] [1875] [1849] [377] [1694] [61] [1471] [1445] [392] [1672] [1500] [300] [711] [1839] [331] [136]\nLos Angeles County, [662] [1772] [558] [1623] [304] [1755] [1388] [1794] [1554] [333] [1662] [1258] [1474] [1764] [1021]\n[409]\nLouisiana, [1584] [326] [1309] [50] [1035] [0] [546] [728] [1130] [297] [1284] [11] [1514] [1929] [1248] [136]\nMichigan, [172] [326] [382] [174] [1364] [1774] [1165] [664] [994] [1480] [1923] [431] [816] [1587] [1443] [860]\nHennepin County, [584] [1890] [1263] [1168] [1990] [656] [784] [1735] [102] [1533] [1979] [1005] [1312] [786] [873]\n[1294]\nPlease generate quantized representations of the top-3 potential answer entities, ranked from highest to lowest:\nLLM Output: 1, [497] [1875] [1849] [377] [1694] [61] [1471] [1445] [392] [1672] [1500] [300] [711] [1839] [331] [136]\n2, [1532] [258] [1837] [357] [923] [1994] [638] [555] [771] [1003] [1736] [1473] [1495] [1436] [1313] [20]\n3, [661] [1243] [542] [1741] [1907] [1799] [858] [1794] [1916] [458] [1844] [909] [438] [1737] [686] [963]\nGround Truth: [497] [1875] [1849] [377] [1694] [61] [1471] [1445] [392] [1672] [1500] [300] [711] [1839] [331] [136]\nTable 9: Case study on FB15k-237 for link prediction using LLaMA2. The code of ground truth Indiana is ranked\nto the first position from 16-th.\nInput: Given a triple in the knowledge graph, you need to predict its validity based on the triple itself and entities’ quantized\nrepresentations.\nThe triple is: (Carl Foreman, /people/person/place_of_birth, Chicago)\nThe quantized representation of entity “Carl Foreman” is: [72] [114] [150] [336] [519] [494] [155] [482] [594] [934] [1012]\n[677] [473] [450] [827] [455]\nThe quantized representation of entity “Chicago” is: [603] [413] [385] [937] [731] [735] [836] [92] [400] [372] [563] [225]\n[82] [526] [81] [229]\nPlease determine the validity of the triple and respond True or False.\nLLM Output: True\nGround Truth: True\nInput: Given a triple in the knowledge graph, you need to predict its validity based on the triple itself and enti-\nties’ quantized representations.\nThe triple is: (Jessica Lange, /people/person/profession, Lawyer)\nThe quantized representation of entity “Jessica Lange” is: [23] [712] [484] [202] [289] [211] [772] [667] [870] [903] [213]\n[693] [360] [59] [868] [722]\nThe quantized representation of entity “Lawyer” is: [760] [84] [976] [802] [430] [735] [912] [480] [966] [411] [284] [113]\n[727] [744] [333] [56]\nPlease determine the validity of the triple and respond True or False.\nLLM Output: False\nGround Truth: False\nTable 10: Two cases on FB15k-237N dataset for triple classification using LLaMA2.')]}
2025-02-05 00:43:05,565 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:44:54,461 - INFO - Papers: {'2025-02-02': [], '2025-02-03': []}
2025-02-05 00:45:48,220 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:50:24,569 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:52:20,631 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:53:51,614 - ERROR - Error downloading/parsing paper 2501.18119: 'KeyboardInterrupt' object is not callable
2025-02-05 00:55:19,603 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 00:55:23,046 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:26,941 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:28,626 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:30,438 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:32,512 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:34,798 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:37,431 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:39,299 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:41,632 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:43,324 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:46,468 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:48,212 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:50,308 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 00:55:50,529 - INFO - Papers: {'2025-02-02': [], '2025-02-03': []}
2025-02-05 00:57:11,911 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 01:00:07,184 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 01:00:11,405 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 01:00:14,780 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 01:00:16,462 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 23:57:41,283 - INFO - Initializing LLM for extracting main content from papers
2025-02-05 23:57:46,211 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 23:57:49,672 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
2025-02-05 23:57:51,226 - ERROR - Error extracting main content: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.
