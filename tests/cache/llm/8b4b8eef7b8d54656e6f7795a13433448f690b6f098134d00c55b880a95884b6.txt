Evaluation Frameworks for Image Generation and Large Language Models

Robust evaluation frameworks are necessary to assess the complexities of generated images.
Early benchmarks primarily focus on image quality and alignment using automated metrics.
Frechet Inception Distance, Inception Score, and CLIPScore are commonly used for quantitative image assessment.
Traditional automated evaluation methods cannot analyze compositional capabilities and lack fine-grained reporting.

T2I-CompBench serves as a comprehensive benchmark for open-world compositional text-to-image generation.
TIFA integrates Large Language Models with Visual Question Answering to enhance text-to-image evaluation.
GenEval advances automatic evaluation by incorporating compositional reasoning tasks.
Subsequent benchmarks leverage human evaluations to assess robustness, creativity, and counting capabilities.

Researchers have focused on evaluating ethical and societal impacts of image generation models.
Text-to-image models have been tested for social biases, stereotypes, and dynamic prompt-specific bias.
FAIntbench pioneered a structured approach to bias evaluation by defining, categorizing, and measuring specific biases.
CPDM dataset facilitates evaluation of potential copyright infringement.

Large Language Models are evaluated across multiple domains and tasks.
LLMs are assessed on traditional NLP tasks like sentiment analysis, language translation, and text summarization.
LLMs demonstrate capabilities in mathematical and logical reasoning.
LLMs perform well in question-answer benchmarks and code-related tasks.

LLMs are evaluated in diverse fields including computational social science, legal tasks, economics, psychology, and search recommendations.
In natural sciences and engineering, LLMs are tested for general scientific and technical capabilities.
Medical domain evaluations assess LLMs' effectiveness in medical queries, examinations, and as medical assistants.
LLM-based agents are evaluated, particularly their tool-using abilities.

Multilingual capabilities of LLMs are also evaluated.
Performance is measured using metrics like ROUGE scores for text summarization and BLEU scores for machine translation.

Various evaluation protocols and frameworks have been proposed.
Dyval series provides dynamic evaluation protocols for reasoning data.
UniGen is a unified framework for generating truthful and diverse textual datasets.
AutoBencher uses language models to automatically search for evaluation datasets.

LLMs themselves have emerged as evaluation tools.
"LLM-as-a-Judge" offers a cost-effective alternative to human evaluations.
Frameworks like ChatEval, EvaluLLM, and Prometheus demonstrate LLMs' utility in evaluation tasks.