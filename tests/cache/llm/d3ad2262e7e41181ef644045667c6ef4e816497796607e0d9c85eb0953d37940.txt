Research Papers on Large Language Model Safety and Jailbreak Prevention

Rigorllm is a research paper about resilient guardrails for large language models against undesired content.
The paper was published as an arXiv preprint in 2024.
The paper has multiple authors including Nathaniel Li, Alexander Pan, Anjali Gopal, and others.
The references include multiple studies focused on large language model safety and jailbreak prevention.
The references cover topics such as measuring malicious use, improving alignment, and developing safety evaluation frameworks.
Key referenced works include the WMDP Benchmark, HarmBench, and JailbreakBench.
The references span publications from conferences like Neural Information Processing Systems and AAAI Conference on Artificial Intelligence.
The research papers explore various approaches to detecting and mitigating potential risks in large language models.