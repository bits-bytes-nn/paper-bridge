topic: Large Language Model Robustness Assessment

  entities:
    GPT-4o-mini|Model
    Claude-3.5-Sonnet|Model
    Gemini-1.5-Flash|Model
    Mixtral-8*7B|Model
    KeyBERT|Tool
    Large Language Models|Technological Concept

  proposition: The research defines 14 types of natural language perturbations across 8 categories.
    entity-attribute relationships:
    Research|DESCRIBED_BY|14 types of natural language perturbations
    Research|DESCRIBED_BY|8 categories

  proposition: Perturbation methods include using KeyBERT to select key terms for spelling mistakes, emoji insertion, and spaced uppercase modifications.
    entity-attribute relationships:
    KeyBERT|USED_FOR|selecting key terms
    Perturbation methods|INCLUDE|spelling mistakes
    Perturbation methods|INCLUDE|emoji insertion
    Perturbation methods|INCLUDE|spaced uppercase modifications

  proposition: Social tagging perturbations involve using an LLM to generate subtitles with hashtags and "@" mentions to simulate social media language.
    entity-attribute relationships:
    Social tagging perturbations|SIMULATE|social media language
    Large Language Models|USED_FOR|generating subtitles

  proposition: Multilingual blend perturbations involve translating selected keywords or phrases into Chinese at word and sentence levels.
    entity-attribute relationships:
    Multilingual blend perturbations|INVOLVE|translating keywords
    Multilingual blend perturbations|INVOLVE|translating phrases
    Perturbations|APPLIED_AT|word level
    Perturbations|APPLIED_AT|sentence level

  proposition: The research created a dynamic dataset by randomly selecting 400 questions from annotated datasets and 400 from open-ended question-answering datasets.
    entity-attribute relationships:
    Research|CREATED|dynamic dataset
    Dataset|CONTAINS|400 questions from annotated datasets
    Dataset|CONTAINS|400 questions from open-ended question-answering datasets

  proposition: GPT-4o-mini, Claude-3.5-Sonnet, and Gemini-1.5-Flash achieved the highest robustness score of 99.36%.
    entity-attribute relationships:
    GPT-4o-mini|ROBUSTNESS_SCORE|99.36%
    Claude-3.5-Sonnet|ROBUSTNESS_SCORE|99.36%
    Gemini-1.5-Flash|ROBUSTNESS_SCORE|99.36%

  proposition: Mixtral-8*7B had the lowest robustness score of 88.78% on annotated datasets.
    entity-attribute relationships:
    Mixtral-8*7B|ROBUSTNESS_SCORE|88.78%
    Robustness score|APPLIED_TO|annotated datasets

  proposition: Models generally performed more consistently on annotated datasets compared to open-ended datasets.
    entity-entity relationships:
    Models|PERFORMED_ON|annotated datasets
    Models|PERFORMED_ON|open-ended datasets

  proposition: Robustness scores on annotated datasets were consistently above 92% across most models.
    entity-attribute relationships:
    Models|ROBUSTNESS_SCORE|above 92%
    Robustness scores|APPLIED_TO|annotated datasets

  proposition: Open-ended dataset robustness scores were significantly lower than annotated dataset scores.
    entity-attribute relationships:
    Open-ended datasets|ROBUSTNESS_SCORE|lower than annotated datasets