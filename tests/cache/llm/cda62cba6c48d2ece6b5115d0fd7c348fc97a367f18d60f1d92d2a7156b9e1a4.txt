topic: Hallucination Detection in Large Language Models

  entities:
    Large Language Models|Model
    HaluEval|Benchmark
    TruthfulQA|Benchmark
    FACTOR|Method
    REALTIMEQA|Benchmark
    FreshQA|Benchmark
    EvolvingQA|Benchmark
    HalluQA|Benchmark
    ChineseFactEval|Benchmark
    SelfCheckGPT-Wikibio|Dataset
    FELM|Benchmark
    PHD|Benchmark
    ChatGPT|Model
    ROME|Method
    SCOTT|Method
    RAG|Approach
    ITI|Method

  proposition: Previous research on hallucination detection in Large Language Models (LLMs) focuses on two primary aspects: factuality and faithfulness.
    entity-entity relationships:
    Large Language Models|RESEARCH_FOCUS|factuality
    Large Language Models|RESEARCH_FOCUS|faithfulness

  proposition: Detecting factual errors in LLM responses involves comparing model-generated content against reliable knowledge sources.
    entity-attribute relationships:
    Large Language Models|DETECTION_METHOD|comparing model-generated content against reliable knowledge sources

  proposition: Some research addresses hallucination detection in a zero-source setting by estimating the uncertainty of generated factual content.
    entity-attribute relationships:
    Large Language Models|DETECTION_METHOD|estimating uncertainty of generated factual content

  proposition: Methods for detecting unfaithful generation can be categorized into fact-based metrics, classifier-based metrics, QA-based metrics, uncertainty estimation, and prompting-based metrics.
    entity-attribute relationships:
    Large Language Models|DETECTION_CATEGORIES|fact-based metrics
    Large Language Models|DETECTION_CATEGORIES|classifier-based metrics
    Large Language Models|DETECTION_CATEGORIES|QA-based metrics
    Large Language Models|DETECTION_CATEGORIES|uncertainty estimation
    Large Language Models|DETECTION_CATEGORIES|prompting-based metrics

topic: Hallucination Detection Benchmarks

  proposition: HaluEval provides a comprehensive collection of generated and human-annotated hallucinated samples.
    entity-attribute relationships:
    HaluEval|TYPE|benchmark
    HaluEval|CONTENT|generated and human-annotated hallucinated samples

  proposition: TruthfulQA consists of adversarially curated questions that mimic human falsehoods.
    entity-attribute relationships:
    TruthfulQA|TYPE|benchmark
    TruthfulQA|CONTENT|adversarially curated questions

  proposition: FACTOR introduces a method for automatically creating benchmarks by perturbing factual statements.
    entity-attribute relationships:
    FACTOR|TYPE|method
    FACTOR|CHARACTERISTIC|automatically creating benchmarks
    FACTOR|TECHNIQUE|perturbing factual statements

  proposition: REALTIMEQA, FreshQA, and EvolvingQA offer questions to evaluate factual accuracy of LLMs in relation to evolving real-world knowledge.
    entity-entity relationships:
    REALTIMEQA|SIMILAR_TO|FreshQA
    REALTIMEQA|SIMILAR_TO|EvolvingQA
    FreshQA|SIMILAR_TO|EvolvingQA

    entity-attribute relationships:
    REALTIMEQA|PURPOSE|evaluate factual accuracy of LLMs
    FreshQA|PURPOSE|evaluate factual accuracy of LLMs
    EvolvingQA|PURPOSE|evaluate factual accuracy of LLMs

  proposition: HalluQA and ChineseFactEval are benchmarks designed to measure hallucination in Chinese large language models.
    entity-attribute relationships:
    HalluQA|TYPE|benchmark
    HalluQA|FOCUS|Chinese large language models
    ChineseFactEval|TYPE|benchmark
    ChineseFactEval|FOCUS|Chinese large language models

  proposition: SelfCheckGPT-Wikibio provides a dataset for detecting sentence-level hallucinations.
    entity-attribute relationships:
    SelfCheckGPT-Wikibio|TYPE|dataset
    SelfCheckGPT-Wikibio|PURPOSE|detecting sentence-level hallucinations

  proposition: FELM assesses factual accuracy across various domains.
    entity-attribute relationships:
    FELM|TYPE|benchmark
    FELM|PURPOSE|assess factual accuracy across domains

  proposition: PHD offers a passage-level hallucination detection benchmark created using ChatGPT and human annotations.
    entity-attribute relationships:
    PHD|TYPE|benchmark
    PHD|LEVEL|passage-level
    PHD|CREATION_METHOD|using ChatGPT and human annotations

topic: Hallucination Mitigation Strategies

  proposition: Enhancing the factual accuracy of the pre-training corpus can improve the model's parametric knowledge.
    entity-attribute relationships:
    Large Language Models|IMPROVEMENT_METHOD|enhancing pre-training corpus factual accuracy

  proposition: Refining training data quality during supervised fine-tuning can help mitigate hallucinations.
    entity-attribute relationships:
    Large Language Models|IMPROVEMENT_METHOD|refining training data quality
    Large Language Models|MITIGATION_TECHNIQUE|supervised fine-tuning

  proposition: Alignment processes can help language models recognize their knowledge boundaries.
    entity-attribute relationships:
    Large Language Models|IMPROVEMENT_METHOD|alignment processes
    Large Language Models|CAPABILITY|recognize knowledge boundaries

  proposition: Inference-time interventions have become a focus for reducing hallucinations.
    entity-attribute relationships:
    Large Language Models|MITIGATION_TECHNIQUE|inference-time interventions

topic: Prompting Techniques for Hallucination Mitigation

  proposition: Prompting plays a crucial role in providing context and controlling model outputs.
    entity-attribute relationships:
    Large Language Models|CONTROL_METHOD|prompting
    Large Language Models|PROMPTING_PURPOSE|provide context
    Large Language Models|PROMPTING_PURPOSE|control model outputs

  proposition: Chain-of-thought and least-to-most prompting help reveal faulty logic or assumptions.
    entity-attribute relationships:
    Large Language Models|PROMPTING_TECHNIQUE|chain-of-thought
    Large Language Models|PROMPTING_TECHNIQUE|least-to-most
    Large Language Models|PROMPTING_PURPOSE|reveal faulty logic
    Large Language Models|PROMPTING_PURPOSE|reveal assumptions

  proposition: Self-consistency, SCOTT, and self-ask methods involve multiple prompts to identify potential hallucinations.
    entity-attribute relationships:
    Large Language Models|PROMPTING_TECHNIQUE|self-consistency
    Large Language Models|PROMPTING_TECHNIQUE|SCOTT
    Large Language Models|PROMPTING_TECHNIQUE|self-ask
    Large Language Models|PROMPTING_PURPOSE|identify potential hallucinations

topic: Retrieval-Augmented Generation

  proposition: RAG methods retrieve information from reliable knowledge sources to reduce hallucinations.
    entity-attribute relationships:
    RAG|PURPOSE|reduce hallucinations
    RAG|METHOD|retrieve information from reliable knowledge sources

  proposition: These methods can use external knowledge bases, structured databases, websites like Wikipedia, search engine APIs, or various external tools.
    entity-attribute relationships:
    RAG|KNOWLEDGE_SOURCES|external knowledge bases
    RAG|KNOWLEDGE_SOURCES|structured databases
    RAG|KNOWLEDGE_SOURCES|websites
    RAG|KNOWLEDGE_SOURCES|search engine APIs
    RAG|KNOWLEDGE_SOURCES|external tools

topic: Model Editing Approaches

  proposition: Model editing allows modification of LLM behavior in a data- and computation-efficient manner.
    entity-attribute relationships:
    Large Language Models|EDITING_CHARACTERISTIC|data-efficient
    Large Language Models|EDITING_CHARACTERISTIC|computation-efficient

  proposition: Methods include incorporating auxiliary sub-networks or directly modifying model parameters.
    entity-attribute relationships:
    Large Language Models|EDITING_METHOD|incorporating auxiliary sub-networks
    Large Language Models|EDITING_METHOD|directly modifying model parameters

  proposition: ROME modifies feedforward weights to update specific factual associations.
    entity-attribute relationships:
    ROME|EDITING_TECHNIQUE|modify feedforward weights
    ROME|PURPOSE|update specific factual associations

  proposition: Inference-time intervention (ITI) identifies attention heads associated with truthfulness and shifts activations to elicit truthful answers.
    entity-attribute relationships:
    ITI|METHOD|identify attention heads
    ITI|FOCUS|truthfulness
    ITI|PURPOSE|elicit truthful answers