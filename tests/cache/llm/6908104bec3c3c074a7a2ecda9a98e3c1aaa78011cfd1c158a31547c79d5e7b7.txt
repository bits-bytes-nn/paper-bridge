Trustworthiness Performance of Large Language Models
GenFM Trustworthiness Evaluation Across Multiple Dimensions

The paper presents a comprehensive evaluation of large language models across multiple trustworthiness dimensions.
The evaluation covers seven key dimensions: truthfulness, safety, fairness, privacy, robustness, machine ethics, and advanced AI risk.
The evaluation includes 19 different large language models from various providers.
The models are scored across each dimension, with an overall average trustworthiness score calculated.
GPT-4o has the highest overall average trustworthiness score of 82.64.
The lowest overall average trustworthiness score is 74.93, belonging to Claude-3-Haiku.
Robustness is consistently the highest-scoring dimension across most models, with many models scoring above 90.
Truthfulness and advanced AI risk dimensions show the most variation between models.
The evaluation provides a standardized approach to assessing the trustworthiness of generative foundation models.
The research is accompanied by a GitHub repository (TrustEval-toolkit) for further reference and exploration.