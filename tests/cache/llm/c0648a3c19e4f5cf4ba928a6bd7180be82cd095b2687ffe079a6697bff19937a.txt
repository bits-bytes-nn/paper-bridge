Jailbreak Techniques and Research in Large Language Models

PAIR algorithm creates semantic jailbreaks with black-box access to an LLM.
Disrupting model alignment can be achieved by altering decoding methods.
Lapid et al. use a Genetic Algorithm to optimize adversarial suffixes.
Li et al. propose a similar method to Lapid et al.'s genetic algorithm approach.
Yao et al. apply fuzzy testing to generate attack instructions.
Yu et al. employ GPTFUZZER, which uses mutation techniques to evolve human-crafted templates into adversarial inputs.
Yuan et al. encode strings to ciphers to bypass safety alignment of LLMs like GPT-4.
Lv et al. propose CodeChameleon, which allows personalized encryption to jailbreak LLMs.

Kour et al. introduce the AttaQ dataset to examine potentially harmful LLM responses.
Zhang et al. propose the JADE platform to challenge LLMs by increasing language complexity.
ObscurePrompt jailbreaks LLMs by transforming queries to be more obscure.
Wu et al. use LLMs to jailbreak large vision models like GPT-4V.
LLM vulnerabilities often differ across languages.
Crescendo is a multi-turn jailbreak that interacts with models in a seemingly benign manner.
Shen et al. propose a jailbreak methodology inspired by psychological concepts of subconsciousness and echopraxia.
Zhang et al. propose PsySafe, a comprehensive framework for psychological-based attack, defense, and evaluation.
Zhu et al. propose AdvPrefix, a prefix-forcing approach for selecting model-dependent prefixes.

Jailbreak Defense Techniques

Xie et al. and Phute et al. use self-evaluation to find potential harm in input queries.
A study uses a secondary LLM to emulate the conscience of a protected primary LLM.
Perplexity-based filtering is effective against attacks like GCG.
SmoothLLM and SemanticSmooth defend by randomly perturbing multiple input prompt copies.
Zhang et al. propose "goal prioritization" to defend against jailbreak attacks.
HateModerate detects harmful content in user inputs.
Xu et al. propose a human-and-model-in-the-loop framework to enhance chatbot safety.
Kim et al. find current defense methods are not sufficiently robust.
AutoDefenes is a multi-agent defense framework that filters harmful LLM responses.