Toxicity Levels in Open-Source and Proprietary Language Models
There is no substantial difference in toxicity levels between open-source and proprietary models.
Open-weight and proprietary models display similar distributions of toxicity scores.
Open-source models can achieve toxicity control comparable to proprietary models.
Toxicity mitigation techniques are broadly accessible across different model types and developers.

Exaggerated Safety in Generative Models
Exaggerated Safety is an emergent alignment issue in generative models.
Exaggerated Safety refers to an overly cautious approach leading models to reject harmless queries.
Google temporarily removed the portrait generation feature of Gemini Pro 1.5 due to false refusals.
False refusals can occur against clearly harmless user requests.
Excessive sensitivity is intended to minimize risk but can suppress legitimate and safe interactions.

Evaluation of Exaggerated Safety
XSTest comprises 250 safe prompts across ten prompt types to evaluate model responses.
OKTest evaluates exaggerated safety using CommonsenseQA and WikiQA datasets.
PHTest is a synthetic dataset generated through prefix optimization to trigger model refusals.
MOSSBench specifically evaluates oversensitivity to harmless multimodal queries.

Mitigation Strategies for Exaggerated Safety
Self-contrastive decoding can modulate output distribution to identify model shortcuts.
Post-safety alignment (PSA) method aims to enhance safety while preserving model utility.

Policies for Evaluating Exaggerated Safety
Homonyms can be safe or harmful depending on context.
Real Discrimination prompts highlight discrimination against absurd or non-existent groups.
Nonsense Group prompts create fictional discriminatory scenarios.
Nonsense Discrimination prompts apply illogical discrimination to real groups.
Historical Events prompts explore sensitive topics through factual historical contexts.
Figurative Language can be interpreted differently in literal versus idiomatic uses.