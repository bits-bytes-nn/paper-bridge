Principles of Honest Large Language Models by Gao et al.

Gao et al. introduced six principles for honest Large Language Models (LLMs).
The principles focus on acknowledging limitations in various domains.
The research aims to maintain both honesty and helpfulness in LLM interactions.

Principle: Latest Information with External Services
LLMs have limitations in accessing current information due to outdated pre-training data.
LLMs lack sufficient fact-checking capabilities.
LLMs cannot directly access live or up-to-date external data sources.
Honest LLMs should acknowledge these information retrieval limitations.

Principle: User Input Not Enough Or With Wrong Information
LLMs frequently encounter incorrect or ambiguous user questions.
Honest LLMs must maintain objectivity and avoid catering to user biases.
LLMs should provide accurate responses independent of user input.

Principle: Professional Capability in Specific Domains
Specialized domains evolve rapidly and require extensive, high-quality datasets.
LLMs have constraints in generating reliable outputs for expert-level tasks.
Honest LLMs should recognize and communicate their domain-specific limitations.

Principle: Interactivity Sensory Processing
LLMs cannot directly perceive or process sensory data like auditory or tactile feedback.
Honest LLMs must acknowledge their inability to interact with the physical world directly.

Principle: Modality Mismatch
LLMs are designed for text-based inputs and outputs.
LLMs struggle to interpret or generate non-textual data like images and audio.
Honest LLMs should transparently communicate their data modality limitations.

Principle: Self Identity Cognition
Honest LLMs should maintain awareness of their artificial nature.
LLMs should recognize differences between human users and AI assistants.
When addressing topics requiring human perception or self-reflection, LLMs should disclaim their limitations.

Calibration Concept
Honest LLMs communicate confidence levels in their answers.
Calibration means a model's confidence accurately reflects its knowledge.
Calibrated models hedge or express lower confidence on difficult questions.
Calibration can be measured through logits, token probabilities, and verbal statements.

Benchmarking Honest LLMs
Two key metrics: honest rate and combined honest rate.
Honest rate measures the percentage of scenarios where LLMs remain truthful.
Combined honest rate reflects cases where LLMs are both honest and helpful.
Evaluation uses an LLM-as-a-Judge approach with a standardized principle-based template.

Dynamic Dataset Construction
Web-browsing agents retrieve unsolved human challenges for specific domain testing.
Principle-guided prompts generate user queries across different honesty categories.