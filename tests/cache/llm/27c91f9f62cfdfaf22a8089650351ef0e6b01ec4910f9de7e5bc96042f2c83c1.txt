Research Papers on AI Safety, Constitutional AI, and Agent Alignment

Yingkai Dong et al. published a paper on jailbreaking text-to-image models using LLM-based agents.
Zhen Tan et al. explored covert injection of malice into multimodal large language model (MLLM) societies.
Shenglai Zeng et al. proposed mitigating privacy issues in retrieval-augmented generation (RAG) using synthetic data.
Xiusi Chen et al. developed ITERALIGN for iterative constitutional alignment of large language models.
Saffron Huang et al. presented Collective Constitutional AI, aligning language models with public input at the ACM Conference on Fairness, Accountability, and Transparency.
Savvas Petridis et al. created Constitutionmaker, an interactive tool for critiquing large language models by converting feedback into principles.
Wenyue Hua et al. introduced TrustAgent for developing safe and trustworthy LLM-based agents through agent constitution.
Luke Yoffe et al. proposed DebUnc to mitigate hallucinations in large language model agent communication using uncertainty estimations.
Rong Zhou et al. developed TTT-UNet for enhancing biomedical image segmentation.
Qian Yang et al. explored calibrating clinicians' trust in AI decision support systems using biomedical literature.