Research Papers on Large Language Model Jailbreaking and Safety

Haibo Jin, Andy Zhou, Joe D Menke, and Haohan Wang published a paper on jailbreaking large language models using cipher characters.
ShengYun Peng, Pin-Yu Chen, Matthew Daniel Hull, and Duen Horng Chau studied risks in fine-tuning large language models.
Hakan Inan et al. developed Llama Guard, an LLM-based input-output safeguard for human-AI conversations.
Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi explored how persuasion can challenge AI safety by humanizing LLMs.
Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang investigated bias and toxicity in zero-shot reasoning.
Zeguan Xiao, Yan Yang, Guanhua Chen, and Yun Chen proposed a method to distract large language models for automatic jailbreak attacks.
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto explored programmatic behavior of LLMs through standard security attacks.
Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, and Yisen Wang developed a method to jailbreak and guard aligned language models using few in-context demonstrations.
Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin improved few-shot jailbreaking to circumvent aligned language models.
Qizhang Li, Xiaochen Yang, Wangmeng Zuo, and Yiwen Guo enhanced jailbreak attacks through adversarial prompt translation.
Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury formalized, analyzed, and detected LLM jailbreaks.
Kai Greshake et al. investigated compromising LLM-integrated applications through indirect prompt injection.
Gelei Deng, Yi Liu, Yuekang Li, and colleagues developed Masterkey, an automated jailbreaking method for large language model chatbots.

Supplementary Research on LLM Toxicity and Safety

Guillermo Villate-Castillo, Javier Del Ser Lorente, and Borja Sanz Urquijo conducted a systematic review of toxicity in large language models.
Lilian Weng explored methods for reducing toxicity in language models.
Yahan Yang, Soham Dan, Dan Roth, and Insup Lee benchmarked LLM guardrails in handling multilingual toxicity.
Johannes Welbl et al. investigated challenges in detoxifying language models.