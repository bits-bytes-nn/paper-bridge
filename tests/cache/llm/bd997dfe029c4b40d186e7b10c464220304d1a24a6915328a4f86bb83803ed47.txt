topic: Large Language Model Evaluation Platforms and Benchmarks

  entities:
    OpenCompass|Platform
    SC-Safety|Benchmark
    Wenxuan Wang|Researcher
    Qinyuan Cheng|Researcher
    Shiqi Chen|Researcher
    FELM|Benchmark
    Mi Zhang|Researcher
    JADE|Platform
    Haoran Li|Researcher
    P-Bench|Benchmark
    Niloofar Mireshghallah|Researcher
    Yanyang Li|Researcher
    CLEVA|Platform
    Allen Nie|Researcher
    MoCa|Benchmark
    Kexin Huang|Researcher
    Flames|Benchmark
    David Esiobu|Researcher
    ROBBIE|Tool
    Shiyao Cui|Researcher
    FFT|Benchmark
    Tinghao Xie|Researcher
    SORRY-Bench|Benchmark
    Hari Shrawgi|Researcher
    Lijun Li|Researcher
    SALAD-Bench|Benchmark
    ACL 2024|Conference

  proposition: Large language models are not fair evaluators.
    entity-attribute relationships:
    Large language models|DESCRIBED_BY|not fair

  proposition: OpenCompass is a universal evaluation platform for foundation models.
    entity-attribute relationships:
    OpenCompass|TYPE|universal evaluation platform
    OpenCompass|EVALUATES|foundation models

  proposition: SC-Safety is a multi-round open-ended question adversarial safety benchmark for large language models in Chinese.
    entity-attribute relationships:
    SC-Safety|TYPE|multi-round open-ended question adversarial safety benchmark
    SC-Safety|LANGUAGE|Chinese
    SC-Safety|EVALUATES|large language models

  proposition: Wenxuan Wang et al. published a paper on multilingual safety of large language models at ACL 2024.
    entity-entity relationships:
    Wenxuan Wang|PUBLISHED_AT|ACL 2024
    Wenxuan Wang|RESEARCHED|multilingual safety of large language models

  proposition: Qinyuan Cheng et al. conducted research on evaluating hallucinations in Chinese large language models.
    entity-attribute relationships:
    Qinyuan Cheng|RESEARCHED|hallucinations in Chinese large language models

  proposition: Shiqi Chen et al. developed FELM, a benchmark for factuality evaluation of large language models.
    entity-attribute relationships:
    FELM|TYPE|benchmark
    FELM|EVALUATES|factuality of large language models
    Shiqi Chen|DEVELOPED|FELM

  proposition: Mi Zhang et al. created JADE, a linguistics-based safety evaluation platform for large language models.
    entity-attribute relationships:
    JADE|TYPE|linguistics-based safety evaluation platform
    JADE|EVALUATES|large language models
    Mi Zhang|CREATED|JADE

  proposition: Haoran Li et al. developed P-Bench, a multi-level privacy evaluation benchmark for language models.
    entity-attribute relationships:
    P-Bench|TYPE|multi-level privacy evaluation benchmark
    P-Bench|EVALUATES|language models
    Haoran Li|DEVELOPED|P-Bench

  proposition: Niloofar Mireshghallah et al. tested privacy implications of language models via contextual integrity theory.
    entity-attribute relationships:
    Niloofar Mireshghallah|TESTED|privacy implications of language models
    Niloofar Mireshghallah|USED|contextual integrity theory

  proposition: Yanyang Li et al. created CLEVA, a Chinese language models evaluation platform.
    entity-attribute relationships:
    CLEVA|TYPE|Chinese language models evaluation platform
    Yanyang Li|CREATED|CLEVA

  proposition: Allen Nie et al. developed MoCa for measuring human-language model alignment on causal and moral judgment tasks.
    entity-attribute relationships:
    MoCa|MEASURES|human-language model alignment
    MoCa|FOCUSES_ON|causal and moral judgment tasks
    Allen Nie|DEVELOPED|MoCa

  proposition: Kexin Huang et al. created Flames, a benchmark for value alignment of Chinese large language models.
    entity-attribute relationships:
    Flames|TYPE|benchmark
    Flames|EVALUATES|value alignment of Chinese large language models
    Kexin Huang|CREATED|Flames

  proposition: David Esiobu et al. developed ROBBIE, a robust bias evaluation tool for large generative language models.
    entity-attribute relationships:
    ROBBIE|TYPE|robust bias evaluation tool
    ROBBIE|EVALUATES|large generative language models
    David Esiobu|DEVELOPED|ROBBIE

  proposition: Shiyao Cui et al. created FFT for evaluating large language models' harmlessness across factuality, fairness, and toxicity.
    entity-attribute relationships:
    FFT|EVALUATES|large language models
    FFT|ASSESSES|harmlessness
    FFT|COVERS|factuality, fairness, and toxicity
    Shiyao Cui|CREATED|FFT

  proposition: Tinghao Xie et al. developed SORRY-Bench to systematically evaluate large language model safety refusal behaviors.
    entity-attribute relationships:
    SORRY-Bench|EVALUATES|large language model safety refusal behaviors
    Tinghao Xie|DEVELOPED|SORRY-Bench

  proposition: Hari Shrawgi et al. investigated uncovering stereotypes in large language models using a task complexity-based approach.
    entity-attribute relationships:
    Hari Shrawgi|INVESTIGATED|stereotypes in large language models
    Hari Shrawgi|USED|task complexity-based approach

  proposition: Lijun Li et al. created SALAD-Bench, a hierarchical and comprehensive safety benchmark for large language models.
    entity-attribute relationships:
    SALAD-Bench|TYPE|hierarchical and comprehensive safety benchmark
    SALAD-Bench|EVALUATES|large language models
    Lijun Li|CREATED|SALAD-Bench