Survey Papers on Multimodal Large Language Models and Benchmarks

Jian Li and Weiheng Lu published a survey on benchmarks of Multimodal Large Language Models in arXiv preprint arXiv:2408.08632 in 2024.
Jiaxing Huang and Jingyi Zhang published a survey on evaluation of Multimodal Large Language Models in arXiv preprint arXiv:2408.15769 in 2024.
Lin Chen and colleagues investigated the current evaluation methods for Large Vision-Language Models in arXiv preprint arXiv:2403.20330 in 2024.

Benchmarking and Evaluation Papers

Andr√©s Villa and colleagues introduced MERLIM, a multi-modal evaluation benchmark for large image-language models in arXiv preprint arXiv:2312.02219 in 2023.
Yuan Liu and colleagues developed MMBench to assess multi-modal model capabilities in 2023.
Bohao Li and colleagues created SEED-Bench for benchmarking Multimodal Large Language Models in the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2024.

Research Papers on Multimodal Models

Hanning Chen and colleagues proposed TaskCLIP for task-oriented object detection in arXiv preprint arXiv:2403.08108 in 2024.
Qirui Jiao and colleagues studied enhancing multimodal large language models with vision detection models in arXiv preprint arXiv:2401.17981 in 2024.
Peng Jin and colleagues developed Chat-UniVi for unified visual representation in large language models in the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2024.
Gongwei Chen and colleagues introduced LION, empowering multimodal large language models with dual-level visual knowledge in the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2024.
Jusung Lee and colleagues worked on visual question answering instruction for domain-specific visual multitasks in arXiv preprint arXiv:2402.08360 in 2024.

Other Notable Papers

Alexander Kirillov and colleagues presented Segment Anything in the 2023 IEEE/CVF International Conference on Computer Vision.
Zongwei Wu and colleagues developed a single-model approach for video object tracking in the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2024.
Niki Maria Foteinopoulou and Ioannis Patras created EmoCLIP for zero-shot video facial expression recognition in the 2024 IEEE International Conference on Automatic Face and Gesture Recognition.
Yao Feng and colleagues developed ChatPose for discussing 3D human pose in the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2024.