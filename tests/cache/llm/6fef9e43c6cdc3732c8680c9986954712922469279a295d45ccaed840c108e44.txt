topic: Large Language Model Security and Attacks

  entities:
    Houda Bouamor|Person
    Juan Pino|Person
    Kalika Bali|Person
    Association for Computational Linguistics|Organization
    Nan Xu|Person
    Gabriel Alon|Person
    Michael Kamfonas|Person
    Yu Fu|Person
    Wei Zhao|Person
    Zhe Li|Person
    Jun Sun|Person
    Yi Liu|Person
    Jason Vega|Person
    Yupei Liu|Person
    Jingwei Yi|Person
    Aleksander Buszydlik|Person
    Aounon Kumar|Person
    Zeyang Sha|Person
    Yang Zhang|Person
    Yujun Zhou|Person
    Zihao Xu|Person
    USENIX Security Symposium|Conference
    Singapore|Location

  proposition: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing was edited by Houda Bouamor, Juan Pino, and Kalika Bali.
    entity-attribute relationships:
    Houda Bouamor|ROLE|Editor
    Juan Pino|ROLE|Editor
    Kalika Bali|ROLE|Editor

    entity-entity relationships:
    Houda Bouamor|EDITED|Conference on Empirical Methods in Natural Language Processing
    Juan Pino|EDITED|Conference on Empirical Methods in Natural Language Processing
    Kalika Bali|EDITED|Conference on Empirical Methods in Natural Language Processing

  proposition: The conference proceedings were published in Singapore in December 2023 by the Association for Computational Linguistics.
    entity-attribute relationships:
    Conference on Empirical Methods in Natural Language Processing|PUBLISHED_IN|Singapore
    Conference on Empirical Methods in Natural Language Processing|PUBLICATION_DATE|December 2023

    entity-entity relationships:
    Conference on Empirical Methods in Natural Language Processing|PUBLISHED_BY|Association for Computational Linguistics

  proposition: Nan Xu and colleagues published a paper on "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking" in 2023.
    entity-attribute relationships:
    Nan Xu|PUBLICATION_YEAR|2023
    Nan Xu|RESEARCH_TOPIC|Jailbreaking Large Language Models

    entity-entity relationships:

  proposition: Gabriel Alon and Michael Kamfonas published a paper on detecting language model attacks using perplexity in 2023.
    entity-attribute relationships:
    Gabriel Alon|PUBLICATION_YEAR|2023
    Michael Kamfonas|PUBLICATION_YEAR|2023
    Gabriel Alon|RESEARCH_TOPIC|Detecting Language Model Attacks
    Michael Kamfonas|RESEARCH_TOPIC|Detecting Language Model Attacks

    entity-entity relationships:

  proposition: Yu Fu and colleagues explored "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack" in 2023.
    entity-attribute relationships:
    Yu Fu|PUBLICATION_YEAR|2023
    Yu Fu|RESEARCH_TOPIC|Safety Alignment in NLP Tasks

    entity-entity relationships:

  proposition: Wei Zhao, Zhe Li, and Jun Sun conducted a causality analysis for evaluating the security of large language models in 2023.
    entity-attribute relationships:
    Wei Zhao|PUBLICATION_YEAR|2023
    Zhe Li|PUBLICATION_YEAR|2023
    Jun Sun|PUBLICATION_YEAR|2023
    Wei Zhao|RESEARCH_TOPIC|Large Language Model Security
    Zhe Li|RESEARCH_TOPIC|Large Language Model Security
    Jun Sun|RESEARCH_TOPIC|Large Language Model Security

    entity-entity relationships:

  proposition: Yi Liu and colleagues studied "Prompt Injection attack against LLM-integrated Applications" in 2023.
    entity-attribute relationships:
    Yi Liu|PUBLICATION_YEAR|2023
    Yi Liu|RESEARCH_TOPIC|Prompt Injection Attack

    entity-entity relationships:

  proposition: Jason Vega and colleagues investigated "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks" in 2023.
    entity-attribute relationships:
    Jason Vega|PUBLICATION_YEAR|2023
    Jason Vega|RESEARCH_TOPIC|Bypassing Safety Training of Open-Source LLMs

    entity-entity relationships:

  proposition: Yupei Liu and colleagues formalized and benchmarked prompt injection attacks and defenses at the USENIX Security Symposium in 2024.
    entity-attribute relationships:
    Yupei Liu|PUBLICATION_YEAR|2024
    Yupei Liu|RESEARCH_TOPIC|Prompt Injection Attacks and Defenses
    Yupei Liu|CONFERENCE|USENIX Security Symposium

    entity-entity relationships:

  proposition: Jingwei Yi and colleagues worked on benchmarking and defending against indirect prompt injection attacks on large language models in 2023.
    entity-attribute relationships:
    Jingwei Yi|PUBLICATION_YEAR|2023
    Jingwei Yi|RESEARCH_TOPIC|Indirect Prompt Injection Attacks

    entity-entity relationships:

  proposition: Aleksander Buszydlik and colleagues conducted red teaming for large language models, focusing on hallucinations in mathematics tasks in 2023.
    entity-attribute relationships:
    Aleksander Buszydlik|PUBLICATION_YEAR|2023
    Aleksander Buszydlik|RESEARCH_TOPIC|Large Language Model Red Teaming

    entity-entity relationships:

  proposition: Aounon Kumar and colleagues researched certifying LLM safety against adversarial prompting in 2023.
    entity-attribute relationships:
    Aounon Kumar|PUBLICATION_YEAR|2023
    Aounon Kumar|RESEARCH_TOPIC|LLM Safety Against Adversarial Prompting

    entity-entity relationships:

  proposition: Zeyang Sha and Yang Zhang studied prompt stealing attacks against large language models in 2024.
    entity-attribute relationships:
    Zeyang Sha|PUBLICATION_YEAR|2024
    Yang Zhang|PUBLICATION_YEAR|2024
    Zeyang Sha|RESEARCH_TOPIC|Prompt Stealing Attacks
    Yang Zhang|RESEARCH_TOPIC|Prompt Stealing Attacks

    entity-entity relationships:

  proposition: Yujun Zhou and colleagues proposed defending jailbreak prompts via an in-context adversarial game in 2024.
    entity-attribute relationships:
    Yujun Zhou|PUBLICATION_YEAR|2024
    Yujun Zhou|RESEARCH_TOPIC|Defending Jailbreak Prompts

    entity-entity relationships:

  proposition: Zihao Xu and colleagues performed a comprehensive study of LLM jailbreak attacks and defense techniques in 2024.
    entity-attribute relationships:
    Zihao Xu|PUBLICATION_YEAR|2024
    Zihao Xu|RESEARCH_TOPIC|LLM Jailbreak Attacks and Defense Techniques

    entity-entity relationships: