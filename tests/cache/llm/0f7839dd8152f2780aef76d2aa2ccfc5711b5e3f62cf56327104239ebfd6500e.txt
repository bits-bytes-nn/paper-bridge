Benchmarking Text-to-Image Model Robustness Propositions

Text-to-image models exhibit varying levels of robustness when exposed to text description perturbations.
CLIPScore is used to measure the performance of text-to-image models before and after input perturbations.
Robustness Score is calculated as 1 minus the absolute difference between original and perturbed CLIPScores, multiplied by 100%.
Robustness scores for evaluated models range from 92.98 to 94.77.
Playground-v2.5 has the lowest robustness score, indicating highest sensitivity to perturbations.
Kolors has the highest robustness score, suggesting greatest stability under perturbations.
CogView-3-Plus and Dall-E-3 show slight increases in CLIPScore after perturbation, demonstrating resilience to input noise.
Perturbation methods are designed to preserve original sentence structure and semantics.
Perturbation types include both programmatic and LLM-based approaches.
Image descriptions are dynamically generated using language models.
Most models demonstrate performance decline under perturbation, with some showing adaptability.
Privacy concerns exist regarding potential exposure of training data in text-to-image models.