Research Papers on Bias and Fairness in Large Language Models

Rameez Qureshi, Naïm Es-Sebbani, Luis Galárraga, Yvette Graham, Miguel Couceiro, and Zied Bouraoui published a research paper on mitigating language model stereotypes using reinforcement learning.
Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, and Jundong Li created a Compositional Evaluation Benchmark for fairness in large language models.
Lucas Dixon, John Li, Jeffrey Scott Sorensen, Nithum Thain, and Lucy Vasserman investigated measuring and mitigating unintended bias in text classification.
Guoliang Dong, Haoyu Wang, Jun Sun, and Xinyu Wang evaluated and proposed methods for mitigating linguistic discrimination in large language models.
Haozhe An, Christabel Acquaye, Colin Wang, Zongxia Li, and Rachel Rudinger studied potential racial, ethnic, and gender discrimination in large language models' hiring decisions.
Y Liu, K Yang, Z Qi, X Liu, Y Yu, and C Zhai developed a statistical framework for measuring social discrimination in large language models.
Shachi H Kumar and colleagues proposed automated methods for detecting gender bias in language models.
Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, and Yang Liu developed a causality-guided framework for steering language models towards unbiased responses.
Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo researched aligning language models to thousands of preferences through system message generalization.
Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu investigated preference leakage as a contamination problem in language model evaluation.
Ali Akbar Septiandri, Marios Constantinides, Mohammad Tahaei, and Daniele Quercia examined the Western, Educated, Industrialized, Rich, and Democratic (WEIRD) aspects of fairness in technology.