Trustworthiness and Safety Concerns in Multimodal and Generative AI Models

He et al. highlight trustworthiness concerns in multimodal generation tasks involving language and other modal outputs.
The safety report for GPT-4o reveals potential safety issues in voice generation tasks.
Chen et al. identify trustworthy problems such as jailbreaks and unexpected variations in interleaved text-and-image generation.
These findings emphasize the urgency of investigating safety challenges in evolving AI models.

Text-to-Video Generation Model Advancements and Safety

Text-to-video generation models have achieved remarkable advancements in recent years.
Sora, developed by OpenAI, can generate intricate scenes and dynamic videos based on user descriptions.
T2VSafetyBench is a comprehensive framework for safety-critical assessments of text-to-video models.
T2VSafetyBench covers 12 essential aspects of video generation safety and includes a malicious prompt dataset.
Pan et al. collect generation prompts to create a dataset of potentially unsafe video content.
Pan et al. develop a Latent Variable Defense approach to prevent generation of harmful videos.
Pang et al. introduce VGMShield, a suite of mitigation strategies for fake video generation.
GPT4Video uses real-toxicity-prompts to train models to avoid producing harmful content.
Dai et al. propose the SafeSora dataset to align text-to-video generation with human values.
New datasets have been constructed for AI-generated video forensics.
Advanced fake video detectors have been proposed to identify and mitigate misinformation impacts.

Audio Generative Models and Safety Challenges

Audio generative models like CoDi and NextGPT can process and generate multiple modalities.
These models synthesize speech for voice-assisted technologies, voice chatbots, and virtual reality experiences.
The primary safety concern is the potential misuse of audio deepfakes for impersonation.
High-fidelity audio generative models can produce speech mimicking an individual's voice and speaking style.
These models pose risks of fraudulent activities, unauthorized system access, and disinformation spread.
Audio generative models might produce incorrect or fabricated information through synthetic speech.
Ethical concerns include unauthorized replication of individuals' voices and potential privacy infringements.
Fairness issues arise from biases in training data that can affect performance across different linguistic backgrounds.
Watermarking and voice safeguarding techniques are being developed to mitigate these risks.