Research Papers on Large Language Model Evaluation and Benchmarking

Giriprasad Sridhara, Ranjani H. G., and Sourav Mazumdar published a research paper in 2023.
Xiaoshuai Song and colleagues developed CS-Bench, a comprehensive benchmark for large language models in computer science mastery.
Jason Holmes and team evaluated large language models on the specialized topic of radiation oncology physics.
Jamil Samaan and colleagues assessed the accuracy of ChatGPT's responses to questions about bariatric surgery.
Aidan Gilson and researchers investigated ChatGPT's performance on the United States Medical Licensing Examination.
Tiffany H. Kung and team examined ChatGPT's performance on USMLE and its potential for AI-assisted medical education.
Zhuo Wang and researchers explored whether LLMs like GPT-4 can outperform traditional AI tools in dementia diagnosis.
Adi Lahat and colleagues evaluated large language models in identifying top research questions in gastroenterology.
Jiaju Lin and team developed AgentSims, an open-source sandbox for large language model evaluation.
Yujia Qin and researchers created ToolLLM to help large language models master over 16,000 real-world APIs.
Minghao Li and colleagues developed API-Bank, a comprehensive benchmark for tool-augmented LLMs.
Liang Xu and team created SuperCLUE, a comprehensive Chinese large language model benchmark.
Haonan Li and researchers developed CMMLU for measuring massive multitask language understanding in Chinese.
Viet Dac Lai and colleagues conducted a comprehensive evaluation of large language models in multilingual learning.
Kaijie Zhu and team proposed Dyval, a graph-informed dynamic evaluation method for large language models.