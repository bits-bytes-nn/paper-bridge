Hallucination Detection in Large Language Models

Previous research on hallucination detection in Large Language Models (LLMs) focuses on two primary aspects: factuality and faithfulness.
Detecting factual errors in LLM responses involves comparing model-generated content against reliable knowledge sources.
Some research addresses hallucination detection in a zero-source setting by estimating the uncertainty of generated factual content.
Methods for detecting unfaithful generation can be categorized into fact-based metrics, classifier-based metrics, QA-based metrics, uncertainty estimation, and prompting-based metrics.

Hallucination Detection Benchmarks

HaluEval provides a comprehensive collection of generated and human-annotated hallucinated samples.
TruthfulQA consists of adversarially curated questions that mimic human falsehoods.
FACTOR introduces a method for automatically creating benchmarks by perturbing factual statements.
REALTIMEQA, FreshQA, and EvolvingQA offer questions to evaluate factual accuracy of LLMs in relation to evolving real-world knowledge.
HalluQA and ChineseFactEval are benchmarks designed to measure hallucination in Chinese large language models.
SelfCheckGPT-Wikibio provides a dataset for detecting sentence-level hallucinations.
FELM assesses factual accuracy across various domains.
PHD offers a passage-level hallucination detection benchmark created using ChatGPT and human annotations.

Hallucination Mitigation Strategies

Enhancing the factual accuracy of the pre-training corpus can improve the model's parametric knowledge.
Refining training data quality during supervised fine-tuning can help mitigate hallucinations.
Alignment processes can help language models recognize their knowledge boundaries.
Inference-time interventions have become a focus for reducing hallucinations.

Prompting Techniques for Hallucination Mitigation

Prompting plays a crucial role in providing context and controlling model outputs.
Chain-of-thought and least-to-most prompting help reveal faulty logic or assumptions.
Self-consistency, SCOTT, and self-ask methods involve multiple prompts to identify potential hallucinations.

Retrieval-Augmented Generation (RAG)

RAG methods retrieve information from reliable knowledge sources to reduce hallucinations.
These methods can use external knowledge bases, structured databases, websites like Wikipedia, search engine APIs, or various external tools.

Model Editing Approaches

Model editing allows modification of LLM behavior in a data- and computation-efficient manner.
Methods include incorporating auxiliary sub-networks or directly modifying model parameters.
ROME modifies feedforward weights to update specific factual associations.
Inference-time intervention (ITI) identifies attention heads associated with truthfulness and shifts activations to elicit truthful answers.