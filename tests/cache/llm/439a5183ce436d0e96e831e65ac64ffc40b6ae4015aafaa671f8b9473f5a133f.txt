Comprehensive Analysis of Trustworthy Generative Foundation Models (GenFMs)

The authors conducted a multidisciplinary collaboration to establish guidelines for trustworthy generative models.
The collaboration involved experts from diverse fields including NLP, Computer Vision, Human-Computer Interaction, Computer Security, Medicine, Computational Social Science, Robotics, Data Mining, Law, and AI for Science.
The research developed comprehensive guidelines systematically structured around legal compliance, ethical and social responsibilities, risk management, user-centered design principles, and adaptability and sustainability.
The authors introduced TrustGen, a holistic and dynamic benchmark for assessing the trustworthiness of generative models.
TrustGen evaluates models across multiple dimensions including truthfulness, safety, fairness, privacy, robustness, machine ethics, and advanced AI risks.
The benchmark covers text-to-image, large language, and vision-language models.
TrustGen's modular components allow dynamic assessment of evolving model capabilities.
The research released an open-source toolkit called TrustEval to facilitate dynamic evaluation of generative model trustworthiness.
The study provides an in-depth analysis of the current trustworthiness landscape of generative foundation models.
The research highlights the complex trade-offs between maximizing utility performance and trustworthiness impact.
The text-to-image model performance analysis reveals varying trustworthiness scores across different models.
CogView-3-Plus achieved the highest average trustworthiness score of 74.96 among the evaluated models.
The research aims to advance the development of trustworthy generative AI by identifying challenges and innovative solutions.
The paper is organized to provide a comprehensive exploration of generative foundation models' trustworthiness, including corporate approaches, evaluation methods, guidelines, benchmark design, and detailed model assessments.