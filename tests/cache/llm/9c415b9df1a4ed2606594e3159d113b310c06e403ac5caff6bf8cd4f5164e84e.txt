Hallucination in Vision-Language Models (VLMs)

Hallucination in VLMs refers to generating content that is factually inconsistent with visual context or common sense.
Hallucination can occur in tasks like image captioning, visual question answering, and visual-language navigation.
Hallucinations arise from misalignment between visual input and generated language.
Misalignment can stem from biases in the language model or limitations in visual content comprehension.

Hallucination Detection Benchmarks

HallusionBench evaluates VLMs' ability to handle complex image-context reasoning.
HallusionBench focuses on two failure modes: language hallucination and visual illusion.
Bingo benchmark evaluates hallucinations caused by biased training data and text prompts.
AutoHallusion develops an automated pipeline to generate diverse hallucination cases.
VHTest generates visual hallucination testing cases using LLM and T2I models.

Object Hallucination Characteristics

Object hallucination involves generating nonexistent objects.
Object hallucination can include attributing incorrect properties to visible objects.
Object hallucination may misrepresent relationships between objects in a scene.
Metrics like CHAIR and POPE assess caption relevance and hallucination levels.

Hallucination Mitigation Strategies

Recent approaches optimize training objectives to reduce hallucinations.
Approaches incorporate grounding constraints during inference stage.
Fine-tuning smaller multimodal models has proven less effective for VLMs.
LRV-Instruction creates balanced positive and negative instructions for VLM fine-tuning.

VLM Truthfulness Performance

GPT-4o has the highest overall accuracy of 60.70% on HallusionBench.
Claude-3.5-Sonnet achieves 62.19% overall accuracy.
Models perform differently on easy and hard questions.
Easy questions align with common sense knowledge.
Hard questions require answers based on specific context and prompts.