Research Papers on Large Language Model Safety and Alignment Challenges

Xiangyu Qi et al. propose that safety alignment in large language models should be more comprehensive than surface-level token modifications.
Xianjun Yang et al. demonstrate the vulnerability of safely-aligned language models through shadow alignment techniques.
Jiaming Ji et al. introduce PKU-SafeRLHF, a safety alignment preference dataset for Llama family models.
Boyi Wei et al. assess the brittleness of safety alignment by examining pruning and low-rank modifications.
Jianhui Chen et al. investigate the identification of safety neurons within large language models.
Haibo Jin et al. conduct a comprehensive survey on jailbreaking techniques for large language and vision-language models.
Yue Liu et al. propose FlipAttack, a method for jailbreaking large language models.
Piyush Jha et al. develop LLMStinger, a jailbreaking approach using reinforcement learning fine-tuned language models.
Researchers are exploring multiple dimensions of large language model safety, including alignment vulnerabilities, jailbreak techniques, and potential defense mechanisms.
The research highlights significant challenges in ensuring the reliable and safe operation of advanced language models.