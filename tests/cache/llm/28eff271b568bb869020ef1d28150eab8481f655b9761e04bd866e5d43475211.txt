Research Papers on Adversarial Attacks, Image Generation, and Vision-Language Models

Raphaël Millière studied adversarial attacks on image generation using made-up words.
Haomin Zhuang, Yihua Zhang, and Sijia Liu conducted a pilot study of query-free adversarial attack against Stable Diffusion.
Peter Sushko and colleagues created REALEDIT, a large-scale empirical dataset of Reddit image transformations.
Boyang Zheng, Chumeng Liang, Xiaoyu Wu, and Yan Liu investigated understanding and improving adversarial attacks on latent diffusion models.
Abenezer Golda and colleagues published a comprehensive survey on privacy and security concerns in generative AI.
Guangcong Zheng and colleagues developed LayoutDiffusion, a controllable diffusion model for layout-to-image generation.
Guillaume Couairon and colleagues proposed DiffEdit, a diffusion-based semantic image editing method with mask guidance.
Kihyuk Sohn and colleagues introduced StyleDrop, a text-to-image generation method that works in any style.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli developed an image quality assessment method based on structural similarity.
Richard Zhang and colleagues explored the effectiveness of deep features as a perceptual metric.
Mathilde Caron and colleagues studied emerging properties in self-supervised vision transformers.
Zixian Ma and colleagues investigated compositional reasoning in vision-language foundation models through the CREPE benchmark.
Cheng-Yu Hsieh and colleagues proposed SugarCREPE to fix hackable benchmarks for vision-language compositionality.
Sri Harsha Dumpala and colleagues created the SUGAR-CREPE++ Dataset to examine vision-language model sensitivity.
Amita Kamath, Jack Hessel, and Kai-Wei Chang identified text encoders as a bottleneck for compositionality in contrastive vision-language models.
Zhiqiu Lin and colleagues revisited the role of language priors in vision-language models.