Corporate Approaches to Generative Foundation Model Trustworthiness

"Advanced" means advanced AI risk.
The text presents a performance comparison of vision-language models across multiple trustworthiness dimensions.
Claude-3.5-Sonnet has the highest average trustworthiness score of 75.46.
Llama-3.2-90B-V has the lowest average trustworthiness score of 47.33.
The performance metrics include truthfulness, safety, fairness, privacy, robustness, and ethics.

Corporate Strategies for Generative Foundation Model Trustworthiness:

Microsoft approaches to trustworthiness include:
Microsoft focuses on unbiased and equitable AI.
Microsoft aims to develop AI for social good.
Microsoft seeks to empower applications and facilities.
Microsoft has established principles and commitments for AI development.

Meta approaches to trustworthiness include:
Meta developed Llama Guard.
Meta created Prompt Guard.
Meta implements responsible model deployment.
Meta conducts CyberSecEval.
Meta performs pre-deployment safety stress tests.

OpenAI approaches to trustworthiness include:
OpenAI established a Red Teaming Network.
OpenAI develops model system cards.
OpenAI focuses on model alignment.
OpenAI creates secure infrastructure for advanced AI.
OpenAI works on identifying AI-generated material.

Amazon approaches to trustworthiness include:
Amazon Bedrock Guardrails enhance model safety.
Amazon conducts model evaluation and selection.
Amazon implements watermarking techniques.
Amazon launched the Trusted AI Challenge.

IBM approaches to trustworthiness include:
IBM developed a framework for securing generative AI.
IBM uses LLMs for threat management.
IBM creates generative models for trust.
IBM provides Trustworthy AI toolkits.
IBM developed the AI Risk Atlas.
IBM created Granite Guardian.

Anthropic approaches to trustworthiness include:
Anthropic runs a Safety Bug Bounty Program.
Anthropic conducts extensive research on interpretability, alignment, and societal impacts.

Google approaches to trustworthiness include:
Google implements responsible AI practices.
Google uses a Secure AI Framework (SAIF).
Google developed ShieldGemma.
Google allows configuring safety settings.