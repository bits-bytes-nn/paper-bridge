Research Papers on Question Answering and Code Generation Benchmarks
Pranav Rajpurkar, Robin Jia, and Percy Liang authored a paper on unanswerable questions for SQuAD.
The paper was published as an arXiv preprint in 2018.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning created HotpotQA.
HotpotQA is a dataset for diverse, explainable multi-hop question answering.
The HotpotQA paper was published as an arXiv preprint in 2018.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer developed TriviaQA.
TriviaQA is a large scale distantly supervised challenge dataset for reading comprehension.
The TriviaQA paper was published as an arXiv preprint in 2017.
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu created PubMedQA.
PubMedQA is a dataset for biomedical research question answering.
The PubMedQA paper was published as an arXiv preprint in 2019.
Multiple research papers explored Large Language Models (LLMs) in code generation and evaluation.
Researchers published studies on ChatGPT's code generation capabilities and limitations in 2023.
Several benchmarks were developed to evaluate code generation performance, including CodeApex and NaturalCodeBench.
Researchers also created multilingual code generation benchmarks like HumanEval-XL.
Additional research explored LLMs in domains like computational social science, legal reasoning, and domain knowledge evaluation.