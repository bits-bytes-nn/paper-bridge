Research References on Language Model Calibration and Uncertainty

Duzhen Zhang and colleagues published a paper on multimodal large language models in 2024.
Baolin Peng and coauthors explored instruction tuning with GPT-4 in a 2023 arXiv preprint.
Kyle Mahowald et al. investigated dissociating language and thought in large language models in 2024.
Robert W Lurz authored a book on the philosophy of animal minds published by Cambridge University Press in 2009.
Lukas Berglund and colleagues examined situational awareness in language models in a 2023 arXiv preprint.
Shrey Desai and Greg Durrett studied calibration of pre-trained transformers at the 2020 EMNLP conference.
Chenglei Si and coauthors explored strategies to make GPT-3 more reliable at the 2023 International Conference on Learning Representations.
Elias Stengel-Eskin and Benjamin Van Durme published research on calibrated interpretation in semantic parsing in 2023.
Kaitlyn Zhou and colleagues investigated how uncertainty and overconfidence affect language models at the 2023 EMNLP conference.
Katherine Tian et al. proposed strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback in 2023.
Kaitlyn Zhou and coauthors examined the impact of language models' reluctance to express uncertainty in a 2024 ACL publication.
Saurav Kadavath and colleagues explored language models' self-awareness of their own knowledge in 2022.
Sabrina J Mielke et al. researched reducing conversational agents' overconfidence through linguistic calibration in 2022.
Neil Band and coauthors studied linguistic calibration of longform generations at the 2024 International Conference on Machine Learning.
Elias Stengel-Eskin, Peter Hase, and Mohit Bansal developed LACIE for calibration in large language models at the 2023 Neural Information Processing Systems conference.