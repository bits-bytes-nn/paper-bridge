Large Language Models: Techniques, Applications, and Trustworthiness Challenges

Large language models leverage deep learning transformer architectures to process language.
Large language models can perform tasks like translation, summarization, and creating conversational agents.
Large language models are prevalent across domains including medical, educational, financial, psychological, software engineering, and creative fields.
Organizations adopting large language models face concerns about ethical use, reliability, and trustworthiness.
A recent study has identified 10 potential security and privacy issues in large language models.
Large language models are vulnerable to membership inference attacks and backdoor attacks.
Large language models can produce hallucinations, generating plausible but incorrect information.
Large language models have introduced potential biases, including gender and racial discrimination.
Extensive datasets used in large language models primarily sourced from the internet raise privacy concerns.
Evaluating large language models requires understanding their trustworthiness from six perspectives: truthfulness, safety, fairness, robustness, privacy, and machine ethics.

Hallucination in Large Language Models

Hallucination in large language models refers to generating content that appears plausible but is inconsistent with facts or user requirements.
Hallucination is a common issue across various large language models.
Researchers are increasing efforts to understand and mitigate hallucinations.
Hallucination detection focuses on two primary aspects: factuality and faithfulness.
Hallucination detection methods include comparing model-generated content against reliable knowledge sources.
Alternative hallucination detection approaches estimate the uncertainty of generated factual content in a zero-source setting.
Hallucination can occur in various natural language generation tasks, including machine translation and abstractive summarization.