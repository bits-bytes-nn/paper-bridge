Research Papers on Language Model Safety and Alignment

Suriya Gunasekar et al. published a research paper titled "Textbooks are all you need" in 2023.
Yuxiang Wei et al. published a research paper titled "Magicoder: Source code is all you need" in 2023.
Xiang Yue et al. published a research paper titled "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning" at the Twelfth International Conference on Learning Representations in 2024.
Longhui Yu et al. published a research paper titled "Metamath: Bootstrap your own mathematical questions for large language models" in 2023.
Fangyu Lei et al. published a research paper titled "S3eval: A synthetic, scalable, systematic evaluation suite for large language models" in 2023.
Zhehao Zhang et al. published a research paper titled "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph" at the Thirty-eighth Annual Conference on Neural Information Processing Systems in 2024.
Jing Xu et al. published a research paper on "Bot-adversarial dialogue for safe conversational agents" in the 2021 Conference of the North American Chapter of the Association for Computational Linguistics.
Josef Dai et al. published a research paper titled "Safe RLHF: Safe Reinforcement Learning from Human Feedback" at the Twelfth International Conference on Learning Representations in 2024.
Taiwei Shi et al. published a research paper titled "Safer-Instruct: Aligning Language Models with Automated Preference Data" at the 2024 Conference of the North American Chapter of the Association for Computational Linguistics.
Jiaming Ji et al. published a research paper titled "Aligner: Efficient alignment by learning to correct" at the Thirty-eighth Annual Conference on Neural Information Processing Systems in 2024.
James O'Neill et al. published a research paper titled "GuardFormer: Guardrail Instruction Pretraining for Efficient SafeGuarding" at the Neurips Safe Generative AI Workshop  2024.
Hasan Abed Al Kader Hammoud et al. published a research paper titled "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch" in 2024.
Makesh Narsimhan Sreedhar et al. published a research paper titled "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues" in 2024.
Fei Wang et al. published a research paper titled "Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models" at the 2024 Conference on Empirical Methods in Natural Language Processing.
Yue Yu et al. published a research paper titled "Self-Generated Critiques Boost Reward Modeling for Language Models" in 2024.