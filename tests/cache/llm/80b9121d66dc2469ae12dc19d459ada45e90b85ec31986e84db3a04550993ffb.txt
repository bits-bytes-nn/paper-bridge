Robustness Assessment of Vision-Language Models (VLMs)

Robustness Evaluation Methodology
MLLM-as-a-Judge is used to calculate the robustness score for image captioning
Robustness score is determined by comparing descriptions under perturbed and unperturbed conditions
An instance is considered robust if the MLLM rates the descriptions as a "Tie"
Final robustness score is the proportion of "Tie" rated instances

Perturbation Domains
Three distinct perturbation domains are designed: image, text, and image-text
Image domain includes 23 perturbation types
Image perturbations include 19 existing corruptions and 4 new transformations
New image perturbations are quarter turn right, quarter turn left, upside down, and horizontal flip
Text domain perturbations exclude multilingual blend and distractive text to maintain semantic integrity
Image-text domain perturbations combine image and text domain perturbations

Dynamic Dataset Creation
VQA and image caption datasets are collected to build a data pool
Data pool will be regularly updated with benchmark datasets
400 VQA questions and 400 image caption questions were randomly selected
Perturbations were randomly applied across image, text, or image-text domains

Robustness Score Observations
Models show varying robustness levels across different tasks
VQA robustness scores range from 82.25% to 97.50%
Image captioning robustness scores range from 9.44% to 51.90%
Qwen-2-VL-72B achieves highest VQA robustness score of 97.50%
GPT-4o-mini leads image captioning robustness with 51.90%
Models consistently demonstrate higher robustness in VQA compared to image captioning
Perturbations have more substantial impact on open-ended generation tasks

Top Performing Models
GPT-4o-mini has highest average robustness score of 69.70%
Claude-3.5-Sonnet has highest VQA robustness score of 96.00%
Llama models show lowest robustness in image captioning