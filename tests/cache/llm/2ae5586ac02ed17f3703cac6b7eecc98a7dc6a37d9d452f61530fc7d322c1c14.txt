Research Papers on AI Safety, Interpretability, and Fairness

Optimal policies tend to seek power.
Alex Turner and Prasad Tadepalli found that parametrically retargetable decision-makers tend to seek power.
Victoria Krakovna and Janos Kramar observed that power-seeking can be probable and predictive for trained agents.
Richard Ngo, Lawrence Chan, and SÃ¶ren Mindermann studied the alignment problem from a deep learning perspective.
Toby Shevlane and colleagues investigated model evaluation for extreme risks.
Chandan Singh and researchers rethought interpretability in the era of large language models.
Neel Nanda and team developed progress measures for grokking via mechanistic interpretability.
Arthur Conmy and colleagues worked towards automated circuit discovery for mechanistic interpretability.
Roland S Zimmermann, Thomas Klein, and Wieland Brendel found that scale alone does not improve mechanistic interpretability in vision models.
Daking Rai and researchers conducted a practical review of mechanistic interpretability for transformer-based language models.
Leonard Bereska and Efstratios Gavves reviewed mechanistic interpretability for AI safety.
Zhibo Chu, Zichong Wang, and Wenbin Zhang conducted a taxonomic survey of fairness in large language models.
Michelle Seng Ah Lee explored context-conscious fairness in machine learning decision-making.
Hilde Weerts and colleagues developed Fairlearn for assessing and improving fairness of AI systems.
Tyna Eloundou and team investigated first-person fairness in chatbots.
Researchers studied various approaches to fairness in AI systems.
Xinyue Shen and colleagues characterized and evaluated in-the-wild jailbreak prompts on large language models.
Rusheb Shah and researchers explored scalable and transferable black-box jailbreaks for language models via persona modulation.
Xuan Li and team developed DeepInception to hypnotize large language models to be jailbreakers.
Anselm Paulus and colleagues created AdvPrompter for fast adaptive adversarial prompting for large language models.