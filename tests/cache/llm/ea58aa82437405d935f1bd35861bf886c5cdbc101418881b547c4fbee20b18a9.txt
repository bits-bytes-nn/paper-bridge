Research Papers on Large Language Model Robustness and Adversarial Challenges

Javad Rafiei Asl et al. proposed a semantic, syntactic, and context-aware natural language adversarial example generator in IEEE Transactions on Dependable and Secure Computing in 2024.
Terry Yue Zhuo et al. conducted an empirical study on the robustness of prompt-based semantic parsing with large pre-trained language models like Codex in 2023.
Freda Shi et al. demonstrated that large language models can be easily distracted by irrelevant context in the 2023 International Conference on Machine Learning.
Seyed Mahed Mousavi et al. investigated the robustness of large language models for spoken dialogues in 2024.
Yue Huang et al. explored the contextual distraction curse in large language models in an arXiv preprint in 2025.
Jindong Wang et al. analyzed the robustness of ChatGPT from adversarial and out-of-distribution perspectives in 2023.
Ridong Han et al. examined ChatGPT's performance, evaluation criteria, robustness, and errors in information extraction in 2023.
Xinyue Shen et al. measured and characterized the reliability of ChatGPT in 2023.
Sungwon Park et al. proposed an adversarial style augmentation method for robust fake news detection using large language models in 2024.
Ming Jiang et al. developed a method for enhancing robustness in large language models by mitigating the impact of irrelevant information in 2024.
Zi Xiong et al. suggested enhancing language model robustness against variation attacks through graph integration in 2024.