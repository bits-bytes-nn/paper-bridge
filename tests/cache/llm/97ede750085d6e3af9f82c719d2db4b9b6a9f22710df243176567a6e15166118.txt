Research Papers on AI Ethics, Privacy, and Multimodal Models
Bernardo Breve, Gaetano Cimino, and Vincenzo Deufemia published a paper on identifying security and privacy violation rules in trigger-action IoT platforms using NLP models.
Sunder Ali Khowaja et al. conducted a review of ChatGPT requiring evaluation across sustainability, privacy, digital divide, and ethics dimensions.
Cunxiang Wang et al. evaluated open question answering evaluation methods.
Yixin Wu et al. quantified privacy risks of prompts in visual prompt learning.
Charly Ashcroft and Kahari Whitaker evaluated domain-specific prompt engineering attacks on large language models.
Yuxin Wen et al. explored privacy backdoors by enhancing membership inference through poisoning pre-trained models.
Guy Amit et al. developed a systematic overview of reducing vulnerability of fine-tuned language models to membership inference attacks.
Mukai Li et al. conducted red teaming of visual language models.
Tribhuvanesh Orekondy et al. worked on understanding and predicting privacy risks in images.
Danna Gurari et al. created VizWiz-Priv dataset for recognizing private visual information in images taken by blind people.
Alexis Roger researched ethical multimodal systems and training large multimodal language models with ethical values.
Haoqin Tu et al. demonstrated that multi-modal training enhances large language models in truthfulness and ethics.
World Health Organization released AI ethics and governance guidance for large multi-modal models.
Hongzhan Lin et al. proposed an approach towards explainable harmful meme detection through multimodal debate between large language models.
Yu Shu et al. developed Large Language and Speech Model (LLASM).
Tsai-Shien Chen et al. created Panda-70m, a dataset of 70m videos captioned with multiple cross-modality teachers.