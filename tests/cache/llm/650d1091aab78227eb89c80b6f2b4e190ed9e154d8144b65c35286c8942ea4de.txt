Multimodal AI Research Papers and Preprints (2022-2024)

OpenAI published GPT-4O Mini: Advancing Cost-Efficient Intelligence in 2024.
Dongping Chen et al. published a paper on Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment in arXiv preprint arXiv:2411.17188 in 2024.
Tanmay Gupta and Aniruddha Kembhavi presented Visual Programming: Compositional Visual Reasoning Without Training at the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2023.
Dídac Surís, Sachit Menon, and Carl Vondrick introduced ViperGPT: Visual Inference via Python Execution for Reasoning at the IEEE/CVF International Conference on Computer Vision in 2023.
Zixian Ma et al. created M&M's: A Benchmark to Evaluate Tool-Use for Multi-Step Multi-Modal Tasks for the Synthetic Data for Computer Vision Workshop at CVPR 2024.
Yushi Hu et al. proposed Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models in arXiv preprint arXiv:2406.09403 in 2024.
Shilong Liu et al. developed LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents in arXiv preprint arXiv:2311.05437 in 2023.
Chunting Zhou et al. published Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model in arXiv preprint arXiv:2408.11039 in 2024.
Jinheng Xie et al. introduced Show-O: One Single Transformer to Unify Multimodal Understanding and Generation in arXiv preprint arXiv:2408.12528 in 2024.
Chameleon Team published Chameleon: Mixed-Modal Early-Fusion Foundation Models in arXiv preprint arXiv:2405.09818 in 2024.
Ethan Chern et al. developed ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation in arXiv preprint arXiv:2407.06135 in 2024.
Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov researched Generating Images with Multimodal Language Models in Advances in Neural Information Processing Systems in 2024.
Zhanyu Wang et al. created GPT4Video: A Unified Multimodal Large Language Model for Instruction-Followed Understanding and Safety-Aware Generation in arXiv preprint arXiv:2311.16511 in 2023.
Yingqing He et al. published a survey on LLMs Meet Multimodal Generation and Editing in arXiv preprint arXiv:2405.19334 in 2024.
Uriel Singer et al. developed Make-a-Video: Text-to-Video Generation Without Text-Video Data in arXiv preprint arXiv:2209.14792 in 2022.
Joseph Cho et al. published a complete survey on Sora as an AGI World Model in arXiv preprint arXiv:2403.05131 in 2024.
OpenAI published Video Generation Models as World Simulators.