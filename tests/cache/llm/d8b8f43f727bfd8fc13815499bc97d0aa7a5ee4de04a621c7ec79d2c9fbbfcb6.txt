Hallucination Evaluation Results Analysis

Questions are constructed based on manipulated objects within a scene.
Questions can be existence questions or spatial relationship questions.
The question construction pipeline is based on AutoHallusion research.
An LLM-powered contextual variator paraphrases questions to increase diversity.

GPT-4o and Claude-3.5-Sonnet are top performers in hallucination evaluation.
Top-performing models show consistent performance across benchmarks.
There is a performance gap of up to 17.91% between top and lower-performing models.

Claude-3.5-Sonnet excels in counterfactual visual question answering.
On HallusionBench, easy questions align with common sense knowledge.
Hard questions are counterfactual and require answers based on provided context.
Claude-3.5-Sonnet outperforms GPT-4o by 6.31% on hard questions.
Models generally show lower accuracy on complex scenarios.

GPT-4o is more effective at handling existence questions.
Claude-3.5-Sonnet leads in addressing spatial relationship questions.
Spatial relationship questions are more challenging than existence questions.

Vision-Language Models (VLMs) introduce new vulnerabilities for potential attacks.
The vision space's continuity makes it easier to generate harmful images that evade detection.
Semantic inconsistency between vision and text modalities can be exploited for harmful behaviors.

Jailbreaking VLMs poses a significant safety risk.
VLMs can be more easily jailbroken compared to Large Language Models (LLMs).
Attackers can format harmful queries into images to bypass safety filters.

Jailbreak attacks on VLMs involve submitting modified prompts with crafted visual inputs.
Jailbreak attack types include prompt-to-image attacks and optimization-based methods.
Prompt-to-image attacks transfer harmful information from text to image modalities.