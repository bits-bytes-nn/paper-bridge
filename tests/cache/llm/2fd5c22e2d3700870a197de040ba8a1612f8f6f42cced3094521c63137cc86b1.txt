topic: Large Language Model Uncertainty and Self-Awareness Research

  entities:
    Yuxin Liang|Researcher
    Zhuoyang Song|Researcher
    Hao Wang|Researcher
    Jiaxing Zhang|Researcher
    Yuqing Yang|Researcher
    Ethan Chern|Researcher
    Xipeng Qiu|Researcher
    Graham Neubig|Researcher
    Pengfei Liu|Researcher
    Hanning Zhang|Researcher
    Stephanie Lin|Researcher
    Jacob Hilton|Researcher
    Owain Evans|Researcher
    Miao Xiong|Researcher
    Aviv Slobodkin|Researcher
    Yukun Zhao|Researcher
    Chao Chen|Researcher
    3rd Workshop on Knowledge Augmented Methods for NLP|Conference
    Thirty-eighth Annual Conference on Neural Information Processing Systems|Conference
    Twelfth International Conference on Learning Representations|Conference
    Conference on Empirical Methods in Natural Language Processing|Conference
    Conference of the North American Chapter of the Association for Computational Linguistics|Conference
    Bangkok|Location
    Thailand|Location
    Singapore|Location
    Mexico City|Location

  proposition: Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxing Zhang published a paper on learning self-awareness in Large Language Models for hallucination mitigation.
    entity-attribute relationships:
    Yuxin Liang|RESEARCHES|Large Language Model self-awareness
    Zhuoyang Song|RESEARCHES|Large Language Model self-awareness
    Hao Wang|RESEARCHES|Large Language Model self-awareness
    Jiaxing Zhang|RESEARCHES|Large Language Model self-awareness
    
    entity-entity relationships:
    Yuxin Liang|CO_AUTHORED|Zhuoyang Song
    Yuxin Liang|CO_AUTHORED|Hao Wang
    Yuxin Liang|CO_AUTHORED|Jiaxing Zhang

  proposition: The paper was presented at the 3rd Workshop on Knowledge Augmented Methods for NLP in Bangkok, Thailand in August 2024.
    entity-attribute relationships:
    3rd Workshop on Knowledge Augmented Methods for NLP|LOCATION|Bangkok
    3rd Workshop on Knowledge Augmented Methods for NLP|COUNTRY|Thailand
    3rd Workshop on Knowledge Augmented Methods for NLP|DATE|August 2024

    entity-entity relationships:
    3rd Workshop on Knowledge Augmented Methods for NLP|HOSTED_IN|Bangkok

  proposition: Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu researched alignment for honesty in Large Language Models.
    entity-attribute relationships:
    Yuqing Yang|RESEARCHES|Large Language Model honesty alignment
    Ethan Chern|RESEARCHES|Large Language Model honesty alignment
    Xipeng Qiu|RESEARCHES|Large Language Model honesty alignment
    Graham Neubig|RESEARCHES|Large Language Model honesty alignment
    Pengfei Liu|RESEARCHES|Large Language Model honesty alignment

    entity-entity relationships:
    Yuqing Yang|CO_AUTHORED|Ethan Chern
    Yuqing Yang|CO_AUTHORED|Xipeng Qiu
    Yuqing Yang|CO_AUTHORED|Graham Neubig
    Yuqing Yang|CO_AUTHORED|Pengfei Liu

  proposition: The research was presented at the Thirty-eighth Annual Conference on Neural Information Processing Systems in 2024.
    entity-attribute relationships:
    Thirty-eighth Annual Conference on Neural Information Processing Systems|DATE|2024

  proposition: Hanning Zhang and colleagues proposed R-Tuning, a method for instructing Large Language Models to express uncertainty by saying "I Don't Know".
    entity-attribute relationships:
    Hanning Zhang|PROPOSED|R-Tuning method
    R-Tuning|PURPOSE|Instructing Large Language Models to express uncertainty

  proposition: Stephanie Lin, Jacob Hilton, and Owain Evans studied teaching models to express their uncertainty in words.
    entity-attribute relationships:
    Stephanie Lin|RESEARCHES|Model uncertainty expression
    Jacob Hilton|RESEARCHES|Model uncertainty expression
    Owain Evans|RESEARCHES|Model uncertainty expression

    entity-entity relationships:
    Stephanie Lin|CO_AUTHORED|Jacob Hilton
    Stephanie Lin|CO_AUTHORED|Owain Evans

  proposition: Miao Xiong and colleagues empirically evaluated confidence elicitation in Large Language Models.
    entity-attribute relationships:
    Miao Xiong|RESEARCHES|Large Language Model confidence elicitation

  proposition: The research was presented at the Twelfth International Conference on Learning Representations in 2024.
    entity-attribute relationships:
    Twelfth International Conference on Learning Representations|DATE|2024

  proposition: Aviv Slobodkin and researchers investigated hallucination and over-confidence in Large Language Models' hidden states.
    entity-attribute relationships:
    Aviv Slobodkin|RESEARCHES|Large Language Model hallucination
    Aviv Slobodkin|RESEARCHES|Large Language Model over-confidence

  proposition: The study was presented at the 2023 Conference on Empirical Methods in Natural Language Processing in Singapore.
    entity-attribute relationships:
    Conference on Empirical Methods in Natural Language Processing|DATE|2023
    Conference on Empirical Methods in Natural Language Processing|LOCATION|Singapore

  proposition: Yukun Zhao and colleagues developed a self-detection method for identifying what Large Language Models do not know.
    entity-attribute relationships:
    Yukun Zhao|DEVELOPED|Self-detection method for Large Language Models

  proposition: The research was presented at the 2024 Conference of the North American Chapter of the Association for Computational Linguistics in Mexico City.
    entity-attribute relationships:
    Conference of the North American Chapter of the Association for Computational Linguistics|DATE|2024
    Conference of the North American Chapter of the Association for Computational Linguistics|LOCATION|Mexico City

  proposition: Chao Chen and researchers explored using internal states of Large Language Models for hallucination detection.
    entity-attribute relationships:
    Chao Chen|RESEARCHES|Large Language Model hallucination detection

  proposition: The study was presented at the Twelfth International Conference on Learning Representations in 2024.
    entity-attribute relationships:
    Twelfth International Conference on Learning Representations|DATE|2024