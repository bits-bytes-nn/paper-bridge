Methodology for Generating Stereotype and Preference Evaluation Dataset

Item selection involves computing average scores across two dimensions.
Only items with an average score exceeding 8 are included in the evaluation.
Selected items represent sufficiently unfair and aggressive stories.

Dataset Generation Process
LLMs (e.g., GPT-4o) break down each story into two to five scenes.
Key scene elements are replaced with placeholders (e.g., "fig1", "fig2").
Text narrative focuses on event flow without specific scene details.
Image descriptions are generated by comparing narrative and original story.
Prompt explicitly includes requirements for consistency and avoiding visual information leakage.
Text-to-image model (Dalle-3) generates corresponding images.
Images are compressed into a composite image.
Contextual variator paraphrases sentences and adjusts lengths.
Human reviewers verify data instance quality.

Performance Analysis of Vision-Language Models (VLMs)

Large performance variations exist across models.
Gemini-1.5-Pro achieves highest accuracy at 91.71%.
Llama-3.2-90B-V scores lowest at 3.08%.
Gemini and Claude series consistently show high accuracy.
Llama-3.2-90B-V struggles likely due to less focused training.

Identification and Stance Accuracy
Identification accuracy often aligns with stance accuracy.
Gemini-1.5-Pro's identification percentage closely matches overall true percentage.
Claude-3-Haiku shows a drop from 44.93% identification to 42.29% overall.
Llama-3.2B-11B-V experiences a 3.52% decline between identification and overall performance.

Preference Evaluation Methodology
LLM-as-a-Judge approach assesses model neutrality and fairness.
GPT-4o generates preference pairs across various domains.
Test case builder expands preference pairs into optional questions.
Contextual variator introduces diversity by replacing specific elements.
Image descriptions generated for placeholders.
Dalle-3 creates images from descriptions.
Composite images created from two images.
Human reviewers verify data instance quality.

Evaluation Results Visualization
Figure 40 compares correct identification percentages across different VLMs.
Models range from 0% to 80% in correct identification.
GPT-4o shows highest performance at 80% correct identification.