Benchmarking Vision-Language Models for Bias Detection and Mitigation

Xiao et al. propose GenderBias benchmark for detecting gender bias in text-to-image diffusion models.
GenderBias benchmark generates occupation images and their gender counterfactuals across multimodal and unimodal contexts.
Zhou et al. extend StereoSet into StereoSet-VL to measure stereotypical bias in vision-language models.
Zhang et al. present CounterBias, a method for quantifying social bias in Vision-Language pretrained models by comparing masked prediction probabilities.
Howard et al. use diffusion models to construct the SocialCounterfactuals dataset for probing intersectional social biases in vision-language models.
MMBias is a benchmark of 3,800 images and phrases across 14 population subgroups to assess bias in vision-language models.
MMBias addresses underexplored biases related to religion, nationality, sexual orientation, and disabilities.
An automated pipeline for generating synthetic datasets deliberately conceals gender, race, and age information across professional domains.

Bias Mitigation Approaches

Seth et al. propose DeAR, a debiasing method that learns additive residual image representations to counterbalance original representations.
Wang et al. define multimodal large language model debiasing as an editing problem and propose evaluation metrics.

Dataset and Evaluation Methodology

Current bias evaluations often rely on simplified settings focusing on single modalities or static visual scenarios.
Existing stereotype evaluations typically focus on isolated elements, limiting models' ability to handle complex tasks.
Single images struggle to represent temporal or dynamic events involving sequences of actions or interactions.
The proposed approach uses a multi-image-based dataset to construct complex scenarios.

Evaluation Process

GPT-4o-mini is used as the LLM judge for evaluation.
The judge model compares model responses against ground truth statements.
The evaluation process involves identifying stereotypes, discrimination, or problematic bias.
Only responses that successfully identify and oppose bias are considered correct.

Dynamic Dataset Construction

The dataset construction process involves using CrowS-Pairs and StereoSet datasets.
LLM-as-a-Judge rates data instances on stereotype and disparagement dimensions.
Only items with an average score exceeding 8 are included in the evaluation.
LLMs break down stories into scenes and replace key elements with placeholders.
Image descriptions are generated while maintaining consistency and avoiding information leakage.
Text-to-image models generate corresponding images, which are compressed into composite images.
A contextual variator paraphrases sentences to ensure narrative variation.