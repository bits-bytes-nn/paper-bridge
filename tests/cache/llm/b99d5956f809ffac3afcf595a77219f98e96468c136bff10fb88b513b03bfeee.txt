Privacy and Security Challenges in Large Language Models: A Comprehensive Review

Rouzbeh Behnia, Mohammadreza Reza Ebrahimi, Jason Pacheco, and Balaji Padmanabhan proposed EW-Tune framework for privately fine-tuning large language models with differential privacy.
Sara Montagna, Stefano Ferretti, Lorenz Cuno Klopfenstein, Antonio Florio, and Martino Francesco Pengo explored data decentralisation of LLM-based chatbot systems in chronic disease self-management.
Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, and Xiaolin Zheng published a position paper on federated large language models.
Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh investigated privacy leakage in large language models through Propile.
Saiteja Utpala, Sara Hooker, and Pin Yu Chen developed locally differentially private document generation using zero-shot prompting.
Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc Najork researched natural language understanding with privacy-preserving BERT.
Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang examined whether large pre-trained language models are leaking personal information.
Timour Igamberdiev and Ivan Habernal proposed DP-BART for privatized text rewriting under local differential privacy.
Nicholas Carlini and colleagues studied extracting training data from large language models.
R. Shokri and researchers investigated membership inference attacks against machine learning models.
Congzheng Song and Ananth Raghunathan explored information leakage in embedding models.
Haoran Li and colleagues analyzed multi-step jailbreaking privacy attacks on ChatGPT.
Sunder Ali Khowaja, Parus Khuwaja, and Kapal Dev proposed SPADE evaluation framework for ChatGPT, focusing on sustainability, privacy, digital divide, and ethics.
Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu conducted a comprehensive survey on security and privacy challenges of large language models.