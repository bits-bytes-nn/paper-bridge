topic: Large Language Model Safety Research

  entities:
    Percy Liang|Researcher
    Boxin Wang|Researcher
    Yuxia Wang|Researcher
    Rishabh Bhardwaj|Researcher
    Soujanya Poria|Researcher
    Kaijie Zhu|Researcher
    Guohai Xu|Researcher
    Linyi Yang|Researcher
    Hao Sun|Researcher
    Bertie Vidgen|Researcher
    Yige Li|Researcher
    Junyi Li|Researcher
    Huachuan Qiu|Researcher
    Peiyi Wang|Researcher
    OpenCompass Contributors|Research Group
    Liang Xu|Researcher
    Wenxuan Wang|Researcher
    Do-not-answer dataset|Dataset
    PromptBench|Toolkit
    HaluEval|Benchmark
    SC-Safety|Benchmark
    MLCommons|Organization
    OpenCompass|Research Platform

  proposition: Percy Liang et al. published a paper on holistic evaluation of language models in 2022.
    entity-attribute relationships:
    Percy Liang|PUBLISHED|holistic evaluation of language models
    Percy Liang|PUBLICATION_YEAR|2022

    entity-entity relationships:
    Percy Liang|AUTHOR_OF|research paper

  proposition: Boxin Wang et al. conducted a comprehensive assessment of trustworthiness in GPT models in 2023.
    entity-attribute relationships:
    Boxin Wang|CONDUCTED|comprehensive assessment of trustworthiness
    Boxin Wang|RESEARCH_YEAR|2023

    entity-entity relationships:
    Boxin Wang|ASSESSED|GPT models

  proposition: Yuxia Wang et al. created the Do-not-answer dataset for evaluating safeguards in large language models in 2023.
    entity-attribute relationships:
    Yuxia Wang|CREATED|Do-not-answer dataset
    Yuxia Wang|RESEARCH_YEAR|2023

    entity-entity relationships:
    Yuxia Wang|DEVELOPED|Do-not-answer dataset
    Do-not-answer dataset|EVALUATES|large language models

  proposition: Rishabh Bhardwaj and Soujanya Poria proposed red-teaming large language models using chain of utterances for safety-alignment in 2023.
    entity-attribute relationships:
    Rishabh Bhardwaj|PROPOSED|red-teaming approach
    Soujanya Poria|PROPOSED|red-teaming approach
    Rishabh Bhardwaj|RESEARCH_YEAR|2023
    Soujanya Poria|RESEARCH_YEAR|2023

    entity-entity relationships:
    Rishabh Bhardwaj|COLLABORATOR|Soujanya Poria
    Rishabh Bhardwaj|RESEARCHED|large language models
    Soujanya Poria|RESEARCHED|large language models

  proposition: Kaijie Zhu et al. developed PromptBench to evaluate the robustness of large language models on adversarial prompts in 2024.
    entity-attribute relationships:
    Kaijie Zhu|DEVELOPED|PromptBench
    Kaijie Zhu|RESEARCH_YEAR|2024

    entity-entity relationships:
    Kaijie Zhu|CREATED|PromptBench
    PromptBench|EVALUATES|large language models

  proposition: Bertie Vidgen et al. introduced version 0.5 of the AI safety benchmark from MLCommons in 2024.
    entity-attribute relationships:
    Bertie Vidgen|INTRODUCED|AI safety benchmark version 0.5
    Bertie Vidgen|RESEARCH_YEAR|2024

    entity-entity relationships:
    Bertie Vidgen|ASSOCIATED_WITH|MLCommons
    Bertie Vidgen|DEVELOPED|AI safety benchmark

  proposition: Junyi Li et al. developed HaluEval, a large-scale hallucination evaluation benchmark for large language models in 2023.
    entity-attribute relationships:
    Junyi Li|DEVELOPED|HaluEval
    Junyi Li|RESEARCH_YEAR|2023

    entity-entity relationships:
    Junyi Li|CREATED|HaluEval
    HaluEval|EVALUATES|large language models

  proposition: Liang Xu et al. created SC-Safety, a multi-round open-ended question adversarial safety benchmark for Chinese large language models in 2023.
    entity-attribute relationships:
    Liang Xu|CREATED|SC-Safety
    Liang Xu|RESEARCH_YEAR|2023

    entity-entity relationships:
    Liang Xu|DEVELOPED|SC-Safety
    SC-Safety|EVALUATES|Chinese large language models

  proposition: Wenxuan Wang et al. studied the multilingual safety of large language models in 2024.
    entity-attribute relationships:
    Wenxuan Wang|STUDIED|multilingual safety
    Wenxuan Wang|RESEARCH_YEAR|2024

    entity-entity relationships:
    Wenxuan Wang|RESEARCHED|large language models