Trustworthiness Benchmarks for Generative Foundation Models

Benchmarks provide frameworks to assess current models and guide future advancements in reliability and safety.
Benchmark development is crucial for fostering collaboration among industry stakeholders to enhance trustworthiness of Generative Foundation Models.

Large Language Model Benchmarks:
TrustLLM benchmark evaluates Large Language Models based on truthfulness, safety, fairness, and robustness.
HELM benchmark provides a broad view of model reliability across multiple dimensions.
DecodingTrust benchmark emphasizes safety, privacy, and ethical considerations.
Do-Not-Answer benchmark aims to reduce potential harm from model outputs.
SafetyBench benchmark focuses on safety and reducing bias.
FairEval benchmark targets issues of bias and harmful content.
CVALUES and ML Commons v0.5 benchmarks contribute to model assessment.
BackdoorLLM benchmark examines vulnerability to backdoor attacks.

Text-to-Image and Vision-Language Model Benchmarks:
HEIM benchmark covers truthfulness, safety, fairness, and robustness in vision domains.
HRS-Bench benchmark focuses on truthful assessment.
Stable Bias benchmark addresses fairness concerns.
DALL-EVAL and GenEVAL benchmarks emphasize truthfulness evaluation.
MultiTrust and MLLM-Guard provide comprehensive multi-dimensional assessments.
MM-SafetyBench and UniCorn focus on safety and privacy considerations.
BenchLMM and Halle-switch prioritize robustness testing.
Red-Teaming VLM and JailBreak-V evaluate security.
GOAT-Bench assesses safety and fairness.
Ch3Ef and GenderBias address specific biases and fairness concerns.

Comprehensive benchmarks like TrustGen aim to evaluate multiple aspects across different model types, including safety, fairness, robustness, privacy, ethics, and diverse model architectures.