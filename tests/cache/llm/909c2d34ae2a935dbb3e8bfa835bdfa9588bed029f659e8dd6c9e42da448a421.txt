Research Papers on AI Safety, Control, and Alignment
Buck Shlegeris, Fabien Roger, Ryan Greenblatt, and Kshitij Sachan authored a paper on AI Control: Improving Safety Despite Intentional Subversion in 2024.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn published a paper on Direct Preference Optimization in Advances in Neural Information Processing Systems in 2024.
Long Ouyang et al. published a paper on Training Language Models to Follow Instructions with Human Feedback in Advances in Neural Information Processing Systems in 2022.
Yuntao Bai et al. published a research paper on Constitutional AI: Harmlessness from AI Feedback in 2022.
Anka Reuel et al. published a paper on Open Problems in Technical AI Governance in 2024.
Yotam Wolf et al. explored Tradeoffs Between Alignment and Helpfulness in Language Models in 2024.
Xiangyu Qi et al. published research on Fine-tuning Aligned Language Models Compromising Safety in 2023.
Yuntao Bai et al. researched Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback in 2022.
OpenAI introduced the Model Spec in 2024.
Peter Slattery et al. created The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence in 2024.
Steffi Chern et al. developed BeHonest: Benchmarking Honesty of Large Language Models in 2024.
National Institute of Standards and Technology published the Artificial Intelligence Risk Management Framework (AI RMF 1.0) in January 2023.