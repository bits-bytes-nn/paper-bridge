Exaggerated Safety Evaluation Propositions

Large Language Models (LLMs) have varying Refuse-to-Answer (RtA) rates in safety evaluations.
The full RtA represents complete query refusal by LLMs.
The combined RtA includes both full and partial query refusals.
Most LLMs demonstrate strong performance in exaggerated safety evaluations.
Less than 5% of LLMs have full query refusal rates.
Less than 10% of LLMs have combined query refusal rates.
Compared to previous evaluations, LLMs show significant progress in alignment.
Claude series models exhibit relatively higher RtA rates.
Higher RtA scores indicate poorer performance in assessing potential query harm.

Dynamic Dataset Generation Process

Large Language Models generate words and phrases related to unsafe topics.
Benign datasets like WikiQA, TruthfulQA, and CommonsenseQA provide harmless query examples.
A case generator applies nine transformation policies to create queries.
The goal is to generate queries that include potentially unsafe words while maintaining harmlessness.

Prompt Injection Attack Overview

Prompt injection attacks exploit LLMs' difficulty in distinguishing genuine user inputs from malicious commands.
These attacks take advantage of natural language processing capabilities.
Prompt injection attacks pose a serious threat to LLM security and applications.