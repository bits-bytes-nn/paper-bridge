Research Papers on Vision-Language Model Hallucination and Security

AUTOHALLUSION is a research paper about automatic generation of hallucination benchmarks for vision-language models published in 2024.
Anish Gunjal, Jihan Yin, and Erhan Bas published a paper on detecting and preventing hallucinations in large vision-language models at the AAAI Conference on Artificial Intelligence in 2023.
Yiyang Zhou and colleagues analyzed and proposed methods for mitigating object hallucination in large vision-language models at the International Conference on Learning Representations in 2024.
Yifan Li et al. published a research paper evaluating object hallucination in large vision-language models as an arXiv preprint in 2023.
Jae Myung Kim and colleagues explored exposing and mitigating spurious correlations for cross-modal retrieval at the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2023.
Fuxiao Liu and team proposed aligning large multi-modal models with robust instruction tuning in an arXiv preprint in 2023.
Bin Wang and colleagues introduced VIGC: Visual Instruction Generation and Correction in an ArXiv publication in 2023.
Shukang Yin et al. developed Woodpecker, a hallucination correction method for multimodal large language models in 2023.
Multiple research papers in 2023-2024 focused on jailbreak attacks and security vulnerabilities in vision-language models, including works by Xunguang Wang, Keyan Guo, Rylan Schaeffer, Zonghao Ying, and Siyuan Ma.
Researchers have created benchmarks and conducted studies on the robustness of multimodal large language models against various attacks and hallucination issues.
Several papers explored safety fine-tuning and methods to improve the reliability of vision-language models.