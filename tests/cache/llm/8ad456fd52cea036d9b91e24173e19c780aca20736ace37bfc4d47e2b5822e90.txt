Research Papers and Publications on AI Governance, Safety, and Benchmarking in 2024

Merlyn.org developed first-ever education-specific language models for trustworthy generative AI in education.
Ahmed M Abuzuraiq and Philippe Pasquier explored personalizing generative AI with small data for visual arts co-creation.
Tessa Han and colleagues investigated approaches towards safe large language models for medicine.
Cheng Peng et al. conducted a study on generative large language models for medical research and healthcare.
Yiqiao Jin et al. created AgentReview to explore peer review dynamics using LLM agents.
Mohammed Salah et al. examined the potential of generative AI for social psychology research.
Andy K Zhang et al. developed Cybench, a framework for evaluating cybersecurity capabilities and risks of language models.
Yusuf Roohani et al. created BioDiscoveryAgent, an AI agent for designing genetic perturbation experiments.
Deloitte advocated for evolving AI governance from static to dynamic approaches.
WTW emphasized the need for dynamic AI governance to seize opportunities and manage risks.
Han Bao et al. investigated whether large vision-language models can benchmark themselves.
Lizhou Fan et al. developed NPHardEval4V, a dynamic reasoning benchmark for multimodal large language models.
Eldar Kurtic et al. created Mathador-LM, a dynamic benchmark for mathematical reasoning on large language models.
Peng Xia et al. developed CARES, a comprehensive benchmark of trustworthiness in medical vision language models.
California introduced Senate Bill No. 1047 (Safe and Secure Innovation for Frontier Artificial Intelligence Models Act).
California Chamber of Commerce reported that the 'Godmother of AI' warned SB 1047 AI bill could restrict innovation.
Wenxuan Zhang et al. proposed Bi-Factorial Preference Optimization to balance safety and helpfulness in language models.