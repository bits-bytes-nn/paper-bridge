Research Papers on Large Language Model Safety Evaluation Benchmarks in 2024

SORRY-Bench is a research paper evaluating large language model safety refusal behaviors in 2024.
Hari Shrawgi and colleagues published a paper on uncovering stereotypes in large language models using a task complexity-based approach.
The paper was presented at the 18th Conference of the European Chapter of the Association for Computational Linguistics in St. Julian's, Malta in March 2024.
Lijun Li and colleagues introduced SALAD-Bench, a hierarchical and comprehensive safety benchmark for large language models in 2024.
Tongxin Yuan and researchers developed R-Judge, a benchmark for assessing safety risk awareness for LLM agents.
R-Judge was published on ArXiv with the identifier abs/2401.10019 in 2024.
Chujie Gao and colleagues explored creating an honest and helpful large language model in their arXiv preprint.
Yuan Li and team investigated awareness in large language models in their research paper.
Simone Tedeschi and researchers created ALERT, a comprehensive benchmark for assessing large language models' safety through red teaming.
Other notable safety evaluation benchmarks include:
OR-Bench, an over-refusal benchmark for large language models.
CLIMB, a benchmark of clinical bias in large language models.
SafeBench, a safety evaluation framework for multimodal large language models.
ChineseSafe, a Chinese benchmark for evaluating safety in large language models.
SG-Bench, a benchmark for evaluating LLM safety generalization across diverse tasks and prompt types.
XTRUST, a study on the multilingual trustworthiness of large language models.