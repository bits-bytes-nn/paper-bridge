Multimodal Large Language Model Benchmarks and Evaluation Papers

Yuan Liu et al. published MMBench for evaluating multi-modal model capabilities.
Bohao Li et al. created SEED-Bench for benchmarking multimodal large language models.
Dingjie Song et al. developed MileBench for evaluating MLLMs in long context scenarios.
Haoning Wu et al. introduced Q-Bench for evaluating foundation models on low-level vision tasks.
Zhenfei Yin et al. developed LAMM as a language-assisted multi-modal instruction-tuning dataset and benchmark.
Haotian Liu et al. proposed visual instruction tuning methodology.
Chaoyou Fu et al. created MME as a comprehensive evaluation benchmark for multimodal large language models.
Bohao Li et al. extended SEED-Bench with SEED-Bench-2-Plus focusing on text-rich visual comprehension.
Jihyung Kil et al. developed CompBench as a comparative reasoning benchmark for multimodal LLMs.
Xiyao Wang et al. created Mementos, a benchmark for reasoning over image sequences.
Xiang Yue et al. introduced MMMU as a massive multi-discipline multimodal understanding and reasoning benchmark.
Xiang Yue et al. further developed MMMU-Pro as a more robust multimodal understanding benchmark.
Ge Zhang et al. created CMMMU, a Chinese massive multi-discipline multimodal understanding benchmark.
Pan Lu et al. developed MathVista for evaluating mathematical reasoning in visual contexts.
Yijia Xiao et al. proposed LogicVista as a multimodal LLM logical reasoning benchmark.