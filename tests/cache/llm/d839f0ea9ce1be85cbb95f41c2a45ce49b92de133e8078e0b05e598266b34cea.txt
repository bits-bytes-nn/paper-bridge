Research Papers on Bias, Fairness, and Robustness in Text-to-Image Models

Eddie L Ungless, Bj√∂rn Ross, and Anne Lauscher studied stereotypes in text-to-image models.
The research examined misrepresentation of non-cisgender identities by text-to-image models.
Abhipsa Basu, R Venkatesh Babu, and Danish Pruthi investigated geographical representativeness of images from text-to-image models.
Rida Qadri, Renee Shelby, Cynthia L Bennett, and Emily Denton conducted a community-centered study of text-to-image models in South Asia.
Lukas Struppek and colleagues explored how cultural biases can be exploited via homoglyphs in text-to-image synthesis.
Laura Gustafson and co-authors developed FACET, a fairness benchmark for computer vision evaluation.
Xudong Shen and colleagues researched finetuning text-to-image diffusion models for fairness.
Nikita Nangia and colleagues created CrowS-pairs, a challenge dataset for measuring social biases in masked language models.
Moin Nadeem and researchers developed StereoSet for measuring stereotypical bias in pretrained language models.
Alicia Parrish and team created BBQ, a hand-built bias benchmark for question answering.
Han Liu and colleagues proposed RIATIG, a method for reliable and imperceptible adversarial text-to-image generation.
Xiaodong Wu and researchers studied robustness of watermarking on text-to-image diffusion models.
Haomin Zhuang and team conducted a pilot study on query-free adversarial attacks against Stable Diffusion.