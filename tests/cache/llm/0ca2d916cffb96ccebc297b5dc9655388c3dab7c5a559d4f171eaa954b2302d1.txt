Research Papers on Large Language Model Safety and Toxicity Mitigation

Chi Han et al. published a paper on Word Embeddings as Steers for Language Models at ACL 2024.
Chi Han et al. received an Outstanding Paper Award for their research.
Liwei Jiang et al. conducted the Delphi experiment to explore whether machines can learn morality.
Bang An et al. proposed a method for Automatic Pseudo-Harmful Prompt Generation to evaluate false refusals in Large Language Models.
Paul RÃ¶ttger et al. developed XSTest, a test suite for identifying exaggerated safety behaviors in Large Language Models.
Min Zhang et al. revealed excessive sensitivity and calibration limitations of LLMs in implicit hate speech detection.
Zouying Cao et al. proposed mitigating exaggerated safety in LLMs via safety-conscious activation steering.
Chenyu Shi et al. investigated the overkill phenomenon in large language models.
Xirui Li et al. created Mossbench to test if multimodal language models are oversensitive to safe queries.
Weixiang Zhao et al. researched comprehensive and efficient post-safety alignment of large language models.
Chaofan Wang et al. explored safeguarding crowdsourcing surveys from ChatGPT with prompt injection.
Facebook and OpenAI have developed content moderation policies and APIs to manage harmful content.
Google Jigsaw suggests machine learning can help reduce toxicity and improve online conversations.