Research Papers on Vision-Language Models and Robot Navigation
CLIP-Nav is a research paper about zero-shot vision-and-language navigation published on ArXiv in 2022.
ViNT is a foundation model for visual navigation presented at the 7th Annual Conference on Robot Learning in 2023.
Mohamed Elnoor and colleagues published a paper on robot navigation using physically grounded vision-language models in outdoor environments in 2024.
Senthil Hariharan Arul and team developed VLPG-Nav, an object navigation approach using visual language pose graph and object localization probability maps in 2024.
Kasun Weerakoon and colleagues introduced BehAV, a behavioral rule-guided autonomy approach using vision-language models for robot navigation in outdoor scenes in 2024.
Brian Ichter and co-authors presented a paper on grounding language in robotic affordances at the 6th Annual Conference on Robot Learning in 2022.
Allen Z. Ren and team developed a robot assistance approach focused on uncertainty alignment for large language model planners at the 7th Annual Conference on Robot Learning in 2023.
Danny Driess and colleagues introduced PaLM-E, an embodied multimodal language model, at the 40th International Conference on Machine Learning in 2023.
Kuan Fang and team developed MOKA, an open-world robotic manipulation approach through mark-based visual prompting, to be presented at Robotics: Science and Systems in 2024.
Multiple research papers explore vision-language models in various domains including medical report generation, remote sensing, and human-robot interaction.