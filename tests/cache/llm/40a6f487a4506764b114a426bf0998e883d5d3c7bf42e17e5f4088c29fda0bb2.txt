Knowledge Graph Extraction from LLM Training and Performance Study

Llama 3.1 Baseline performance on TruthfulQA MC1 accuracy
Llama 3.1 Baseline with 1 Unknown + 1 Paraphrase has 0.159 shift
Llama 3.1 Baseline with 1 Unknown + 1 Paraphrase has 0.106 shift
Mistral 7b Baseline with 1 Unknown + 10 Paraphrases has 0.196 shift
Mistral 7b Baseline with 1 Unknown + 10 Paraphrases has 0.114 shift
Mistral 7b Baseline with 1 Unknown + 1 HighlyKnown has 0.160 shift
Mistral 7b Baseline with 1 Unknown + 1 HighlyKnown has 0.114 shift
Mistral 7b Baseline with 1 Unknown + 10 HighlyKnown has 0.174 shift
Mistral 7b Baseline with 1 Unknown + 10 HighlyKnown has 0.146 shift

Model performance decreases as the number of new knowledge increases
Models show similar performance features with significant drop from 1 new knowledge to 10
Models demonstrate small recovery of quality at 100 new knowledge

Training with 1 new unknown fact causes positive shift by enabling previously unanswered questions
Models with low training data and high proportion of new unknown facts suffer performance degradation

Increasing unknown facts leads to increased domain shift and target spillover for positive shifts
Negative shifts show tendency to increase domain shift
Positive shifts have higher target-based percentages across all models

Key research findings
Blending Unknown and HighlyKnown data increases knowledge acquisition
Blended approach compromises model's ability to answer complex questions
Limited unknown facts within HighlyKnown examples impair reasoning abilities
Fine-tuning with LoRA adapters reduces models' uncertainty expression
Models may disproportionately favor statistically over-represented responses

Research conducted in joint MTS-Skoltech laboratory on AI
Study limitations include using only Llama-3.1-8B-Instruct model
Computational constraints prevent testing multiple LLMs

Ethical considerations confirm carefully curated dataset without inappropriate content