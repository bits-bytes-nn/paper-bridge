Research Papers on Language Model Evaluation and Benchmarking

Steffi Chern and colleagues published a paper titled "BeHonest: Benchmarking Honesty of Large Language Models" in 2024.
The National Institute of Standards and Technology released the Artificial Intelligence Risk Management Framework (AI RMF 1.0) in January 2023.
Yuexing Hao and colleagues developed the i-SDM Framework for AI-based tools in shared decision-making for cancer treatment in 2024.
Jieyu Zhang and researchers published "Task Me Anything" as an arXiv preprint in 2024.
Xiang Lisa Li and colleagues created "AutoBencher" for generating salient, novel, and difficult datasets for language models in 2024.
Jinjie Ni and team introduced MixEval, a method for deriving crowd wisdom from LLM benchmark mixtures in 2024.
Yucheng Li and colleagues proposed LatestEval to address data contamination in language model evaluation in 2023.
Ali Shirali, Rediet Abebe, and Moritz Hardt developed a theory of dynamic benchmarks in 2022.
Irena Gao and researchers worked on adaptive testing of computer vision models in 2022.
Marco Tulio Ribeiro and Scott M. Lundberg focused on adaptive testing and debugging of NLP models in 2022.
Guillaume Leclerc and colleagues created 3DB, a framework for debugging computer vision models in 2021.
Shuo Yang and team proposed rethinking benchmark and contamination for language models in 2023.
Nikita Soni and colleagues discussed large human language models, their needs and challenges in 2024.
Colin White and researchers developed Livebench, a challenging and contamination-free LLM benchmark in 2024.