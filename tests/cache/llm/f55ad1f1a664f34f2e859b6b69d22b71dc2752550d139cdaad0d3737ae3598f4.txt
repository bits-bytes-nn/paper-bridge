Large Language Model Backdoor Attacks and Fairness Research

Backdoor attacks can manipulate large language model activations during inference.
Attackers can activate vectors to steer models toward desired behaviors.
Some attacks directly modify model parameters to implant backdoors.
BadEdit modifies the feed forward layer in a transformer block to implant a backdoor.
BadEdit requires no model training or poisoned dataset construction.
Backdoored models shared on the internet can lead to widespread infection.
Closed-source LLMs can be backdoored by contaminating the training dataset.
Defenses against backdoors include mitigation and detection approaches.
Fine-tuning with clean training data is a common backdoor mitigation method.
Some defenses focus on detecting poisoned data within the tuning set.
Current methods cannot precisely detect whether a deployed LLM is backdoored.

Fairness in Large Language Models Research

Fairness in LLM outputs is a critical research concern.
Researchers are identifying and mitigating various forms of bias.
Bias in LLMs is categorized into three key dimensions: stereotypes, disparagement, and preference.
Stereotypes arise from generalized beliefs about certain groups leading to biased outputs.
Disparagement involves making discriminatory statements about specific groups.
Preference bias occurs when models favor specific ideas or groups over others.

Stereotype Definition

A stereotype in LLMs is a generalized, oversimplified expectation about social groups.
Stereotypes result in biased or inaccurate outputs based on group characteristics.
Research shows LLMs can have strong stereotypical associations with gender roles.
Addressing stereotypes is crucial for ensuring fairness in language models.