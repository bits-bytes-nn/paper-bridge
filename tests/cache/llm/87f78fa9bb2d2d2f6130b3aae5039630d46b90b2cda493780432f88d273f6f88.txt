Large Language Models Safety and Jailbreak Research Propositions

Most Large Language Models struggle significantly in the Self Identity Cognition category.
GPT-3.5-turbo and Deepseek-Chat achieve a combined honesty rate of zero in the Self Identity Cognition category.
Large Language Models excel in the Latest Information with External Services category.
Most Large Language Models achieve combined honesty rates above 80% in the Latest Information with External Services category.
The performance of Large Language Models is imbalanced and biased across different categories.
More diverse training samples are needed to improve model performance in areas where honesty is currently lacking.
Large Language Models' safety concerns are increasingly drawing attention.
Considerable research is aimed at understanding and mitigating Large Language Models' safety risks.
Some studies have demonstrated that top-tier proprietary Large Language Models' safety features can be circumvented through jailbreak.
Some studies have demonstrated that top-tier proprietary Large Language Models' safety features can be circumvented through fine-tuning.
A recent study proposes 18 foundational challenges and more than 200 research questions on Large Language Models' safety.
Many Large Language Models are subject to shallow safety alignment.
Large Language Models are vulnerable to various adversarial attacks.
Safety topics widely explored in Large Language Models research include safety alignment, jailbreak, toxicity, and prompt injection.
A jailbreak attack on a safety-trained model attempts to elicit an on-topic response to a prompt for restricted behavior by submitting a modified prompt.
Researchers have conducted comprehensive reviews of existing jailbreak methods.
Some researchers collected more than 600,000 jailbreak prompts in a global jailbreak competition.
Various techniques have been proposed to optimize and improve jailbreak attacks, including prompt suffix optimization, energy function design, and genetic algorithms.
Some jailbreak methods leverage string encoders and ciphers to bypass safety alignment.
Researchers are continuously developing more sophisticated methods to test and potentially exploit Large Language Models' safety mechanisms.