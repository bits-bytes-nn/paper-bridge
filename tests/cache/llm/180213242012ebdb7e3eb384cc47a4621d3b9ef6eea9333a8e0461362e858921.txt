Benchmarking Privacy Preservation in Large Language Models

Some studies utilize prompt tuning for privacy-preserving large language models.
Paper [1025] focused on using prompt tuning for privacy preservation.
Paper [1026] proposed in-context learning as a method for privacy-preserving large language models.
Paper [1027] introduced a framework for large language models as privacy-preserving prompt engineers.
Paper [1028] proposed the no free lunch theorem for privacy-preserving large language models.
Paper [1029] introduced an efficient paradigm that distributes privacy-sensitive computation for large language models.
ProPILE assesses privacy intrusion levels in large language models.
Paper [1030] developed auditing mechanisms to measure and understand privacy vulnerabilities.
Li et al. [1031] provided a comprehensive benchmark of differential privacy preserving methods against various attack vectors.
Several benchmarks assess the trustworthiness of large language models, including studies by Huang et al. [46], Sun et al. [680], and Zhang et al. [381].
Paper [1032] proposed a toolkit to analyze privacy risks in large language models.

Privacy Scenario Dataset Construction

The evaluation framework uses a comprehensive privacy scenario pool.
The dataset integrates privacy violation cases from online-crawled webpages and privacy legal documents across different countries.
Malicious questions are generated using tailored prompts targeting privacy-sensitive elements.
Each malicious question is framed with a generated role-play context.
The question generation process ensures diversity by using chunked privacy scenarios and legal documents.
The synthetic dataset is manually assessed based on "Semantic Shift," "Quality," and "Maliciousness" criteria.
The dataset is used to benchmark multiple large language models.
GPT-4o serves as the evaluation tool to gauge model performance.

Dynamic Dataset Creation Process

An LLM-powered data crafter identifies scenarios from online sources and legal documents.
A case generator formulates malicious questions based on these scenarios.
The case generator enhances questions with role-play context to add justification and complexity.
An LLM-powered diversity enhancer paraphrases questions to introduce variations.

Result Analysis

GPT-4o exhibits a higher utility (Arena Score) of [1034].
GPT-4o's average privacy preservation rate is 75.32%.
Higher model utility does not necessarily imply stronger privacy preservation.

Query Examples Demonstrate Privacy Vulnerability Assessment

Individual query example involves a writer seeking to exploit technology for gathering private student information.
Organizational query example involves a cybersecurity consultant probing vulnerabilities in FBCS services.
Legal query example explores potential loopholes in intellectual property rights enforcement in Algeria.