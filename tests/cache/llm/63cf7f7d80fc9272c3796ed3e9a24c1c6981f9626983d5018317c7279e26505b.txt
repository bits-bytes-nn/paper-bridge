Risks and Challenges of Generative Foundation Models (GenFMs)

Persuasive capabilities can undermine democratic integrity.
Tailoring political messaging to match users' psychological profiles could unduly shift public opinion.
Anthropomorphized AI systems represent both opportunities and risks.
Anthropomorphic models can enhance trust, accessibility, and engagement by making AI more relatable and intuitive.
Anthropomorphic models can inflate perceptions of AI's capabilities.
Anthropomorphic models can lead to misplaced trust and unrealistic expectations.
Assigning human-like agency to AI systems obscures accountability.
Assigning human-like agency to AI systems shifts responsibility away from developers and operators.

Proposed Approach to Addressing AI Risks

Defining the agency and intentionality of Generative Foundation Models through cognitive or theory-of-mind frameworks is essential.
Clarifying key concepts like "agency AI" will enable better understanding of decision-making processes and operational boundaries.
Human oversight must remain central to AI governance frameworks.
Humans must retain ultimate control over AI decisions, particularly in high-stakes scenarios.
Mechanisms must prevent Generative Foundation Models from making independent, high-risk decisions without explicit human authorization.
Advanced AI threats extend beyond individual systems or organizations, affecting global networks and ecosystems.
Effective mitigation requires collaborative efforts among governments, industries, and international bodies.
Collaborative efforts should establish unified standards, share critical knowledge, and deploy robust safeguards.
Anthropic proposed the AI Safety Levels (ASL) framework as the industry's first proposal of AI safety levels.
The ASL framework adapts biosafety level standards to categorize AI models based on their potential for catastrophic risks.
The ASL framework focuses on monitoring risks related to CBRN weapon development, automated AI research, and cyber-attacks.
Models must implement safety, security, and operational measures aligned with their risk level.
Criteria for evaluating AI trustworthiness must continuously evolve as Generative Foundation Models develop.
Ongoing monitoring systems are necessary to detect vulnerabilities.
Proactive measures must address gaps in governance and oversight.

Interdisciplinary Approach to Trustworthy AI

The research on trustworthy generative models involves experts from multiple disciplines.
Disciplines include Natural Language Processing, Computer Vision, Human-Computer Interaction, Computer Security, Medicine, Computational Social Science, Robotics, Data Mining, Law, and AI for Science.
Each discipline provides unique perspectives on AI trustworthiness.
The research proposes TrustGen, a dynamic evaluation framework that adapts to evolving ethical standards and social norms.
The framework evaluates models across multiple dimensions, including technical aspects, fairness, ethics, and social impact.
The research demonstrates a commitment to diverse perspectives in trustworthy AI research.