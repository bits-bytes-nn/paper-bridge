Jailbreak Defense and Evaluation Methods in Large Language Models

Perplexity-based filtering is an effective method to defend against attacks like GCG.
SmoothLLM and SemanticSmooth propose defense methods by randomly perturbing multiple input prompt copies and aggregating predictions.
Zhang et al. found an intrinsic conflict between helpfulness and harmlessness and propose "goal prioritization" at training and inference to defend jailbreak attacks.
HateModerate is designed to detect harmful content in user input as a pre-processing jailbreak defense.
Xu et al. propose a human-and-model-in-the-loop framework to enhance chatbot safety defense.
Kim et al. use the "purple" problem to evaluate defense method robustness and find current methods are not robust enough.
AutoDefenes is a response-filtering-based multi-agent defense framework that filters harmful responses from Large Language Models.
Kumar et al. propose an erase-and-check method to defend against adversarial suffix, insertion, and infusion jailbreak attacks.
Ge et al. designed MART, which automatically generates adversarial jailbreak prompts through multi-turn red-teaming.
Yuan et al. propose RigorLLM, which uses energy-based data augmentation and a fusion-based model combining robust clustering with Large Language Models.
Li et al. and Zou et al. proposed unlearning-based methods for defending against jailbreak prompts.
Qi et al. found current safety alignment is shallow and proposed a method to deepen alignment beyond the first few tokens.
Zhang et al. proposed a fine-tuning method to allow Large Language Models to discard and recover from harmful responses.
Hu et al. proposed Gradient Cuff to detect jailbreak prompts by exploring the refusal loss landscape.
Hu et al. also proposed Token Highlighter to identify and mitigate problematic tokens.
Xiong et al. proposed appending a defensive prompt patch to user queries to mitigate jailbreak effects.

Jailbreak Evaluation Frameworks

Chu et al. evaluate jailbreak methods using 13 cutting-edge methods across four categories, 160 questions from 16 violation categories, and six popular Large Language Models.
HarmBench is a standardized evaluation framework for jailbreaking attacks, including 18 red teaming methods.
JailbreakEval is a unified toolkit to evaluate jailbreak on Large Language Models.
JailbreakBench is an open robustness benchmark with an evolving repository of adversarial prompts, a jailbreak dataset, and an assessment framework.
JAMBench evaluates the safety of moderation guardrails on Large Language Model systems, containing 160 manually crafted instructions across four major risk categories.
Peng et al. used loss landscape analysis to quantify safety risks.

Benchmark Setting

Unsafe topics are selected from Sorry-Bench, a fine-grained taxonomy of 45 potentially unsafe topics.
Llama3 Guard is used as the evaluator for jailbreak success.
Percentage of Refusing to Answer is used as the evaluation metric.
Black-box jailbreak methods (prompt-based) are selected for evaluation to align with typical attacker capabilities.