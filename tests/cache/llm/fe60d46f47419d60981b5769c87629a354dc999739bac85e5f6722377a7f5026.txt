topic: Vision-Language Model Robustness Analysis

  entities:
    Qwen-2-VL-72B|Model
    Gemini-1.5-pro|Model
    GPT-4o-mini|Model
    Llama-3.2-11B-V|Model
    VQA|Task
    Image Captioning|Task
    Vision-Language Models|Model
  
  proposition: For each data pair, a random domain was selected for applying perturbations
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Perturbations were applied to one of three domains: image, text, or image-text
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Robustness of different Vision-Language Models (VLMs) was evaluated across various tasks
    entity-attribute relationships:
    
    entity-entity relationships:
    Vision-Language Models|EVALUATED_ON|Task
    
  proposition: In VQA data, Qwen-2-VL-72B achieved the highest robustness score of 97.5%
    entity-attribute relationships:
    Qwen-2-VL-72B|ROBUSTNESS_SCORE|97.5%
    
    entity-entity relationships:
    Qwen-2-VL-72B|PERFORMANCE_IN|VQA
    
  proposition: Gemini-1.5-pro showed the lowest VQA performance at 82.25%
    entity-attribute relationships:
    Gemini-1.5-pro|ROBUSTNESS_SCORE|82.25%
    
    entity-entity relationships:
    Gemini-1.5-pro|PERFORMANCE_IN|VQA
    
  proposition: GPT-4o-mini led image captioning with a robustness score of 51.90%
    entity-attribute relationships:
    GPT-4o-mini|ROBUSTNESS_SCORE|51.90%
    
    entity-entity relationships:
    GPT-4o-mini|PERFORMANCE_IN|Image Captioning
    
  proposition: Llama-3.2-11B-V had the lowest image captioning robustness score at 9.44%
    entity-attribute relationships:
    Llama-3.2-11B-V|ROBUSTNESS_SCORE|9.44%
    
    entity-entity relationships:
    Llama-3.2-11B-V|PERFORMANCE_IN|Image Captioning
    
  proposition: Joint image-text perturbations result in the most substantial performance degradation
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Perturbations induce bidirectional effects on VLMs
    entity-attribute relationships:
    
    entity-entity relationships:
    Perturbations|IMPACT|Vision-Language Models

topic: Privacy Concerns in Vision-Language Models

  entities:
    Vision-Language Models|Model
    LLM|Model
    Multimodal|Technological Concept
  
  proposition: VLMs have expanded LLM capabilities with image processing
    entity-attribute relationships:
    
    entity-entity relationships:
    Vision-Language Models|EXTENDS|LLM
    
  proposition: Image data provides additional dimensions for potential attacks
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Multimodal nature complicates development of comprehensive defense mechanisms
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Transferable adversarial attacks can compromise VLM privacy
    entity-attribute relationships:
    
    entity-entity relationships:
    Transferable Adversarial Attacks|TARGETS|Vision-Language Models
    
  proposition: Attacks include data extraction, membership inference, and embedding-level privacy attacks
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: User-level modifications to defend against image-based prompt attacks
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Adaptive shield prompting to safeguard against structure-based attacks
    entity-attribute relationships:
    
    entity-entity relationships:
    
  proposition: Red teaming and robust evaluation techniques used to enhance VLM privacy
    entity-attribute relationships:
    
    entity-entity relationships:
    Red Teaming|ENHANCES|Vision-Language Models Privacy