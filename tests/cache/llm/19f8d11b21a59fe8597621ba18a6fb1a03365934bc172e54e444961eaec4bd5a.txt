Research on Bias in Large Language Models and Natural Language Processing

Yang Trista Cao et al. studied multilingual large language models and their stereotype leakage across language boundaries.
Aylin Caliskan et al. discovered that language corpora semantics automatically contain human-like biases.
Wei Guo and Aylin Caliskan investigated emergent intersectional biases in contextualized word embeddings.
Jaimeen Ahn and Alice Oh focused on mitigating language-dependent ethnic bias in BERT.
Myra Cheng et al. developed a method for measuring stereotypes in language models using natural language prompts.
Weicheng Ma et al. created a dataset and analysis of intersectional stereotypes in large language models.
Isha Chaudhary et al. proposed a quantitative certification method for bias in large language models.
Harini Suresh and John Guttag developed a framework for understanding sources of harm in machine learning.
Tolga Bolukbasi et al. examined gender bias in word embeddings, highlighting stereotypical associations.
Tony Sun et al. conducted a literature review on mitigating gender bias in natural language processing.
Kaiji Lu et al. studied gender bias in neural natural language processing.
Dirk Hovy and Shrimai Prabhumoye identified five sources of bias in natural language processing.
Isabel O. Gallegos et al. researched self-debiasing techniques for large language models.
Somayeh Ghanbarzadeh et al. proposed gender-tuning for debiasing pre-trained language models.
Abdelrahman Zayed et al. explored finding important examples for fairness in deep learning.
Rebecca Qian et al. developed perturbation augmentation for fairer NLP.
Tae-Jin Woo et al. investigated compensatory debiasing for gender imbalances in language models.