Safety and Fairness Considerations for Generative AI Models

[117] provides a detailed discussion of safety and fairness considerations for generative models.
Google PaLM API evaluates content based on a safety attribute list and filters accordingly.
Google Gemini API introduces configurable filters for dynamically setting safety attribute blocking thresholds.
Google's Secure AI Framework (SAIF) is a conceptual framework for mitigating AI-specific risks.
SAIF addresses risks including model theft, training data poisoning, prompt injection attacks, and confidential information extraction.
ShieldGemma offers advanced predictions of safety risks across various harm types.
ShieldGemma can effectively filter both inputs and outputs.
DeepMind introduced the Frontier Safety Framework to evaluate critical capabilities in frontier models.
DeepMind's Frontier Safety Framework adopts the Responsible Capability Scaling approach.
DeepMind introduced the Search-Augmented Factuality Evaluator (SAFE) for long-form factuality assessment.
SAFE uses a large language model to break down long-form responses into individual facts.
SAFE evaluates fact accuracy through a multi-step reasoning process.
SAFE sends search queries to Google Search to verify fact support.

IBM Trustworthy AI Initiatives

IBM has proposed frameworks and products focused on Trustworthy AI.
IBM's Framework for Securing Generative AI helps identify common AI attacks.
The framework prioritizes defense strategies to protect generative AI efforts.
The framework focuses on securing data, models, and usage.
IBM provides a suite of detectors to improve LLM safety and reliability.
IBM leverages large language models for next-generation threat management.
IBM's Granite foundation models are designed with trust in mind.
Granite models use a "HAP detector" to filter hateful and profane content.
IBM released Granite Guardian models for risk detection in prompts and responses.
Risks are categorized using an AI risk atlas.
Watsonx Assistant ensures chatbot data privacy and protects against vulnerabilities.

Salesforce Generative AI Trust Initiatives

Salesforce has developed five guiding principles for trusted generative AI.
The principles are Accuracy, Safety, Transparency, Empowerment, and Sustainability.
Salesforce's Einstein AI platform includes a comprehensive "Trust Layer".
The Trust Layer grounds AI outputs in accurate CRM data.
The Trust Layer masks sensitive information and mitigates risks.
Salesforce employs mechanisms to detect and prevent LLM hallucinations.
Salesforce uses zero retention agreements with third-party model providers.
Salesforce maintains an audit trail to track data use and feedback.
Salesforce released tools like Robustness Gym and SummVis for model evaluation.
Salesforce improves factual consistency by grounding entities and ensembling models.
Salesforce introduced Socratic pretraining to enhance model control and reliability.

NVIDIA Trustworthy AI Efforts

NVIDIA emphasizes safety and transparency in AI development.
NVIDIA joined the NIST Artificial Intelligence Safety Institute Consortium.
NVIDIA offers NeMo Guardrails, an open-source tool for ensuring accurate AI responses.
NVIDIA maintains a GitHub repository dedicated to trustworthy AI.
NVIDIA collaborated with EQTY Lab and Intel on 'Verifiable Compute'.
Verifiable Compute enhances trust in AI workflows using hardware security and distributed ledger technology.

Cohere focuses on detailed discussions of AI safety and responsibility.