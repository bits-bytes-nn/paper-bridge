Research Papers on Large Language Model Alignment, Trustworthiness, and Deception

Research paper by Dawei Li et al. explores dynamic co-augmentation of large language models and knowledge graphs for Alzheimer's disease questions.
Research paper by Long Ouyang et al. focuses on training language models to follow instructions with human feedback.
Tianhao Shen et al. published a survey on large language model alignment.
Yufei Wang et al. published a survey on aligning large language models with human preferences.
Lichao Sun et al. published a research paper titled "TrustLLM: Trustworthiness in Large Language Models".
Dan Hendrycks et al. published an overview of catastrophic AI risks.
Peter S Park et al. published a survey on AI deception, including examples, risks, and potential solutions.
Jacob Steinhardt explored emergent deception and emergent optimization.
Alexander Matt Turner et al. proposed activation addition for steering language models without optimization.
Nina Panickssery et al. published research on steering Llama 2 via contrastive activation addition.
Elias Stengel-Eskin et al. researched teaching models to balance resisting and accepting persuasion.
Philippe Laban et al. challenged LLMs, leading to performance drops in the flipflop experiment.
Zihao Yi et al. published a survey on recent advances in LLM-based multi-turn dialogue systems.