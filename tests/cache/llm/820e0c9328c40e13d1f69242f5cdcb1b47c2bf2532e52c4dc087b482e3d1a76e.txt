Evaluation Methods for LoRA Adapter Fine-Tuning

The primary objective is to refine LoRA adapters without substantial performance degradation.
Evaluation methods include both intrinsic and extrinsic approaches.
Intrinsic evaluation assesses knowledge category shifts in the model.
A fact can shift from Unknown to HighlyKnown or vice versa.
Minimal 'negative' shifts after fine-tuning indicate safe knowledge addition.

Extrinsic evaluation uses two benchmarks: MMLU and TruthfulQA.
MMLU is a benchmark for knowledge and reasoning abilities.
TruthfulQA tests the model's ability to provide truthful answers.
Evaluation uses lm-evaluation-harness tool.
MMLU uses 5-shot prompting.
TruthfulQA uses 0-shot prompting.
Accuracy is the final metric for both benchmarks.

TruthfulQA has two accuracy modes:
MC1 mode requires selecting a single correct answer from 4-5 options.
MC2 mode requires identifying multiple correct answers.

Fact categories are defined based on answer probability:
Unknown (UK) category: Model never returns the correct answer.
MaybeKnown (MK) category: Model returns correct answer occasionally.
HighlyKnown (HK) category: Model always returns the correct answer.

Experiments used Llama-3.1-8B-Instruct model.
Dataset created using DBpedia Knowledge Graph entities.
Entities are categorized by popularity: head, torso, and tail.
Training dataset augmented with synthetic data including paraphrases.

LoRA training parameters:
10 epochs
Learning rate: 1e-3
Batch size: 16
LoRA rank: 1
LoRA alpha: 2
LoRA dropout: 0.1
LoRA layers: down_proj, gate_proj, up_proj

Total dataset: 21,036 question-answer pairs
Paraphrasing used Llama-3-70B-Instruct to generate variations.
Training configurations include 0, 1, and 10 paraphrases per question.

Experimental results show models can learn up to 500 unknown samples with 100% reliability.