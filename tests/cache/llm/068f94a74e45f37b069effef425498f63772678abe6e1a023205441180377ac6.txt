Sampling Process Methodology and Evaluation Techniques for Text-to-Image Models

Sampling process implements two diversity checks
Similarity Checking prevents oversampling of identical elements
Group Checking maintains distinction between different groups of elements
Sampled data distribution is stored to enhance dataset diversity
Evaluation uses VQA-based approach
TIFA is leveraged for atomic and interpretable evaluation
Truthfulness is calculated sample-wise and averaged into final score
Evaluation assesses truthfulness within image generative models
Dynamic updating setting records element sampling frequency
New samples are designed to avoid duplicating previous elements
Ensures caption diversity across real-world element distributions

Truthfulness Scores for Text-to-Image Models

Dall-E-3 has a truthfulness score of 0.45
FLUX-1.1-Pro has a truthfulness score of 0.36
SD-3.5-large has a truthfulness score of 0.35
CogView-3-Plus has a truthfulness score of 0.32
SD-3.5-large-turbo has a truthfulness score of 0.32
HunyuanDiT has a truthfulness score of 0.31
Playground-v2.5 has a truthfulness score of 0.30
Kolors has a truthfulness score of 0.28

Text-to-Image Models Challenges in Complex Scene Generation

T2I models struggle with generating complex scenes containing multiple objects
Models demonstrate strong aesthetic achievement and internal stylistic coherence
Models encounter significant challenges in organizing spatial relationships between objects
Objects are often placed without meaningful connection
Models tend to focus on primary objects during image generation
Inadequate rendering of secondary elements compromises overall truthfulness

Safety Challenges in Text-to-Image Models

T2I models have strong image generation capacity
Models are prone to producing harmful content
Training datasets may contain toxic content
Models can generate discriminatory images targeting specific groups
Safety research explores multiple threat vectors
Threat vectors include jailbreak attacks, backdoor attacks, inversion attacks
Researchers develop frameworks to evaluate and mitigate safety risks

Safety Evaluation Dimensions

Safety evaluation covers multiple dimensions
Dimensions include NSFW keywords, topics, self-harm, sexual content
Evaluation also covers gore, violence, hate speech, drugs, illegal activities
Robustness testing includes perturbations like spelling mistakes and emoji insertion
Fairness assessment involves entity fuzzification and syntactic disruptions
Privacy evaluation examines individual and social tagging aspects