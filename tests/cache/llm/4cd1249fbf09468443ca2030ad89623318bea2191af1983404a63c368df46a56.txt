Research Papers on Text-to-Image AI Model Vulnerabilities and Attacks

Jeongho Kim and colleagues proposed StableViton for virtual try-on using latent diffusion model.
Yuhao Xu and team developed OOTDiffusion for controllable virtual try-on using latent diffusion.
Sensen Gao and researchers introduced RT-Attack for jailbreaking text-to-image models via random token.
Zhi-Yi Chin and colleagues created Prompting4Debugging to red-team text-to-image diffusion models.
Yu-Lin Tsai and team investigated reliability of concept removal methods for diffusion models.
Yijun Yang and researchers developed MMA-Diffusion as a multimodal attack on diffusion models.
Jiachen Ma and colleagues proposed a jailbreaking prompt attack against diffusion models.
Technology Review reported that text-to-image AI models can be tricked into generating disturbing images.
Le Monde highlighted concerns about AI's problematic representation of beauty.
Liang Shi and team developed anonymization prompt learning for facial privacy-preserving text-to-image generation.
Yixin Wu and colleagues conducted membership inference attacks against text-to-image generation models.
Yixin Wan and researchers surveyed bias in text-to-image generation.
Alexander Lin and team explored word-level explanations for analyzing bias in text-to-image models.
Ranjita Naik and Besmira Nushi examined social biases through text-to-image generation.
Hongcheng Gao and researchers evaluated robustness of text-to-image diffusion models against real-world attacks.
Raphaël Millière studied adversarial attacks on image generation using made-up words.
Haomin Zhuang and team conducted a pilot study of query-free adversarial attack against Stable Diffusion.