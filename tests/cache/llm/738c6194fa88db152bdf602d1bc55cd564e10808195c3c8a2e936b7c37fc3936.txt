Research Papers on Prompt Injection and Large Language Model Security

Mossbench investigates multimodal language model sensitivity to safe queries.
Weixiang Zhao et al. propose comprehensive and efficient post safety alignment of large language models via safety patching.
Chaofan Wang et al. explore safeguarding crowdsourcing surveys from ChatGPT with prompt injection techniques.
Rodrigo Pedro et al. examine prompt injections leading to SQL injection attacks in LLM-integrated web applications.
Jun Yan et al. study virtual prompt injection for instruction-tuned large language models.
Jiahao Yu et al. assess prompt injection risks in over 200 custom GPTs.
Daniel Wankit Yip et al. develop a novel evaluation framework for assessing resilience against prompt injection attacks in large language models.
Ahmed Salem et al. introduce Maatphor for automated variant analysis of prompt injection attacks.
OWASP provides guidance on SQL injection prevention.
Riley Goodside discusses prompt injection attacks against GPT-3.
Rich Harang offers insights on securing LLM systems against prompt injection.
FÃ¡bio Perez and Ian Ribeiro explore attack techniques for language models.
Jiawen Shi et al. propose an optimization-based prompt injection attack to LLM-as-a-Judge.
Zeyi Liao et al. investigate environmental injection attacks on web agents for privacy leakage.
Chejian Xu et al. develop controllable black-box attacks on VLM-powered web agents.
Julien Piet et al. present Jatmo, a prompt injection defense through task-specific fine-tuning.
Sizhe Chen et al. propose StruQ for defending against prompt injection with structured queries.
Eric Wallace et al. explore the instruction hierarchy for training LLMs to prioritize privileged instructions.