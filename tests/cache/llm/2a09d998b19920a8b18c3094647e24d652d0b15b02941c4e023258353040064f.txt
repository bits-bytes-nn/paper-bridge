topic: Text-to-Video Generation Research

  entities:
    Make-a-Video|Model
    Sora|Model
    LLMs Meet Multimodal Generation and Editing|Research Paper
    Video Generation Models as World Simulators|Report
    T2VSafetyBench|Toolkit
    VGMShield|Tool
    SafeSora|Approach
    DeMamba|Module
    DeCoF|Method
    Videofact|Tool
    Vita|Platform
    Baichuan-omni|Model
    Uriel Singer|Researcher
    Joseph Cho|Researcher
    Yibo Miao|Researcher
    Yan Pang|Researcher
    Samuel Gehman|Researcher
    Josef Dai|Researcher
    Haoxing Chen|Researcher
    Peisong He|Researcher
    Danial Samadi Vahdati|Researcher
    Long Ma|Researcher
    Chirui Chang|Researcher
    Tai D Nguyen|Researcher
    Chaoyou Fu|Researcher
    Yadong Li|Researcher
    OpenAI|Organization
    arXiv|Publication Platform

  proposition: Research paper titled "LLMs Meet Multimodal Generation and Editing: A Survey" published in arXiv in 2024
    entity-attribute relationships:
    LLMs Meet Multimodal Generation and Editing|PUBLISHED_IN|arXiv
    LLMs Meet Multimodal Generation and Editing|PUBLICATION_YEAR|2024

  proposition: Research paper by Uriel Singer et al. introduces Make-a-Video for text-to-video generation without text-video data
    entity-attribute relationships:
    Make-a-Video|CREATED_BY|Uriel Singer
    Make-a-Video|CHARACTERISTIC|text-to-video generation without text-video data

  proposition: Research paper by Joseph Cho et al. explores Sora as a potential AGI world model
    entity-attribute relationships:
    Sora|EXPLORED_BY|Joseph Cho
    Sora|POTENTIAL_CHARACTERISTIC|AGI world model

  proposition: OpenAI published a technical report on Video Generation Models as World Simulators
    entity-attribute relationships:
    Video Generation Models as World Simulators|PUBLISHED_BY|OpenAI

  proposition: OpenAI introduced Sora, a text-to-video AI model
    entity-attribute relationships:
    Sora|INTRODUCED_BY|OpenAI
    Sora|TYPE|text-to-video AI model

  proposition: Research paper by Yibo Miao et al. presents T2VSafetyBench for evaluating text-to-video generative model safety
    entity-attribute relationships:
    T2VSafetyBench|CREATED_BY|Yibo Miao
    T2VSafetyBench|PURPOSE|evaluating text-to-video generative model safety

  proposition: Research paper by Yan Pang et al. investigates unsafe video generation
    entity-attribute relationships:
    Yan Pang|RESEARCHED|unsafe video generation

  proposition: Yan Pang et al. proposed VGMShield to mitigate misuse of video generative models
    entity-attribute relationships:
    VGMShield|CREATED_BY|Yan Pang
    VGMShield|PURPOSE|mitigate misuse of video generative models

  proposition: Josef Dai et al. proposed SafeSora for safety alignment of text-to-video generation
    entity-attribute relationships:
    SafeSora|CREATED_BY|Josef Dai
    SafeSora|PURPOSE|safety alignment of text-to-video generation

  proposition: Haoxing Chen et al. developed DeMamba for AI-generated video detection
    entity-attribute relationships:
    DeMamba|CREATED_BY|Haoxing Chen
    DeMamba|PURPOSE|AI-generated video detection

  proposition: Long Ma et al. introduced DeCoF for generated video detection via frame consistency
    entity-attribute relationships:
    DeCoF|CREATED_BY|Long Ma
    DeCoF|PURPOSE|generated video detection via frame consistency

  proposition: Tai D Nguyen et al. proposed Videofact for detecting video forgeries
    entity-attribute relationships:
    Videofact|CREATED_BY|Tai D Nguyen
    Videofact|PURPOSE|detecting video forgeries

topic: AI Research and Multimodal Models

  entities:
    Vita|Platform
    Baichuan-omni|Model
    Chaoyou Fu|Researcher
    Yadong Li|Researcher

  proposition: Chaoyou Fu et al. developed Vita, an open-source interactive omni multimodal LLM
    entity-attribute relationships:
    Vita|CREATED_BY|Chaoyou Fu
    Vita|CHARACTERISTIC|open-source interactive omni multimodal LLM

  proposition: Yadong Li et al. published a technical report on Baichuan-omni
    entity-attribute relationships:
    Baichuan-omni|PUBLISHED_BY|Yadong Li