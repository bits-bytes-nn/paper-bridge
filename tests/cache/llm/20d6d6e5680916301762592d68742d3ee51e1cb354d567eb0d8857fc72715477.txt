topic: Hallucination in Large Language Models Research

  entities:
    Lei Huang|Researcher
    Yue Zhang|Researcher
    Mrinank Sharma|Researcher
    Ethan Perez|Researcher
    Jerry Wei|Researcher
    Owain Evans|Researcher
    Katherine Lee|Researcher
    Joshua Maynez|Researcher
    Potsawee Manakul|Researcher
    Yuyan Chen|Researcher
    Ziwei Xu|Researcher
    Shehzaad Dhuliawala|Researcher
    Abhika Mishra|Researcher
    Kenneth Li|Researcher
    Junyi Li|Researcher
    Yuji Zhang|Researcher
    ACM Computing Surveys|Journal
    arXiv|Publication Platform
    Advances in Neural Information Processing Systems|Conference

  proposition: "Survey of hallucination in natural language generation" published in ACM Computing Surveys in 2023
    entity-attribute relationships:
    "Survey of hallucination in natural language generation"|PUBLISHED_IN|ACM Computing Surveys
    "Survey of hallucination in natural language generation"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Lei Huang and colleagues published "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions" on arXiv in 2023
    entity-attribute relationships:
    Lei Huang|PUBLISHED|"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions"
    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions"|PUBLISHED_IN|arXiv
    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Yue Zhang and colleagues published "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models" on arXiv in 2023
    entity-attribute relationships:
    Yue Zhang|PUBLISHED|"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"
    "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"|PUBLISHED_IN|arXiv
    "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Mrinank Sharma and colleagues published research on "Towards Understanding Sycophancy in Language Models" in 2023
    entity-attribute relationships:
    Mrinank Sharma|PUBLISHED|"Towards Understanding Sycophancy in Language Models"
    "Towards Understanding Sycophancy in Language Models"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Ethan Perez and colleagues published "Discovering language model behaviors with model-written evaluations" on arXiv in 2022
    entity-attribute relationships:
    Ethan Perez|PUBLISHED|"Discovering language model behaviors with model-written evaluations"
    "Discovering language model behaviors with model-written evaluations"|PUBLISHED_IN|arXiv
    "Discovering language model behaviors with model-written evaluations"|PUBLICATION_YEAR|2022

    entity-entity relationships:
    None

  proposition: Jerry Wei and colleagues published "Simple synthetic data reduces sycophancy in large language models" on arXiv in 2023
    entity-attribute relationships:
    Jerry Wei|PUBLISHED|"Simple synthetic data reduces sycophancy in large language models"
    "Simple synthetic data reduces sycophancy in large language models"|PUBLISHED_IN|arXiv
    "Simple synthetic data reduces sycophancy in large language models"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Owain Evans and colleagues published "Truthful AI: Developing and governing AI that does not lie" on arXiv in 2021
    entity-attribute relationships:
    Owain Evans|PUBLISHED|"Truthful AI: Developing and governing AI that does not lie"
    "Truthful AI: Developing and governing AI that does not lie"|PUBLISHED_IN|arXiv
    "Truthful AI: Developing and governing AI that does not lie"|PUBLICATION_YEAR|2021

    entity-entity relationships:
    None

  proposition: Katherine Lee and colleagues published research on "Hallucinations in neural machine translation" in 2018
    entity-attribute relationships:
    Katherine Lee|PUBLISHED|"Hallucinations in neural machine translation"
    "Hallucinations in neural machine translation"|PUBLICATION_YEAR|2018

    entity-entity relationships:
    None

  proposition: Joshua Maynez and colleagues published "On faithfulness and factuality in abstractive summarization" on arXiv in 2020
    entity-attribute relationships:
    Joshua Maynez|PUBLISHED|"On faithfulness and factuality in abstractive summarization"
    "On faithfulness and factuality in abstractive summarization"|PUBLISHED_IN|arXiv
    "On faithfulness and factuality in abstractive summarization"|PUBLICATION_YEAR|2020

    entity-entity relationships:
    None

  proposition: Potsawee Manakul and colleagues published "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models" on arXiv in 2023
    entity-attribute relationships:
    Potsawee Manakul|PUBLISHED|"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"
    "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"|PUBLISHED_IN|arXiv
    "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Yuyan Chen and colleagues published "Hallucination detection: Robustly discerning reliable answers in large language models" in ACM conference proceedings in 2023
    entity-attribute relationships:
    Yuyan Chen|PUBLISHED|"Hallucination detection: Robustly discerning reliable answers in large language models"
    "Hallucination detection: Robustly discerning reliable answers in large language models"|PUBLISHED_IN|ACM conference proceedings
    "Hallucination detection: Robustly discerning reliable answers in large language models"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Ziwei Xu and colleagues published "Hallucination is inevitable: An innate limitation of large language models" on arXiv in 2024
    entity-attribute relationships:
    Ziwei Xu|PUBLISHED|"Hallucination is inevitable: An innate limitation of large language models"
    "Hallucination is inevitable: An innate limitation of large language models"|PUBLISHED_IN|arXiv
    "Hallucination is inevitable: An innate limitation of large language models"|PUBLICATION_YEAR|2024

    entity-entity relationships:
    None

  proposition: Shehzaad Dhuliawala and colleagues published "Chain-of-verification reduces hallucination in large language models" on arXiv in 2023
    entity-attribute relationships:
    Shehzaad Dhuliawala|PUBLISHED|"Chain-of-verification reduces hallucination in large language models"
    "Chain-of-verification reduces hallucination in large language models"|PUBLISHED_IN|arXiv
    "Chain-of-verification reduces hallucination in large language models"|PUBLICATION_YEAR|2023

    entity-entity relationships:
    None

  proposition: Abhika Mishra and colleagues published "Fine-grained hallucination detection and editing for language models" on arXiv in 2024
    entity-attribute relationships:
    Abhika Mishra|PUBLISHED|"Fine-grained hallucination detection and editing for language models"
    "Fine-grained hallucination detection and editing for language models"|PUBLISHED_IN|arXiv
    "Fine-grained hallucination detection and editing for language models"|PUBLICATION_YEAR|2024

    entity-entity relationships:
    None

  proposition: Kenneth Li and colleagues published "Inference-time intervention: Eliciting truthful answers from a language model" in Advances in Neural Information Processing Systems in 2024
    entity-attribute relationships:
    Kenneth Li|PUBLISHED|"Inference-time intervention: Eliciting truthful answers from a language model"
    "Inference-time intervention: Eliciting truthful answers from a language model"|PUBLISHED_IN|Advances in Neural Information Processing Systems
    "Inference-time intervention: Eliciting truthful answers from a language model"|PUBLICATION_YEAR|2024

    entity-entity relationships:
    None

  proposition: Junyi Li and colleagues published "The dawn after the dark: An empirical study on factuality hallucination in large language models" on arXiv in 2024
    entity-attribute relationships:
    Junyi Li|PUBLISHED|"The dawn after the dark: An empirical study on factuality hallucination in large language models"
    "The dawn after the dark: An empirical study on factuality hallucination in large language models"|PUBLISHED_IN|arXiv
    "The dawn after the dark: An empirical study on factuality hallucination in large language models"|PUBLICATION_YEAR|2024

    entity-entity relationships:
    None

  proposition: Yuji Zhang and colleagues published research on "Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models: Analysis and Solution"
    entity-attribute relationships:
    Yuji Zhang|PUBLISHED|"Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models: Analysis and Solution"

    entity-entity relationships:
    None