Benchmarks and Datasets for Evaluating AI Agents in Interactive Environments

Alfred is a benchmark for interpreting grounded instructions for everyday tasks.
Alfred was presented at the IEEE/CVF conference on computer vision and pattern recognition in 2020.

AlfWorld aligns text and embodied environments for interactive learning.
AlfWorld was published as an arXiv preprint in 2020.

SmartPlay is a benchmark for evaluating large language models as intelligent agents.
SmartPlay was published as an arXiv preprint in 2023.

Mind2Web aims to develop a generalist agent for the web.
Mind2Web was published in the Advances in Neural Information Processing Systems in 2024.

VisualWebArena evaluates multimodal agents on realistic visual web tasks.
VisualWebArena was published as an arXiv preprint in 2024.

VideoWebErena evaluates long context multimodal agents with video understanding web tasks.
VideoWebErena was published as an arXiv preprint in 2024.

AndroidInTheWild is a large-scale dataset for Android device control.
AndroidInTheWild was published in the Advances in Neural Information Processing Systems in 2024.

Meta-GUI focuses on multi-modal conversational agents on mobile GUI.
Meta-GUI was published as an arXiv preprint in 2022.

AndroidWorld is a dynamic benchmarking environment for autonomous agents.
AndroidWorld was published as an arXiv preprint in 2024.

OSWorld benchmarks multimodal agents for open-ended tasks in real computer environments.
OSWorld was published as an arXiv preprint in 2024.

Windows Agent Arena evaluates multi-modal OS agents at scale.
Windows Agent Arena was published as an arXiv preprint in 2024.

OmniACT is a dataset and benchmark for multimodal generalist autonomous agents for desktop and web.
OmniACT was presented at the European Conference on Computer Vision in 2025.

GUI-WORLD is a dataset for GUI-oriented multimodal LLM-based agents.
GUI-WORLD was published as an arXiv preprint in 2024.