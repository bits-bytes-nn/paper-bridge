topic: Hallucination in Vision-Language Models

  entities:
    Hallucination|Social Concept
    Vision-Language Models|Model
    HallusionBench|Benchmark
    Bingo|Benchmark
    AutoHallusion|Tool
    VHTest|Tool
    GPT-4o|Model
    Claude-3.5-Sonnet|Model
    CHAIR|Metric
    POPE|Metric
    LRV-Instruction|Method

  proposition: Hallucination in VLMs refers to generating content that is factually inconsistent with visual context or common sense.
    entity-attribute relationships:
    Hallucination|DESCRIBED_BY|factually inconsistent with visual context
    Hallucination|DESCRIBED_BY|inconsistent with common sense
    
    entity-entity relationships:
    Hallucination|OCCURS_IN|Vision-Language Models

  proposition: Hallucination can occur in tasks like image captioning, visual question answering, and visual-language navigation.
    entity-attribute relationships:
    Hallucination|OCCURS_IN|image captioning
    Hallucination|OCCURS_IN|visual question answering
    Hallucination|OCCURS_IN|visual-language navigation

  proposition: Hallucinations arise from misalignment between visual input and generated language.
    entity-attribute relationships:
    Hallucination|CAUSED_BY|misalignment between visual input and generated language

  proposition: HallusionBench evaluates VLMs' ability to handle complex image-context reasoning.
    entity-attribute relationships:
    HallusionBench|PURPOSE|evaluate VLMs' image-context reasoning
    
    entity-entity relationships:
    HallusionBench|EVALUATES|Vision-Language Models

  proposition: HallusionBench focuses on two failure modes: language hallucination and visual illusion.
    entity-attribute relationships:
    HallusionBench|FOCUSES_ON|language hallucination
    HallusionBench|FOCUSES_ON|visual illusion

  proposition: Object hallucination involves generating nonexistent objects.
    entity-attribute relationships:
    Object hallucination|INVOLVES|generating nonexistent objects

  proposition: Metrics like CHAIR and POPE assess caption relevance and hallucination levels.
    entity-attribute relationships:
    CHAIR|PURPOSE|assess caption relevance
    POPE|PURPOSE|assess hallucination levels

  proposition: GPT-4o has the highest overall accuracy of 60.70% on HallusionBench.
    entity-attribute relationships:
    GPT-4o|ACCURACY|60.70%
    
    entity-entity relationships:
    GPT-4o|EVALUATED_ON|HallusionBench

  proposition: Claude-3.5-Sonnet achieves 62.19% overall accuracy.
    entity-attribute relationships:
    Claude-3.5-Sonnet|ACCURACY|62.19%

topic: Hallucination Mitigation Strategies

  entities:
    LRV-Instruction|Method

  proposition: Recent approaches optimize training objectives to reduce hallucinations.
    entity-attribute relationships:
    Approaches|PURPOSE|reduce hallucinations

  proposition: LRV-Instruction creates balanced positive and negative instructions for VLM fine-tuning.
    entity-attribute relationships:
    LRV-Instruction|PURPOSE|create balanced instructions
    
    entity-entity relationships:
    LRV-Instruction|APPLIES_TO|Vision-Language Models