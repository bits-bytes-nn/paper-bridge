Preference Bias in Large Language Models: Extraction and Analysis

An LLM-powered case generator creates queries by providing instructions based on scenario examples.
An LLM-powered diversity enhancer paraphrases queries to introduce variations and enhance diversity.
Preference is a form of bias that can significantly impact the objectivity and reliability of LLM responses.
Preference refers to situations where LLMs have stronger tendencies or preferences for certain types of people, things, or ideas.
Current latest LLMs tend to lean slightly left politically.
Preference bias can potentially undermine the credibility of LLMs by producing non-impartial outputs.
Previous research reveals preference bias extends beyond political, scientific, and societal matters.
Rozado's research examines political preferences embedded in LLMs, revealing left-leaning biases.
These preferences may result from supervised fine-tuning processes.
Rutinowski et al. and McGee uncover that ChatGPT tends to favor progressive libertarian views.
Recent studies in recommendation systems have found bias can impact the quality of LLMs' recommendations.

Preference Bias Mitigation Strategies:
Solaiman et al. introduced PALMS method to mitigate preference bias through iterative fine-tuning using custom values-targeted datasets.
Allam introduced BiasDPO framework to mitigate bias using a specific loss function that rewards generating less biased text.
Zhou et al. proposed UniBias method to identify and mask biased neural network vectors and attention heads during inference.
Gao et al. propose a two-stage fine-tuning approach inspired by curriculum learning to help LLMs maintain honesty and neutrality.
Some researchers suggest offering alternative viewpoints backed by references as an effective bias mitigation strategy.

Preference Assessment Domains:
Ideology domain includes pairs like Capitalism / Socialism.
Culture and lifestyle domain compares witty jokes with slapstick comedy.
Social equality and diversity domain explores age diversity versus seniority-based systems.
Health and well-being domain contrasts natural remedies with pharmaceutical solutions.
Technology, science, and education domain examines attitudes toward technological advancements.

Dataset Construction Process:
An LLM-powered data crafter generates preference pairs in specific domains.
The domains can be easily expanded by modifying instruction parameters.
A case generator creates queries to elicit preferences from the model.
An LLM-powered diversity enhancer paraphrases queries to introduce variations.
This approach addresses challenges in preference evaluation by providing comprehensive and adaptable assessment methods.