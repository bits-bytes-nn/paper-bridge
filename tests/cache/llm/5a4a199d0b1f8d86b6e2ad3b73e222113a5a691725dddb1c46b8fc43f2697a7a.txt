Hallucination Evaluation and Benchmarking of Large Language Models

LLMs require alternative evaluation metrics beyond traditional exact match and F1 scores.
The LLM-as-judge paradigm is adopted for consistent evaluation across tasks.
A dynamic data collection pipeline is used for hallucination evaluation.
A web browsing agent retrieves question-answer pairs and claim-label pairs from reliable sources like Wikipedia, Snopes, and FactCheck.org.
Retrieved URLs are filtered to ensure they belong to target sites.
A contextual variator is used to diversify prompt formats and reduce prompt sensitivity.
Benchmark data can be randomly selected from a dataset pool of well-known truthfulness assessment datasets.
The initial dataset pool includes datasets from TrustLLM.
The framework allows easy integration of new datasets for truthfulness evaluation.

Performance Analysis of LLMs on Dynamic Datasets

LLMs generally perform better on dynamically generated datasets compared to established benchmark datasets.
Most LLMs show better performance on dynamic datasets created by retrieval agents.
For QA tasks, the performance trend is consistent across all LLMs.
In fact-checking tasks, the performance pattern is mostly consistent, with exceptions in models like Llama-3.1-8B and Llama-3.1-70B.

Performance Highlights from Table 11

GPT-4o achieves 81.25% accuracy on Dynamic-QA and 70.95% on Dynamic-FC.
Yi-lightning shows high performance with 77.08% Dynamic-QA and 76.54% Dynamic-FC accuracy.
Llama-3.1-70B performs well in Dynamic-QA with 78.12% accuracy but lower in Dynamic-FC at 53.63%.
Qwen-2.5-72B demonstrates balanced performance across tasks.

Sycophancy in Large Language Models

Large language models are distinguished by their ability to follow instructions and align with human values.
Reinforcement learning from human feedback (RLHF) enhances model alignment.
The alignment process can unintentionally introduce sycophancy.
Sycophancy occurs when LLMs prioritize aligning with user beliefs over providing accurate information.