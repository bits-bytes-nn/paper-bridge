topic: Robustness Assessment of Vision-Language Models (VLMs)

  entities:
    MLLM-as-a-Judge|Method
    GPT-4o-mini|Model
    Qwen-2-VL-72B|Model
    Claude-3.5-Sonnet|Model
    Llama|Model
    VQA|Task
    Image Captioning|Task

  proposition: MLLM-as-a-Judge is used to calculate the robustness score for image captioning
    entity-entity relationships:
    MLLM-as-a-Judge|EVALUATES|Image Captioning

  proposition: Robustness score is determined by comparing descriptions under perturbed and unperturbed conditions
    entity-attribute relationships:
    Robustness Score|MEASURED_BY|Perturbed and Unperturbed Descriptions

  proposition: An instance is considered robust if the MLLM rates the descriptions as a "Tie"
    entity-attribute relationships:
    Robustness|DEFINED_BY|Tie Rating

  proposition: Final robustness score is the proportion of "Tie" rated instances
    entity-attribute relationships:
    Robustness Score|CALCULATED_AS|Proportion of Tie Ratings

  proposition: Three distinct perturbation domains are designed: image, text, and image-text
    entities:
      Image Domain|Domain
      Text Domain|Domain
      Image-Text Domain|Domain

  proposition: Image domain includes 23 perturbation types
    entity-attribute relationships:
    Image Domain|CONTAINS|23 Perturbation Types

  proposition: Image perturbations include 19 existing corruptions and 4 new transformations
    entity-attribute relationships:
    Image Perturbations|CONSISTS_OF|19 Existing Corruptions
    Image Perturbations|CONSISTS_OF|4 New Transformations

  proposition: New image perturbations are quarter turn right, quarter turn left, upside down, and horizontal flip
    entity-attribute relationships:
    New Image Perturbations|INCLUDES|Quarter Turn Right
    New Image Perturbations|INCLUDES|Quarter Turn Left
    New Image Perturbations|INCLUDES|Upside Down
    New Image Perturbations|INCLUDES|Horizontal Flip

  proposition: VQA and image caption datasets are collected to build a data pool
    entities:
      VQA Dataset|Dataset
      Image Caption Dataset|Dataset
      Data Pool|Data Source

    entity-entity relationships:
    VQA Dataset|CONTRIBUTES_TO|Data Pool
    Image Caption Dataset|CONTRIBUTES_TO|Data Pool

  proposition: 400 VQA questions and 400 image caption questions were randomly selected
    entity-attribute relationships:
    VQA Questions|COUNT|400
    Image Caption Questions|COUNT|400

  proposition: Qwen-2-VL-72B achieves highest VQA robustness score of 97.50%
    entity-attribute relationships:
    Qwen-2-VL-72B|VQA_ROBUSTNESS_SCORE|97.50%

  proposition: GPT-4o-mini leads image captioning robustness with 51.90%
    entity-attribute relationships:
    GPT-4o-mini|IMAGE_CAPTIONING_ROBUSTNESS_SCORE|51.90%

  proposition: GPT-4o-mini has highest average robustness score of 69.70%
    entity-attribute relationships:
    GPT-4o-mini|AVERAGE_ROBUSTNESS_SCORE|69.70%

  proposition: Claude-3.5-Sonnet has highest VQA robustness score of 96.00%
    entity-attribute relationships:
    Claude-3.5-Sonnet|VQA_ROBUSTNESS_SCORE|96.00%

  proposition: Llama models show lowest robustness in image captioning
    entity-attribute relationships:
    Llama|IMAGE_CAPTIONING_ROBUSTNESS|Low