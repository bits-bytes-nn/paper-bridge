Research Papers on Language Model Safety and Vulnerabilities

Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau published a paper on reducing conversational agents' overconfidence through linguistic calibration in Transactions of the Association for Computational Linguistics.
Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto researched linguistic calibration of longform generations at the Forty-first International Conference on Machine Learning in 2024.
Elias Stengel-Eskin, Peter Hase, and Mohit Bansal developed LACIE: Listener-Aware Finetuning for Calibration in Large Language Models at the Thirty-eighth Annual Conference on Neural Information Processing Systems.
Abhinav Rao and colleagues studied tricking LLMs into disobedience, analyzing and preventing jailbreaks in an arXiv preprint.
Yi Liu and researchers examined jailbreaking ChatGPT via prompt engineering in an empirical study.
Haoran Li and team investigated multi-step jailbreaking privacy attacks on ChatGPT.
Huachuan Qiu and colleagues created a Latent Jailbreak benchmark for evaluating text safety and output robustness of large language models.
Stephen Casper and researchers proposed an explore, establish, exploit approach to red teaming language models from scratch.
Xi Zhiheng, Zheng Rui, and Gui Tao discussed safety and ethical concerns of large language models at the Chinese National Conference on Computational Linguistics.
Jiaming Ji and team developed BeaverTails to improve safety alignment of LLMs through a human-preference dataset.
Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach explored how low-resource languages can jailbreak GPT-4.
Nanna Inie, Jonathan Stray, and Leon Derczynski conducted a grounded theory study on LLM red teaming in the wild.
Yixu Wang and colleagues investigated the concept of fake alignment in large language models.
Norman Mu and researchers examined whether LLMs can follow simple rules.
Sander Schulhoff and team exposed systemic vulnerabilities of LLMs through a global prompt hacking competition.
Nan Xu and researchers studied cognitive overload as a method of jailbreaking large language models.
Gabriel Alon and Michael Kamfonas developed a method for detecting language model attacks using perplexity.
Yu Fu and colleagues explored safety alignment in NLP tasks through weakly aligned summarization as an in-context attack.
Wei Zhao, Zhe Li, and Jun Sun conducted causality analysis for evaluating the security of large language models.