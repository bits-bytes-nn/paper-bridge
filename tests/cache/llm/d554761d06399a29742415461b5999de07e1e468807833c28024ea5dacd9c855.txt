Research Papers on Large Language Model Security and Vulnerabilities
Poisonedrag is a research paper about knowledge poisoning attacks to retrieval-augmented generation of large language models
The research paper was published as an arXiv preprint in 2024
Multiple research papers explore vulnerabilities in large language models and multi-agent systems
Researchers have investigated various attack vectors including:
Backdoor threats to LLM-based agents
Malicious attacks in multi-agent collaborations
Poisoning memory or knowledge bases of agents
Compromising autonomous LLM agents through malfunction amplification
Privacy issues in retrieval-augmented generation
Researchers are developing diagnostic benchmarks and safety assessment methodologies for multi-agent systems
Research papers cover topics such as:
Red teaming of language models
Autonomous language agent testing
Safety exploration of multi-agent networks
Injection of malice into multi-modal large language model societies
Jailbreaking text-to-image models using LLM-based agents