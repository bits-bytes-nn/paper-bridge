topic: Large Language Model Robustness Assessment

  entities:
    GPT-4o-mini|Model
    Claude-3.5-Sonnet|Model
    Gemini-1.5-Flash|Model
    Mixtral-8*7B|Model
    KeyBERT|Tool
    Large Language Models|Technological Concept
    Perturbation Methods|Method

  proposition: The research defines 14 types of natural language perturbations across 8 categories.
    entity-attribute relationships:
    Perturbation Methods|COUNT|14
    Perturbation Methods|CATEGORY_COUNT|8

  proposition: Perturbation methods include using KeyBERT to select key terms for spelling mistakes, emoji insertion, and spaced uppercase modifications.
    entity-attribute relationships:
    KeyBERT|USED_FOR|key term selection
    Perturbation Methods|INCLUDES|spelling mistakes
    Perturbation Methods|INCLUDES|emoji insertion
    Perturbation Methods|INCLUDES|spaced uppercase modifications

  proposition: Social tagging perturbations involve using an LLM to generate subtitles with hashtags and "@" mentions to simulate social media language.
    entity-attribute relationships:
    Perturbation Methods|INCLUDES|social tagging
    Large Language Models|USED_FOR|subtitle generation

  proposition: Multilingual blend perturbations involve translating selected keywords or phrases into Chinese at word and sentence levels.
    entity-attribute relationships:
    Perturbation Methods|INCLUDES|multilingual blend
    Perturbation Methods|TRANSLATION_LEVEL|word
    Perturbation Methods|TRANSLATION_LEVEL|sentence

  proposition: The research created a dynamic dataset by randomly selecting 400 questions from annotated datasets and 400 from open-ended question-answering datasets.
    entity-attribute relationships:
    Dataset|SIZE|800
    Dataset|COMPOSITION|annotated datasets
    Dataset|COMPOSITION|open-ended question-answering datasets

  proposition: GPT-4o-mini, Claude-3.5-Sonnet, and Gemini-1.5-Flash achieved the highest robustness score of 99.36%.
    entity-attribute relationships:
    GPT-4o-mini|ROBUSTNESS_SCORE|99.36%
    Claude-3.5-Sonnet|ROBUSTNESS_SCORE|99.36%
    Gemini-1.5-Flash|ROBUSTNESS_SCORE|99.36%

  proposition: Mixtral-8*7B had the lowest robustness score of 88.78% on annotated datasets.
    entity-attribute relationships:
    Mixtral-8*7B|ROBUSTNESS_SCORE|88.78%
    Mixtral-8*7B|DATASET_PERFORMANCE|annotated datasets

  proposition: Models generally performed more consistently on annotated datasets compared to open-ended datasets.
    entity-attribute relationships:
    Large Language Models|PERFORMANCE_CONSISTENCY|annotated datasets

  proposition: Robustness scores on annotated datasets were consistently above 92% across most models.
    entity-attribute relationships:
    Large Language Models|MINIMUM_ROBUSTNESS_SCORE|92%
    Large Language Models|DATASET_PERFORMANCE|annotated datasets

  proposition: Open-ended dataset robustness scores were significantly lower than annotated dataset scores.
    entity-attribute relationships:
    Large Language Models|DATASET_PERFORMANCE|open-ended datasets