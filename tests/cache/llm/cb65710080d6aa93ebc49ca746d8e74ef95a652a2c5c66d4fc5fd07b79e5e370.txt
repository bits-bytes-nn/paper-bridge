Hallucination Evaluation Methodology and Benchmarking in Large Language Models

Information Retrieval Methods
[647-653, 597, 654, 589, 655, 595, 596] retrieve information from external knowledge bases, structured databases, specific websites, search engine APIs, and various external tools.

Model Editing Techniques
Model editing allows for modification of Large Language Model (LLM) behavior in a data- and computation-efficient manner.
Model editing methods often involve incorporating an auxiliary sub-network or directly modifying original model parameters.
Meng et al. propose ROME, a method that modifies feedforward weights to update specific factual associations in GPT.
Li et al. introduce inference-time intervention (ITI), a technique that identifies attention heads associated with truthfulness.
ITI shifts activations along truth-correlated directions to elicit truthful answers from Llama.
Liu et al. propose event-based knowledge editing with deductive editing boundaries.

Decoding Strategies
Decoding strategies determine how the next token is selected from the probability distribution generated by LLMs.
These strategies can significantly influence model responses.
Li et al. propose contrastive decoding, leveraging differences between expert and amateur models.
Lee et al. conduct factuality assessments of LLM-generated content using various decoding strategies.
Lee et al. introduce factual-nucleus sampling decoding algorithm.
Shi et al. propose a context-aware decoding strategy to encourage LLMs to pay closer attention to context during generation.
The context-aware strategy aims to override prior knowledge with reliable information to reduce hallucinations.

Hallucination Evaluation Approach
LLM hallucinations often arise from unreliable knowledge in noisy training data.
Retrieval-Augmented Generation (RAG) adds controllability to LLMs' knowledge sources.
Even with RAG, LLMs remain susceptible to hallucination.
The evaluation examines LLMs' hallucination tendencies in two scenarios:
First scenario: Relying exclusively on models' parametric (internal) knowledge.
Second scenario: Retrieving information from reliable external sources.
Internal knowledge scenario uses existing QA datasets covering adversarial QA, commonsense QA, and human falsehood QA.
External knowledge scenario simulates RAG using a fact-checking task.

Evaluation Methodology
The evaluation employs the LLM-as-a-Judge paradigm to assess model output.
Traditional metrics like exact match and F1 scores may not suit LLM response diversity.
The same LLM-as-judge approach is used consistently across tasks.

Dynamic Dataset Construction
A web browsing agent retrieves question-answer and claim-label pairs.
QA task data is retrieved from reliable sources like Wikipedia.
Fact-checking task data is gathered from websites like Snopes and FactCheck.org.
Additional checks filter out irrelevant URLs.
A contextual variator diversifies prompt formats to reduce prompt sensitivity.