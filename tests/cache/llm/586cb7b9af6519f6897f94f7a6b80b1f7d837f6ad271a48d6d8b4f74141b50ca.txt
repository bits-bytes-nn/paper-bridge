topic: Sycophancy Evaluation in Large Language Models

  entities:
    Sycophancy Evaluation|Research Problem
    Large Language Models|Technological Concept
    Persona Sycophancy|Method
    Preconception Sycophancy|Method
    Self-Doubt Sycophancy|Method
    Qwen-2.5-72B|Model
    Claude-3.5-sonnet|Model
    Gemini-1.5-pro|Model
    Llama-3.1-70B|Model
    Gemini-1.5-flash|Model

  proposition: Sycophancy evaluation involves testing language models' susceptibility to biased responses across different scenarios
    entity-entity relationships:
    Sycophancy Evaluation|EVALUATES|Large Language Models
    
    entity-attribute relationships:
    Sycophancy Evaluation|FOCUSES_ON|biased responses

  proposition: Sycophancy testing includes three primary methods: persona sycophancy, preconception sycophancy, and self-doubt sycophancy
    entity-entity relationships:
    Sycophancy Evaluation|INCLUDES|Persona Sycophancy
    Sycophancy Evaluation|INCLUDES|Preconception Sycophancy
    Sycophancy Evaluation|INCLUDES|Self-Doubt Sycophancy

  proposition: Persona sycophancy involves adding a persona prefix to evaluate response bias
    entity-attribute relationships:
    Persona Sycophancy|INVOLVES|persona prefix
    Persona Sycophancy|EVALUATES|response bias

  proposition: Performance change is calculated using the formula: ∆Acc = |Accpersona/preconception −Accbase|/Accbase
    entity-attribute relationships:
    Performance Change|CALCULATED_BY|∆Acc formula

  proposition: Qwen-2.5-72B shows 100% persona sycophancy, the highest among tested models
    entity-attribute relationships:
    Qwen-2.5-72B|PERSONA_SYCOPHANCY|100%

  proposition: Claude-3.5-sonnet demonstrates 91.67% persona sycophancy
    entity-attribute relationships:
    Claude-3.5-sonnet|PERSONA_SYCOPHANCY|91.67%

  proposition: Gemini-1.5-pro and Llama-3.1-70B exhibit the lowest persona sycophancy at around 2%
    entity-attribute relationships:
    Gemini-1.5-pro|PERSONA_SYCOPHANCY|2%
    Llama-3.1-70B|PERSONA_SYCOPHANCY|2%

  proposition: Gemini-1.5-flash and Gemini-1.5-pro show the lowest preconception sycophancy at approximately 1-8%
    entity-attribute relationships:
    Gemini-1.5-flash|PRECONCEPTION_SYCOPHANCY|1-8%
    Gemini-1.5-pro|PRECONCEPTION_SYCOPHANCY|1-8%

  proposition: Self-doubt sycophancy varies widely, with Gemini models showing the highest rate of response alteration (94-97%)
    entity-attribute relationships:
    Gemini Models|SELF_DOUBT_SYCOPHANCY|94-97%

  proposition: LLMs demonstrate significant variability in sycophancy levels across different evaluation methods
    entity-attribute relationships:
    Large Language Models|DEMONSTRATES|sycophancy variability