Research Papers on Large Language Model Uncertainty and Self-Awareness

Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxing Zhang published a paper on learning self-awareness in Large Language Models for hallucination mitigation.
The paper was presented at the 3rd Workshop on Knowledge Augmented Methods for NLP in Bangkok, Thailand in August 2024.

Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu researched alignment for honesty in Large Language Models.
The research was presented at the Thirty-eighth Annual Conference on Neural Information Processing Systems in 2024.

Hanning Zhang and colleagues proposed R-Tuning, a method for instructing Large Language Models to express uncertainty by saying "I Don't Know".

Stephanie Lin, Jacob Hilton, and Owain Evans studied teaching models to express their uncertainty in words.

Miao Xiong and colleagues empirically evaluated confidence elicitation in Large Language Models.
The research was presented at the Twelfth International Conference on Learning Representations in 2024.

Aviv Slobodkin and researchers investigated hallucination and over-confidence in Large Language Models' hidden states.
The study was presented at the 2023 Conference on Empirical Methods in Natural Language Processing in Singapore.

Yukun Zhao and colleagues developed a self-detection method for identifying what Large Language Models do not know.
The research was presented at the 2024 Conference of the North American Chapter of the Association for Computational Linguistics in Mexico City.

Chao Chen and researchers explored using internal states of Large Language Models for hallucination detection.
The study was presented at the Twelfth International Conference on Learning Representations in 2024.