Interdisciplinary Collaboration in Trustworthy AI Research

Computational social scientists and HCI experts provide perspectives on fairness, societal biases, machine ethics, and human-centric safety considerations.
Security experts guide model evaluation for robustness against adversarial attacks and privacy preservation mechanisms.
Roboticists, medical and AI for science researchers help evaluate model truthfulness and reliability in physical interactions, healthcare, and scientific research scenarios.
Legal scholars assess advanced AI risks and develop guidelines aligned with global regulatory requirements and ethical standards.
The interdisciplinary collaboration enables a comprehensive evaluation of models across multiple dimensions.
TrustGen is proposed as a dynamic evaluation framework that adapts to evolving ethical standards and social norms.
The research spans technical challenges in model trustworthiness and alignment to ethical considerations in downstream applications like medicine, robotics, AI for sciences, and human-AI collaboration.

Student Contributors' Involvement

All contributing professors made direct contributions to the paper.
Professors were invited to revise specific sections based on their areas of expertise.
Contributions included direct revisions to Introduction, Guideline, and Benchmark Design.
Professors provided conceptual input to enhance guideline rationale and benchmark standardization.
Contributions involved feedback on paper structure, toolkit usability, and targeted content revisions.

Acknowledgment Support

Max Lamparth receives partial support from the Stanford Center for AI Safety.
Max Lamparth receives partial support from the Center for International Security and Cooperation.
Max Lamparth receives partial support from the Stanford Existential Risk Initiative.