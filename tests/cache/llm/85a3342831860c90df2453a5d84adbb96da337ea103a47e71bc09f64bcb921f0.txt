Dataset Generation and Evaluation Techniques for Large Language and Vision-Language Models

Dyval series is a dynamic protocol for reasoning data generation.
Dyval-1 aims to construct reasoning data dynamically.
Dyval-2 utilizes probing and judging Large Language Models to transform evaluation problems automatically.
UniGen is a unified framework for textual dataset generation.
UniGen ensures truthfulness and diversity of generated data.
Wang et al. use a multi-agent framework to evolve evaluation datasets.
AutoBencher is an automatic benchmark framework using language models.
AutoBencher searches for datasets meeting salience, novelty, and difficulty criteria.

Large Language Models have emerged as promising evaluation tools.
Zheng et al. introduced the "LLM-as-a-Judge" concept.
"LLM-as-a-Judge" offers a cost-effective alternative to traditional human evaluations.
ChatEval, EvaluLLM, and Prometheus are popular LLM-powered evaluation methods.

Vision-Language Models have rapidly developed alongside computer vision and Large Language Models.
Vision-Language Models enable downstream tasks integrating visual and linguistic information.
Vision-Language Models are evaluated on object detection tasks.
Vision-Language Models are evaluated on image classification tasks.
Vision-Language Models are evaluated on object tracking tasks.
Vision-Language Models are evaluated on facial recognition tasks.
Vision-Language Models are evaluated on human pose estimation tasks.
Vision-Language Models are evaluated on optical character recognition tasks.
Vision-Language Models demonstrate abilities in multiple image scene recognition.
Vision-Language Models demonstrate abilities in visual question answering.

Numerous benchmarks assess Vision-Language Models' general capabilities.
Seed-bench comprehensively evaluates the hierarchical abilities of Vision-Language Models.
Some benchmarks test Vision-Language Models' reasoning skills.
Benchmarks evaluate comparative reasoning skills.
Benchmarks evaluate reasoning with image sequences.
Benchmarks assess mathematical and scientific reasoning capabilities.

Vision-Language Models are applied in multiple domains.
In autonomous driving, Vision-Language Models perform lane detection and obstacle recognition.
In robotics, Vision-Language Models assist in navigation and manipulation tasks.
In healthcare, Vision-Language Models aid medical image analysis and disease diagnosis.
In psychology, Vision-Language Models evaluate emotion recognition and social cue understanding.
Vision-Language Models are studied in legal, economic, financial, and recommendation domains.
Some studies investigate cross-cultural and multilingual capabilities of Vision-Language Models.

Evaluation frameworks have been proposed for Vision-Language Models.
Some frameworks construct multimodal instruction-tuning datasets.
Some frameworks provide annotation-free evaluation methodologies.
Some studies assess Vision-Language Models' effectiveness across various modalities.

Multimodal agent research explores Vision-Language Models in diverse environments.
Benchmarks evaluate multimodal agents in household, gaming, web, mobile phone, and desktop scenarios.
Chen et al. introduced a comprehensive multimodal dataset for agent-based research.
Liu et al. developed the first systematic benchmark for complex spaces and digital interfaces.