Propositions for Large Language Model Fairness and Bias Detection

Tailored queries facilitate the detection of unintended biases in large language models.
An LLM-powered diversity enhancer paraphrases preference queries to introduce variations in style, length, and format.
The diversity enhancer supports robust evaluation by providing a comprehensive range of examples.
The evaluation framework enables adaptability to nuanced biases across different contexts and query formats.

Domains for preference assessment include ideology, culture and lifestyle, social equality and diversity, health and well-being, and technology, science, and education.

Fairness analysis of large language models involves measuring stereotype accuracy, disparagement Refuse-to-Answer (RtA) rate, and preference RtA rate.

GLM-4-Plus achieved the highest stereotype accuracy at 91.08%.
Gemini-1.5-Pro demonstrated a disparagement response accuracy of 65.48%.
Higher stereotype accuracy does not necessarily correlate with improved disparagement response across models.

Most models demonstrate strong performance in preference responses.
Mixtral-8*22B achieved an outstanding preference response accuracy of 99.56%.
Claude-3.5-Sonnet and Gemini-1.5-Pro achieved 98.22% preference response accuracy.

Smaller models tend to underperform across fairness metrics compared to larger models within the same series.
Llama-3.1-8B scored 73.25% in stereotype, 60.00% in disparagement, and 88.89% in preference.
Llama-3.1-70B scored 85.99% in stereotype, 63.00% in disparagement, and 89.33% in preference.

Robustness in large language models refers to their capacity to maintain consistent performance when faced with diverse, unexpected, or perturbed inputs.
Robustness studies encompass potential factors that may lead to erroneous system outputs.
The research focuses specifically on LLM robustness when confronted with natural language perturbations.