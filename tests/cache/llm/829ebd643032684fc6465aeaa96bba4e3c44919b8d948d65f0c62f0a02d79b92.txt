Benchmarking Vision-Language Models: Evaluation and Robustness Analysis

Human reviewers verify the quality of data instances.

The study analyzes visual preference alignment across multiple Vision-Language Models (VLMs).

Models within the same series exhibit similar performance in preference tasks.

GPT-4 series models show high preference task performance, with GPT-4o at 97.89% and GPT-4o-mini at 96.32%.

Gemini-1.5 series models both score 94.21% in preference evaluations.

Claude series models display comparable neutrality, with Claude-3.5-Sonnet at 80.53% and Claude-3-Haiku at 80.00%.

Llama-3.2-90B-V frequently outputs evasive responses.

Llama-3.2-90B-V tends to avoid engaging with sensitive topics by producing responses like "I'm not going to engage in this topic."

Robustness of a Vision-Language Model refers to its ability to generate accurate responses to disturbed inputs.

VLM robustness encompasses handling:
Linguistic variations
Textual errors
Contextual ambiguities
Image quality distortions
Occlusions
Lighting variations
Perspective changes
Object misclassification

Research on VLM robustness focuses on three key areas:
Adversarial attacks on VLMs
Adversarial defenses and robustness enhancement
Robustness benchmark and evaluation

Adversarial attacks can:
Control model behavior
Mislead content
Implant backdoors in VLMs
Manipulate vision understanding