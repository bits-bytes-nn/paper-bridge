topic: Multimodal Large Language Models Research Benchmarks

  entities:
    MMMU-Pro|Benchmark
    CMMMU|Benchmark
    MathVista|Benchmark
    LogicVista|Benchmark
    MMRel|Dataset
    MIA-Bench|Benchmark
    MMDU|Benchmark
    DriveVLM|Model
    LOC-ZSON|Approach
    CLIP-Nav|Approach
    ViNT|Foundation Model
    Robot Navigation|Research Field

  proposition: MMMU-Pro is a multi-discipline multimodal understanding benchmark.
    entity-attribute relationships:
    MMMU-Pro|TYPE|multi-discipline multimodal understanding benchmark

  proposition: CMMMU is a Chinese massive multi-discipline multimodal understanding benchmark.
    entity-attribute relationships:
    CMMMU|TYPE|Chinese massive multi-discipline multimodal understanding benchmark

  proposition: MathVista evaluates mathematical reasoning of foundation models in visual contexts.
    entity-attribute relationships:
    MathVista|PURPOSE|evaluate mathematical reasoning
    MathVista|CONTEXT|visual contexts

  proposition: LogicVista is a multimodal LLM logical reasoning benchmark in visual contexts.
    entity-attribute relationships:
    LogicVista|TYPE|multimodal LLM logical reasoning benchmark
    LogicVista|CONTEXT|visual contexts

  proposition: MMRel is a relation understanding dataset and benchmark in the MLLM era.
    entity-attribute relationships:
    MMRel|TYPE|relation understanding dataset and benchmark
    MMRel|CONTEXT|MLLM era

  proposition: MIA-Bench aims to evaluate instruction following of multimodal LLMs.
    entity-attribute relationships:
    MIA-Bench|PURPOSE|evaluate instruction following
    MIA-Bench|CONTEXT|multimodal LLMs

  proposition: MMDU is a multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for LVLMs.
    entity-attribute relationships:
    MMDU|TYPE|multi-turn multi-image dialog understanding benchmark
    MMDU|PURPOSE|instruction-tuning dataset for LVLMs

topic: Autonomous Driving and Vision-Language Models

  proposition: A survey explores multimodal large language models for autonomous driving.
    entity-attribute relationships:
    Autonomous Driving|RESEARCH_AREA|multimodal large language models

  proposition: DriveVLM represents the convergence of autonomous driving and large vision-language models.
    entity-entity relationships:
    DriveVLM|CONNECTS|Autonomous Driving
    DriveVLM|CONNECTS|Large Vision-Language Models

topic: Navigation Approaches

  proposition: LOC-ZSON is a language-driven object-centric zero-shot object retrieval and navigation approach.
    entity-attribute relationships:
    LOC-ZSON|TYPE|language-driven object-centric zero-shot navigation approach

  proposition: CLIP-Nav uses CLIP for zero-shot vision-and-language navigation.
    entity-attribute relationships:
    CLIP-Nav|TECHNOLOGY|CLIP
    CLIP-Nav|TYPE|zero-shot vision-and-language navigation approach

  proposition: ViNT is a foundation model for visual navigation.
    entity-attribute relationships:
    ViNT|TYPE|foundation model
    ViNT|PURPOSE|visual navigation

  proposition: Robot navigation research explores using physically grounded vision-language models in outdoor environments.
    entity-attribute relationships:
    Robot Navigation|RESEARCH_FOCUS|physically grounded vision-language models
    Robot Navigation|ENVIRONMENT|outdoor environments