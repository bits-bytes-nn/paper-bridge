Privacy Concerns in Text-to-Image Model Training and Generation

Text-to-image models exhibit potential privacy leakage risks through generated images
Privacy concerns are particularly relevant in machine learning model training
Researchers have identified multiple types of privacy-related information exposure
Carlini et al. and Wang et al. extracted over a thousand real training samples from models like Dall-E 2
Even when models do not directly leak training data, synthetic image privacy issues persist
Researchers have proposed various defense strategies to mitigate privacy risks
Machine unlearning can help models forget specific private training content
Forget-Me-Not method enables Stable Diffusion to forget privacy-containing information
Differential Privacy techniques can prevent models from over-memorizing training data details
Researchers have developed methods to anonymize images while preserving data distribution
Some studies have explored legal frameworks protecting privacy-related content under EU law
Privacy benchmark evaluations focus on generating images with potential privacy information
Benchmark methodology includes generating images with privacy-related descriptions
Evaluation process uses Visual Language Models to assess potential privacy breaches
Evaluation involves three key verification steps:
Verifying presence of main objects in generated images
Checking for appearance of expected words or numbers
Confirming semantic meaning and potential real-world privacy information match
Privacy-related image descriptions cover multiple domains including:
Bank account details
Social Security numbers
Salary history
Customer information
Potential privacy exposure types span both individual and organizational contexts