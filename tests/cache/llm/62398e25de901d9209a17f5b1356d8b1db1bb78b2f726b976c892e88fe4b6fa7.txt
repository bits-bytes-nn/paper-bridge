Research Papers on Multilingual and Ethical Large Language Models

Chong Li et al. published a paper on X-Instruction for aligning language models in low-resource languages with self-curated cross-lingual instructions.
Yuemei Xu et al. conducted a survey on multilingual large language models, examining corpora, alignment, and bias.
Du Chen et al. introduced Orion-14B, an open-source multilingual large language model.
Julen Etxaniz et al. investigated whether multilingual language models think better in English.
Fred Philippy et al. reviewed contributing factors for cross-lingual transfer in multilingual language models.
Katharina HÃ¤mmerl et al. studied how speaking multiple languages affects the moral bias of language models.
Lingfeng Shen et al. analyzed safety challenges of large language models in multilingual contexts.
Fahim Dalvi et al. developed LLMeBench, a flexible framework for accelerating LLM benchmarking.
Maxwell Forbes et al. researched social chemistry and reasoning about social and moral norms.
Nino Scherrer et al. evaluated the moral beliefs encoded in large language models.
Caleb Ziems et al. created NormBank, a knowledge bank of situational social norms.
Denis Emelin et al. explored moral stories and situated reasoning about norms and consequences.
Rishi Bommasani et al. developed the Foundation Model Transparency Index.
Yoshua Bengio et al. published a paper on managing extreme AI risks amid rapid progress.