topic: Large Language Model Jailbreak Research

  entities:
    Jiahao Yu|Researcher
    Xingwei Lin|Researcher
    Xinyu Xing|Researcher
    Youliang Yuan|Researcher
    Huijie Lv|Researcher
    George Kour|Researcher
    Yue Huang|Researcher
    Yuanwei Wu|Researcher
    Yue Deng|Researcher
    Yifan Cao|Researcher
    Mark Russinovich|Researcher
    Guangyu Shen|Researcher
    GPTFuzzer|Research Paper
    GPT-4 Is Too Smart To Be Safe|Research Paper
    CodeChameleon|Framework
    ObscurePrompt|Method
    Crescendo|Attack Method
    GPT-4|Model
    GPT-4V|Model
    arXiv|Publication Platform

  proposition: Jiahao Yu, Xingwei Lin, and Xinyu Xing published a research paper titled "GPTFuzzer" about red teaming large language models with auto-generated jailbreak prompts.
    entity-attribute relationships:
    Jiahao Yu|AUTHOR_OF|GPTFuzzer
    Xingwei Lin|AUTHOR_OF|GPTFuzzer
    Xinyu Xing|AUTHOR_OF|GPTFuzzer
    GPTFuzzer|FOCUSES_ON|jailbreak prompts
    GPTFuzzer|RESEARCH_METHOD|red teaming

    entity-entity relationships:
    Jiahao Yu|CO_AUTHOR|Xingwei Lin
    Jiahao Yu|CO_AUTHOR|Xinyu Xing
    Xingwei Lin|CO_AUTHOR|Xinyu Xing

  proposition: The paper was published as an arXiv preprint in 2023.
    entity-attribute relationships:
    GPTFuzzer|PUBLICATION_YEAR|2023
    GPTFuzzer|PUBLISHED_ON|arXiv

  proposition: Youliang Yuan and colleagues published a research paper titled "GPT-4 Is Too Smart To Be Safe" about stealthy chat with large language models via cipher.
    entity-attribute relationships:
    Youliang Yuan|AUTHOR_OF|GPT-4 Is Too Smart To Be Safe
    GPT-4 Is Too Smart To Be Safe|RESEARCH_FOCUS|stealthy chat
    GPT-4 Is Too Smart To Be Safe|RESEARCH_METHOD|cipher

    entity-entity relationships:
    GPT-4 Is Too Smart To Be Safe|INVESTIGATES|GPT-4

  proposition: The paper was published as an arXiv preprint in 2023.
    entity-attribute relationships:
    GPT-4 Is Too Smart To Be Safe|PUBLICATION_YEAR|2023
    GPT-4 Is Too Smart To Be Safe|PUBLISHED_ON|arXiv

  proposition: Huijie Lv and colleagues developed "CodeChameleon", a personalized encryption framework for jailbreaking large language models.
    entity-attribute relationships:
    Huijie Lv|AUTHOR_OF|CodeChameleon
    CodeChameleon|TYPE|Framework
    CodeChameleon|PURPOSE|jailbreaking large language models
    CodeChameleon|FEATURE|personalized encryption

  proposition: The paper was published as an arXiv preprint in 2024.
    entity-attribute relationships:
    CodeChameleon|PUBLICATION_YEAR|2024
    CodeChameleon|PUBLISHED_ON|arXiv

  proposition: George Kour and colleagues published a research paper about unveiling safety vulnerabilities of large language models.
    entity-attribute relationships:
    George Kour|AUTHOR_OF|Research Paper
    Research Paper|RESEARCH_FOCUS|safety vulnerabilities of large language models

  proposition: The paper was published as an arXiv preprint in 2023.
    entity-attribute relationships:
    Research Paper|PUBLICATION_YEAR|2023
    Research Paper|PUBLISHED_ON|arXiv

  proposition: Yue Huang and colleagues proposed "ObscurePrompt", a method for jailbreaking large language models via obscure input.
    entity-attribute relationships:
    Yue Huang|AUTHOR_OF|ObscurePrompt
    ObscurePrompt|TYPE|Method
    ObscurePrompt|PURPOSE|jailbreaking large language models
    ObscurePrompt|TECHNIQUE|obscure input

  proposition: The paper was published as an arXiv preprint in 2024.
    entity-attribute relationships:
    ObscurePrompt|PUBLICATION_YEAR|2024
    ObscurePrompt|PUBLISHED_ON|arXiv

  proposition: Yuanwei Wu and colleagues investigated whether large language models can automatically jailbreak GPT-4V.
    entity-attribute relationships:
    Yuanwei Wu|AUTHOR_OF|Research Paper
    Research Paper|RESEARCH_FOCUS|automatic jailbreaking

    entity-entity relationships:
    Research Paper|INVESTIGATES|GPT-4V

  proposition: The paper was published as an arXiv preprint in 2024.
    entity-attribute relationships:
    Research Paper|PUBLICATION_YEAR|2024
    Research Paper|PUBLISHED_ON|arXiv

  proposition: Yue Deng and colleagues studied multilingual jailbreak challenges in large language models.
    entity-attribute relationships:
    Yue Deng|AUTHOR_OF|Research Paper
    Research Paper|RESEARCH_FOCUS|multilingual jailbreak challenges

  proposition: The paper was published as an arXiv preprint in 2023.
    entity-attribute relationships:
    Research Paper|PUBLICATION_YEAR|2023
    Research Paper|PUBLISHED_ON|arXiv

  proposition: Yifan Cao and colleagues conducted a cross-language investigation into jailbreak attacks in large language models.
    entity-attribute relationships:
    Yifan Cao|AUTHOR_OF|Research Paper
    Research Paper|RESEARCH_FOCUS|cross-language jailbreak attacks

  proposition: The paper was published as an arXiv preprint in 2023.
    entity-attribute relationships:
    Research Paper|PUBLICATION_YEAR|2023
    Research Paper|PUBLISHED_ON|arXiv

  proposition: Mark Russinovich and colleagues developed the "Crescendo" multi-turn LLM jailbreak attack.
    entity-attribute relationships:
    Mark Russinovich|AUTHOR_OF|Crescendo
    Crescendo|TYPE|Attack Method
    Crescendo|CHARACTERISTIC|multi-turn

  proposition: The paper was published as an arXiv preprint in 2024.
    entity-attribute relationships:
    Crescendo|PUBLICATION_YEAR|2024
    Crescendo|PUBLISHED_ON|arXiv

  proposition: Guangyu Shen and colleagues proposed a method for rapid optimization of jailbreaking LLMs via subconscious exploitation and echopraxia.
    entity-attribute relationships:
    Guangyu Shen|AUTHOR_OF|Research Paper
    Research Paper|RESEARCH_FOCUS|jailbreaking LLMs
    Research Paper|TECHNIQUE|subconscious exploitation
    Research Paper|TECHNIQUE|echopraxia
    Research Paper|GOAL|rapid optimization

  proposition: The paper was published as an arXiv preprint in 2024.
    entity-attribute relationships:
    Research Paper|PUBLICATION_YEAR|2024
    Research Paper|PUBLISHED_ON|arXiv