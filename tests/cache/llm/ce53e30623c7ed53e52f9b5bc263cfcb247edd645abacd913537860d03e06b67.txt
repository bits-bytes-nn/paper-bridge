Benchmarks and Evaluations for Large Language Models and Vision-Language Models

Yutao Mou, Shikun Zhang, and Wei Ye published SG-Bench for evaluating LLM safety generalization across diverse tasks and prompt types.
Yahan Li, Yi Wang, Yi Chang, and Yuan Wu published XTRUST for evaluating multilingual trustworthiness of large language models.
Tony Lee and colleagues conducted a holistic evaluation of text-to-image models in 2023.
Eslam Mohamed Bakr and colleagues developed HRS-Bench, a holistic, reliable, and scalable benchmark for text-to-image models.
Jaemin Cho, Abhay Zala, and Mohit Bansal probed reasoning skills and social biases of text-to-image generation models.
Yichi Zhang and colleagues conducted a comprehensive study benchmarking trustworthiness of multimodal large language models.
Tianle Gu and colleagues developed MLLMGuard, a multi-dimensional safety evaluation suite for multimodal large language models.
Xin Liu and colleagues created MM-SafetyBench for safety evaluation of multimodal large language models.
Haoqin Tu and colleagues developed a safety evaluation benchmark for vision LLMs focusing on object counting.
Rizhao Cai and colleagues created BenchLMM for benchmarking cross-style visual capability of large multimodal models.
Bohan Zhai and colleagues investigated object existence hallucinations in large vision language models.
Mukai Li and colleagues conducted red teaming of visual language models.
Weidi Luo and colleagues developed JailBreakV-28K to assess robustness against jailbreak attacks.
Jie Zhang and colleagues created VLBiasBench for evaluating bias in large vision-language models.
Hongzhan Lin and colleagues developed Goat-bench for safety insights through meme-based social abuse analysis.
Zhe Hu and colleagues created VIVA, a benchmark for vision-grounded decision-making with human values.
Zhelun Shi and colleagues assessed multimodal large language models' alignment with human values.