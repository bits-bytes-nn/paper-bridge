Benchmarking Vision-Language Models: Dynamic Dataset and Result Analysis

A dynamic harmful query dataset was developed for evaluating jailbreaks on Large Language Models and Vision-Language Models.
The same dataset will be used for VLMs with attack methods from Table 25.
Figure 38 and Table 31 present the refuse to answer (RtA) rate of various VLMs across five different jailbreak attacks.
Larger models tend to have higher RtA rates, indicating better defense against attacks.
This trend is consistent across model pairs like GPT-4o and GPT-4o-mini, Claude-3.5-sonnet and Claude-3-haiku, Gemini-1.5-Pro and Gemini-1.5-flash, and Llama-3.2-90B-V and Llama-3.2-11B-V.
Prompt-to-image attacks typically yield lower RtAs compared to optimization-based attacks.
Optimization-based attacks often generate jailbreak images using an open-source VLM.
The effectiveness of these attacks varies depending on the specific model implementation.
Jailbreak in Pieces attack shows lower RtAs for models with similar adaptor architectures like Qwen-2-VL-72B and GLM-4v-Plus.
Some models like GPT-4o cannot understand optimized noisy images.
Prompt-to-image attacks produce semantically meaningful images that all VLMs can interpret, leading to better transferability and lower RtAs.

Fairness in Vision-Language Models

Fairness in VLMs is more complex due to the introduction of visual modality.
There is limited understanding of VLM fairness.
Researchers are studying VLM fairness through:
Creating related datasets
Evaluating and identifying fairness in VLMs
Mitigating biases in VLM outputs
Stereotypes and disparagement exist in VLMs.
Researchers have proposed various methods to measure and address bias:
GenderBias benchmark uses text-to-image diffusion models to generate occupation images with gender counterfactuals.
StereoSet-VL extends StereoSet to measure stereotypical bias in multimodal contexts.
CounterBias quantifies social bias by comparing masked prediction probabilities between factual and counterfactual samples.